# Comparing `tmp/elwood-0.1.2-py2.py3-none-any.whl.zip` & `tmp/elwood-0.1.3-py2.py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,30 +1,31 @@
-Zip file size: 63120 bytes, number of entries: 28
--rw-rw-r--  2.0 unx      126 b- defN 23-Apr-19 20:19 elwood/__init__.py
--rw-rw-r--  2.0 unx    17988 b- defN 23-Apr-19 20:19 elwood/cli.py
+Zip file size: 64340 bytes, number of entries: 29
+-rw-rw-r--  2.0 unx      126 b- defN 23-Jul-18 16:28 elwood/__init__.py
+-rw-rw-r--  2.0 unx    18004 b- defN 23-May-03 18:37 elwood/cli.py
 -rw-rw-r--  2.0 unx      409 b- defN 23-Feb-16 15:03 elwood/constants.py
 -rw-rw-r--  2.0 unx     3042 b- defN 23-Mar-20 16:57 elwood/download.py
--rw-rw-r--  2.0 unx    11168 b- defN 23-Apr-19 20:19 elwood/elwood.py
--rw-rw-r--  2.0 unx     4607 b- defN 23-Apr-19 20:19 elwood/feature_normalization.py
+-rw-rw-r--  2.0 unx    10730 b- defN 23-Jul-18 16:28 elwood/elwood.py
+-rw-rw-r--  2.0 unx     4370 b- defN 23-Apr-24 20:00 elwood/feature_normalization.py
 -rw-rw-r--  2.0 unx      689 b- defN 23-Mar-31 16:42 elwood/feature_scaling.py
--rw-rw-r--  2.0 unx     9756 b- defN 23-Mar-20 16:57 elwood/file_processor.py
+-rw-rw-r--  2.0 unx    10050 b- defN 23-May-16 15:57 elwood/file_processor.py
 -rw-rw-r--  2.0 unx    12542 b- defN 23-Mar-20 16:57 elwood/geo_processor.py
 -rw-rw-r--  2.0 unx    24705 b- defN 23-Mar-20 16:57 elwood/normalizer.py
--rw-rw-r--  2.0 unx    24707 b- defN 23-Apr-19 20:19 elwood/standardizer.py
+-rw-rw-r--  2.0 unx    24648 b- defN 23-May-03 18:37 elwood/standardizer.py
 -rw-rw-r--  2.0 unx     2313 b- defN 23-Mar-20 16:57 elwood/time_helpers.py
--rw-rw-r--  2.0 unx    11713 b- defN 23-Mar-20 16:57 elwood/time_processor.py
+-rw-rw-r--  2.0 unx    11711 b- defN 23-May-03 18:37 elwood/time_processor.py
+-rw-rw-r--  2.0 unx     1688 b- defN 23-Jun-27 17:01 elwood/utils.py
 -rw-rw-r--  2.0 unx     4602 b- defN 23-Feb-15 22:42 elwood/data/iso_lookup.csv
 -rw-rw-r--  2.0 unx        0 b- defN 23-Feb-16 15:03 elwood/transformations/__init__.py
--rw-rw-r--  2.0 unx     2007 b- defN 23-Mar-20 16:57 elwood/transformations/clipping.py
--rw-rw-r--  2.0 unx      424 b- defN 23-Feb-16 15:03 elwood/transformations/geo_utils.py
+-rw-rw-r--  2.0 unx     2029 b- defN 23-Jul-18 16:28 elwood/transformations/clipping.py
+-rw-rw-r--  2.0 unx      462 b- defN 23-Jul-18 16:28 elwood/transformations/geo_utils.py
 -rw-rw-r--  2.0 unx    16372 b- defN 23-Apr-19 20:19 elwood/transformations/outlier_scaling.py
--rw-rw-r--  2.0 unx     2410 b- defN 23-Apr-19 20:19 elwood/transformations/regridding.py
--rw-rw-r--  2.0 unx     1474 b- defN 23-Apr-19 20:19 elwood/transformations/scaling.py
+-rw-rw-r--  2.0 unx     2903 b- defN 23-Jul-18 16:28 elwood/transformations/regridding.py
+-rw-rw-r--  2.0 unx     1929 b- defN 23-Jul-18 16:28 elwood/transformations/scaling.py
 -rw-rw-r--  2.0 unx      141 b- defN 23-Feb-17 18:00 elwood/transformations/temporal_utils.py
--rw-rw-r--  2.0 unx      123 b- defN 23-Apr-19 20:19 elwood-0.1.2.dist-info/AUTHORS.md
--rw-rw-r--  2.0 unx    35149 b- defN 23-Apr-19 20:19 elwood-0.1.2.dist-info/LICENSE
--rw-rw-r--  2.0 unx     4943 b- defN 23-Apr-19 20:19 elwood-0.1.2.dist-info/METADATA
--rw-rw-r--  2.0 unx      110 b- defN 23-Apr-19 20:19 elwood-0.1.2.dist-info/WHEEL
--rw-rw-r--  2.0 unx       43 b- defN 23-Apr-19 20:19 elwood-0.1.2.dist-info/entry_points.txt
--rw-rw-r--  2.0 unx        7 b- defN 23-Apr-19 20:19 elwood-0.1.2.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx     2315 b- defN 23-Apr-19 20:19 elwood-0.1.2.dist-info/RECORD
-28 files, 193885 bytes uncompressed, 59410 bytes compressed:  69.4%
+-rw-rw-r--  2.0 unx      123 b- defN 23-Jul-18 16:29 elwood-0.1.3.dist-info/AUTHORS.md
+-rw-rw-r--  2.0 unx    35149 b- defN 23-Jul-18 16:29 elwood-0.1.3.dist-info/LICENSE
+-rw-rw-r--  2.0 unx     5837 b- defN 23-Jul-18 16:29 elwood-0.1.3.dist-info/METADATA
+-rw-rw-r--  2.0 unx      110 b- defN 23-Jul-18 16:29 elwood-0.1.3.dist-info/WHEEL
+-rw-rw-r--  2.0 unx       43 b- defN 23-Jul-18 16:29 elwood-0.1.3.dist-info/entry_points.txt
+-rw-rw-r--  2.0 unx        7 b- defN 23-Jul-18 16:29 elwood-0.1.3.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx     2388 b- defN 23-Jul-18 16:29 elwood-0.1.3.dist-info/RECORD
+29 files, 197122 bytes uncompressed, 60524 bytes compressed:  69.3%
```

## zipnote {}

```diff
@@ -33,14 +33,17 @@
 
 Filename: elwood/time_helpers.py
 Comment: 
 
 Filename: elwood/time_processor.py
 Comment: 
 
+Filename: elwood/utils.py
+Comment: 
+
 Filename: elwood/data/iso_lookup.csv
 Comment: 
 
 Filename: elwood/transformations/__init__.py
 Comment: 
 
 Filename: elwood/transformations/clipping.py
@@ -57,29 +60,29 @@
 
 Filename: elwood/transformations/scaling.py
 Comment: 
 
 Filename: elwood/transformations/temporal_utils.py
 Comment: 
 
-Filename: elwood-0.1.2.dist-info/AUTHORS.md
+Filename: elwood-0.1.3.dist-info/AUTHORS.md
 Comment: 
 
-Filename: elwood-0.1.2.dist-info/LICENSE
+Filename: elwood-0.1.3.dist-info/LICENSE
 Comment: 
 
-Filename: elwood-0.1.2.dist-info/METADATA
+Filename: elwood-0.1.3.dist-info/METADATA
 Comment: 
 
-Filename: elwood-0.1.2.dist-info/WHEEL
+Filename: elwood-0.1.3.dist-info/WHEEL
 Comment: 
 
-Filename: elwood-0.1.2.dist-info/entry_points.txt
+Filename: elwood-0.1.3.dist-info/entry_points.txt
 Comment: 
 
-Filename: elwood-0.1.2.dist-info/top_level.txt
+Filename: elwood-0.1.3.dist-info/top_level.txt
 Comment: 
 
-Filename: elwood-0.1.2.dist-info/RECORD
+Filename: elwood-0.1.3.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## elwood/__init__.py

```diff
@@ -1,5 +1,5 @@
 """Top-level package for elwood."""
 
 __author__ = """Brandon Rose"""
 __email__ = "brandon@jataware.com"
-__version__ = "0.1.2"
+__version__ = "0.1.3"
```

## elwood/cli.py

```diff
@@ -435,23 +435,23 @@
 @click.option("--date", type=str, default=None)
 @click.option("--x", type=str, help="longitude Column", default=None)
 @click.option("--y", type=str, help="latitude Column", default=None)
 def mix(xform, geo, input_file, output_file, feature_name, band, nodataval, date, x, y):
     """Console script to flexibly run elwood."""
     click.echo("Mixing...")
     # If geocoding: check if gadm feather file exists; if not, download it
-    if geo != None:
+    if geo is not None:
         download_and_clean(geo)
 
     if xform == "netcdf":
 
         print(f"Transforming {input_file} netcdf to csv")
         df = netcdf2df_processor(input_file)
 
-        if geo != None:
+        if geo is not None:
             print(f"Geocoding {input_file} to {geo}")
             df_geo = geocode(geo, df, x, y)
             df_geo.to_csv(output_file, index=False)
             print(df_geo.head())
 
         else:
             print("Writing netcdf to csv")
@@ -459,15 +459,15 @@
             print(df.head())
 
     elif xform == "geotiff":
 
         print(f"Transforming {input_file} geotiff to csv")
         df = raster2df_processor(input_file, feature_name, band, nodataval, date)
 
-        if geo != None:
+        if geo is not None:
 
             print(f"Geocoding {input_file} to {geo}")
             df_geo = geocode(geo, df, x, y)
             df_geo.to_csv(output_file, index=False)
             print(df_geo.head())
 
         else:
@@ -475,15 +475,15 @@
             df.to_csv(output_file, index=False)
             print(df.head())
 
     elif xform == "geocode":
 
         df = pd.read_csv(input_file)
 
-        if geo != None:
+        if geo is not None:
             print(f"Geocoding {input_file} to {geo}")
             df_geo = geocode(geo, df, x, y)
             df_geo.to_csv(output_file, index=False)
             print(df_geo.head())
 
 
 @cli.command()
```

## elwood/elwood.py

```diff
@@ -155,15 +155,15 @@
 
 
 def clip_geo(dataframe, geo_columns, polygons_list):
     """Clips data based on geographical shape(s) or shapefile (NOT IMPLEMENTED).
 
     Args:
         dataframe (pandas.Dataframe): A pandas dataframe containing geographical data.
-        geo_columns (list): A list containing the two column names for the lat/lon columns in the dataframe.
+        geo_columns (Dict): A dict containing the two column names for the lat/lon columns in the dataframe.
         polygons_list (list[list[obj]]): A list containing lists of objects that represent polygon shapes to clip to.
 
     Returns:
         pandas.Dataframe: A pandas dataframe only containing the clipped data.
     """
 
     mask = construct_multipolygon(polygons_list=polygons_list)
@@ -207,38 +207,23 @@
         time_column=time_column,
         time_bucket=time_bucket,
         aggregation_functions=aggregation_functions,
         geo_columns=geo_columns,
     )
 
 
-def regrid_dataframe_geo(dataframe, geo_columns, time_column, scale_multi, scale=None):
-    """Regrids a dataframe with detectable geo-resolution
-
-    Args:
-        dataframe (_type_): _description_
-        geo_columns (_type_): _description_
-        scale_multi (_type_): _description_
-    """
-
-    return regrid_dataframe(
-        dataframe=dataframe,
-        geo_columns=geo_columns,
-        time_column=time_column,
-        scale_multi=scale_multi,
-        scale=scale,
-    )
+regrid_dataframe_geo = regrid_dataframe
 
 
 def get_boundary_box(dataframe, geo_columns):
     """Returns the minimum and maximum x,y coordinates for a geographical dataset with latitude and longitude.
 
     Args:
         dataframe (pandas.Dataframe): Pandas dataframe with latitude and longitude
-        geo_columns (List[string]): A list of the two column names that represent latitude and longitude.
+        geo_columns (Dict[str,str]): A dict with the two column names that represent latitude and longitude.
 
     Returns:
         Dict: An object containing key value pairs for xmin, xmax,  ymin, and ymax.
     """
 
     return calculate_boundary_box(dataframe=dataframe, geo_columns=geo_columns)
 
@@ -311,15 +296,15 @@
         gadm3 = gadm3[["geometry", "country", "admin1", "admin2", "admin3"]]
         self.gadm3 = gadm3
 
 
 def optimize_df_types(df: pd.DataFrame):
     """
     Pandas will upcast essentially everything. This will use the built-in
-    Pandas function to_numeeric to downcast dataframe series to types that use
+    Pandas function to_numeric to downcast dataframe series to types that use
     less memory e.g. float64 to float32.
 
     For very large dataframes the memory reduction should translate into
     increased efficieny.
     """
     floats = df.select_dtypes(include=["float64"]).columns.tolist()
     df[floats] = df[floats].apply(pd.to_numeric, downcast="float")
```

## elwood/feature_normalization.py

```diff
@@ -1,13 +1,10 @@
 import numpy as np
 import pandas
 from numpy.typing import NDArray
-from scipy.stats import yeojohnson, gaussian_kde
-from matplotlib import pyplot as plt
-from scipy.stats import rankdata
 
 
 def zero_to_one_normalization(dataframe):
     """
     This function accepts a dataframe in the canonical format
     and min/max scales each feature to between 0 to 1
     """
@@ -34,17 +31,14 @@
 BoolMask = NDArray[np.bool_]  # type alias for boolean mask
 
 
 def min_max_clip(X: np.ndarray, outliers: BoolMask = None) -> np.ndarray:
     """Scale data to [0-1] range using min and max from non-outlier data. Outliers are clamped to range."""
     masked_X = X[~outliers] if outliers is not None else X
     min_value, max_value = np.min(masked_X, axis=0), np.max(masked_X, axis=0)
-    assert (
-        max_value > min_value
-    ), f"max_value {max_value} must be greater than min_value {min_value}"
     min_bound, max_bound = 0.0, 1.0
     return np.clip((X - min_value) / (max_value - min_value), min_bound, max_bound)
 
 
 def log_lin_log(X: np.ndarray, outliers: BoolMask = None) -> np.ndarray:
     """all non-outlier data is spaced linearly, while more extreme data is spaced logarithmically"""
```

## elwood/file_processor.py

```diff
@@ -183,15 +183,18 @@
 
         for ThisRow in RowRange:
             RowData = rBand.ReadAsArray(0, ThisRow, data_source.RasterXSize, 1)[0]
             for ThisCol in ColRange:
                 # need to exclude NaN values since there is no nodataval
                 row_value = RowData[ThisCol]
 
-                if (row_value > nData) and not (numpy.isnan(row_value)):
+                # if the null data value is a straight up nan then we should only check if the row_value is nan
+                # however, if the null data value is not nan (e.g. a number) we should make sure that row_value
+                # does not equal that number
+                if (numpy.isnan(nData) and not (numpy.isnan(row_value))) or (row_value != nData):
 
                     # TODO: implement filters on valid pixels
                     # for example, the below would ensure pixel values are between -100 and 100
                     # if (RowData[ThisCol] <= 100) and (RowData[ThisCol] >= -100):
 
                     X = GeoTrans[0] + (ThisCol * GeoTrans[1])
                     Y = GeoTrans[3] + (
```

## elwood/standardizer.py

```diff
@@ -148,15 +148,15 @@
         if result is None:
             # Special case to handle triplet date of day, month, year column.
             mapper_date_list.remove(date_dict)
             build_date_components = {
                 date_assoc["name"]: date_assoc
                 for date_assoc in mapper_date_list
                 if date_assoc["name"]
-                in [value for value in date_dict["associated_columns"].values()]
+                in [value for value in date_dict.get("associated_columns", {}).values()]
             }
             print(f"BUILD COMPONENTS: {build_date_components}")
             for item in build_date_components.values():
                 mapper_date_list.remove(item)
             build_date_components[date_dict["name"]] = date_dict
             result = build_a_date_handler(
                 date_mapper=build_date_components, dataframe=df
@@ -236,15 +236,15 @@
                 staple_col_name = "country"
                 df.rename(columns={geo_field_name: staple_col_name}, inplace=True)
                 # renamed_col_dict[staple_col_name] = [geo_field_name] # 7/2/2021 do not include primary cols
 
         elif "qualifies" in geo_dict and geo_dict["qualifies"]:
             # Note that any "qualifier" column that is not primary geo/date
             # will just be lopped on to the right as its own column. It'’'s
-            # column name will just be the name and Uncharted will deal with
+            # column name will just be the name and the client will deal with
             # it. The key takeaway is that qualifier columns grow the width,
             # not the length of the dataset.
             # Want to add the qualified col as the dictionary key.
             # e.g. "name": "region", "qualifies": ["probability", "color"]
             # should produce two dict entries for prob and color, with region
             # in a list as the value for both.
             for k in geo_dict["qualifies"]:
@@ -463,15 +463,15 @@
     df: pandas.DataFrame, mapper: dict, protected_cols: list
 ) -> (pandas.DataFrame, dict, dict):
     """
     Description
     -----------
     Identify mapper columns that match protected column names. When found,
     update the mapper and dataframe, and keep a dict of these changes
-    to return to the caller e.g. SpaceTag.
+    to return to the caller e.g. Dojo.
 
     Parameters
     ----------
     df: pandas.DataFrame
         submitted data
     mapper: dict
         a dictionary for the schema mapping (JSON) for the dataframe.
@@ -506,15 +506,15 @@
     ]
 
     # Only need to change a feature column name if it qualifies another field,
     # and therefore will be appended as a column to the output.
     feature_cols = [
         d["name"]
         for d in mapper["feature"]
-        if d["name"] in protected_cols and "qualifies" in d and d["qualifies"]
+        if d["name"] in protected_cols and d.get("qualifies")
     ]
 
     # Verbose build of the collision_list, could have combined above.
     collision_list = non_primary_geo_cols + non_primary_time_cols + feature_cols
 
     # Bail if no column name collisions.
     if not collision_list:
@@ -530,21 +530,21 @@
         renamed_col_dict[col + suffix] = [col]
 
     # Update mapper
     for k, vlist in mapper.items():
         for dct in vlist:
             if dct["name"] in collision_list:
                 dct["name"] = dct["name"] + suffix
-            elif "qualifies" in dct and dct["qualifies"]:
+            elif dct.get("qualifies"):
                 # change any instances of this column name qualified by another field
                 dct["qualifies"] = [
                     w.replace(w, w + suffix) if w in collision_list else w
                     for w in dct["qualifies"]
                 ]
-            elif "associated_columns" in dct and dct["associated_columns"]:
+            elif dct.get("associated_columns"):
                 # change any instances of this column name in an associated_columns dict
                 dct["associated_columns"] = {
                     k: v.replace(v, v + suffix) if v in collision_list else v
                     for k, v in dct["associated_columns"].items()
                 }
 
     return df, mapper, renamed_col_dict
```

## elwood/time_processor.py

```diff
@@ -257,15 +257,15 @@
 
     # Add the first popped tuple column name to the list of associated fields.
     assoc_fields.append(date_field_tuple[0])
 
     # TODO: If day and year are associated to each other and month, but
     # month is not associated to those fields, then at this point assoc_fields
     # will be the three values, and assoc_columns will contain only day and
-    # year. This will error out below. It is assumed that SpaceTag will
+    # year. This will error out below. It is assumed that client will
     # control for this instance.
 
     # If there is no primary_time column for timestamp, which would have
     # been created above with primary_date_group_mapper, or farther above
     # looping mapper["date"], attempt to generate from date_type = Month,
     # Day, Year features. Otherwise, create a new column name from the
     # concatenation of the associated date fields here.
```

## elwood/transformations/clipping.py

```diff
@@ -19,16 +19,16 @@
 
     shape = MultiPolygon(final_multipolygon_list)
 
     return shape
 
 
 def clip_dataframe(dataframe, geo_columns, mask):
-    x_geo = dataframe[geo_columns[0]]
-    y_geo = dataframe[geo_columns[1]]
+    x_geo = dataframe[geo_columns['lon_column']]
+    y_geo = dataframe[geo_columns['lat_column']]
     geo_dataframe = geopandas.GeoDataFrame(
         dataframe, geometry=geopandas.points_from_xy(x_geo, y_geo)
     )
 
     clipped_gpd = geopandas.clip(gdf=geo_dataframe, mask=mask)
 
     return pandas.DataFrame(clipped_gpd)
```

## elwood/transformations/geo_utils.py

```diff
@@ -1,13 +1,15 @@
 import geopandas
 
 
 def calculate_boundary_box(dataframe, geo_columns):
-    x_geo = dataframe[geo_columns[0]]
-    y_geo = dataframe[geo_columns[1]]
+    """
+    """
+    x_geo = dataframe[geo_columns['lon_column']]
+    y_geo = dataframe[geo_columns['lat_column']]
     geo_dataframe = geopandas.GeoDataFrame(
         dataframe, geometry=geopandas.points_from_xy(x_geo, y_geo)
     )
 
     xmin, ymin, xmax, ymax = geo_dataframe.total_bounds
 
     boundary_dict = {"xmin": xmin, "xmax": xmax, "ymin": ymin, "ymax": ymax}
```

## elwood/transformations/regridding.py

```diff
@@ -1,60 +1,67 @@
 import math
 import xarray
 import numpy
+import pandas as pd
+from typing import Dict, List
 
 
-def regrid_dataframe(dataframe, geo_columns, time_column, scale_multi, scale=None):
+def regrid_dataframe(dataframe: pd.core.frame.DataFrame,
+                     geo_columns: Dict[str, str],
+                     time_column: List[str],
+                     scale_multi: int,
+                     scale=None) -> pd.core.frame.DataFrame:
     """Uses xarray interpolation to regrid geography in a dataframe.
 
     Args:
         dataframe (pandas.Dataframe): Dataframe of a dataset that has detectable gridden geographical resolution ie. points that represent 1sqkm areas
-        geo_columns (List[str]): A list containing the geo_columns for the latitude and longitude pairs.
+        geo_columns (Dict[str]): A dictionary containing the geo_columns for the latitude and longitude pairs, with keys 'lat_column' and 'lon_column'.
         time_column (List[str]): A list containing the name of the datetime column(s) in the dataset.
         scale_multi (int): The number by which to divide to geographical scale to regrid larger.
 
     Returns:
         pandas.Dataframe: Dataframe with geographical extend regridded to
     """
 
-    geo_columns.extend(time_column)
+    geo_columns_list = [geo_columns['lat_column'], geo_columns['lon_column']]
+    geo_columns_list.extend(time_column)
     ds = None
 
     try:
 
-        ds = xarray.Dataset.from_dataframe(dataframe.set_index(geo_columns))
+        ds = xarray.Dataset.from_dataframe(dataframe.set_index(geo_columns_list))
 
     except KeyError as error:
         print(error)
 
     ds_scale = 0
     if scale:
         ds_scale = scale
     else:
         ds_scale = getScale(
-            ds[geo_columns[0]][0],
-            ds[geo_columns[1]][0],
-            ds[geo_columns[0]][1],
-            ds[geo_columns[1]][1],
+            ds[geo_columns['lat_column']][0],
+            ds[geo_columns['lon_column']][0],
+            ds[geo_columns['lat_column']][1],
+            ds[geo_columns['lon_column']][1],
         )
 
     multiplier = ds_scale / scale_multi
 
-    new_0 = numpy.linspace(
-        ds[geo_columns[0]][0],
-        ds[geo_columns[0]][-1],
-        round(ds.dims[geo_columns[0]] * multiplier),
+    new_lat = numpy.linspace(
+        ds[geo_columns['lat_column']][0],
+        ds[geo_columns['lat_column']][-1],
+        round(ds.dims[geo_columns['lat_column']] * multiplier),
     )
-    new_1 = numpy.linspace(
-        ds[geo_columns[1]][0],
-        ds[geo_columns[1]][-1],
-        round(ds.dims[geo_columns[1]] * multiplier),
+    new_lon = numpy.linspace(
+        ds[geo_columns['lon_column']][0],
+        ds[geo_columns['lon_column']][-1],
+        round(ds.dims[geo_columns['lon_column']] * multiplier),
     )
 
-    interpolation = {geo_columns[0]: new_0, geo_columns[1]: new_1}
+    interpolation = {geo_columns['lat_column']: new_lat, geo_columns['lon_column']: new_lon}
 
     ds2 = ds.interp(**interpolation)
 
     final_dataframe = ds2.to_dataframe()
     final_dataframe.reset_index(inplace=True)
 
     return final_dataframe
```

## elwood/transformations/scaling.py

```diff
@@ -1,33 +1,41 @@
 import pandas
 
 
 def scale_time(
-    dataframe, time_column, time_bucket, aggregation_functions, geo_columns=None
+        dataframe: pandas.core.frame.DataFrame,
+        time_column: str,
+        time_bucket,
+        aggregation_functions,
+        geo_columns=None
 ):
     """Scales timestamp data in a dataframe to a less granular time frequency
 
     Args:
         dataframe (pandas.Dataframe): pandas dataframe with a timestamp field.
         time_column (string): Name of the time column to scale in the dataframe.
         time_bucket (DateOffset, Timedelta or str): The offset string or object representing target conversion. ex. "2H", "M"
         aggregation_functions (List[string] or Dict{str:str}): A list containing one aggregation function, or a Dict containing column
             names and aggregation functions like sum, average, median, mean, mode, etc. Example: ['sum'], or {'temp': 'min', 'rainfall': 'max', 'visitors': 'sum'}
-        geo_columns (List[string]): A list of the geographical columns that should not be modified by the transformation.
+        geo_columns (dict): A dict containing the column names for the lat/lon columns in the dataframe, with keys 'lat' and 'lon'.
     """
 
     dataframe[time_column] = pandas.to_datetime(dataframe[time_column])
 
-    if geo_columns:
-        dataframe = dataframe.set_index(geo_columns)
-
-        dataframe = dataframe.groupby(geo_columns)
-
-    aggregator = aggregation_functions
-    if isinstance(aggregation_functions, list):
-        aggregator = aggregation_functions[0]
-    scaled_frame = dataframe.resample(time_bucket, on=time_column).agg(aggregator)
-
-    # scaled_frame.columns = scaled_frame.columns.get_level_values(0)
-    scaled_frame.reset_index(inplace=True)
-
-    return scaled_frame
+    if geo_columns and 'lat_column' in geo_columns and 'lon_column' in geo_columns:
+        geo_columns_list = [geo_columns['lat_column'], geo_columns['lon_column']]  # Extracts the values (column names) from the dict
+        dataframe = dataframe.set_index(geo_columns_list)
+        dataframe = dataframe.groupby(geo_columns_list)
+
+    if aggregation_functions and all(aggregation_functions):
+        aggregator = aggregation_functions
+        if isinstance(aggregation_functions, list):
+            aggregator = aggregation_functions[0]
+        scaled_frame = dataframe.resample(time_bucket, on=time_column).agg(aggregator)
+
+
+        # scaled_frame.columns = scaled_frame.columns.get_level_values(0)
+        scaled_frame.reset_index(inplace=True)
+
+        return scaled_frame
+    else:
+        raise ValueError("Invalid aggregation functions provided.")
```

## Comparing `elwood-0.1.2.dist-info/LICENSE` & `elwood-0.1.3.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `elwood-0.1.2.dist-info/METADATA` & `elwood-0.1.3.dist-info/METADATA`

 * *Files 21% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: elwood
-Version: 0.1.2
+Version: 0.1.3
 Summary: An open source dataset transformation, standardization, and normalization python library.
 Home-page: https://github.com/jataware/elwood
 Author: Brandon Rose, Powell Fendley
 Author-email: info@jataware.com
 License: MIT license
 Keywords: elwood
 Platform: UNKNOWN
@@ -28,14 +28,15 @@
 Requires-Dist: fuzzywuzzy (>=0.18.0)
 Requires-Dist: GDAL (==3.1.4)
 Requires-Dist: geofeather (>=0.3.0)
 Requires-Dist: geopandas (==0.8.1)
 Requires-Dist: netCDF4 (==1.5.3)
 Requires-Dist: numpy (==1.22)
 Requires-Dist: openpyxl (==3.0.7)
+Requires-Dist: pandas (==1.5.3)
 Requires-Dist: pip (>=21.1)
 Requires-Dist: pydantic (>=1.8.2)
 Requires-Dist: pyproj (==2.6.1.post1)
 Requires-Dist: python-Levenshtein (>=0.12.2)
 Requires-Dist: Rtree (==0.8.3)
 Requires-Dist: shapely
 Requires-Dist: Sphinx (==1.8.5)
@@ -57,25 +58,28 @@
 `pip install elwood`
 
 Now you are able to use any of the dataset transformation, standardization, or normalization functions exposed through this library. To start, simply include `from elwood import elwood` in your python file. 
 
 ## Standardization
 `elwood.process(args)`
 
-#TODO STUB
+Given an arbitrary dataset containing geospatial data (with columns and rows) with arbitrary non-standard format, and given some annotations/dictionary about the dataset, Elwood can standardize. Standardization means creating an output dataset with stable and predictable columns. The data can be normalized, regridded, scaled, and resolved using GADM to standard country names, as well as resolve the latitude,longitude of the event/measurement. A usual standard output will contain the following columns: `timestamp`, `country`, `admin1`, `admin2`, `admin3`, `lat`, `lng`, alongside other measurements/events/features of interest (additional columns to the right of the standard ones) contained within the input dataset.
+
+#TODO document standardization further
 
 ## Transformation
 
 The transformation functions include geographical extent clipping (latitude/longitude), geographical regridding (gridded data such as NetCDF or GeoTIFF), temporal clipping, and temporal scaling. 
 
 ### Geospatial Clipping
 
 `elwood.clip_geo(dataframe, geo_columns, polygons_list)`
 
-This function takes a pandas dataframe, a geo_columns list of the column names for latitude and longitude, ex: `["lat", "lng"]`, and a list containing lists of objects representing the polygons to clip the data to. ex: 
+This function takes a pandas dataframe, a geo_columns dict of the column names for latitude and longitude, ex:
+`{'lat_column': 'latitude', 'lon_column': 'longitude'}`, and a list containing lists of objects representing the polygons to clip the data to. ex: 
 ```
 [
      [
         {
             "lat": 11.0,
             "lng": 42.0
         },
@@ -95,15 +99,15 @@
     ...
 ]
 ```
 ### Geospatial regridding
 
 `elwood.regrid_dataframe_geo(dataframe, geo_columns, scale_multi)`
 
-This function takes a dataframe and regrids it's geography by some scale multiplier that is provided. This multiplier will be used to divide the current geographical scale in order to make a more coarse grained resolution dataset. The dataframe must have a detectable geographical scale, meaning each lat/lon represents a point in the middle of a gridded cell for the data provided. Lat and lon and determined by the geo_columns passed in: a list of the column names ex: `["lat", "lng"]`
+This function takes a dataframe and regrids it's geography by some scale multiplier that is provided. This multiplier will be used to divide the current geographical scale in order to make a more coarse grained resolution dataset. The dataframe must have a detectable geographical scale, meaning each lat/lon represents a point in the middle of a gridded cell for the data provided. Lat and lon and determined by the geo_columns passed in: a dict of the column names ex: `{'lat_column': 'my_latitude', 'lon_column': 'my_longitude'}`
 
 ### Temporal Clipping
 `elwood.clip_dataframe_time(dataframe, time_column, time_ranges)`
 
 This function will produce a dataframe that only includes rows with `time_column` values contained within `time_ranges`. The time_ranges argument is a list of objects containing a start and end time. ex: `[{"start": datetime, "end": datetime}, ...]`
 
 ### Temporal Scaling
@@ -116,11 +120,18 @@
 `elwood.normalize_features(dataframe, output_file)`
 
 This function expects a dataframe with a "feature" column and a "value" column, or long data. Each entry for a feature has its own feature/value row.
 This function returns a dataframe in which all numerical values under the "value" column for each "feature" have been 0 to 1 scaled.
 Optionally you may specify an `output_file` name to generate a parquet file of the dataframe.
 
 
-# Historys
+# History
+
+0.1.3
+
+0.1.2
+
+0.1.1
 
+0.1.0
```

## Comparing `elwood-0.1.2.dist-info/RECORD` & `elwood-0.1.3.dist-info/RECORD`

 * *Files 14% similar despite different names*

```diff
@@ -1,28 +1,29 @@
-elwood/__init__.py,sha256=Dq2HwvPsxX7-Y_TV0v4Us0Icx86C4ODbIZ_nPh8k7HM,126
-elwood/cli.py,sha256=M4-2TU-q9tyBDSlj9HN4w2_2YcROr2JMqYj5nUySe6o,17988
+elwood/__init__.py,sha256=M1f-09dZ6k6nN0ZoH78_GrzQsYhOOu70mbkl_nPR0YU,126
+elwood/cli.py,sha256=DddheI7ROE63sX1lWDnBE8nftlSul3R8LceADKZ0JvE,18004
 elwood/constants.py,sha256=ZthYZ6NWPGrBpsHuVgYTY4CfGW5TDY7woUiuuGVNAvw,409
 elwood/download.py,sha256=E0oI7P3rR5nt7TYN00TnJmtgCalGiuJS1TPn4F4qX3A,3042
-elwood/elwood.py,sha256=ZqtD5C6Xlcg0S6ulUwpEC8aST4ePyyriZeGdV7XrZBQ,11168
-elwood/feature_normalization.py,sha256=-vAFEqEI5xt3aqrv-gLlZxjENlIW3aI44riqmu-n6uA,4607
+elwood/elwood.py,sha256=TqeqfCCncc5bkofuuDfGpy6clR3U0u44P_ok3-67K7c,10730
+elwood/feature_normalization.py,sha256=-nj1PzS4_oqg1QHGmF3O-pwQJYw9OGwpkm3L6uWOKEY,4370
 elwood/feature_scaling.py,sha256=gZaWWJ9n69tMw8fX7AAd5-FLc4uyYYaK-2tsQDkxYiI,689
-elwood/file_processor.py,sha256=V9FNr5WX11LCGQTx4zBYoXMKjZd8hAAiYmH7xS6Re3s,9756
+elwood/file_processor.py,sha256=Ix371yYemNEX9zZv2Lc9Lftwql1oGUBWNFno8QGT_N0,10050
 elwood/geo_processor.py,sha256=a0P90yBmrNwAq_JiDZbyyW8pihVjNUCO2C4kpapf62s,12542
 elwood/normalizer.py,sha256=cFM4B4-J5tg0aykdm_lSlyR5BQDKbB2rMrDsxsEg5JQ,24705
-elwood/standardizer.py,sha256=2MjtmPNMzgAwRGic9rErlTZbYk571wSsVt-JkMhTyDo,24707
+elwood/standardizer.py,sha256=XKL9gtiwIpMngFdxzT8MK4G8BE7AAYPj3G6D-QpbzQQ,24648
 elwood/time_helpers.py,sha256=MX55zyBRggJ-lcVAhDRM-1HrfiaBeQXFt6D86dz0v9c,2313
-elwood/time_processor.py,sha256=_cLtRBX478CewVOMtSJjWhRx_S4z0TzMgDqyGs8Y8aY,11713
+elwood/time_processor.py,sha256=ZnnFnjyEobciVuOPzNIiQ4xZwUBSiQXPf-Pi44HXIRM,11711
+elwood/utils.py,sha256=H6flLw_x68l9gAeq4CbhYNDwvQyTWPhITlzz-lOcKDU,1688
 elwood/data/iso_lookup.csv,sha256=nPPErF9kP2fU_s4IkldxgyUc5NnbYMbePrDYLQAoHSQ,4602
 elwood/transformations/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-elwood/transformations/clipping.py,sha256=6LKx8qap0xiWiDgm2Hb7_aavxCVUcpAYEc1EOFvyfx8,2007
-elwood/transformations/geo_utils.py,sha256=p_c14Gy3k5dC8XWcPfeWldwDyhufjBWNptCyUUOeaD4,424
+elwood/transformations/clipping.py,sha256=fzif6Z8Ee1o6DH4KtiF4gM8cFGSdsuqTDpQ9alMLZjg,2029
+elwood/transformations/geo_utils.py,sha256=ZdUnpHZU5WpPT8T3SNUJM7jfTeazsAzsD57WK3-TqlM,462
 elwood/transformations/outlier_scaling.py,sha256=O_K57Xx-eqOVC2MHr1h3E-QpyK70QRumPVmoPLtMtjI,16372
-elwood/transformations/regridding.py,sha256=N2Mo1MKJ2OrzfPr-Ac3l-S2yIiWINn-W5SsvPFsKKOo,2410
-elwood/transformations/scaling.py,sha256=skRvRSuudgViBDo3HmAuOh6G27pUmTwILCd0gelk7j4,1474
+elwood/transformations/regridding.py,sha256=wTX5S53dR4QAFvSTICXISIEP1uPDucZLQhpegDyDz0Y,2903
+elwood/transformations/scaling.py,sha256=VqzTnaGlrrorttVcQk-Cr9Tg-4VJGuoJ89ynv7yk1Jk,1929
 elwood/transformations/temporal_utils.py,sha256=CxT29J3GfK4h6onFlNphojBP_IA9nL5G3c1r9Vh8QDE,141
-elwood-0.1.2.dist-info/AUTHORS.md,sha256=_nf9LY5n7wib7wT7oPRZqxRil0STaMq7JcvAwTph7tI,123
-elwood-0.1.2.dist-info/LICENSE,sha256=OXLcl0T2SZ8Pmy2_dmlvKuetivmyPd5m1q-Gyd-zaYY,35149
-elwood-0.1.2.dist-info/METADATA,sha256=uuXgt-5tPPwhDx6EBJCzMvX5gzUbpeExr6z8V97wwo0,4943
-elwood-0.1.2.dist-info/WHEEL,sha256=WzZ8cwjh8l0jtULNjYq1Hpr-WCqCRgPr--TX4P5I1Wo,110
-elwood-0.1.2.dist-info/entry_points.txt,sha256=TzJi2WjTTVW62q1YxjU0wNUn9F3QbPILW6A8VRe9Xes,43
-elwood-0.1.2.dist-info/top_level.txt,sha256=MKkVgr_DJlxlN-IlwnW98ogymP3HYNUDq7_aK-pftXg,7
-elwood-0.1.2.dist-info/RECORD,,
+elwood-0.1.3.dist-info/AUTHORS.md,sha256=_nf9LY5n7wib7wT7oPRZqxRil0STaMq7JcvAwTph7tI,123
+elwood-0.1.3.dist-info/LICENSE,sha256=OXLcl0T2SZ8Pmy2_dmlvKuetivmyPd5m1q-Gyd-zaYY,35149
+elwood-0.1.3.dist-info/METADATA,sha256=QfYYYqMflfBKUSypEiqy4OMipPQ6Ma9wpfyOcjdKtEE,5837
+elwood-0.1.3.dist-info/WHEEL,sha256=8zNYZbwQSXoB9IfXOjPfeNwvAsALAjffgk27FqvCWbo,110
+elwood-0.1.3.dist-info/entry_points.txt,sha256=TzJi2WjTTVW62q1YxjU0wNUn9F3QbPILW6A8VRe9Xes,43
+elwood-0.1.3.dist-info/top_level.txt,sha256=MKkVgr_DJlxlN-IlwnW98ogymP3HYNUDq7_aK-pftXg,7
+elwood-0.1.3.dist-info/RECORD,,
```

