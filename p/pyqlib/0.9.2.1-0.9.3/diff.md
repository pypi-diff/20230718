# Comparing `tmp/pyqlib-0.9.2.1-cp38-cp38-macosx_10_15_x86_64.whl.zip` & `tmp/pyqlib-0.9.3-cp38-cp38-win_amd64.whl.zip`

## zipinfo {}

```diff
@@ -1,235 +1,240 @@
-Zip file size: 641881 bytes, number of entries: 233
--rw-r--r--  2.0 unx    11664 b- defN 23-Jul-05 05:01 qlib/__init__.py
--rw-r--r--  2.0 unx    17640 b- defN 23-Jul-05 05:00 qlib/config.py
--rw-r--r--  2.0 unx      499 b- defN 23-Jul-05 05:00 qlib/constant.py
--rw-r--r--  2.0 unx     7601 b- defN 23-Jul-05 05:00 qlib/log.py
--rw-r--r--  2.0 unx     1995 b- defN 23-Jul-05 05:00 qlib/typehint.py
--rw-r--r--  2.0 unx    11817 b- defN 23-Jul-05 05:00 qlib/backtest/__init__.py
--rw-r--r--  2.0 unx    17999 b- defN 23-Jul-05 05:00 qlib/backtest/account.py
--rw-r--r--  2.0 unx     4039 b- defN 23-Jul-05 05:00 qlib/backtest/backtest.py
--rw-r--r--  2.0 unx    21870 b- defN 23-Jul-05 05:00 qlib/backtest/decision.py
--rw-r--r--  2.0 unx    44185 b- defN 23-Jul-05 05:00 qlib/backtest/exchange.py
--rw-r--r--  2.0 unx    26612 b- defN 23-Jul-05 05:00 qlib/backtest/executor.py
--rw-r--r--  2.0 unx    23234 b- defN 23-Jul-05 05:00 qlib/backtest/high_performance_ds.py
--rw-r--r--  2.0 unx    20047 b- defN 23-Jul-05 05:00 qlib/backtest/position.py
--rw-r--r--  2.0 unx    14992 b- defN 23-Jul-05 05:00 qlib/backtest/profit_attribution.py
--rw-r--r--  2.0 unx    27673 b- defN 23-Jul-05 05:00 qlib/backtest/report.py
--rw-r--r--  2.0 unx     4003 b- defN 23-Jul-05 05:00 qlib/backtest/signal.py
--rw-r--r--  2.0 unx    10536 b- defN 23-Jul-05 05:00 qlib/backtest/utils.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jul-05 05:00 qlib/contrib/__init__.py
--rw-r--r--  2.0 unx    14067 b- defN 23-Jul-05 05:00 qlib/contrib/evaluate.py
--rw-r--r--  2.0 unx     6395 b- defN 23-Jul-05 05:00 qlib/contrib/evaluate_portfolio.py
--rw-r--r--  2.0 unx     1074 b- defN 23-Jul-05 05:00 qlib/contrib/torch.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jul-05 05:00 qlib/contrib/data/__init__.py
--rw-r--r--  2.0 unx     2130 b- defN 23-Jul-05 05:00 qlib/contrib/data/data.py
--rw-r--r--  2.0 unx    13599 b- defN 23-Jul-05 05:00 qlib/contrib/data/dataset.py
--rw-r--r--  2.0 unx    20209 b- defN 23-Jul-05 05:00 qlib/contrib/data/handler.py
--rw-r--r--  2.0 unx    18736 b- defN 23-Jul-05 05:00 qlib/contrib/data/highfreq_handler.py
--rw-r--r--  2.0 unx     3070 b- defN 23-Jul-05 05:00 qlib/contrib/data/highfreq_processor.py
--rw-r--r--  2.0 unx    12284 b- defN 23-Jul-05 05:00 qlib/contrib/data/highfreq_provider.py
--rw-r--r--  2.0 unx     4456 b- defN 23-Jul-05 05:00 qlib/contrib/data/processor.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jul-05 05:00 qlib/contrib/data/utils/__init__.py
--rw-r--r--  2.0 unx     6953 b- defN 23-Jul-05 05:00 qlib/contrib/data/utils/sepdf.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jul-05 05:00 qlib/contrib/eva/__init__.py
--rw-r--r--  2.0 unx     6635 b- defN 23-Jul-05 05:00 qlib/contrib/eva/alpha.py
--rw-r--r--  2.0 unx      200 b- defN 23-Jul-05 05:00 qlib/contrib/meta/__init__.py
--rw-r--r--  2.0 unx      211 b- defN 23-Jul-05 05:00 qlib/contrib/meta/data_selection/__init__.py
--rw-r--r--  2.0 unx    17758 b- defN 23-Jul-05 05:00 qlib/contrib/meta/data_selection/dataset.py
--rw-r--r--  2.0 unx     6431 b- defN 23-Jul-05 05:00 qlib/contrib/meta/data_selection/model.py
--rw-r--r--  2.0 unx     3024 b- defN 23-Jul-05 05:00 qlib/contrib/meta/data_selection/net.py
--rw-r--r--  2.0 unx     3962 b- defN 23-Jul-05 05:00 qlib/contrib/meta/data_selection/utils.py
--rw-r--r--  2.0 unx     1711 b- defN 23-Jul-05 05:00 qlib/contrib/model/__init__.py
--rw-r--r--  2.0 unx     3778 b- defN 23-Jul-05 05:00 qlib/contrib/model/catboost_model.py
--rw-r--r--  2.0 unx    12182 b- defN 23-Jul-05 05:00 qlib/contrib/model/double_ensemble.py
--rw-r--r--  2.0 unx     4937 b- defN 23-Jul-05 05:00 qlib/contrib/model/gbdt.py
--rw-r--r--  2.0 unx     6645 b- defN 23-Jul-05 05:00 qlib/contrib/model/highfreq_gdbt_model.py
--rw-r--r--  2.0 unx     4203 b- defN 23-Jul-05 05:00 qlib/contrib/model/linear.py
--rw-r--r--  2.0 unx    27941 b- defN 23-Jul-05 05:00 qlib/contrib/model/pytorch_adarnn.py
--rw-r--r--  2.0 unx    21515 b- defN 23-Jul-05 05:00 qlib/contrib/model/pytorch_add.py
--rw-r--r--  2.0 unx    11339 b- defN 23-Jul-05 05:00 qlib/contrib/model/pytorch_alstm.py
--rw-r--r--  2.0 unx    11569 b- defN 23-Jul-05 05:00 qlib/contrib/model/pytorch_alstm_ts.py
--rw-r--r--  2.0 unx    12709 b- defN 23-Jul-05 05:00 qlib/contrib/model/pytorch_gats.py
--rw-r--r--  2.0 unx    13149 b- defN 23-Jul-05 05:00 qlib/contrib/model/pytorch_gats_ts.py
--rw-r--r--  2.0 unx     9652 b- defN 23-Jul-05 05:00 qlib/contrib/model/pytorch_gru.py
--rw-r--r--  2.0 unx     9863 b- defN 23-Jul-05 05:00 qlib/contrib/model/pytorch_gru_ts.py
--rw-r--r--  2.0 unx    18671 b- defN 23-Jul-05 05:00 qlib/contrib/model/pytorch_hist.py
--rw-r--r--  2.0 unx    15847 b- defN 23-Jul-05 05:00 qlib/contrib/model/pytorch_igmtf.py
--rw-r--r--  2.0 unx    16228 b- defN 23-Jul-05 05:00 qlib/contrib/model/pytorch_krnn.py
--rw-r--r--  2.0 unx    11054 b- defN 23-Jul-05 05:00 qlib/contrib/model/pytorch_localformer.py
--rw-r--r--  2.0 unx    10377 b- defN 23-Jul-05 05:00 qlib/contrib/model/pytorch_localformer_ts.py
--rw-r--r--  2.0 unx     9416 b- defN 23-Jul-05 05:00 qlib/contrib/model/pytorch_lstm.py
--rw-r--r--  2.0 unx     9655 b- defN 23-Jul-05 05:00 qlib/contrib/model/pytorch_lstm_ts.py
--rw-r--r--  2.0 unx    17140 b- defN 23-Jul-05 05:00 qlib/contrib/model/pytorch_nn.py
--rw-r--r--  2.0 unx    11661 b- defN 23-Jul-05 05:00 qlib/contrib/model/pytorch_sandwich.py
--rw-r--r--  2.0 unx    15892 b- defN 23-Jul-05 05:00 qlib/contrib/model/pytorch_sfm.py
--rw-r--r--  2.0 unx    22859 b- defN 23-Jul-05 05:00 qlib/contrib/model/pytorch_tabnet.py
--rw-r--r--  2.0 unx     9587 b- defN 23-Jul-05 05:00 qlib/contrib/model/pytorch_tcn.py
--rw-r--r--  2.0 unx     9167 b- defN 23-Jul-05 05:00 qlib/contrib/model/pytorch_tcn_ts.py
--rw-r--r--  2.0 unx    14296 b- defN 23-Jul-05 05:00 qlib/contrib/model/pytorch_tcts.py
--rw-r--r--  2.0 unx    34231 b- defN 23-Jul-05 05:00 qlib/contrib/model/pytorch_tra.py
--rw-r--r--  2.0 unx     9881 b- defN 23-Jul-05 05:00 qlib/contrib/model/pytorch_transformer.py
--rw-r--r--  2.0 unx     9179 b- defN 23-Jul-05 05:00 qlib/contrib/model/pytorch_transformer_ts.py
--rw-r--r--  2.0 unx     1197 b- defN 23-Jul-05 05:00 qlib/contrib/model/pytorch_utils.py
--rw-r--r--  2.0 unx     2606 b- defN 23-Jul-05 05:00 qlib/contrib/model/tcn.py
--rw-r--r--  2.0 unx     3084 b- defN 23-Jul-05 05:00 qlib/contrib/model/xgboost.py
--rw-r--r--  2.0 unx      586 b- defN 23-Jul-05 05:00 qlib/contrib/online/__init__.py
--rw-r--r--  2.0 unx     5489 b- defN 23-Jul-05 05:00 qlib/contrib/online/manager.py
--rw-r--r--  2.0 unx     1110 b- defN 23-Jul-05 05:00 qlib/contrib/online/online_model.py
--rw-r--r--  2.0 unx    13138 b- defN 23-Jul-05 05:00 qlib/contrib/online/operator.py
--rw-r--r--  2.0 unx     2980 b- defN 23-Jul-05 05:00 qlib/contrib/online/user.py
--rw-r--r--  2.0 unx     3079 b- defN 23-Jul-05 05:00 qlib/contrib/online/utils.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jul-05 05:00 qlib/contrib/ops/__init__.py
--rw-r--r--  2.0 unx     8151 b- defN 23-Jul-05 05:00 qlib/contrib/ops/high_freq.py
--rw-r--r--  2.0 unx      356 b- defN 23-Jul-05 05:00 qlib/contrib/report/__init__.py
--rw-r--r--  2.0 unx    11189 b- defN 23-Jul-05 05:00 qlib/contrib/report/graph.py
--rw-r--r--  2.0 unx     2467 b- defN 23-Jul-05 05:00 qlib/contrib/report/utils.py
--rw-r--r--  2.0 unx      178 b- defN 23-Jul-05 05:00 qlib/contrib/report/analysis_model/__init__.py
--rw-r--r--  2.0 unx    11382 b- defN 23-Jul-05 05:00 qlib/contrib/report/analysis_model/analysis_model_performance.py
--rw-r--r--  2.0 unx      404 b- defN 23-Jul-05 05:00 qlib/contrib/report/analysis_position/__init__.py
--rw-r--r--  2.0 unx     9452 b- defN 23-Jul-05 05:00 qlib/contrib/report/analysis_position/cumulative_return.py
--rw-r--r--  2.0 unx     6559 b- defN 23-Jul-05 05:00 qlib/contrib/report/analysis_position/parse_position.py
--rw-r--r--  2.0 unx     4322 b- defN 23-Jul-05 05:00 qlib/contrib/report/analysis_position/rank_label.py
--rw-r--r--  2.0 unx     8413 b- defN 23-Jul-05 05:00 qlib/contrib/report/analysis_position/report.py
--rw-r--r--  2.0 unx    10651 b- defN 23-Jul-05 05:00 qlib/contrib/report/analysis_position/risk_analysis.py
--rw-r--r--  2.0 unx     2572 b- defN 23-Jul-05 05:00 qlib/contrib/report/analysis_position/score_ic.py
--rw-r--r--  2.0 unx      124 b- defN 23-Jul-05 05:00 qlib/contrib/report/data/__init__.py
--rw-r--r--  2.0 unx     6608 b- defN 23-Jul-05 05:00 qlib/contrib/report/data/ana.py
--rw-r--r--  2.0 unx      931 b- defN 23-Jul-05 05:00 qlib/contrib/report/data/base.py
--rw-r--r--  2.0 unx      512 b- defN 23-Jul-05 05:00 qlib/contrib/strategy/__init__.py
--rw-r--r--  2.0 unx     3827 b- defN 23-Jul-05 05:00 qlib/contrib/strategy/cost_control.py
--rw-r--r--  2.0 unx     8260 b- defN 23-Jul-05 05:00 qlib/contrib/strategy/order_generator.py
--rw-r--r--  2.0 unx    29374 b- defN 23-Jul-05 05:00 qlib/contrib/strategy/rule_strategy.py
--rw-r--r--  2.0 unx    22498 b- defN 23-Jul-05 05:00 qlib/contrib/strategy/signal_strategy.py
--rw-r--r--  2.0 unx      286 b- defN 23-Jul-05 05:00 qlib/contrib/strategy/optimizer/__init__.py
--rw-r--r--  2.0 unx      314 b- defN 23-Jul-05 05:00 qlib/contrib/strategy/optimizer/base.py
--rw-r--r--  2.0 unx     6515 b- defN 23-Jul-05 05:00 qlib/contrib/strategy/optimizer/enhanced_indexing.py
--rw-r--r--  2.0 unx     8640 b- defN 23-Jul-05 05:00 qlib/contrib/strategy/optimizer/optimizer.py
--rw-r--r--  2.0 unx       35 b- defN 23-Jul-05 05:00 qlib/contrib/tuner/__init__.py
--rw-r--r--  2.0 unx     3629 b- defN 23-Jul-05 05:00 qlib/contrib/tuner/config.py
--rw-r--r--  2.0 unx      821 b- defN 23-Jul-05 05:00 qlib/contrib/tuner/launcher.py
--rw-r--r--  2.0 unx     3451 b- defN 23-Jul-05 05:00 qlib/contrib/tuner/pipeline.py
--rw-r--r--  2.0 unx      433 b- defN 23-Jul-05 05:00 qlib/contrib/tuner/space.py
--rw-r--r--  2.0 unx     8009 b- defN 23-Jul-05 05:00 qlib/contrib/tuner/tuner.py
--rw-r--r--  2.0 unx      206 b- defN 23-Jul-05 05:00 qlib/contrib/workflow/__init__.py
--rw-r--r--  2.0 unx     3368 b- defN 23-Jul-05 05:00 qlib/contrib/workflow/record_temp.py
--rw-r--r--  2.0 unx     1410 b- defN 23-Jul-05 05:00 qlib/data/__init__.py
--rw-r--r--  2.0 unx     8387 b- defN 23-Jul-05 05:00 qlib/data/base.py
--rw-r--r--  2.0 unx    47251 b- defN 23-Jul-05 05:00 qlib/data/cache.py
--rw-r--r--  2.0 unx     3749 b- defN 23-Jul-05 05:00 qlib/data/client.py
--rw-r--r--  2.0 unx    49488 b- defN 23-Jul-05 05:00 qlib/data/data.py
--rw-r--r--  2.0 unx    13924 b- defN 23-Jul-05 05:00 qlib/data/filter.py
--rw-r--r--  2.0 unx      598 b- defN 23-Jul-05 05:00 qlib/data/inst_processor.py
--rw-r--r--  2.0 unx    45446 b- defN 23-Jul-05 05:00 qlib/data/ops.py
--rw-r--r--  2.0 unx     3232 b- defN 23-Jul-05 05:00 qlib/data/pit.py
--rw-r--r--  2.0 unx       73 b- defN 23-Jul-05 05:00 qlib/data/_libs/__init__.py
--rwxr-xr-x  2.0 unx   156960 b- defN 23-Jul-05 05:08 qlib/data/_libs/expanding.cpython-38-darwin.so
--rw-r--r--  2.0 unx     4151 b- defN 23-Jul-05 05:00 qlib/data/_libs/expanding.pyx
--rwxr-xr-x  2.0 unx   104808 b- defN 23-Jul-05 05:08 qlib/data/_libs/rolling.cpython-38-darwin.so
--rw-r--r--  2.0 unx     6111 b- defN 23-Jul-05 05:00 qlib/data/_libs/rolling.pyx
--rw-r--r--  2.0 unx    27197 b- defN 23-Jul-05 05:00 qlib/data/dataset/__init__.py
--rw-r--r--  2.0 unx    26735 b- defN 23-Jul-05 05:00 qlib/data/dataset/handler.py
--rw-r--r--  2.0 unx    12348 b- defN 23-Jul-05 05:00 qlib/data/dataset/loader.py
--rw-r--r--  2.0 unx    14380 b- defN 23-Jul-05 05:00 qlib/data/dataset/processor.py
--rw-r--r--  2.0 unx     6349 b- defN 23-Jul-05 05:00 qlib/data/dataset/storage.py
--rw-r--r--  2.0 unx     4184 b- defN 23-Jul-05 05:00 qlib/data/dataset/utils.py
--rw-r--r--  2.0 unx      754 b- defN 23-Jul-05 05:00 qlib/data/dataset/weight.py
--rw-r--r--  2.0 unx      269 b- defN 23-Jul-05 05:00 qlib/data/storage/__init__.py
--rw-r--r--  2.0 unx    14386 b- defN 23-Jul-05 05:00 qlib/data/storage/file_storage.py
--rw-r--r--  2.0 unx    14662 b- defN 23-Jul-05 05:00 qlib/data/storage/storage.py
--rw-r--r--  2.0 unx      149 b- defN 23-Jul-05 05:00 qlib/model/__init__.py
--rw-r--r--  2.0 unx     3771 b- defN 23-Jul-05 05:00 qlib/model/base.py
--rw-r--r--  2.0 unx    22767 b- defN 23-Jul-05 05:00 qlib/model/trainer.py
--rw-r--r--  2.0 unx      579 b- defN 23-Jul-05 05:00 qlib/model/utils.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jul-05 05:00 qlib/model/ens/__init__.py
--rw-r--r--  2.0 unx     4535 b- defN 23-Jul-05 05:00 qlib/model/ens/ensemble.py
--rw-r--r--  2.0 unx     3912 b- defN 23-Jul-05 05:00 qlib/model/ens/group.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jul-05 05:00 qlib/model/interpret/__init__.py
--rw-r--r--  2.0 unx     1133 b- defN 23-Jul-05 05:00 qlib/model/interpret/base.py
--rw-r--r--  2.0 unx      182 b- defN 23-Jul-05 05:00 qlib/model/meta/__init__.py
--rw-r--r--  2.0 unx     2779 b- defN 23-Jul-05 05:00 qlib/model/meta/dataset.py
--rw-r--r--  2.0 unx     2355 b- defN 23-Jul-05 05:00 qlib/model/meta/model.py
--rw-r--r--  2.0 unx     1655 b- defN 23-Jul-05 05:00 qlib/model/meta/task.py
--rw-r--r--  2.0 unx      336 b- defN 23-Jul-05 05:00 qlib/model/riskmodel/__init__.py
--rw-r--r--  2.0 unx     5035 b- defN 23-Jul-05 05:00 qlib/model/riskmodel/base.py
--rw-r--r--  2.0 unx     3206 b- defN 23-Jul-05 05:00 qlib/model/riskmodel/poet.py
--rw-r--r--  2.0 unx    10441 b- defN 23-Jul-05 05:00 qlib/model/riskmodel/shrink.py
--rw-r--r--  2.0 unx     3801 b- defN 23-Jul-05 05:00 qlib/model/riskmodel/structured.py
--rw-r--r--  2.0 unx      339 b- defN 23-Jul-05 05:00 qlib/rl/__init__.py
--rw-r--r--  2.0 unx     1122 b- defN 23-Jul-05 05:00 qlib/rl/aux_info.py
--rw-r--r--  2.0 unx     5235 b- defN 23-Jul-05 05:00 qlib/rl/interpreter.py
--rw-r--r--  2.0 unx     2703 b- defN 23-Jul-05 05:00 qlib/rl/reward.py
--rw-r--r--  2.0 unx      339 b- defN 23-Jul-05 05:00 qlib/rl/seed.py
--rw-r--r--  2.0 unx     3031 b- defN 23-Jul-05 05:00 qlib/rl/simulator.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jul-05 05:00 qlib/rl/contrib/__init__.py
--rw-r--r--  2.0 unx    13230 b- defN 23-Jul-05 05:00 qlib/rl/contrib/backtest.py
--rw-r--r--  2.0 unx     3387 b- defN 23-Jul-05 05:00 qlib/rl/contrib/naive_config_parser.py
--rw-r--r--  2.0 unx    10105 b- defN 23-Jul-05 05:00 qlib/rl/contrib/train_onpolicy.py
--rw-r--r--  2.0 unx      836 b- defN 23-Jul-05 05:00 qlib/rl/contrib/utils.py
--rw-r--r--  2.0 unx      245 b- defN 23-Jul-05 05:00 qlib/rl/data/__init__.py
--rw-r--r--  2.0 unx     1754 b- defN 23-Jul-05 05:00 qlib/rl/data/base.py
--rw-r--r--  2.0 unx     3111 b- defN 23-Jul-05 05:00 qlib/rl/data/integration.py
--rw-r--r--  2.0 unx     7412 b- defN 23-Jul-05 05:00 qlib/rl/data/native.py
--rw-r--r--  2.0 unx    10544 b- defN 23-Jul-05 05:00 qlib/rl/data/pickle_styled.py
--rw-r--r--  2.0 unx     1008 b- defN 23-Jul-05 05:00 qlib/rl/order_execution/__init__.py
--rw-r--r--  2.0 unx     9669 b- defN 23-Jul-05 05:00 qlib/rl/order_execution/interpreter.py
--rw-r--r--  2.0 unx     4830 b- defN 23-Jul-05 05:00 qlib/rl/order_execution/network.py
--rw-r--r--  2.0 unx     7008 b- defN 23-Jul-05 05:00 qlib/rl/order_execution/policy.py
--rw-r--r--  2.0 unx     3549 b- defN 23-Jul-05 05:00 qlib/rl/order_execution/reward.py
--rw-r--r--  2.0 unx     4765 b- defN 23-Jul-05 05:00 qlib/rl/order_execution/simulator_qlib.py
--rw-r--r--  2.0 unx    14742 b- defN 23-Jul-05 05:00 qlib/rl/order_execution/simulator_simple.py
--rw-r--r--  2.0 unx     3690 b- defN 23-Jul-05 05:00 qlib/rl/order_execution/state.py
--rw-r--r--  2.0 unx    21177 b- defN 23-Jul-05 05:00 qlib/rl/order_execution/strategy.py
--rw-r--r--  2.0 unx     1631 b- defN 23-Jul-05 05:00 qlib/rl/order_execution/utils.py
--rw-r--r--  2.0 unx      154 b- defN 23-Jul-05 05:00 qlib/rl/strategy/__init__.py
--rw-r--r--  2.0 unx     1049 b- defN 23-Jul-05 05:00 qlib/rl/strategy/single_order.py
--rw-r--r--  2.0 unx      463 b- defN 23-Jul-05 05:00 qlib/rl/trainer/__init__.py
--rw-r--r--  2.0 unx     3650 b- defN 23-Jul-05 05:00 qlib/rl/trainer/api.py
--rw-r--r--  2.0 unx    11576 b- defN 23-Jul-05 05:00 qlib/rl/trainer/callbacks.py
--rw-r--r--  2.0 unx    13476 b- defN 23-Jul-05 05:00 qlib/rl/trainer/trainer.py
--rw-r--r--  2.0 unx     9887 b- defN 23-Jul-05 05:00 qlib/rl/trainer/vessel.py
--rw-r--r--  2.0 unx      527 b- defN 23-Jul-05 05:00 qlib/rl/utils/__init__.py
--rw-r--r--  2.0 unx     6597 b- defN 23-Jul-05 05:00 qlib/rl/utils/data_queue.py
--rw-r--r--  2.0 unx     9844 b- defN 23-Jul-05 05:00 qlib/rl/utils/env_wrapper.py
--rw-r--r--  2.0 unx    13367 b- defN 23-Jul-05 05:00 qlib/rl/utils/finite_env.py
--rw-r--r--  2.0 unx    18541 b- defN 23-Jul-05 05:00 qlib/rl/utils/log.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jul-05 05:00 qlib/run/__init__.py
--rw-r--r--  2.0 unx      176 b- defN 23-Jul-05 05:00 qlib/run/get_data.py
--rw-r--r--  2.0 unx       73 b- defN 23-Jul-05 05:00 qlib/strategy/__init__.py
--rw-r--r--  2.0 unx    11155 b- defN 23-Jul-05 05:00 qlib/strategy/base.py
--rw-r--r--  2.0 unx    12220 b- defN 23-Jul-05 05:00 qlib/tests/__init__.py
--rw-r--r--  2.0 unx     4834 b- defN 23-Jul-05 05:00 qlib/tests/config.py
--rw-r--r--  2.0 unx     8001 b- defN 23-Jul-05 05:00 qlib/tests/data.py
--rw-r--r--  2.0 unx    34192 b- defN 23-Jul-05 05:00 qlib/utils/__init__.py
--rw-r--r--  2.0 unx     3085 b- defN 23-Jul-05 05:00 qlib/utils/data.py
--rw-r--r--  2.0 unx      438 b- defN 23-Jul-05 05:00 qlib/utils/exceptions.py
--rw-r--r--  2.0 unx     5738 b- defN 23-Jul-05 05:00 qlib/utils/file.py
--rw-r--r--  2.0 unx    21893 b- defN 23-Jul-05 05:00 qlib/utils/index_data.py
--rw-r--r--  2.0 unx     3302 b- defN 23-Jul-05 05:00 qlib/utils/objm.py
--rw-r--r--  2.0 unx     9118 b- defN 23-Jul-05 05:00 qlib/utils/paral.py
--rw-r--r--  2.0 unx     9387 b- defN 23-Jul-05 05:00 qlib/utils/resam.py
--rw-r--r--  2.0 unx     6074 b- defN 23-Jul-05 05:00 qlib/utils/serial.py
--rw-r--r--  2.0 unx    11778 b- defN 23-Jul-05 05:00 qlib/utils/time.py
--rw-r--r--  2.0 unx    24582 b- defN 23-Jul-05 05:00 qlib/workflow/__init__.py
--rw-r--r--  2.0 unx     3690 b- defN 23-Jul-05 05:00 qlib/workflow/cli.py
--rw-r--r--  2.0 unx    15192 b- defN 23-Jul-05 05:00 qlib/workflow/exp.py
--rw-r--r--  2.0 unx    17579 b- defN 23-Jul-05 05:00 qlib/workflow/expm.py
--rw-r--r--  2.0 unx    21860 b- defN 23-Jul-05 05:00 qlib/workflow/record_temp.py
--rw-r--r--  2.0 unx    18257 b- defN 23-Jul-05 05:00 qlib/workflow/recorder.py
--rw-r--r--  2.0 unx     1616 b- defN 23-Jul-05 05:00 qlib/workflow/utils.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jul-05 05:00 qlib/workflow/online/__init__.py
--rw-r--r--  2.0 unx    17366 b- defN 23-Jul-05 05:00 qlib/workflow/online/manager.py
--rw-r--r--  2.0 unx     8411 b- defN 23-Jul-05 05:00 qlib/workflow/online/strategy.py
--rw-r--r--  2.0 unx    10586 b- defN 23-Jul-05 05:00 qlib/workflow/online/update.py
--rw-r--r--  2.0 unx     6475 b- defN 23-Jul-05 05:00 qlib/workflow/online/utils.py
--rw-r--r--  2.0 unx      535 b- defN 23-Jul-05 05:00 qlib/workflow/task/__init__.py
--rw-r--r--  2.0 unx    10057 b- defN 23-Jul-05 05:00 qlib/workflow/task/collect.py
--rw-r--r--  2.0 unx    11767 b- defN 23-Jul-05 05:00 qlib/workflow/task/gen.py
--rw-r--r--  2.0 unx    18412 b- defN 23-Jul-05 05:00 qlib/workflow/task/manage.py
--rw-r--r--  2.0 unx     8612 b- defN 23-Jul-05 05:00 qlib/workflow/task/utils.py
--rw-r--r--  2.0 unx     1141 b- defN 23-Jul-05 05:08 pyqlib-0.9.2.1.dist-info/LICENSE
--rw-r--r--  2.0 unx    39367 b- defN 23-Jul-05 05:08 pyqlib-0.9.2.1.dist-info/METADATA
--rw-r--r--  2.0 unx      110 b- defN 23-Jul-05 05:08 pyqlib-0.9.2.1.dist-info/WHEEL
--rw-r--r--  2.0 unx       47 b- defN 23-Jul-05 05:08 pyqlib-0.9.2.1.dist-info/entry_points.txt
--rw-r--r--  2.0 unx        5 b- defN 23-Jul-05 05:08 pyqlib-0.9.2.1.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx    20183 b- defN 23-Jul-05 05:08 pyqlib-0.9.2.1.dist-info/RECORD
-233 files, 2276535 bytes uncompressed, 610257 bytes compressed:  73.2%
+Zip file size: 690808 bytes, number of entries: 238
+-rw-rw-rw-  2.0 fat    11957 b- defN 23-Jul-18 11:31 qlib/__init__.py
+-rw-rw-rw-  2.0 fat    18129 b- defN 23-Jul-18 11:31 qlib/config.py
+-rw-rw-rw-  2.0 fat      521 b- defN 23-Jul-18 11:31 qlib/constant.py
+-rw-rw-rw-  2.0 fat     7862 b- defN 23-Jul-18 11:31 qlib/log.py
+-rw-rw-rw-  2.0 fat     2058 b- defN 23-Jul-18 11:31 qlib/typehint.py
+-rw-rw-rw-  2.0 fat    12163 b- defN 23-Jul-18 11:31 qlib/backtest/__init__.py
+-rw-rw-rw-  2.0 fat    18416 b- defN 23-Jul-18 11:31 qlib/backtest/account.py
+-rw-rw-rw-  2.0 fat     4149 b- defN 23-Jul-18 11:31 qlib/backtest/backtest.py
+-rw-rw-rw-  2.0 fat    22466 b- defN 23-Jul-18 11:31 qlib/backtest/decision.py
+-rw-rw-rw-  2.0 fat    45141 b- defN 23-Jul-18 11:31 qlib/backtest/exchange.py
+-rw-rw-rw-  2.0 fat    27240 b- defN 23-Jul-18 11:31 qlib/backtest/executor.py
+-rw-rw-rw-  2.0 fat    23892 b- defN 23-Jul-18 11:31 qlib/backtest/high_performance_ds.py
+-rw-rw-rw-  2.0 fat    20612 b- defN 23-Jul-18 11:31 qlib/backtest/position.py
+-rw-rw-rw-  2.0 fat    15326 b- defN 23-Jul-18 11:31 qlib/backtest/profit_attribution.py
+-rw-rw-rw-  2.0 fat    28314 b- defN 23-Jul-18 11:31 qlib/backtest/report.py
+-rw-rw-rw-  2.0 fat     4108 b- defN 23-Jul-18 11:31 qlib/backtest/signal.py
+-rw-rw-rw-  2.0 fat    10826 b- defN 23-Jul-18 11:31 qlib/backtest/utils.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-Jul-18 11:31 qlib/contrib/__init__.py
+-rw-rw-rw-  2.0 fat    14471 b- defN 23-Jul-18 11:31 qlib/contrib/evaluate.py
+-rw-rw-rw-  2.0 fat     6639 b- defN 23-Jul-18 11:31 qlib/contrib/evaluate_portfolio.py
+-rw-rw-rw-  2.0 fat     1105 b- defN 23-Jul-18 11:31 qlib/contrib/torch.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-Jul-18 11:31 qlib/contrib/data/__init__.py
+-rw-rw-rw-  2.0 fat     2185 b- defN 23-Jul-18 11:31 qlib/contrib/data/data.py
+-rw-rw-rw-  2.0 fat    13947 b- defN 23-Jul-18 11:31 qlib/contrib/data/dataset.py
+-rw-rw-rw-  2.0 fat    20641 b- defN 23-Jul-18 11:31 qlib/contrib/data/handler.py
+-rw-rw-rw-  2.0 fat    19273 b- defN 23-Jul-18 11:31 qlib/contrib/data/highfreq_handler.py
+-rw-rw-rw-  2.0 fat     3149 b- defN 23-Jul-18 11:31 qlib/contrib/data/highfreq_processor.py
+-rw-rw-rw-  2.0 fat    12588 b- defN 23-Jul-18 11:31 qlib/contrib/data/highfreq_provider.py
+-rw-rw-rw-  2.0 fat     4571 b- defN 23-Jul-18 11:31 qlib/contrib/data/processor.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-Jul-18 11:31 qlib/contrib/data/utils/__init__.py
+-rw-rw-rw-  2.0 fat     7163 b- defN 23-Jul-18 11:31 qlib/contrib/data/utils/sepdf.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-Jul-18 11:31 qlib/contrib/eva/__init__.py
+-rw-rw-rw-  2.0 fat     6847 b- defN 23-Jul-18 11:31 qlib/contrib/eva/alpha.py
+-rw-rw-rw-  2.0 fat      207 b- defN 23-Jul-18 11:31 qlib/contrib/meta/__init__.py
+-rw-rw-rw-  2.0 fat      219 b- defN 23-Jul-18 11:31 qlib/contrib/meta/data_selection/__init__.py
+-rw-rw-rw-  2.0 fat    18286 b- defN 23-Jul-18 11:31 qlib/contrib/meta/data_selection/dataset.py
+-rw-rw-rw-  2.0 fat     6615 b- defN 23-Jul-18 11:31 qlib/contrib/meta/data_selection/model.py
+-rw-rw-rw-  2.0 fat     3098 b- defN 23-Jul-18 11:31 qlib/contrib/meta/data_selection/net.py
+-rw-rw-rw-  2.0 fat     4075 b- defN 23-Jul-18 11:31 qlib/contrib/meta/data_selection/utils.py
+-rw-rw-rw-  2.0 fat     1754 b- defN 23-Jul-18 11:31 qlib/contrib/model/__init__.py
+-rw-rw-rw-  2.0 fat     3878 b- defN 23-Jul-18 11:31 qlib/contrib/model/catboost_model.py
+-rw-rw-rw-  2.0 fat    12459 b- defN 23-Jul-18 11:31 qlib/contrib/model/double_ensemble.py
+-rw-rw-rw-  2.0 fat     5061 b- defN 23-Jul-18 11:31 qlib/contrib/model/gbdt.py
+-rw-rw-rw-  2.0 fat     6810 b- defN 23-Jul-18 11:31 qlib/contrib/model/highfreq_gdbt_model.py
+-rw-rw-rw-  2.0 fat     4315 b- defN 23-Jul-18 11:31 qlib/contrib/model/linear.py
+-rw-rw-rw-  2.0 fat    28729 b- defN 23-Jul-18 11:31 qlib/contrib/model/pytorch_adarnn.py
+-rw-rw-rw-  2.0 fat    22112 b- defN 23-Jul-18 11:31 qlib/contrib/model/pytorch_add.py
+-rw-rw-rw-  2.0 fat    11676 b- defN 23-Jul-18 11:31 qlib/contrib/model/pytorch_alstm.py
+-rw-rw-rw-  2.0 fat    11911 b- defN 23-Jul-18 11:31 qlib/contrib/model/pytorch_alstm_ts.py
+-rw-rw-rw-  2.0 fat    13089 b- defN 23-Jul-18 11:31 qlib/contrib/model/pytorch_gats.py
+-rw-rw-rw-  2.0 fat    13532 b- defN 23-Jul-18 11:31 qlib/contrib/model/pytorch_gats_ts.py
+-rw-rw-rw-  2.0 fat     9959 b- defN 23-Jul-18 11:31 qlib/contrib/model/pytorch_gru.py
+-rw-rw-rw-  2.0 fat    10173 b- defN 23-Jul-18 11:31 qlib/contrib/model/pytorch_gru_ts.py
+-rw-rw-rw-  2.0 fat    19168 b- defN 23-Jul-18 11:31 qlib/contrib/model/pytorch_hist.py
+-rw-rw-rw-  2.0 fat    16285 b- defN 23-Jul-18 11:31 qlib/contrib/model/pytorch_igmtf.py
+-rw-rw-rw-  2.0 fat    16228 b- defN 23-Jul-18 11:31 qlib/contrib/model/pytorch_krnn.py
+-rw-rw-rw-  2.0 fat    11038 b- defN 23-Jul-18 11:31 qlib/contrib/model/pytorch_localformer.py
+-rw-rw-rw-  2.0 fat    10365 b- defN 23-Jul-18 11:31 qlib/contrib/model/pytorch_localformer_ts.py
+-rw-rw-rw-  2.0 fat     9716 b- defN 23-Jul-18 11:31 qlib/contrib/model/pytorch_lstm.py
+-rw-rw-rw-  2.0 fat     9960 b- defN 23-Jul-18 11:31 qlib/contrib/model/pytorch_lstm_ts.py
+-rw-rw-rw-  2.0 fat    17587 b- defN 23-Jul-18 11:31 qlib/contrib/model/pytorch_nn.py
+-rw-rw-rw-  2.0 fat    12042 b- defN 23-Jul-18 11:31 qlib/contrib/model/pytorch_sandwich.py
+-rw-rw-rw-  2.0 fat    16365 b- defN 23-Jul-18 11:31 qlib/contrib/model/pytorch_sfm.py
+-rw-rw-rw-  2.0 fat    23498 b- defN 23-Jul-18 11:31 qlib/contrib/model/pytorch_tabnet.py
+-rw-rw-rw-  2.0 fat     9891 b- defN 23-Jul-18 11:31 qlib/contrib/model/pytorch_tcn.py
+-rw-rw-rw-  2.0 fat     9460 b- defN 23-Jul-18 11:31 qlib/contrib/model/pytorch_tcn_ts.py
+-rw-rw-rw-  2.0 fat    14713 b- defN 23-Jul-18 11:31 qlib/contrib/model/pytorch_tcts.py
+-rw-rw-rw-  2.0 fat    35154 b- defN 23-Jul-18 11:31 qlib/contrib/model/pytorch_tra.py
+-rw-rw-rw-  2.0 fat     9865 b- defN 23-Jul-18 11:31 qlib/contrib/model/pytorch_transformer.py
+-rw-rw-rw-  2.0 fat     9167 b- defN 23-Jul-18 11:31 qlib/contrib/model/pytorch_transformer_ts.py
+-rw-rw-rw-  2.0 fat     1234 b- defN 23-Jul-18 11:31 qlib/contrib/model/pytorch_utils.py
+-rw-rw-rw-  2.0 fat     2682 b- defN 23-Jul-18 11:31 qlib/contrib/model/tcn.py
+-rw-rw-rw-  2.0 fat     3168 b- defN 23-Jul-18 11:31 qlib/contrib/model/xgboost.py
+-rw-rw-rw-  2.0 fat      607 b- defN 23-Jul-18 11:31 qlib/contrib/online/__init__.py
+-rw-rw-rw-  2.0 fat     5636 b- defN 23-Jul-18 11:31 qlib/contrib/online/manager.py
+-rw-rw-rw-  2.0 fat     1149 b- defN 23-Jul-18 11:31 qlib/contrib/online/online_model.py
+-rw-rw-rw-  2.0 fat    13138 b- defN 23-Jul-18 11:31 qlib/contrib/online/operator.py
+-rw-rw-rw-  2.0 fat     3057 b- defN 23-Jul-18 11:31 qlib/contrib/online/user.py
+-rw-rw-rw-  2.0 fat     3177 b- defN 23-Jul-18 11:31 qlib/contrib/online/utils.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-Jul-18 11:31 qlib/contrib/ops/__init__.py
+-rw-rw-rw-  2.0 fat     8428 b- defN 23-Jul-18 11:31 qlib/contrib/ops/high_freq.py
+-rw-rw-rw-  2.0 fat      367 b- defN 23-Jul-18 11:31 qlib/contrib/report/__init__.py
+-rw-rw-rw-  2.0 fat    11572 b- defN 23-Jul-18 11:31 qlib/contrib/report/graph.py
+-rw-rw-rw-  2.0 fat     2541 b- defN 23-Jul-18 11:31 qlib/contrib/report/utils.py
+-rw-rw-rw-  2.0 fat      185 b- defN 23-Jul-18 11:31 qlib/contrib/report/analysis_model/__init__.py
+-rw-rw-rw-  2.0 fat    11719 b- defN 23-Jul-18 11:31 qlib/contrib/report/analysis_model/analysis_model_performance.py
+-rw-rw-rw-  2.0 fat      415 b- defN 23-Jul-18 11:31 qlib/contrib/report/analysis_position/__init__.py
+-rw-rw-rw-  2.0 fat     9725 b- defN 23-Jul-18 11:31 qlib/contrib/report/analysis_position/cumulative_return.py
+-rw-rw-rw-  2.0 fat     6734 b- defN 23-Jul-18 11:31 qlib/contrib/report/analysis_position/parse_position.py
+-rw-rw-rw-  2.0 fat     4450 b- defN 23-Jul-18 11:31 qlib/contrib/report/analysis_position/rank_label.py
+-rw-rw-rw-  2.0 fat     8661 b- defN 23-Jul-18 11:31 qlib/contrib/report/analysis_position/report.py
+-rw-rw-rw-  2.0 fat    10946 b- defN 23-Jul-18 11:31 qlib/contrib/report/analysis_position/risk_analysis.py
+-rw-rw-rw-  2.0 fat     2641 b- defN 23-Jul-18 11:31 qlib/contrib/report/analysis_position/score_ic.py
+-rw-rw-rw-  2.0 fat      131 b- defN 23-Jul-18 11:31 qlib/contrib/report/data/__init__.py
+-rw-rw-rw-  2.0 fat     6808 b- defN 23-Jul-18 11:31 qlib/contrib/report/data/ana.py
+-rw-rw-rw-  2.0 fat      965 b- defN 23-Jul-18 11:31 qlib/contrib/report/data/base.py
+-rw-rw-rw-  2.0 fat      326 b- defN 23-Jul-18 11:31 qlib/contrib/rolling/__init__.py
+-rw-rw-rw-  2.0 fat      658 b- defN 23-Jul-18 11:31 qlib/contrib/rolling/__main__.py
+-rw-rw-rw-  2.0 fat    10803 b- defN 23-Jul-18 11:31 qlib/contrib/rolling/base.py
+-rw-rw-rw-  2.0 fat    13592 b- defN 23-Jul-18 11:31 qlib/contrib/rolling/ddgda.py
+-rw-rw-rw-  2.0 fat      540 b- defN 23-Jul-18 11:31 qlib/contrib/strategy/__init__.py
+-rw-rw-rw-  2.0 fat     3928 b- defN 23-Jul-18 11:31 qlib/contrib/strategy/cost_control.py
+-rw-rw-rw-  2.0 fat     8478 b- defN 23-Jul-18 11:31 qlib/contrib/strategy/order_generator.py
+-rw-rw-rw-  2.0 fat    30043 b- defN 23-Jul-18 11:31 qlib/contrib/strategy/rule_strategy.py
+-rw-rw-rw-  2.0 fat    23019 b- defN 23-Jul-18 11:31 qlib/contrib/strategy/signal_strategy.py
+-rw-rw-rw-  2.0 fat      295 b- defN 23-Jul-18 11:31 qlib/contrib/strategy/optimizer/__init__.py
+-rw-rw-rw-  2.0 fat      326 b- defN 23-Jul-18 11:31 qlib/contrib/strategy/optimizer/base.py
+-rw-rw-rw-  2.0 fat     6717 b- defN 23-Jul-18 11:31 qlib/contrib/strategy/optimizer/enhanced_indexing.py
+-rw-rw-rw-  2.0 fat     8904 b- defN 23-Jul-18 11:31 qlib/contrib/strategy/optimizer/optimizer.py
+-rw-rw-rw-  2.0 fat       37 b- defN 23-Jul-18 11:31 qlib/contrib/tuner/__init__.py
+-rw-rw-rw-  2.0 fat     3716 b- defN 23-Jul-18 11:31 qlib/contrib/tuner/config.py
+-rw-rw-rw-  2.0 fat      858 b- defN 23-Jul-18 11:31 qlib/contrib/tuner/launcher.py
+-rw-rw-rw-  2.0 fat     3532 b- defN 23-Jul-18 11:31 qlib/contrib/tuner/pipeline.py
+-rw-rw-rw-  2.0 fat      453 b- defN 23-Jul-18 11:31 qlib/contrib/tuner/space.py
+-rw-rw-rw-  2.0 fat     8217 b- defN 23-Jul-18 11:31 qlib/contrib/tuner/tuner.py
+-rw-rw-rw-  2.0 fat      213 b- defN 23-Jul-18 11:31 qlib/contrib/workflow/__init__.py
+-rw-rw-rw-  2.0 fat     3454 b- defN 23-Jul-18 11:31 qlib/contrib/workflow/record_temp.py
+-rw-rw-rw-  2.0 fat     1476 b- defN 23-Jul-18 11:31 qlib/data/__init__.py
+-rw-rw-rw-  2.0 fat     8668 b- defN 23-Jul-18 11:31 qlib/data/base.py
+-rw-rw-rw-  2.0 fat    48444 b- defN 23-Jul-18 11:31 qlib/data/cache.py
+-rw-rw-rw-  2.0 fat     3852 b- defN 23-Jul-18 11:31 qlib/data/client.py
+-rw-rw-rw-  2.0 fat    50820 b- defN 23-Jul-18 11:31 qlib/data/data.py
+-rw-rw-rw-  2.0 fat    14298 b- defN 23-Jul-18 11:31 qlib/data/filter.py
+-rw-rw-rw-  2.0 fat      620 b- defN 23-Jul-18 11:31 qlib/data/inst_processor.py
+-rw-rw-rw-  2.0 fat    47127 b- defN 23-Jul-18 11:31 qlib/data/ops.py
+-rw-rw-rw-  2.0 fat     3302 b- defN 23-Jul-18 11:31 qlib/data/pit.py
+-rw-rw-rw-  2.0 fat       75 b- defN 23-Jul-18 11:31 qlib/data/_libs/__init__.py
+-rw-rw-rw-  2.0 fat   156160 b- defN 23-Jul-18 11:33 qlib/data/_libs/expanding.cp38-win_amd64.pyd
+-rw-rw-rw-  2.0 fat     4303 b- defN 23-Jul-18 11:31 qlib/data/_libs/expanding.pyx
+-rw-rw-rw-  2.0 fat   120320 b- defN 23-Jul-18 11:33 qlib/data/_libs/rolling.cp38-win_amd64.pyd
+-rw-rw-rw-  2.0 fat     6318 b- defN 23-Jul-18 11:31 qlib/data/_libs/rolling.pyx
+-rw-rw-rw-  2.0 fat    27919 b- defN 23-Jul-18 11:31 qlib/data/dataset/__init__.py
+-rw-rw-rw-  2.0 fat    27480 b- defN 23-Jul-18 11:31 qlib/data/dataset/handler.py
+-rw-rw-rw-  2.0 fat    12690 b- defN 23-Jul-18 11:31 qlib/data/dataset/loader.py
+-rw-rw-rw-  2.0 fat    14799 b- defN 23-Jul-18 11:31 qlib/data/dataset/processor.py
+-rw-rw-rw-  2.0 fat     6507 b- defN 23-Jul-18 11:31 qlib/data/dataset/storage.py
+-rw-rw-rw-  2.0 fat     4294 b- defN 23-Jul-18 11:31 qlib/data/dataset/utils.py
+-rw-rw-rw-  2.0 fat      781 b- defN 23-Jul-18 11:31 qlib/data/dataset/weight.py
+-rw-rw-rw-  2.0 fat      276 b- defN 23-Jul-18 11:31 qlib/data/storage/__init__.py
+-rw-rw-rw-  2.0 fat    14762 b- defN 23-Jul-18 11:31 qlib/data/storage/file_storage.py
+-rw-rw-rw-  2.0 fat    15156 b- defN 23-Jul-18 11:31 qlib/data/storage/storage.py
+-rw-rw-rw-  2.0 fat      158 b- defN 23-Jul-18 11:31 qlib/model/__init__.py
+-rw-rw-rw-  2.0 fat     3881 b- defN 23-Jul-18 11:31 qlib/model/base.py
+-rw-rw-rw-  2.0 fat    23384 b- defN 23-Jul-18 11:31 qlib/model/trainer.py
+-rw-rw-rw-  2.0 fat      605 b- defN 23-Jul-18 11:31 qlib/model/utils.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-Jul-18 11:31 qlib/model/ens/__init__.py
+-rw-rw-rw-  2.0 fat     4669 b- defN 23-Jul-18 11:31 qlib/model/ens/ensemble.py
+-rw-rw-rw-  2.0 fat     4027 b- defN 23-Jul-18 11:31 qlib/model/ens/group.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-Jul-18 11:31 qlib/model/interpret/__init__.py
+-rw-rw-rw-  2.0 fat     1178 b- defN 23-Jul-18 11:31 qlib/model/interpret/base.py
+-rw-rw-rw-  2.0 fat      190 b- defN 23-Jul-18 11:31 qlib/model/meta/__init__.py
+-rw-rw-rw-  2.0 fat     2856 b- defN 23-Jul-18 11:31 qlib/model/meta/dataset.py
+-rw-rw-rw-  2.0 fat     2430 b- defN 23-Jul-18 11:31 qlib/model/meta/model.py
+-rw-rw-rw-  2.0 fat     1708 b- defN 23-Jul-18 11:31 qlib/model/meta/task.py
+-rw-rw-rw-  2.0 fat      351 b- defN 23-Jul-18 11:31 qlib/model/riskmodel/__init__.py
+-rw-rw-rw-  2.0 fat     5182 b- defN 23-Jul-18 11:31 qlib/model/riskmodel/base.py
+-rw-rw-rw-  2.0 fat     3288 b- defN 23-Jul-18 11:31 qlib/model/riskmodel/poet.py
+-rw-rw-rw-  2.0 fat    10702 b- defN 23-Jul-18 11:31 qlib/model/riskmodel/shrink.py
+-rw-rw-rw-  2.0 fat     3895 b- defN 23-Jul-18 11:31 qlib/model/riskmodel/structured.py
+-rw-rw-rw-  2.0 fat      347 b- defN 23-Jul-18 11:31 qlib/rl/__init__.py
+-rw-rw-rw-  2.0 fat     1165 b- defN 23-Jul-18 11:31 qlib/rl/aux_info.py
+-rw-rw-rw-  2.0 fat     5376 b- defN 23-Jul-18 11:31 qlib/rl/interpreter.py
+-rw-rw-rw-  2.0 fat     2788 b- defN 23-Jul-18 11:31 qlib/rl/reward.py
+-rw-rw-rw-  2.0 fat      351 b- defN 23-Jul-18 11:31 qlib/rl/seed.py
+-rw-rw-rw-  2.0 fat     3106 b- defN 23-Jul-18 11:31 qlib/rl/simulator.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-Jul-18 11:31 qlib/rl/contrib/__init__.py
+-rw-rw-rw-  2.0 fat    13614 b- defN 23-Jul-18 11:31 qlib/rl/contrib/backtest.py
+-rw-rw-rw-  2.0 fat     3494 b- defN 23-Jul-18 11:31 qlib/rl/contrib/naive_config_parser.py
+-rw-rw-rw-  2.0 fat    10373 b- defN 23-Jul-18 11:31 qlib/rl/contrib/train_onpolicy.py
+-rw-rw-rw-  2.0 fat      865 b- defN 23-Jul-18 11:31 qlib/rl/contrib/utils.py
+-rw-rw-rw-  2.0 fat      253 b- defN 23-Jul-18 11:31 qlib/rl/data/__init__.py
+-rw-rw-rw-  2.0 fat     1819 b- defN 23-Jul-18 11:31 qlib/rl/data/base.py
+-rw-rw-rw-  2.0 fat     3193 b- defN 23-Jul-18 11:31 qlib/rl/data/integration.py
+-rw-rw-rw-  2.0 fat     7645 b- defN 23-Jul-18 11:31 qlib/rl/data/native.py
+-rw-rw-rw-  2.0 fat    10840 b- defN 23-Jul-18 11:31 qlib/rl/data/pickle_styled.py
+-rw-rw-rw-  2.0 fat     1046 b- defN 23-Jul-18 11:31 qlib/rl/order_execution/__init__.py
+-rw-rw-rw-  2.0 fat     9926 b- defN 23-Jul-18 11:31 qlib/rl/order_execution/interpreter.py
+-rw-rw-rw-  2.0 fat     4970 b- defN 23-Jul-18 11:31 qlib/rl/order_execution/network.py
+-rw-rw-rw-  2.0 fat     7245 b- defN 23-Jul-18 11:31 qlib/rl/order_execution/policy.py
+-rw-rw-rw-  2.0 fat     3648 b- defN 23-Jul-18 11:31 qlib/rl/order_execution/reward.py
+-rw-rw-rw-  2.0 fat     4906 b- defN 23-Jul-18 11:31 qlib/rl/order_execution/simulator_qlib.py
+-rw-rw-rw-  2.0 fat    15104 b- defN 23-Jul-18 11:31 qlib/rl/order_execution/simulator_simple.py
+-rw-rw-rw-  2.0 fat     3791 b- defN 23-Jul-18 11:31 qlib/rl/order_execution/state.py
+-rw-rw-rw-  2.0 fat    21728 b- defN 23-Jul-18 11:31 qlib/rl/order_execution/strategy.py
+-rw-rw-rw-  2.0 fat     1683 b- defN 23-Jul-18 11:31 qlib/rl/order_execution/utils.py
+-rw-rw-rw-  2.0 fat      159 b- defN 23-Jul-18 11:31 qlib/rl/strategy/__init__.py
+-rw-rw-rw-  2.0 fat     1082 b- defN 23-Jul-18 11:31 qlib/rl/strategy/single_order.py
+-rw-rw-rw-  2.0 fat      483 b- defN 23-Jul-18 11:31 qlib/rl/trainer/__init__.py
+-rw-rw-rw-  2.0 fat     3768 b- defN 23-Jul-18 11:31 qlib/rl/trainer/api.py
+-rw-rw-rw-  2.0 fat    11867 b- defN 23-Jul-18 11:31 qlib/rl/trainer/callbacks.py
+-rw-rw-rw-  2.0 fat    13831 b- defN 23-Jul-18 11:31 qlib/rl/trainer/trainer.py
+-rw-rw-rw-  2.0 fat    10103 b- defN 23-Jul-18 11:31 qlib/rl/trainer/vessel.py
+-rw-rw-rw-  2.0 fat      548 b- defN 23-Jul-18 11:31 qlib/rl/utils/__init__.py
+-rw-rw-rw-  2.0 fat     6785 b- defN 23-Jul-18 11:31 qlib/rl/utils/data_queue.py
+-rw-rw-rw-  2.0 fat    10094 b- defN 23-Jul-18 11:31 qlib/rl/utils/env_wrapper.py
+-rw-rw-rw-  2.0 fat    13736 b- defN 23-Jul-18 11:31 qlib/rl/utils/finite_env.py
+-rw-rw-rw-  2.0 fat    19064 b- defN 23-Jul-18 11:31 qlib/rl/utils/log.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-Jul-18 11:31 qlib/run/__init__.py
+-rw-rw-rw-  2.0 fat      185 b- defN 23-Jul-18 11:31 qlib/run/get_data.py
+-rw-rw-rw-  2.0 fat       75 b- defN 23-Jul-18 11:31 qlib/strategy/__init__.py
+-rw-rw-rw-  2.0 fat    11451 b- defN 23-Jul-18 11:31 qlib/strategy/base.py
+-rw-rw-rw-  2.0 fat    12507 b- defN 23-Jul-18 11:31 qlib/tests/__init__.py
+-rw-rw-rw-  2.0 fat     5001 b- defN 23-Jul-18 11:31 qlib/tests/config.py
+-rw-rw-rw-  2.0 fat     8193 b- defN 23-Jul-18 11:31 qlib/tests/data.py
+-rw-rw-rw-  2.0 fat    30095 b- defN 23-Jul-18 11:31 qlib/utils/__init__.py
+-rw-rw-rw-  2.0 fat     3190 b- defN 23-Jul-18 11:31 qlib/utils/data.py
+-rw-rw-rw-  2.0 fat      457 b- defN 23-Jul-18 11:31 qlib/utils/exceptions.py
+-rw-rw-rw-  2.0 fat     5928 b- defN 23-Jul-18 11:31 qlib/utils/file.py
+-rw-rw-rw-  2.0 fat    22535 b- defN 23-Jul-18 11:31 qlib/utils/index_data.py
+-rw-rw-rw-  2.0 fat     7669 b- defN 23-Jul-18 11:31 qlib/utils/mod.py
+-rw-rw-rw-  2.0 fat     3435 b- defN 23-Jul-18 11:31 qlib/utils/objm.py
+-rw-rw-rw-  2.0 fat     9433 b- defN 23-Jul-18 11:31 qlib/utils/paral.py
+-rw-rw-rw-  2.0 fat     9626 b- defN 23-Jul-18 11:31 qlib/utils/resam.py
+-rw-rw-rw-  2.0 fat     6263 b- defN 23-Jul-18 11:31 qlib/utils/serial.py
+-rw-rw-rw-  2.0 fat    12155 b- defN 23-Jul-18 11:31 qlib/utils/time.py
+-rw-rw-rw-  2.0 fat    25250 b- defN 23-Jul-18 11:31 qlib/workflow/__init__.py
+-rw-rw-rw-  2.0 fat     3809 b- defN 23-Jul-18 11:31 qlib/workflow/cli.py
+-rw-rw-rw-  2.0 fat    15192 b- defN 23-Jul-18 11:31 qlib/workflow/exp.py
+-rw-rw-rw-  2.0 fat    18008 b- defN 23-Jul-18 11:31 qlib/workflow/expm.py
+-rw-rw-rw-  2.0 fat    22414 b- defN 23-Jul-18 11:31 qlib/workflow/record_temp.py
+-rw-rw-rw-  2.0 fat    18257 b- defN 23-Jul-18 11:31 qlib/workflow/recorder.py
+-rw-rw-rw-  2.0 fat     1663 b- defN 23-Jul-18 11:31 qlib/workflow/utils.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-Jul-18 11:31 qlib/workflow/online/__init__.py
+-rw-rw-rw-  2.0 fat    17748 b- defN 23-Jul-18 11:31 qlib/workflow/online/manager.py
+-rw-rw-rw-  2.0 fat     8620 b- defN 23-Jul-18 11:31 qlib/workflow/online/strategy.py
+-rw-rw-rw-  2.0 fat    10884 b- defN 23-Jul-18 11:31 qlib/workflow/online/update.py
+-rw-rw-rw-  2.0 fat     6662 b- defN 23-Jul-18 11:31 qlib/workflow/online/utils.py
+-rw-rw-rw-  2.0 fat      548 b- defN 23-Jul-18 11:31 qlib/workflow/task/__init__.py
+-rw-rw-rw-  2.0 fat    10315 b- defN 23-Jul-18 11:31 qlib/workflow/task/collect.py
+-rw-rw-rw-  2.0 fat    12117 b- defN 23-Jul-18 11:31 qlib/workflow/task/gen.py
+-rw-rw-rw-  2.0 fat    18968 b- defN 23-Jul-18 11:31 qlib/workflow/task/manage.py
+-rw-rw-rw-  2.0 fat    10322 b- defN 23-Jul-18 11:31 qlib/workflow/task/utils.py
+-rw-rw-rw-  2.0 fat     1162 b- defN 23-Jul-18 11:33 pyqlib-0.9.3.dist-info/LICENSE
+-rw-rw-rw-  2.0 fat    41184 b- defN 23-Jul-18 11:33 pyqlib-0.9.3.dist-info/METADATA
+-rw-rw-rw-  2.0 fat      100 b- defN 23-Jul-18 11:33 pyqlib-0.9.3.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat       48 b- defN 23-Jul-18 11:32 pyqlib-0.9.3.dist-info/entry_points.txt
+-rw-rw-rw-  2.0 fat        5 b- defN 23-Jul-18 11:32 pyqlib-0.9.3.dist-info/top_level.txt
+-rw-rw-r--  2.0 fat    20594 b- defN 23-Jul-18 11:33 pyqlib-0.9.3.dist-info/RECORD
+238 files, 2374279 bytes uncompressed, 658560 bytes compressed:  72.3%
```

## zipnote {}

```diff
@@ -282,14 +282,26 @@
 
 Filename: qlib/contrib/report/data/ana.py
 Comment: 
 
 Filename: qlib/contrib/report/data/base.py
 Comment: 
 
+Filename: qlib/contrib/rolling/__init__.py
+Comment: 
+
+Filename: qlib/contrib/rolling/__main__.py
+Comment: 
+
+Filename: qlib/contrib/rolling/base.py
+Comment: 
+
+Filename: qlib/contrib/rolling/ddgda.py
+Comment: 
+
 Filename: qlib/contrib/strategy/__init__.py
 Comment: 
 
 Filename: qlib/contrib/strategy/cost_control.py
 Comment: 
 
 Filename: qlib/contrib/strategy/order_generator.py
@@ -363,21 +375,21 @@
 
 Filename: qlib/data/pit.py
 Comment: 
 
 Filename: qlib/data/_libs/__init__.py
 Comment: 
 
-Filename: qlib/data/_libs/expanding.cpython-38-darwin.so
+Filename: qlib/data/_libs/expanding.cp38-win_amd64.pyd
 Comment: 
 
 Filename: qlib/data/_libs/expanding.pyx
 Comment: 
 
-Filename: qlib/data/_libs/rolling.cpython-38-darwin.so
+Filename: qlib/data/_libs/rolling.cp38-win_amd64.pyd
 Comment: 
 
 Filename: qlib/data/_libs/rolling.pyx
 Comment: 
 
 Filename: qlib/data/dataset/__init__.py
 Comment: 
@@ -609,14 +621,17 @@
 
 Filename: qlib/utils/file.py
 Comment: 
 
 Filename: qlib/utils/index_data.py
 Comment: 
 
+Filename: qlib/utils/mod.py
+Comment: 
+
 Filename: qlib/utils/objm.py
 Comment: 
 
 Filename: qlib/utils/paral.py
 Comment: 
 
 Filename: qlib/utils/resam.py
@@ -675,26 +690,26 @@
 
 Filename: qlib/workflow/task/manage.py
 Comment: 
 
 Filename: qlib/workflow/task/utils.py
 Comment: 
 
-Filename: pyqlib-0.9.2.1.dist-info/LICENSE
+Filename: pyqlib-0.9.3.dist-info/LICENSE
 Comment: 
 
-Filename: pyqlib-0.9.2.1.dist-info/METADATA
+Filename: pyqlib-0.9.3.dist-info/METADATA
 Comment: 
 
-Filename: pyqlib-0.9.2.1.dist-info/WHEEL
+Filename: pyqlib-0.9.3.dist-info/WHEEL
 Comment: 
 
-Filename: pyqlib-0.9.2.1.dist-info/entry_points.txt
+Filename: pyqlib-0.9.3.dist-info/entry_points.txt
 Comment: 
 
-Filename: pyqlib-0.9.2.1.dist-info/top_level.txt
+Filename: pyqlib-0.9.3.dist-info/top_level.txt
 Comment: 
 
-Filename: pyqlib-0.9.2.1.dist-info/RECORD
+Filename: pyqlib-0.9.3.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## qlib/__init__.py

```diff
@@ -1,297 +1,296 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-from pathlib import Path
-
-__version__ = "0.9.2.1"
-__version__bak = __version__  # This version is backup for QlibConfig.reset_qlib_version
-import os
-from typing import Union
-import yaml
-import logging
-import platform
-import subprocess
-from .log import get_module_logger
-
-
-# init qlib
-def init(default_conf="client", **kwargs):
-    """
-
-    Parameters
-    ----------
-    default_conf: str
-        the default value is client. Accepted values: client/server.
-    **kwargs :
-        clear_mem_cache: str
-            the default value is True;
-            Will the memory cache be clear.
-            It is often used to improve performance when init will be called for multiple times
-        skip_if_reg: bool: str
-            the default value is True;
-            When using the recorder, skip_if_reg can set to True to avoid loss of recorder.
-
-    """
-    from .config import C  # pylint: disable=C0415
-    from .data.cache import H  # pylint: disable=C0415
-
-    logger = get_module_logger("Initialization")
-
-    skip_if_reg = kwargs.pop("skip_if_reg", False)
-    if skip_if_reg and C.registered:
-        # if we reinitialize Qlib during running an experiment `R.start`.
-        # it will result in loss of the recorder
-        logger.warning("Skip initialization because `skip_if_reg is True`")
-        return
-
-    clear_mem_cache = kwargs.pop("clear_mem_cache", True)
-    if clear_mem_cache:
-        H.clear()
-    C.set(default_conf, **kwargs)
-    get_module_logger.setLevel(C.logging_level)
-
-    # mount nfs
-    for _freq, provider_uri in C.provider_uri.items():
-        mount_path = C["mount_path"][_freq]
-        # check path if server/local
-        uri_type = C.dpm.get_uri_type(provider_uri)
-        if uri_type == C.LOCAL_URI:
-            if not Path(provider_uri).exists():
-                if C["auto_mount"]:
-                    logger.error(
-                        f"Invalid provider uri: {provider_uri}, please check if a valid provider uri has been set. This path does not exist."
-                    )
-                else:
-                    logger.warning(f"auto_path is False, please make sure {mount_path} is mounted")
-        elif uri_type == C.NFS_URI:
-            _mount_nfs_uri(provider_uri, C.dpm.get_data_uri(_freq), C["auto_mount"])
-        else:
-            raise NotImplementedError(f"This type of URI is not supported")
-
-    C.register()
-
-    if "flask_server" in C:
-        logger.info(f"flask_server={C['flask_server']}, flask_port={C['flask_port']}")
-    logger.info("qlib successfully initialized based on %s settings." % default_conf)
-    data_path = {_freq: C.dpm.get_data_uri(_freq) for _freq in C.dpm.provider_uri.keys()}
-    logger.info(f"data_path={data_path}")
-
-
-def _mount_nfs_uri(provider_uri, mount_path, auto_mount: bool = False):
-
-    LOG = get_module_logger("mount nfs", level=logging.INFO)
-    if mount_path is None:
-        raise ValueError(f"Invalid mount path: {mount_path}!")
-    # FIXME: the C["provider_uri"] is modified in this function
-    # If it is not modified, we can pass only  provider_uri or mount_path instead of C
-    mount_command = "sudo mount.nfs %s %s" % (provider_uri, mount_path)
-    # If the provider uri looks like this 172.23.233.89//data/csdesign'
-    # It will be a nfs path. The client provider will be used
-    if not auto_mount:  # pylint: disable=R1702
-        if not Path(mount_path).exists():
-            raise FileNotFoundError(
-                f"Invalid mount path: {mount_path}! Please mount manually: {mount_command} or Set init parameter `auto_mount=True`"
-            )
-    else:
-        # Judging system type
-        sys_type = platform.system()
-        if "windows" in sys_type.lower():
-            # system: window
-            exec_result = os.popen(f"mount -o anon {provider_uri} {mount_path}")
-            result = exec_result.read()
-            if "85" in result:
-                LOG.warning(f"{provider_uri} on Windows:{mount_path} is already mounted")
-            elif "53" in result:
-                raise OSError("not find network path")
-            elif "error" in result or "错误" in result:
-                raise OSError("Invalid mount path")
-            elif provider_uri in result:
-                LOG.info("window success mount..")
-            else:
-                raise OSError(f"unknown error: {result}")
-
-        else:
-            # system: linux/Unix/Mac
-            # check mount
-            _remote_uri = provider_uri[:-1] if provider_uri.endswith("/") else provider_uri
-            # `mount a /b/c` is different from `mount a /b/c/`. So we convert it into string to make sure handling it accurately
-            mount_path = str(mount_path)
-            _mount_path = mount_path[:-1] if mount_path.endswith("/") else mount_path
-            _check_level_num = 2
-            _is_mount = False
-            while _check_level_num:
-                with subprocess.Popen(
-                    'mount | grep "{}"'.format(_remote_uri),
-                    shell=True,
-                    stdout=subprocess.PIPE,
-                    stderr=subprocess.STDOUT,
-                ) as shell_r:
-                    _command_log = shell_r.stdout.readlines()
-                if len(_command_log) > 0:
-                    for _c in _command_log:
-                        _temp_mount = _c.decode("utf-8").split(" ")[2]
-                        _temp_mount = _temp_mount[:-1] if _temp_mount.endswith("/") else _temp_mount
-                        if _temp_mount == _mount_path:
-                            _is_mount = True
-                            break
-                if _is_mount:
-                    break
-                _remote_uri = "/".join(_remote_uri.split("/")[:-1])
-                _mount_path = "/".join(_mount_path.split("/")[:-1])
-                _check_level_num -= 1
-
-            if not _is_mount:
-                try:
-                    Path(mount_path).mkdir(parents=True, exist_ok=True)
-                except Exception as e:
-                    raise OSError(
-                        f"Failed to create directory {mount_path}, please create {mount_path} manually!"
-                    ) from e
-
-                # check nfs-common
-                command_res = os.popen("dpkg -l | grep nfs-common")
-                command_res = command_res.readlines()
-                if not command_res:
-                    raise OSError("nfs-common is not found, please install it by execute: sudo apt install nfs-common")
-                # manually mount
-                command_status = os.system(mount_command)
-                if command_status == 256:
-                    raise OSError(
-                        f"mount {provider_uri} on {mount_path} error! Needs SUDO! Please mount manually: {mount_command}"
-                    )
-                elif command_status == 32512:
-                    # LOG.error("Command error")
-                    raise OSError(f"mount {provider_uri} on {mount_path} error! Command error")
-                elif command_status == 0:
-                    LOG.info("Mount finished")
-            else:
-                LOG.warning(f"{_remote_uri} on {_mount_path} is already mounted")
-
-
-def init_from_yaml_conf(conf_path, **kwargs):
-    """init_from_yaml_conf
-
-    :param conf_path: A path to the qlib config in yml format
-    """
-
-    if conf_path is None:
-        config = {}
-    else:
-        with open(conf_path) as f:
-            config = yaml.safe_load(f)
-    config.update(kwargs)
-    default_conf = config.pop("default_conf", "client")
-    init(default_conf, **config)
-
-
-def get_project_path(config_name="config.yaml", cur_path: Union[Path, str, None] = None) -> Path:
-    """
-    If users are building a project follow the following pattern.
-    - Qlib is a sub folder in project path
-    - There is a file named `config.yaml` in qlib.
-
-    For example:
-        If your project file system structure follows such a pattern
-
-            <project_path>/
-              - config.yaml
-              - ...some folders...
-                - qlib/
-
-        This folder will return <project_path>
-
-        NOTE: link is not supported here.
-
-
-    This method is often used when
-    - user want to use a relative config path instead of hard-coding qlib config path in code
-
-    Raises
-    ------
-    FileNotFoundError:
-        If project path is not found
-    """
-    if cur_path is None:
-        cur_path = Path(__file__).absolute().resolve()
-    cur_path = Path(cur_path)
-    while True:
-        if (cur_path / config_name).exists():
-            return cur_path
-        if cur_path == cur_path.parent:
-            raise FileNotFoundError("We can't find the project path")
-        cur_path = cur_path.parent
-
-
-def auto_init(**kwargs):
-    """
-    This function will init qlib automatically with following priority
-    - Find the project configuration and init qlib
-        - The parsing process will be affected by the `conf_type` of the configuration file
-    - Init qlib with default config
-    - Skip initialization if already initialized
-
-    :**kwargs: it may contain following parameters
-                cur_path: the start path to find the project path
-
-    Here are two examples of the configuration
-
-    Example 1)
-    If you want to create a new project-specific config based on a shared configure, you can use  `conf_type: ref`
-
-    .. code-block:: yaml
-
-        conf_type: ref
-        qlib_cfg: '<shared_yaml_config_path>'    # this could be null reference no config from other files
-        # following configs in `qlib_cfg_update` is project=specific
-        qlib_cfg_update:
-            exp_manager:
-                class: "MLflowExpManager"
-                module_path: "qlib.workflow.expm"
-                kwargs:
-                    uri: "file://<your mlflow experiment path>"
-                    default_exp_name: "Experiment"
-
-    Example 2)
-    If you want to create simple a standalone config, you can use following config(a.k.a. `conf_type: origin`)
-
-    .. code-block:: python
-
-        exp_manager:
-            class: "MLflowExpManager"
-            module_path: "qlib.workflow.expm"
-            kwargs:
-                uri: "file://<your mlflow experiment path>"
-                default_exp_name: "Experiment"
-
-    """
-    kwargs["skip_if_reg"] = kwargs.get("skip_if_reg", True)
-
-    try:
-        pp = get_project_path(cur_path=kwargs.pop("cur_path", None))
-    except FileNotFoundError:
-        init(**kwargs)
-    else:
-        logger = get_module_logger("Initialization")
-        conf_pp = pp / "config.yaml"
-        with conf_pp.open() as f:
-            conf = yaml.safe_load(f)
-
-        conf_type = conf.get("conf_type", "origin")
-        if conf_type == "origin":
-            # The type of config is just like original qlib config
-            init_from_yaml_conf(conf_pp, **kwargs)
-        elif conf_type == "ref":
-            # This config type will be more convenient in following scenario
-            # - There is a shared configure file, and you don't want to edit it inplace.
-            # - The shared configure may be updated later, and you don't want to copy it.
-            # - You have some customized config.
-            qlib_conf_path = conf.get("qlib_cfg", None)
-
-            # merge the arguments
-            qlib_conf_update = conf.get("qlib_cfg_update", {})
-            for k, v in kwargs.items():
-                if k in qlib_conf_update:
-                    logger.warning(f"`qlib_conf_update` from conf_pp is override by `kwargs` on key '{k}'")
-            qlib_conf_update.update(kwargs)
-
-            init_from_yaml_conf(qlib_conf_path, **qlib_conf_update)
-        logger.info(f"Auto load project config: {conf_pp}")
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+from pathlib import Path
+
+__version__ = "0.9.3"
+__version__bak = __version__  # This version is backup for QlibConfig.reset_qlib_version
+import os
+from typing import Union
+import yaml
+import logging
+import platform
+import subprocess
+from .log import get_module_logger
+
+
+# init qlib
+def init(default_conf="client", **kwargs):
+    """
+
+    Parameters
+    ----------
+    default_conf: str
+        the default value is client. Accepted values: client/server.
+    **kwargs :
+        clear_mem_cache: str
+            the default value is True;
+            Will the memory cache be clear.
+            It is often used to improve performance when init will be called for multiple times
+        skip_if_reg: bool: str
+            the default value is True;
+            When using the recorder, skip_if_reg can set to True to avoid loss of recorder.
+
+    """
+    from .config import C  # pylint: disable=C0415
+    from .data.cache import H  # pylint: disable=C0415
+
+    logger = get_module_logger("Initialization")
+
+    skip_if_reg = kwargs.pop("skip_if_reg", False)
+    if skip_if_reg and C.registered:
+        # if we reinitialize Qlib during running an experiment `R.start`.
+        # it will result in loss of the recorder
+        logger.warning("Skip initialization because `skip_if_reg is True`")
+        return
+
+    clear_mem_cache = kwargs.pop("clear_mem_cache", True)
+    if clear_mem_cache:
+        H.clear()
+    C.set(default_conf, **kwargs)
+    get_module_logger.setLevel(C.logging_level)
+
+    # mount nfs
+    for _freq, provider_uri in C.provider_uri.items():
+        mount_path = C["mount_path"][_freq]
+        # check path if server/local
+        uri_type = C.dpm.get_uri_type(provider_uri)
+        if uri_type == C.LOCAL_URI:
+            if not Path(provider_uri).exists():
+                if C["auto_mount"]:
+                    logger.error(
+                        f"Invalid provider uri: {provider_uri}, please check if a valid provider uri has been set. This path does not exist."
+                    )
+                else:
+                    logger.warning(f"auto_path is False, please make sure {mount_path} is mounted")
+        elif uri_type == C.NFS_URI:
+            _mount_nfs_uri(provider_uri, C.dpm.get_data_uri(_freq), C["auto_mount"])
+        else:
+            raise NotImplementedError(f"This type of URI is not supported")
+
+    C.register()
+
+    if "flask_server" in C:
+        logger.info(f"flask_server={C['flask_server']}, flask_port={C['flask_port']}")
+    logger.info("qlib successfully initialized based on %s settings." % default_conf)
+    data_path = {_freq: C.dpm.get_data_uri(_freq) for _freq in C.dpm.provider_uri.keys()}
+    logger.info(f"data_path={data_path}")
+
+
+def _mount_nfs_uri(provider_uri, mount_path, auto_mount: bool = False):
+    LOG = get_module_logger("mount nfs", level=logging.INFO)
+    if mount_path is None:
+        raise ValueError(f"Invalid mount path: {mount_path}!")
+    # FIXME: the C["provider_uri"] is modified in this function
+    # If it is not modified, we can pass only  provider_uri or mount_path instead of C
+    mount_command = "sudo mount.nfs %s %s" % (provider_uri, mount_path)
+    # If the provider uri looks like this 172.23.233.89//data/csdesign'
+    # It will be a nfs path. The client provider will be used
+    if not auto_mount:  # pylint: disable=R1702
+        if not Path(mount_path).exists():
+            raise FileNotFoundError(
+                f"Invalid mount path: {mount_path}! Please mount manually: {mount_command} or Set init parameter `auto_mount=True`"
+            )
+    else:
+        # Judging system type
+        sys_type = platform.system()
+        if "windows" in sys_type.lower():
+            # system: window
+            exec_result = os.popen(f"mount -o anon {provider_uri} {mount_path}")
+            result = exec_result.read()
+            if "85" in result:
+                LOG.warning(f"{provider_uri} on Windows:{mount_path} is already mounted")
+            elif "53" in result:
+                raise OSError("not find network path")
+            elif "error" in result or "错误" in result:
+                raise OSError("Invalid mount path")
+            elif provider_uri in result:
+                LOG.info("window success mount..")
+            else:
+                raise OSError(f"unknown error: {result}")
+
+        else:
+            # system: linux/Unix/Mac
+            # check mount
+            _remote_uri = provider_uri[:-1] if provider_uri.endswith("/") else provider_uri
+            # `mount a /b/c` is different from `mount a /b/c/`. So we convert it into string to make sure handling it accurately
+            mount_path = str(mount_path)
+            _mount_path = mount_path[:-1] if mount_path.endswith("/") else mount_path
+            _check_level_num = 2
+            _is_mount = False
+            while _check_level_num:
+                with subprocess.Popen(
+                    'mount | grep "{}"'.format(_remote_uri),
+                    shell=True,
+                    stdout=subprocess.PIPE,
+                    stderr=subprocess.STDOUT,
+                ) as shell_r:
+                    _command_log = shell_r.stdout.readlines()
+                if len(_command_log) > 0:
+                    for _c in _command_log:
+                        _temp_mount = _c.decode("utf-8").split(" ")[2]
+                        _temp_mount = _temp_mount[:-1] if _temp_mount.endswith("/") else _temp_mount
+                        if _temp_mount == _mount_path:
+                            _is_mount = True
+                            break
+                if _is_mount:
+                    break
+                _remote_uri = "/".join(_remote_uri.split("/")[:-1])
+                _mount_path = "/".join(_mount_path.split("/")[:-1])
+                _check_level_num -= 1
+
+            if not _is_mount:
+                try:
+                    Path(mount_path).mkdir(parents=True, exist_ok=True)
+                except Exception as e:
+                    raise OSError(
+                        f"Failed to create directory {mount_path}, please create {mount_path} manually!"
+                    ) from e
+
+                # check nfs-common
+                command_res = os.popen("dpkg -l | grep nfs-common")
+                command_res = command_res.readlines()
+                if not command_res:
+                    raise OSError("nfs-common is not found, please install it by execute: sudo apt install nfs-common")
+                # manually mount
+                command_status = os.system(mount_command)
+                if command_status == 256:
+                    raise OSError(
+                        f"mount {provider_uri} on {mount_path} error! Needs SUDO! Please mount manually: {mount_command}"
+                    )
+                elif command_status == 32512:
+                    # LOG.error("Command error")
+                    raise OSError(f"mount {provider_uri} on {mount_path} error! Command error")
+                elif command_status == 0:
+                    LOG.info("Mount finished")
+            else:
+                LOG.warning(f"{_remote_uri} on {_mount_path} is already mounted")
+
+
+def init_from_yaml_conf(conf_path, **kwargs):
+    """init_from_yaml_conf
+
+    :param conf_path: A path to the qlib config in yml format
+    """
+
+    if conf_path is None:
+        config = {}
+    else:
+        with open(conf_path) as f:
+            config = yaml.safe_load(f)
+    config.update(kwargs)
+    default_conf = config.pop("default_conf", "client")
+    init(default_conf, **config)
+
+
+def get_project_path(config_name="config.yaml", cur_path: Union[Path, str, None] = None) -> Path:
+    """
+    If users are building a project follow the following pattern.
+    - Qlib is a sub folder in project path
+    - There is a file named `config.yaml` in qlib.
+
+    For example:
+        If your project file system structure follows such a pattern
+
+            <project_path>/
+              - config.yaml
+              - ...some folders...
+                - qlib/
+
+        This folder will return <project_path>
+
+        NOTE: link is not supported here.
+
+
+    This method is often used when
+    - user want to use a relative config path instead of hard-coding qlib config path in code
+
+    Raises
+    ------
+    FileNotFoundError:
+        If project path is not found
+    """
+    if cur_path is None:
+        cur_path = Path(__file__).absolute().resolve()
+    cur_path = Path(cur_path)
+    while True:
+        if (cur_path / config_name).exists():
+            return cur_path
+        if cur_path == cur_path.parent:
+            raise FileNotFoundError("We can't find the project path")
+        cur_path = cur_path.parent
+
+
+def auto_init(**kwargs):
+    """
+    This function will init qlib automatically with following priority
+    - Find the project configuration and init qlib
+        - The parsing process will be affected by the `conf_type` of the configuration file
+    - Init qlib with default config
+    - Skip initialization if already initialized
+
+    :**kwargs: it may contain following parameters
+                cur_path: the start path to find the project path
+
+    Here are two examples of the configuration
+
+    Example 1)
+    If you want to create a new project-specific config based on a shared configure, you can use  `conf_type: ref`
+
+    .. code-block:: yaml
+
+        conf_type: ref
+        qlib_cfg: '<shared_yaml_config_path>'    # this could be null reference no config from other files
+        # following configs in `qlib_cfg_update` is project=specific
+        qlib_cfg_update:
+            exp_manager:
+                class: "MLflowExpManager"
+                module_path: "qlib.workflow.expm"
+                kwargs:
+                    uri: "file://<your mlflow experiment path>"
+                    default_exp_name: "Experiment"
+
+    Example 2)
+    If you want to create simple a standalone config, you can use following config(a.k.a. `conf_type: origin`)
+
+    .. code-block:: python
+
+        exp_manager:
+            class: "MLflowExpManager"
+            module_path: "qlib.workflow.expm"
+            kwargs:
+                uri: "file://<your mlflow experiment path>"
+                default_exp_name: "Experiment"
+
+    """
+    kwargs["skip_if_reg"] = kwargs.get("skip_if_reg", True)
+
+    try:
+        pp = get_project_path(cur_path=kwargs.pop("cur_path", None))
+    except FileNotFoundError:
+        init(**kwargs)
+    else:
+        logger = get_module_logger("Initialization")
+        conf_pp = pp / "config.yaml"
+        with conf_pp.open() as f:
+            conf = yaml.safe_load(f)
+
+        conf_type = conf.get("conf_type", "origin")
+        if conf_type == "origin":
+            # The type of config is just like original qlib config
+            init_from_yaml_conf(conf_pp, **kwargs)
+        elif conf_type == "ref":
+            # This config type will be more convenient in following scenario
+            # - There is a shared configure file, and you don't want to edit it inplace.
+            # - The shared configure may be updated later, and you don't want to copy it.
+            # - You have some customized config.
+            qlib_conf_path = conf.get("qlib_cfg", None)
+
+            # merge the arguments
+            qlib_conf_update = conf.get("qlib_cfg_update", {})
+            for k, v in kwargs.items():
+                if k in qlib_conf_update:
+                    logger.warning(f"`qlib_conf_update` from conf_pp is override by `kwargs` on key '{k}'")
+            qlib_conf_update.update(kwargs)
+
+            init_from_yaml_conf(qlib_conf_path, **qlib_conf_update)
+        logger.info(f"Auto load project config: {conf_pp}")
```

## qlib/config.py

```diff
@@ -1,491 +1,490 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-"""
-About the configs
-=================
-
-The config will be based on _default_config.
-Two modes are supported
-- client
-- server
-
-"""
-from __future__ import annotations
-
-import os
-import re
-import copy
-import logging
-import platform
-import multiprocessing
-from pathlib import Path
-from typing import Callable, Optional, Union
-from typing import TYPE_CHECKING
-
-from qlib.constant import REG_CN, REG_US, REG_TW
-
-if TYPE_CHECKING:
-    from qlib.utils.time import Freq
-
-
-class Config:
-    def __init__(self, default_conf):
-        self.__dict__["_default_config"] = copy.deepcopy(default_conf)  # avoiding conflicts with __getattr__
-        self.reset()
-
-    def __getitem__(self, key):
-        return self.__dict__["_config"][key]
-
-    def __getattr__(self, attr):
-        if attr in self.__dict__["_config"]:
-            return self.__dict__["_config"][attr]
-
-        raise AttributeError(f"No such `{attr}` in self._config")
-
-    def get(self, key, default=None):
-        return self.__dict__["_config"].get(key, default)
-
-    def __setitem__(self, key, value):
-        self.__dict__["_config"][key] = value
-
-    def __setattr__(self, attr, value):
-        self.__dict__["_config"][attr] = value
-
-    def __contains__(self, item):
-        return item in self.__dict__["_config"]
-
-    def __getstate__(self):
-        return self.__dict__
-
-    def __setstate__(self, state):
-        self.__dict__.update(state)
-
-    def __str__(self):
-        return str(self.__dict__["_config"])
-
-    def __repr__(self):
-        return str(self.__dict__["_config"])
-
-    def reset(self):
-        self.__dict__["_config"] = copy.deepcopy(self._default_config)
-
-    def update(self, *args, **kwargs):
-        self.__dict__["_config"].update(*args, **kwargs)
-
-    def set_conf_from_C(self, config_c):
-        self.update(**config_c.__dict__["_config"])
-
-    @staticmethod
-    def register_from_C(config, skip_register=True):
-        from .utils import set_log_with_config  # pylint: disable=C0415
-
-        if C.registered and skip_register:
-            return
-
-        C.set_conf_from_C(config)
-        if C.logging_config:
-            set_log_with_config(C.logging_config)
-        C.register()
-
-
-# pickle.dump protocol version: https://docs.python.org/3/library/pickle.html#data-stream-format
-PROTOCOL_VERSION = 4
-
-NUM_USABLE_CPU = max(multiprocessing.cpu_count() - 2, 1)
-
-DISK_DATASET_CACHE = "DiskDatasetCache"
-SIMPLE_DATASET_CACHE = "SimpleDatasetCache"
-DISK_EXPRESSION_CACHE = "DiskExpressionCache"
-
-DEPENDENCY_REDIS_CACHE = (DISK_DATASET_CACHE, DISK_EXPRESSION_CACHE)
-
-_default_config = {
-    # data provider config
-    "calendar_provider": "LocalCalendarProvider",
-    "instrument_provider": "LocalInstrumentProvider",
-    "feature_provider": "LocalFeatureProvider",
-    "pit_provider": "LocalPITProvider",
-    "expression_provider": "LocalExpressionProvider",
-    "dataset_provider": "LocalDatasetProvider",
-    "provider": "LocalProvider",
-    # config it in qlib.init()
-    # "provider_uri" str or dict:
-    #   # str
-    #   "~/.qlib/stock_data/cn_data"
-    #   # dict
-    #   {"day": "~/.qlib/stock_data/cn_data", "1min": "~/.qlib/stock_data/cn_data_1min"}
-    # NOTE: provider_uri priority:
-    #   1. backend_config: backend_obj["kwargs"]["provider_uri"]
-    #   2. backend_config: backend_obj["kwargs"]["provider_uri_map"]
-    #   3. qlib.init: provider_uri
-    "provider_uri": "",
-    # cache
-    "expression_cache": None,
-    "calendar_cache": None,
-    # for simple dataset cache
-    "local_cache_path": None,
-    # kernels can be a fixed value or a callable function lie `def (freq: str) -> int`
-    # If the kernels are arctic_kernels, `min(NUM_USABLE_CPU, 30)` may be a good value
-    "kernels": NUM_USABLE_CPU,
-    # pickle.dump protocol version
-    "dump_protocol_version": PROTOCOL_VERSION,
-    # How many tasks belong to one process. Recommend 1 for high-frequency data and None for daily data.
-    "maxtasksperchild": None,
-    # If joblib_backend is None, use loky
-    "joblib_backend": "multiprocessing",
-    "default_disk_cache": 1,  # 0:skip/1:use
-    "mem_cache_size_limit": 500,
-    "mem_cache_limit_type": "length",
-    # memory cache expire second, only in used 'DatasetURICache' and 'client D.calendar'
-    # default 1 hour
-    "mem_cache_expire": 60 * 60,
-    # cache dir name
-    "dataset_cache_dir_name": "dataset_cache",
-    "features_cache_dir_name": "features_cache",
-    # redis
-    # in order to use cache
-    "redis_host": "127.0.0.1",
-    "redis_port": 6379,
-    "redis_task_db": 1,
-    "redis_password": None,
-    # This value can be reset via qlib.init
-    "logging_level": logging.INFO,
-    # Global configuration of qlib log
-    # logging_level can control the logging level more finely
-    "logging_config": {
-        "version": 1,
-        "formatters": {
-            "logger_format": {
-                "format": "[%(process)s:%(threadName)s](%(asctime)s) %(levelname)s - %(name)s - [%(filename)s:%(lineno)d] - %(message)s"
-            }
-        },
-        "filters": {
-            "field_not_found": {
-                "()": "qlib.log.LogFilter",
-                "param": [".*?WARN: data not found for.*?"],
-            }
-        },
-        "handlers": {
-            "console": {
-                "class": "logging.StreamHandler",
-                "level": logging.DEBUG,
-                "formatter": "logger_format",
-                "filters": ["field_not_found"],
-            }
-        },
-        "loggers": {"qlib": {"level": logging.DEBUG, "handlers": ["console"]}},
-        # To let qlib work with other packages, we shouldn't disable existing loggers.
-        # Note that this param is default to True according to the documentation of logging.
-        "disable_existing_loggers": False,
-    },
-    # Default config for experiment manager
-    "exp_manager": {
-        "class": "MLflowExpManager",
-        "module_path": "qlib.workflow.expm",
-        "kwargs": {
-            "uri": "file:" + str(Path(os.getcwd()).resolve() / "mlruns"),
-            "default_exp_name": "Experiment",
-        },
-    },
-    "pit_record_type": {
-        "date": "I",  # uint32
-        "period": "I",  # uint32
-        "value": "d",  # float64
-        "index": "I",  # uint32
-    },
-    "pit_record_nan": {
-        "date": 0,
-        "period": 0,
-        "value": float("NAN"),
-        "index": 0xFFFFFFFF,
-    },
-    # Default config for MongoDB
-    "mongo": {
-        "task_url": "mongodb://localhost:27017/",
-        "task_db_name": "default_task_db",
-    },
-    # Shift minute for highfreq minute data, used in backtest
-    # if min_data_shift == 0, use default market time [9:30, 11:29, 1:00, 2:59]
-    # if min_data_shift != 0, use shifted market time [9:30, 11:29, 1:00, 2:59] - shift*minute
-    "min_data_shift": 0,
-}
-
-MODE_CONF = {
-    "server": {
-        # config it in qlib.init()
-        "provider_uri": "",
-        # redis
-        "redis_host": "127.0.0.1",
-        "redis_port": 6379,
-        "redis_task_db": 1,
-        # cache
-        "expression_cache": DISK_EXPRESSION_CACHE,
-        "dataset_cache": DISK_DATASET_CACHE,
-        "local_cache_path": Path("~/.cache/qlib_simple_cache").expanduser().resolve(),
-        "mount_path": None,
-    },
-    "client": {
-        # config it in user's own code
-        "provider_uri": "~/.qlib/qlib_data/cn_data",
-        # cache
-        # Using parameter 'remote' to announce the client is using server_cache, and the writing access will be disabled.
-        # Disable cache by default. Avoid introduce advanced features for beginners
-        "dataset_cache": None,
-        # SimpleDatasetCache directory
-        "local_cache_path": Path("~/.cache/qlib_simple_cache").expanduser().resolve(),
-        # client config
-        "mount_path": None,
-        "auto_mount": False,  # The nfs is already mounted on our server[auto_mount: False].
-        # The nfs should be auto-mounted by qlib on other
-        # serversS(such as PAI) [auto_mount:True]
-        "timeout": 100,
-        "logging_level": logging.INFO,
-        "region": REG_CN,
-        # custom operator
-        # each element of custom_ops should be Type[ExpressionOps] or dict
-        # if element of custom_ops is Type[ExpressionOps], it represents the custom operator class
-        # if element of custom_ops is dict, it represents the config of custom operator and should include `class` and `module_path` keys.
-        "custom_ops": [],
-    },
-}
-
-HIGH_FREQ_CONFIG = {
-    "provider_uri": "~/.qlib/qlib_data/cn_data_1min",
-    "dataset_cache": None,
-    "expression_cache": "DiskExpressionCache",
-    "region": REG_CN,
-}
-
-_default_region_config = {
-    REG_CN: {
-        "trade_unit": 100,
-        "limit_threshold": 0.095,
-        "deal_price": "close",
-    },
-    REG_US: {
-        "trade_unit": 1,
-        "limit_threshold": None,
-        "deal_price": "close",
-    },
-    REG_TW: {
-        "trade_unit": 1000,
-        "limit_threshold": 0.1,
-        "deal_price": "close",
-    },
-}
-
-
-class QlibConfig(Config):
-    # URI_TYPE
-    LOCAL_URI = "local"
-    NFS_URI = "nfs"
-    DEFAULT_FREQ = "__DEFAULT_FREQ"
-
-    def __init__(self, default_conf):
-        super().__init__(default_conf)
-        self._registered = False
-
-    class DataPathManager:
-        """
-        Motivation:
-        - get the right path (e.g. data uri) for accessing data based on given information(e.g. provider_uri, mount_path and frequency)
-        - some helper functions to process uri.
-        """
-
-        def __init__(self, provider_uri: Union[str, Path, dict], mount_path: Union[str, Path, dict]):
-
-            """
-            The relation of `provider_uri` and `mount_path`
-            - `mount_path` is used only if provider_uri is an NFS path
-            - otherwise, provider_uri will be used for accessing data
-            """
-            self.provider_uri = provider_uri
-            self.mount_path = mount_path
-
-        @staticmethod
-        def format_provider_uri(provider_uri: Union[str, dict, Path]) -> dict:
-            if provider_uri is None:
-                raise ValueError("provider_uri cannot be None")
-            if isinstance(provider_uri, (str, dict, Path)):
-                if not isinstance(provider_uri, dict):
-                    provider_uri = {QlibConfig.DEFAULT_FREQ: provider_uri}
-            else:
-                raise TypeError(f"provider_uri does not support {type(provider_uri)}")
-            for freq, _uri in provider_uri.items():
-                if QlibConfig.DataPathManager.get_uri_type(_uri) == QlibConfig.LOCAL_URI:
-                    provider_uri[freq] = str(Path(_uri).expanduser().resolve())
-            return provider_uri
-
-        @staticmethod
-        def get_uri_type(uri: Union[str, Path]):
-            uri = uri if isinstance(uri, str) else str(uri.expanduser().resolve())
-            is_win = re.match("^[a-zA-Z]:.*", uri) is not None  # such as 'C:\\data', 'D:'
-            # such as 'host:/data/'   (User may define short hostname by themselves or use localhost)
-            is_nfs_or_win = re.match("^[^/]+:.+", uri) is not None
-
-            if is_nfs_or_win and not is_win:
-                return QlibConfig.NFS_URI
-            else:
-                return QlibConfig.LOCAL_URI
-
-        def get_data_uri(self, freq: Optional[Union[str, Freq]] = None) -> Path:
-            """
-            please refer DataPathManager's __init__ and class doc
-            """
-            if freq is not None:
-                freq = str(freq)  # converting Freq to string
-            if freq is None or freq not in self.provider_uri:
-                freq = QlibConfig.DEFAULT_FREQ
-            _provider_uri = self.provider_uri[freq]
-            if self.get_uri_type(_provider_uri) == QlibConfig.LOCAL_URI:
-                return Path(_provider_uri)
-            elif self.get_uri_type(_provider_uri) == QlibConfig.NFS_URI:
-                if "win" in platform.system().lower():
-                    # windows, mount_path is the drive
-                    _path = str(self.mount_path[freq])
-                    return Path(f"{_path}:\\") if ":" not in _path else Path(_path)
-                return Path(self.mount_path[freq])
-            else:
-                raise NotImplementedError(f"This type of uri is not supported")
-
-    def set_mode(self, mode):
-        # raise KeyError
-        self.update(MODE_CONF[mode])
-        # TODO: update region based on kwargs
-
-    def set_region(self, region):
-        # raise KeyError
-        self.update(_default_region_config[region])
-
-    @staticmethod
-    def is_depend_redis(cache_name: str):
-        return cache_name in DEPENDENCY_REDIS_CACHE
-
-    @property
-    def dpm(self):
-        return self.DataPathManager(self["provider_uri"], self["mount_path"])
-
-    def resolve_path(self):
-        # resolve path
-        _mount_path = self["mount_path"]
-        _provider_uri = self.DataPathManager.format_provider_uri(self["provider_uri"])
-        if not isinstance(_mount_path, dict):
-            _mount_path = {_freq: _mount_path for _freq in _provider_uri.keys()}
-
-        # check provider_uri and mount_path
-        _miss_freq = set(_provider_uri.keys()) - set(_mount_path.keys())
-        assert len(_miss_freq) == 0, f"mount_path is missing freq: {_miss_freq}"
-
-        # resolve
-        for _freq in _provider_uri.keys():
-            # mount_path
-            _mount_path[_freq] = (
-                _mount_path[_freq] if _mount_path[_freq] is None else str(Path(_mount_path[_freq]).expanduser())
-            )
-        self["provider_uri"] = _provider_uri
-        self["mount_path"] = _mount_path
-
-    def set(self, default_conf: str = "client", **kwargs):
-        """
-        configure qlib based on the input parameters
-
-        The configuration will act like a dictionary.
-
-        Normally, it literally is replaced the value according to the keys.
-        However, sometimes it is hard for users to set the config when the configuration is nested and complicated
-
-        So this API provides some special parameters for users to set the keys in a more convenient way.
-        - region:  REG_CN, REG_US
-            - several region-related config will be changed
-
-        Parameters
-        ----------
-        default_conf : str
-            the default config template chosen by user: "server", "client"
-        """
-        from .utils import set_log_with_config, get_module_logger, can_use_cache  # pylint: disable=C0415
-
-        self.reset()
-
-        _logging_config = kwargs.get("logging_config", self.logging_config)
-
-        # set global config
-        if _logging_config:
-            set_log_with_config(_logging_config)
-
-        logger = get_module_logger("Initialization", kwargs.get("logging_level", self.logging_level))
-        logger.info(f"default_conf: {default_conf}.")
-
-        self.set_mode(default_conf)
-        self.set_region(kwargs.get("region", self["region"] if "region" in self else REG_CN))
-
-        for k, v in kwargs.items():
-            if k not in self:
-                logger.warning("Unrecognized config %s" % k)
-            self[k] = v
-
-        self.resolve_path()
-
-        if not (self["expression_cache"] is None and self["dataset_cache"] is None):
-            # check redis
-            if not can_use_cache():
-                log_str = ""
-                # check expression cache
-                if self.is_depend_redis(self["expression_cache"]):
-                    log_str += self["expression_cache"]
-                    self["expression_cache"] = None
-                # check dataset cache
-                if self.is_depend_redis(self["dataset_cache"]):
-                    log_str += f" and {self['dataset_cache']}" if log_str else self["dataset_cache"]
-                    self["dataset_cache"] = None
-                if log_str:
-                    logger.warning(
-                        f"redis connection failed(host={self['redis_host']} port={self['redis_port']}), "
-                        f"{log_str} will not be used!"
-                    )
-
-    def register(self):
-        from .utils import init_instance_by_config  # pylint: disable=C0415
-        from .data.ops import register_all_ops  # pylint: disable=C0415
-        from .data.data import register_all_wrappers  # pylint: disable=C0415
-        from .workflow import R, QlibRecorder  # pylint: disable=C0415
-        from .workflow.utils import experiment_exit_handler  # pylint: disable=C0415
-
-        register_all_ops(self)
-        register_all_wrappers(self)
-        # set up QlibRecorder
-        exp_manager = init_instance_by_config(self["exp_manager"])
-        qr = QlibRecorder(exp_manager)
-        R.register(qr)
-        # clean up experiment when python program ends
-        experiment_exit_handler()
-
-        # Supporting user reset qlib version (useful when user want to connect to qlib server with old version)
-        self.reset_qlib_version()
-
-        self._registered = True
-
-    def reset_qlib_version(self):
-        import qlib  # pylint: disable=C0415
-
-        reset_version = self.get("qlib_reset_version", None)
-        if reset_version is not None:
-            qlib.__version__ = reset_version
-        else:
-            qlib.__version__ = getattr(qlib, "__version__bak")
-            # Due to a bug? that converting __version__ to _QlibConfig__version__bak
-            # Using  __version__bak instead of __version__
-
-    def get_kernels(self, freq: str):
-        """get number of processors given frequency"""
-        if isinstance(self["kernels"], Callable):
-            return self["kernels"](freq)
-        return self["kernels"]
-
-    @property
-    def registered(self):
-        return self._registered
-
-
-# global config
-C = QlibConfig(_default_config)
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+"""
+About the configs
+=================
+
+The config will be based on _default_config.
+Two modes are supported
+- client
+- server
+
+"""
+from __future__ import annotations
+
+import os
+import re
+import copy
+import logging
+import platform
+import multiprocessing
+from pathlib import Path
+from typing import Callable, Optional, Union
+from typing import TYPE_CHECKING
+
+from qlib.constant import REG_CN, REG_US, REG_TW
+
+if TYPE_CHECKING:
+    from qlib.utils.time import Freq
+
+
+class Config:
+    def __init__(self, default_conf):
+        self.__dict__["_default_config"] = copy.deepcopy(default_conf)  # avoiding conflicts with __getattr__
+        self.reset()
+
+    def __getitem__(self, key):
+        return self.__dict__["_config"][key]
+
+    def __getattr__(self, attr):
+        if attr in self.__dict__["_config"]:
+            return self.__dict__["_config"][attr]
+
+        raise AttributeError(f"No such `{attr}` in self._config")
+
+    def get(self, key, default=None):
+        return self.__dict__["_config"].get(key, default)
+
+    def __setitem__(self, key, value):
+        self.__dict__["_config"][key] = value
+
+    def __setattr__(self, attr, value):
+        self.__dict__["_config"][attr] = value
+
+    def __contains__(self, item):
+        return item in self.__dict__["_config"]
+
+    def __getstate__(self):
+        return self.__dict__
+
+    def __setstate__(self, state):
+        self.__dict__.update(state)
+
+    def __str__(self):
+        return str(self.__dict__["_config"])
+
+    def __repr__(self):
+        return str(self.__dict__["_config"])
+
+    def reset(self):
+        self.__dict__["_config"] = copy.deepcopy(self._default_config)
+
+    def update(self, *args, **kwargs):
+        self.__dict__["_config"].update(*args, **kwargs)
+
+    def set_conf_from_C(self, config_c):
+        self.update(**config_c.__dict__["_config"])
+
+    @staticmethod
+    def register_from_C(config, skip_register=True):
+        from .utils import set_log_with_config  # pylint: disable=C0415
+
+        if C.registered and skip_register:
+            return
+
+        C.set_conf_from_C(config)
+        if C.logging_config:
+            set_log_with_config(C.logging_config)
+        C.register()
+
+
+# pickle.dump protocol version: https://docs.python.org/3/library/pickle.html#data-stream-format
+PROTOCOL_VERSION = 4
+
+NUM_USABLE_CPU = max(multiprocessing.cpu_count() - 2, 1)
+
+DISK_DATASET_CACHE = "DiskDatasetCache"
+SIMPLE_DATASET_CACHE = "SimpleDatasetCache"
+DISK_EXPRESSION_CACHE = "DiskExpressionCache"
+
+DEPENDENCY_REDIS_CACHE = (DISK_DATASET_CACHE, DISK_EXPRESSION_CACHE)
+
+_default_config = {
+    # data provider config
+    "calendar_provider": "LocalCalendarProvider",
+    "instrument_provider": "LocalInstrumentProvider",
+    "feature_provider": "LocalFeatureProvider",
+    "pit_provider": "LocalPITProvider",
+    "expression_provider": "LocalExpressionProvider",
+    "dataset_provider": "LocalDatasetProvider",
+    "provider": "LocalProvider",
+    # config it in qlib.init()
+    # "provider_uri" str or dict:
+    #   # str
+    #   "~/.qlib/stock_data/cn_data"
+    #   # dict
+    #   {"day": "~/.qlib/stock_data/cn_data", "1min": "~/.qlib/stock_data/cn_data_1min"}
+    # NOTE: provider_uri priority:
+    #   1. backend_config: backend_obj["kwargs"]["provider_uri"]
+    #   2. backend_config: backend_obj["kwargs"]["provider_uri_map"]
+    #   3. qlib.init: provider_uri
+    "provider_uri": "",
+    # cache
+    "expression_cache": None,
+    "calendar_cache": None,
+    # for simple dataset cache
+    "local_cache_path": None,
+    # kernels can be a fixed value or a callable function lie `def (freq: str) -> int`
+    # If the kernels are arctic_kernels, `min(NUM_USABLE_CPU, 30)` may be a good value
+    "kernels": NUM_USABLE_CPU,
+    # pickle.dump protocol version
+    "dump_protocol_version": PROTOCOL_VERSION,
+    # How many tasks belong to one process. Recommend 1 for high-frequency data and None for daily data.
+    "maxtasksperchild": None,
+    # If joblib_backend is None, use loky
+    "joblib_backend": "multiprocessing",
+    "default_disk_cache": 1,  # 0:skip/1:use
+    "mem_cache_size_limit": 500,
+    "mem_cache_limit_type": "length",
+    # memory cache expire second, only in used 'DatasetURICache' and 'client D.calendar'
+    # default 1 hour
+    "mem_cache_expire": 60 * 60,
+    # cache dir name
+    "dataset_cache_dir_name": "dataset_cache",
+    "features_cache_dir_name": "features_cache",
+    # redis
+    # in order to use cache
+    "redis_host": "127.0.0.1",
+    "redis_port": 6379,
+    "redis_task_db": 1,
+    "redis_password": None,
+    # This value can be reset via qlib.init
+    "logging_level": logging.INFO,
+    # Global configuration of qlib log
+    # logging_level can control the logging level more finely
+    "logging_config": {
+        "version": 1,
+        "formatters": {
+            "logger_format": {
+                "format": "[%(process)s:%(threadName)s](%(asctime)s) %(levelname)s - %(name)s - [%(filename)s:%(lineno)d] - %(message)s"
+            }
+        },
+        "filters": {
+            "field_not_found": {
+                "()": "qlib.log.LogFilter",
+                "param": [".*?WARN: data not found for.*?"],
+            }
+        },
+        "handlers": {
+            "console": {
+                "class": "logging.StreamHandler",
+                "level": logging.DEBUG,
+                "formatter": "logger_format",
+                "filters": ["field_not_found"],
+            }
+        },
+        "loggers": {"qlib": {"level": logging.DEBUG, "handlers": ["console"]}},
+        # To let qlib work with other packages, we shouldn't disable existing loggers.
+        # Note that this param is default to True according to the documentation of logging.
+        "disable_existing_loggers": False,
+    },
+    # Default config for experiment manager
+    "exp_manager": {
+        "class": "MLflowExpManager",
+        "module_path": "qlib.workflow.expm",
+        "kwargs": {
+            "uri": "file:" + str(Path(os.getcwd()).resolve() / "mlruns"),
+            "default_exp_name": "Experiment",
+        },
+    },
+    "pit_record_type": {
+        "date": "I",  # uint32
+        "period": "I",  # uint32
+        "value": "d",  # float64
+        "index": "I",  # uint32
+    },
+    "pit_record_nan": {
+        "date": 0,
+        "period": 0,
+        "value": float("NAN"),
+        "index": 0xFFFFFFFF,
+    },
+    # Default config for MongoDB
+    "mongo": {
+        "task_url": "mongodb://localhost:27017/",
+        "task_db_name": "default_task_db",
+    },
+    # Shift minute for highfreq minute data, used in backtest
+    # if min_data_shift == 0, use default market time [9:30, 11:29, 1:00, 2:59]
+    # if min_data_shift != 0, use shifted market time [9:30, 11:29, 1:00, 2:59] - shift*minute
+    "min_data_shift": 0,
+}
+
+MODE_CONF = {
+    "server": {
+        # config it in qlib.init()
+        "provider_uri": "",
+        # redis
+        "redis_host": "127.0.0.1",
+        "redis_port": 6379,
+        "redis_task_db": 1,
+        # cache
+        "expression_cache": DISK_EXPRESSION_CACHE,
+        "dataset_cache": DISK_DATASET_CACHE,
+        "local_cache_path": Path("~/.cache/qlib_simple_cache").expanduser().resolve(),
+        "mount_path": None,
+    },
+    "client": {
+        # config it in user's own code
+        "provider_uri": "~/.qlib/qlib_data/cn_data",
+        # cache
+        # Using parameter 'remote' to announce the client is using server_cache, and the writing access will be disabled.
+        # Disable cache by default. Avoid introduce advanced features for beginners
+        "dataset_cache": None,
+        # SimpleDatasetCache directory
+        "local_cache_path": Path("~/.cache/qlib_simple_cache").expanduser().resolve(),
+        # client config
+        "mount_path": None,
+        "auto_mount": False,  # The nfs is already mounted on our server[auto_mount: False].
+        # The nfs should be auto-mounted by qlib on other
+        # serversS(such as PAI) [auto_mount:True]
+        "timeout": 100,
+        "logging_level": logging.INFO,
+        "region": REG_CN,
+        # custom operator
+        # each element of custom_ops should be Type[ExpressionOps] or dict
+        # if element of custom_ops is Type[ExpressionOps], it represents the custom operator class
+        # if element of custom_ops is dict, it represents the config of custom operator and should include `class` and `module_path` keys.
+        "custom_ops": [],
+    },
+}
+
+HIGH_FREQ_CONFIG = {
+    "provider_uri": "~/.qlib/qlib_data/cn_data_1min",
+    "dataset_cache": None,
+    "expression_cache": "DiskExpressionCache",
+    "region": REG_CN,
+}
+
+_default_region_config = {
+    REG_CN: {
+        "trade_unit": 100,
+        "limit_threshold": 0.095,
+        "deal_price": "close",
+    },
+    REG_US: {
+        "trade_unit": 1,
+        "limit_threshold": None,
+        "deal_price": "close",
+    },
+    REG_TW: {
+        "trade_unit": 1000,
+        "limit_threshold": 0.1,
+        "deal_price": "close",
+    },
+}
+
+
+class QlibConfig(Config):
+    # URI_TYPE
+    LOCAL_URI = "local"
+    NFS_URI = "nfs"
+    DEFAULT_FREQ = "__DEFAULT_FREQ"
+
+    def __init__(self, default_conf):
+        super().__init__(default_conf)
+        self._registered = False
+
+    class DataPathManager:
+        """
+        Motivation:
+        - get the right path (e.g. data uri) for accessing data based on given information(e.g. provider_uri, mount_path and frequency)
+        - some helper functions to process uri.
+        """
+
+        def __init__(self, provider_uri: Union[str, Path, dict], mount_path: Union[str, Path, dict]):
+            """
+            The relation of `provider_uri` and `mount_path`
+            - `mount_path` is used only if provider_uri is an NFS path
+            - otherwise, provider_uri will be used for accessing data
+            """
+            self.provider_uri = provider_uri
+            self.mount_path = mount_path
+
+        @staticmethod
+        def format_provider_uri(provider_uri: Union[str, dict, Path]) -> dict:
+            if provider_uri is None:
+                raise ValueError("provider_uri cannot be None")
+            if isinstance(provider_uri, (str, dict, Path)):
+                if not isinstance(provider_uri, dict):
+                    provider_uri = {QlibConfig.DEFAULT_FREQ: provider_uri}
+            else:
+                raise TypeError(f"provider_uri does not support {type(provider_uri)}")
+            for freq, _uri in provider_uri.items():
+                if QlibConfig.DataPathManager.get_uri_type(_uri) == QlibConfig.LOCAL_URI:
+                    provider_uri[freq] = str(Path(_uri).expanduser().resolve())
+            return provider_uri
+
+        @staticmethod
+        def get_uri_type(uri: Union[str, Path]):
+            uri = uri if isinstance(uri, str) else str(uri.expanduser().resolve())
+            is_win = re.match("^[a-zA-Z]:.*", uri) is not None  # such as 'C:\\data', 'D:'
+            # such as 'host:/data/'   (User may define short hostname by themselves or use localhost)
+            is_nfs_or_win = re.match("^[^/]+:.+", uri) is not None
+
+            if is_nfs_or_win and not is_win:
+                return QlibConfig.NFS_URI
+            else:
+                return QlibConfig.LOCAL_URI
+
+        def get_data_uri(self, freq: Optional[Union[str, Freq]] = None) -> Path:
+            """
+            please refer DataPathManager's __init__ and class doc
+            """
+            if freq is not None:
+                freq = str(freq)  # converting Freq to string
+            if freq is None or freq not in self.provider_uri:
+                freq = QlibConfig.DEFAULT_FREQ
+            _provider_uri = self.provider_uri[freq]
+            if self.get_uri_type(_provider_uri) == QlibConfig.LOCAL_URI:
+                return Path(_provider_uri)
+            elif self.get_uri_type(_provider_uri) == QlibConfig.NFS_URI:
+                if "win" in platform.system().lower():
+                    # windows, mount_path is the drive
+                    _path = str(self.mount_path[freq])
+                    return Path(f"{_path}:\\") if ":" not in _path else Path(_path)
+                return Path(self.mount_path[freq])
+            else:
+                raise NotImplementedError(f"This type of uri is not supported")
+
+    def set_mode(self, mode):
+        # raise KeyError
+        self.update(MODE_CONF[mode])
+        # TODO: update region based on kwargs
+
+    def set_region(self, region):
+        # raise KeyError
+        self.update(_default_region_config[region])
+
+    @staticmethod
+    def is_depend_redis(cache_name: str):
+        return cache_name in DEPENDENCY_REDIS_CACHE
+
+    @property
+    def dpm(self):
+        return self.DataPathManager(self["provider_uri"], self["mount_path"])
+
+    def resolve_path(self):
+        # resolve path
+        _mount_path = self["mount_path"]
+        _provider_uri = self.DataPathManager.format_provider_uri(self["provider_uri"])
+        if not isinstance(_mount_path, dict):
+            _mount_path = {_freq: _mount_path for _freq in _provider_uri.keys()}
+
+        # check provider_uri and mount_path
+        _miss_freq = set(_provider_uri.keys()) - set(_mount_path.keys())
+        assert len(_miss_freq) == 0, f"mount_path is missing freq: {_miss_freq}"
+
+        # resolve
+        for _freq in _provider_uri.keys():
+            # mount_path
+            _mount_path[_freq] = (
+                _mount_path[_freq] if _mount_path[_freq] is None else str(Path(_mount_path[_freq]).expanduser())
+            )
+        self["provider_uri"] = _provider_uri
+        self["mount_path"] = _mount_path
+
+    def set(self, default_conf: str = "client", **kwargs):
+        """
+        configure qlib based on the input parameters
+
+        The configuration will act like a dictionary.
+
+        Normally, it literally is replaced the value according to the keys.
+        However, sometimes it is hard for users to set the config when the configuration is nested and complicated
+
+        So this API provides some special parameters for users to set the keys in a more convenient way.
+        - region:  REG_CN, REG_US
+            - several region-related config will be changed
+
+        Parameters
+        ----------
+        default_conf : str
+            the default config template chosen by user: "server", "client"
+        """
+        from .utils import set_log_with_config, get_module_logger, can_use_cache  # pylint: disable=C0415
+
+        self.reset()
+
+        _logging_config = kwargs.get("logging_config", self.logging_config)
+
+        # set global config
+        if _logging_config:
+            set_log_with_config(_logging_config)
+
+        logger = get_module_logger("Initialization", kwargs.get("logging_level", self.logging_level))
+        logger.info(f"default_conf: {default_conf}.")
+
+        self.set_mode(default_conf)
+        self.set_region(kwargs.get("region", self["region"] if "region" in self else REG_CN))
+
+        for k, v in kwargs.items():
+            if k not in self:
+                logger.warning("Unrecognized config %s" % k)
+            self[k] = v
+
+        self.resolve_path()
+
+        if not (self["expression_cache"] is None and self["dataset_cache"] is None):
+            # check redis
+            if not can_use_cache():
+                log_str = ""
+                # check expression cache
+                if self.is_depend_redis(self["expression_cache"]):
+                    log_str += self["expression_cache"]
+                    self["expression_cache"] = None
+                # check dataset cache
+                if self.is_depend_redis(self["dataset_cache"]):
+                    log_str += f" and {self['dataset_cache']}" if log_str else self["dataset_cache"]
+                    self["dataset_cache"] = None
+                if log_str:
+                    logger.warning(
+                        f"redis connection failed(host={self['redis_host']} port={self['redis_port']}), "
+                        f"{log_str} will not be used!"
+                    )
+
+    def register(self):
+        from .utils import init_instance_by_config  # pylint: disable=C0415
+        from .data.ops import register_all_ops  # pylint: disable=C0415
+        from .data.data import register_all_wrappers  # pylint: disable=C0415
+        from .workflow import R, QlibRecorder  # pylint: disable=C0415
+        from .workflow.utils import experiment_exit_handler  # pylint: disable=C0415
+
+        register_all_ops(self)
+        register_all_wrappers(self)
+        # set up QlibRecorder
+        exp_manager = init_instance_by_config(self["exp_manager"])
+        qr = QlibRecorder(exp_manager)
+        R.register(qr)
+        # clean up experiment when python program ends
+        experiment_exit_handler()
+
+        # Supporting user reset qlib version (useful when user want to connect to qlib server with old version)
+        self.reset_qlib_version()
+
+        self._registered = True
+
+    def reset_qlib_version(self):
+        import qlib  # pylint: disable=C0415
+
+        reset_version = self.get("qlib_reset_version", None)
+        if reset_version is not None:
+            qlib.__version__ = reset_version
+        else:
+            qlib.__version__ = getattr(qlib, "__version__bak")
+            # Due to a bug? that converting __version__ to _QlibConfig__version__bak
+            # Using  __version__bak instead of __version__
+
+    def get_kernels(self, freq: str):
+        """get number of processors given frequency"""
+        if isinstance(self["kernels"], Callable):
+            return self["kernels"](freq)
+        return self["kernels"]
+
+    @property
+    def registered(self):
+        return self._registered
+
+
+# global config
+C = QlibConfig(_default_config)
```

## qlib/constant.py

 * *Ordering differences only*

```diff
@@ -1,22 +1,22 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-# REGION CONST
-from typing import TypeVar
-
-import numpy as np
-import pandas as pd
-
-REG_CN = "cn"
-REG_US = "us"
-REG_TW = "tw"
-
-# Epsilon for avoiding division by zero.
-EPS = 1e-12
-
-# Infinity in integer
-INF = int(1e18)
-ONE_DAY = pd.Timedelta("1day")
-ONE_MIN = pd.Timedelta("1min")
-EPS_T = pd.Timedelta("1s")  # use 1 second to exclude the right interval point
-float_or_ndarray = TypeVar("float_or_ndarray", float, np.ndarray)
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+# REGION CONST
+from typing import TypeVar
+
+import numpy as np
+import pandas as pd
+
+REG_CN = "cn"
+REG_US = "us"
+REG_TW = "tw"
+
+# Epsilon for avoiding division by zero.
+EPS = 1e-12
+
+# Infinity in integer
+INF = int(1e18)
+ONE_DAY = pd.Timedelta("1day")
+ONE_MIN = pd.Timedelta("1min")
+EPS_T = pd.Timedelta("1s")  # use 1 second to exclude the right interval point
+float_or_ndarray = TypeVar("float_or_ndarray", float, np.ndarray)
```

## qlib/log.py

```diff
@@ -1,263 +1,262 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-
-import logging
-from typing import Optional, Text, Dict, Any
-import re
-from logging import config as logging_config
-from time import time
-from contextlib import contextmanager
-
-from .config import C
-
-
-class MetaLogger(type):
-    def __new__(mcs, name, bases, attrs):  # pylint: disable=C0204
-        wrapper_dict = logging.Logger.__dict__.copy()
-        for key, val in wrapper_dict.items():
-            if key not in attrs and key != "__reduce__":
-                attrs[key] = val
-        return type.__new__(mcs, name, bases, attrs)
-
-
-class QlibLogger(metaclass=MetaLogger):
-    """
-    Customized logger for Qlib.
-    """
-
-    def __init__(self, module_name):
-        self.module_name = module_name
-        # this feature name conflicts with the attribute with Logger
-        # rename it to avoid some corner cases that result in comparing `str` and `int`
-        self.__level = 0
-
-    @property
-    def logger(self):
-        logger = logging.getLogger(self.module_name)
-        logger.setLevel(self.__level)
-        return logger
-
-    def setLevel(self, level):
-        self.__level = level
-
-    def __getattr__(self, name):
-        # During unpickling, python will call __getattr__. Use this line to avoid maximum recursion error.
-        if name in {"__setstate__"}:
-            raise AttributeError
-        return self.logger.__getattribute__(name)
-
-
-class _QLibLoggerManager:
-    def __init__(self):
-        self._loggers = {}
-
-    def setLevel(self, level):
-        for logger in self._loggers.values():
-            logger.setLevel(level)
-
-    def __call__(self, module_name, level: Optional[int] = None) -> QlibLogger:
-        """
-        Get a logger for a specific module.
-
-        :param module_name: str
-            Logic module name.
-        :param level: int
-        :return: Logger
-            Logger object.
-        """
-        if level is None:
-            level = C.logging_level
-
-        if not module_name.startswith("qlib."):
-            # Add a prefix of qlib. when the requested ``module_name`` doesn't start with ``qlib.``.
-            # If the module_name is already qlib.xxx, we do not format here. Otherwise, it will become qlib.qlib.xxx.
-            module_name = "qlib.{}".format(module_name)
-
-        # Get logger.
-        module_logger = self._loggers.setdefault(module_name, QlibLogger(module_name))
-        module_logger.setLevel(level)
-        return module_logger
-
-
-get_module_logger = _QLibLoggerManager()
-
-
-class TimeInspector:
-
-    timer_logger = get_module_logger("timer")
-
-    time_marks = []
-
-    @classmethod
-    def set_time_mark(cls):
-        """
-        Set a time mark with current time, and this time mark will push into a stack.
-        :return: float
-            A timestamp for current time.
-        """
-        _time = time()
-        cls.time_marks.append(_time)
-        return _time
-
-    @classmethod
-    def pop_time_mark(cls):
-        """
-        Pop last time mark from stack.
-        """
-        return cls.time_marks.pop()
-
-    @classmethod
-    def get_cost_time(cls):
-        """
-        Get last time mark from stack, calculate time diff with current time.
-        :return: float
-            Time diff calculated by last time mark with current time.
-        """
-        cost_time = time() - cls.time_marks.pop()
-        return cost_time
-
-    @classmethod
-    def log_cost_time(cls, info="Done"):
-        """
-        Get last time mark from stack, calculate time diff with current time, and log time diff and info.
-        :param info: str
-            Info that will be logged into stdout.
-        """
-        cost_time = time() - cls.time_marks.pop()
-        cls.timer_logger.info("Time cost: {0:.3f}s | {1}".format(cost_time, info))
-
-    @classmethod
-    @contextmanager
-    def logt(cls, name="", show_start=False):
-        """logt.
-        Log the time of the inside code
-
-        Parameters
-        ----------
-        name :
-            name
-        show_start :
-            show_start
-        """
-        if show_start:
-            cls.timer_logger.info(f"{name} Begin")
-        cls.set_time_mark()
-        try:
-            yield None
-        finally:
-            pass
-        cls.log_cost_time(info=f"{name} Done")
-
-
-def set_log_with_config(log_config: Dict[Text, Any]):
-    """set log with config
-
-    :param log_config:
-    :return:
-    """
-    logging_config.dictConfig(log_config)
-
-
-class LogFilter(logging.Filter):
-    def __init__(self, param=None):
-        super().__init__()
-        self.param = param
-
-    @staticmethod
-    def match_msg(filter_str, msg):
-        match = False
-        try:
-            if re.match(filter_str, msg):
-                match = True
-        except Exception:
-            pass
-        return match
-
-    def filter(self, record):
-        allow = True
-        if isinstance(self.param, str):
-            allow = not self.match_msg(self.param, record.msg)
-        elif isinstance(self.param, list):
-            allow = not any(self.match_msg(p, record.msg) for p in self.param)
-        return allow
-
-
-def set_global_logger_level(level: int, return_orig_handler_level: bool = False):
-    """set qlib.xxx logger handlers level
-
-    Parameters
-    ----------
-    level: int
-        logger level
-
-    return_orig_handler_level: bool
-        return origin handler level map
-
-    Examples
-    ---------
-
-        .. code-block:: python
-
-            import qlib
-            import logging
-            from qlib.log import get_module_logger, set_global_logger_level
-            qlib.init()
-
-            tmp_logger_01 = get_module_logger("tmp_logger_01", level=logging.INFO)
-            tmp_logger_01.info("1. tmp_logger_01 info show")
-
-            global_level = logging.WARNING + 1
-            set_global_logger_level(global_level)
-            tmp_logger_02 = get_module_logger("tmp_logger_02", level=logging.INFO)
-            tmp_logger_02.log(msg="2. tmp_logger_02 log show", level=global_level)
-
-            tmp_logger_01.info("3. tmp_logger_01 info do not show")
-
-    """
-    _handler_level_map = {}
-    qlib_logger = logging.root.manager.loggerDict.get("qlib", None)  # pylint: disable=E1101
-    if qlib_logger is not None:
-        for _handler in qlib_logger.handlers:
-            _handler_level_map[_handler] = _handler.level
-            _handler.level = level
-    return _handler_level_map if return_orig_handler_level else None
-
-
-@contextmanager
-def set_global_logger_level_cm(level: int):
-    """set qlib.xxx logger handlers level to use contextmanager
-
-    Parameters
-    ----------
-    level: int
-        logger level
-
-    Examples
-    ---------
-
-        .. code-block:: python
-
-            import qlib
-            import logging
-            from qlib.log import get_module_logger, set_global_logger_level_cm
-            qlib.init()
-
-            tmp_logger_01 = get_module_logger("tmp_logger_01", level=logging.INFO)
-            tmp_logger_01.info("1. tmp_logger_01 info show")
-
-            global_level = logging.WARNING + 1
-            with set_global_logger_level_cm(global_level):
-                tmp_logger_02 = get_module_logger("tmp_logger_02", level=logging.INFO)
-                tmp_logger_02.log(msg="2. tmp_logger_02 log show", level=global_level)
-                tmp_logger_01.info("3. tmp_logger_01 info do not show")
-
-            tmp_logger_01.info("4. tmp_logger_01 info show")
-
-    """
-    _handler_level_map = set_global_logger_level(level, return_orig_handler_level=True)
-    try:
-        yield
-    finally:
-        for _handler, _level in _handler_level_map.items():
-            _handler.level = _level
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+
+import logging
+from typing import Optional, Text, Dict, Any
+import re
+from logging import config as logging_config
+from time import time
+from contextlib import contextmanager
+
+from .config import C
+
+
+class MetaLogger(type):
+    def __new__(mcs, name, bases, attrs):  # pylint: disable=C0204
+        wrapper_dict = logging.Logger.__dict__.copy()
+        for key, val in wrapper_dict.items():
+            if key not in attrs and key != "__reduce__":
+                attrs[key] = val
+        return type.__new__(mcs, name, bases, attrs)
+
+
+class QlibLogger(metaclass=MetaLogger):
+    """
+    Customized logger for Qlib.
+    """
+
+    def __init__(self, module_name):
+        self.module_name = module_name
+        # this feature name conflicts with the attribute with Logger
+        # rename it to avoid some corner cases that result in comparing `str` and `int`
+        self.__level = 0
+
+    @property
+    def logger(self):
+        logger = logging.getLogger(self.module_name)
+        logger.setLevel(self.__level)
+        return logger
+
+    def setLevel(self, level):
+        self.__level = level
+
+    def __getattr__(self, name):
+        # During unpickling, python will call __getattr__. Use this line to avoid maximum recursion error.
+        if name in {"__setstate__"}:
+            raise AttributeError
+        return self.logger.__getattribute__(name)
+
+
+class _QLibLoggerManager:
+    def __init__(self):
+        self._loggers = {}
+
+    def setLevel(self, level):
+        for logger in self._loggers.values():
+            logger.setLevel(level)
+
+    def __call__(self, module_name, level: Optional[int] = None) -> QlibLogger:
+        """
+        Get a logger for a specific module.
+
+        :param module_name: str
+            Logic module name.
+        :param level: int
+        :return: Logger
+            Logger object.
+        """
+        if level is None:
+            level = C.logging_level
+
+        if not module_name.startswith("qlib."):
+            # Add a prefix of qlib. when the requested ``module_name`` doesn't start with ``qlib.``.
+            # If the module_name is already qlib.xxx, we do not format here. Otherwise, it will become qlib.qlib.xxx.
+            module_name = "qlib.{}".format(module_name)
+
+        # Get logger.
+        module_logger = self._loggers.setdefault(module_name, QlibLogger(module_name))
+        module_logger.setLevel(level)
+        return module_logger
+
+
+get_module_logger = _QLibLoggerManager()
+
+
+class TimeInspector:
+    timer_logger = get_module_logger("timer")
+
+    time_marks = []
+
+    @classmethod
+    def set_time_mark(cls):
+        """
+        Set a time mark with current time, and this time mark will push into a stack.
+        :return: float
+            A timestamp for current time.
+        """
+        _time = time()
+        cls.time_marks.append(_time)
+        return _time
+
+    @classmethod
+    def pop_time_mark(cls):
+        """
+        Pop last time mark from stack.
+        """
+        return cls.time_marks.pop()
+
+    @classmethod
+    def get_cost_time(cls):
+        """
+        Get last time mark from stack, calculate time diff with current time.
+        :return: float
+            Time diff calculated by last time mark with current time.
+        """
+        cost_time = time() - cls.time_marks.pop()
+        return cost_time
+
+    @classmethod
+    def log_cost_time(cls, info="Done"):
+        """
+        Get last time mark from stack, calculate time diff with current time, and log time diff and info.
+        :param info: str
+            Info that will be logged into stdout.
+        """
+        cost_time = time() - cls.time_marks.pop()
+        cls.timer_logger.info("Time cost: {0:.3f}s | {1}".format(cost_time, info))
+
+    @classmethod
+    @contextmanager
+    def logt(cls, name="", show_start=False):
+        """logt.
+        Log the time of the inside code
+
+        Parameters
+        ----------
+        name :
+            name
+        show_start :
+            show_start
+        """
+        if show_start:
+            cls.timer_logger.info(f"{name} Begin")
+        cls.set_time_mark()
+        try:
+            yield None
+        finally:
+            pass
+        cls.log_cost_time(info=f"{name} Done")
+
+
+def set_log_with_config(log_config: Dict[Text, Any]):
+    """set log with config
+
+    :param log_config:
+    :return:
+    """
+    logging_config.dictConfig(log_config)
+
+
+class LogFilter(logging.Filter):
+    def __init__(self, param=None):
+        super().__init__()
+        self.param = param
+
+    @staticmethod
+    def match_msg(filter_str, msg):
+        match = False
+        try:
+            if re.match(filter_str, msg):
+                match = True
+        except Exception:
+            pass
+        return match
+
+    def filter(self, record):
+        allow = True
+        if isinstance(self.param, str):
+            allow = not self.match_msg(self.param, record.msg)
+        elif isinstance(self.param, list):
+            allow = not any(self.match_msg(p, record.msg) for p in self.param)
+        return allow
+
+
+def set_global_logger_level(level: int, return_orig_handler_level: bool = False):
+    """set qlib.xxx logger handlers level
+
+    Parameters
+    ----------
+    level: int
+        logger level
+
+    return_orig_handler_level: bool
+        return origin handler level map
+
+    Examples
+    ---------
+
+        .. code-block:: python
+
+            import qlib
+            import logging
+            from qlib.log import get_module_logger, set_global_logger_level
+            qlib.init()
+
+            tmp_logger_01 = get_module_logger("tmp_logger_01", level=logging.INFO)
+            tmp_logger_01.info("1. tmp_logger_01 info show")
+
+            global_level = logging.WARNING + 1
+            set_global_logger_level(global_level)
+            tmp_logger_02 = get_module_logger("tmp_logger_02", level=logging.INFO)
+            tmp_logger_02.log(msg="2. tmp_logger_02 log show", level=global_level)
+
+            tmp_logger_01.info("3. tmp_logger_01 info do not show")
+
+    """
+    _handler_level_map = {}
+    qlib_logger = logging.root.manager.loggerDict.get("qlib", None)  # pylint: disable=E1101
+    if qlib_logger is not None:
+        for _handler in qlib_logger.handlers:
+            _handler_level_map[_handler] = _handler.level
+            _handler.level = level
+    return _handler_level_map if return_orig_handler_level else None
+
+
+@contextmanager
+def set_global_logger_level_cm(level: int):
+    """set qlib.xxx logger handlers level to use contextmanager
+
+    Parameters
+    ----------
+    level: int
+        logger level
+
+    Examples
+    ---------
+
+        .. code-block:: python
+
+            import qlib
+            import logging
+            from qlib.log import get_module_logger, set_global_logger_level_cm
+            qlib.init()
+
+            tmp_logger_01 = get_module_logger("tmp_logger_01", level=logging.INFO)
+            tmp_logger_01.info("1. tmp_logger_01 info show")
+
+            global_level = logging.WARNING + 1
+            with set_global_logger_level_cm(global_level):
+                tmp_logger_02 = get_module_logger("tmp_logger_02", level=logging.INFO)
+                tmp_logger_02.log(msg="2. tmp_logger_02 log show", level=global_level)
+                tmp_logger_01.info("3. tmp_logger_01 info do not show")
+
+            tmp_logger_01.info("4. tmp_logger_01 info show")
+
+    """
+    _handler_level_map = set_global_logger_level(level, return_orig_handler_level=True)
+    try:
+        yield
+    finally:
+        for _handler, _level in _handler_level_map.items():
+            _handler.level = _level
```

## qlib/typehint.py

 * *Ordering differences only*

```diff
@@ -1,63 +1,63 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-"""Commonly used types."""
-
-import sys
-from typing import Union
-from pathlib import Path
-
-__all__ = ["Literal", "TypedDict", "final"]
-
-if sys.version_info >= (3, 8):
-    from typing import Literal, TypedDict, final  # type: ignore  # pylint: disable=no-name-in-module
-else:
-    from typing_extensions import Literal, TypedDict, final
-
-
-class InstDictConf(TypedDict):
-    """
-    InstDictConf  is a Dict-based config to describe an instance
-
-        case 1)
-        {
-            'class': 'ClassName',
-            'kwargs': dict, #  It is optional. {} will be used if not given
-            'model_path': path, # It is optional if module is given in the class
-        }
-        case 2)
-        {
-            'class': <The class it self>,
-            'kwargs': dict, #  It is optional. {} will be used if not given
-        }
-    """
-
-    # class: str  # because class is a keyword of Python. We have to comment it
-    kwargs: dict  # It is optional. {} will be used if not given
-    module_path: str  # It is optional if module is given in the class
-
-
-InstConf = Union[InstDictConf, str, object, Path]
-"""
-InstConf is a type to describe an instance; it will be passed into init_instance_by_config for Qlib
-
-    config : Union[str, dict, object, Path]
-
-        InstDictConf example.
-            please refer to the docs of InstDictConf
-
-        str example.
-            1) specify a pickle object
-                - path like 'file:///<path to pickle file>/obj.pkl'
-            2) specify a class name
-                - "ClassName":  getattr(module, "ClassName")() will be used.
-            3) specify module path with class name
-                - "a.b.c.ClassName" getattr(<a.b.c.module>, "ClassName")() will be used.
-
-        object example:
-            instance of accept_types
-
-        Path example:
-            specify a pickle object
-                - it will be treated like 'file:///<path to pickle file>/obj.pkl'
-"""
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+"""Commonly used types."""
+
+import sys
+from typing import Union
+from pathlib import Path
+
+__all__ = ["Literal", "TypedDict", "final"]
+
+if sys.version_info >= (3, 8):
+    from typing import Literal, TypedDict, final  # type: ignore  # pylint: disable=no-name-in-module
+else:
+    from typing_extensions import Literal, TypedDict, final
+
+
+class InstDictConf(TypedDict):
+    """
+    InstDictConf  is a Dict-based config to describe an instance
+
+        case 1)
+        {
+            'class': 'ClassName',
+            'kwargs': dict, #  It is optional. {} will be used if not given
+            'model_path': path, # It is optional if module is given in the class
+        }
+        case 2)
+        {
+            'class': <The class it self>,
+            'kwargs': dict, #  It is optional. {} will be used if not given
+        }
+    """
+
+    # class: str  # because class is a keyword of Python. We have to comment it
+    kwargs: dict  # It is optional. {} will be used if not given
+    module_path: str  # It is optional if module is given in the class
+
+
+InstConf = Union[InstDictConf, str, object, Path]
+"""
+InstConf is a type to describe an instance; it will be passed into init_instance_by_config for Qlib
+
+    config : Union[str, dict, object, Path]
+
+        InstDictConf example.
+            please refer to the docs of InstDictConf
+
+        str example.
+            1) specify a pickle object
+                - path like 'file:///<path to pickle file>/obj.pkl'
+            2) specify a class name
+                - "ClassName":  getattr(module, "ClassName")() will be used.
+            3) specify module path with class name
+                - "a.b.c.ClassName" getattr(<a.b.c.module>, "ClassName")() will be used.
+
+        object example:
+            instance of accept_types
+
+        Path example:
+            specify a pickle object
+                - it will be treated like 'file:///<path to pickle file>/obj.pkl'
+"""
```

## qlib/backtest/__init__.py

```diff
@@ -1,348 +1,347 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-from __future__ import annotations
-
-import copy
-from pathlib import Path
-from typing import TYPE_CHECKING, Any, Generator, List, Optional, Tuple, Union
-
-import pandas as pd
-
-from .account import Account
-
-if TYPE_CHECKING:
-    from ..strategy.base import BaseStrategy
-    from .executor import BaseExecutor
-    from .decision import BaseTradeDecision
-
-from ..config import C
-from ..log import get_module_logger
-from ..utils import init_instance_by_config
-from .backtest import INDICATOR_METRIC, PORT_METRIC, backtest_loop, collect_data_loop
-from .decision import Order
-from .exchange import Exchange
-from .utils import CommonInfrastructure
-
-# make import more user-friendly by adding `from qlib.backtest import STH`
-
-
-logger = get_module_logger("backtest caller")
-
-
-def get_exchange(
-    exchange: Union[str, dict, object, Path] = None,
-    freq: str = "day",
-    start_time: Union[pd.Timestamp, str] = None,
-    end_time: Union[pd.Timestamp, str] = None,
-    codes: Union[list, str] = "all",
-    subscribe_fields: list = [],
-    open_cost: float = 0.0015,
-    close_cost: float = 0.0025,
-    min_cost: float = 5.0,
-    limit_threshold: Union[Tuple[str, str], float, None] | None = None,
-    deal_price: Union[str, Tuple[str, str], List[str]] | None = None,
-    **kwargs: Any,
-) -> Exchange:
-    """get_exchange
-
-    Parameters
-    ----------
-
-    # exchange related arguments
-    exchange: Exchange
-        It could be None or any types that are acceptable by `init_instance_by_config`.
-    freq: str
-        frequency of data.
-    start_time: Union[pd.Timestamp, str]
-        closed start time for backtest.
-    end_time: Union[pd.Timestamp, str]
-        closed end time for backtest.
-    codes: Union[list, str]
-        list stock_id list or a string of instruments (i.e. all, csi500, sse50)
-    subscribe_fields: list
-        subscribe fields.
-    open_cost : float
-        open transaction cost. It is a ratio. The cost is proportional to your order's deal amount.
-    close_cost : float
-        close transaction cost. It is a ratio. The cost is proportional to your order's deal amount.
-    min_cost : float
-        min transaction cost.  It is an absolute amount of cost instead of a ratio of your order's deal amount.
-        e.g. You must pay at least 5 yuan of commission regardless of your order's deal amount.
-    deal_price: Union[str, Tuple[str, str], List[str]]
-                The `deal_price` supports following two types of input
-                - <deal_price> : str
-                - (<buy_price>, <sell_price>): Tuple[str, str] or List[str]
-
-                <deal_price>, <buy_price> or <sell_price> := <price>
-                <price> := str
-                - for example '$close', '$open', '$vwap' ("close" is OK. `Exchange` will help to prepend
-                  "$" to the expression)
-    limit_threshold : float
-        limit move 0.1 (10%) for example, long and short with same limit.
-
-    Returns
-    -------
-    :class: Exchange
-    an initialized Exchange object
-    """
-
-    if limit_threshold is None:
-        limit_threshold = C.limit_threshold
-    if exchange is None:
-        logger.info("Create new exchange")
-
-        exchange = Exchange(
-            freq=freq,
-            start_time=start_time,
-            end_time=end_time,
-            codes=codes,
-            deal_price=deal_price,
-            subscribe_fields=subscribe_fields,
-            limit_threshold=limit_threshold,
-            open_cost=open_cost,
-            close_cost=close_cost,
-            min_cost=min_cost,
-            **kwargs,
-        )
-        return exchange
-    else:
-        return init_instance_by_config(exchange, accept_types=Exchange)
-
-
-def create_account_instance(
-    start_time: Union[pd.Timestamp, str],
-    end_time: Union[pd.Timestamp, str],
-    benchmark: Optional[str],
-    account: Union[float, int, dict],
-    pos_type: str = "Position",
-) -> Account:
-    """
-    # TODO: is very strange pass benchmark_config in the account (maybe for report)
-    # There should be a post-step to process the report.
-
-    Parameters
-    ----------
-    start_time
-        start time of the benchmark
-    end_time
-        end time of the benchmark
-    benchmark : str
-        the benchmark for reporting
-    account :   Union[
-                    float,
-                    {
-                        "cash": float,
-                        "stock1": Union[
-                                        int,    # it is equal to {"amount": int}
-                                        {"amount": int, "price"(optional): float},
-                                  ]
-                    },
-                ]
-        information for describing how to creating the account
-        For `float`:
-            Using Account with only initial cash
-        For `dict`:
-            key "cash" means initial cash.
-            key "stock1" means the information of first stock with amount and price(optional).
-            ...
-    pos_type: str
-        Postion type.
-    """
-    if isinstance(account, (int, float)):
-        init_cash = account
-        position_dict = {}
-    elif isinstance(account, dict):
-        init_cash = account.pop("cash")
-        position_dict = account
-    else:
-        raise ValueError("account must be in (int, float, dict)")
-
-    return Account(
-        init_cash=init_cash,
-        position_dict=position_dict,
-        pos_type=pos_type,
-        benchmark_config={}
-        if benchmark is None
-        else {
-            "benchmark": benchmark,
-            "start_time": start_time,
-            "end_time": end_time,
-        },
-    )
-
-
-def get_strategy_executor(
-    start_time: Union[pd.Timestamp, str],
-    end_time: Union[pd.Timestamp, str],
-    strategy: Union[str, dict, object, Path],
-    executor: Union[str, dict, object, Path],
-    benchmark: Optional[str] = "SH000300",
-    account: Union[float, int, dict] = 1e9,
-    exchange_kwargs: dict = {},
-    pos_type: str = "Position",
-) -> Tuple[BaseStrategy, BaseExecutor]:
-
-    # NOTE:
-    # - for avoiding recursive import
-    # - typing annotations is not reliable
-    from ..strategy.base import BaseStrategy  # pylint: disable=C0415
-    from .executor import BaseExecutor  # pylint: disable=C0415
-
-    trade_account = create_account_instance(
-        start_time=start_time,
-        end_time=end_time,
-        benchmark=benchmark,
-        account=account,
-        pos_type=pos_type,
-    )
-
-    exchange_kwargs = copy.copy(exchange_kwargs)
-    if "start_time" not in exchange_kwargs:
-        exchange_kwargs["start_time"] = start_time
-    if "end_time" not in exchange_kwargs:
-        exchange_kwargs["end_time"] = end_time
-    trade_exchange = get_exchange(**exchange_kwargs)
-
-    common_infra = CommonInfrastructure(trade_account=trade_account, trade_exchange=trade_exchange)
-    trade_strategy = init_instance_by_config(strategy, accept_types=BaseStrategy)
-    trade_strategy.reset_common_infra(common_infra)
-    trade_executor = init_instance_by_config(executor, accept_types=BaseExecutor)
-    trade_executor.reset_common_infra(common_infra)
-
-    return trade_strategy, trade_executor
-
-
-def backtest(
-    start_time: Union[pd.Timestamp, str],
-    end_time: Union[pd.Timestamp, str],
-    strategy: Union[str, dict, object, Path],
-    executor: Union[str, dict, object, Path],
-    benchmark: str = "SH000300",
-    account: Union[float, int, dict] = 1e9,
-    exchange_kwargs: dict = {},
-    pos_type: str = "Position",
-) -> Tuple[PORT_METRIC, INDICATOR_METRIC]:
-    """initialize the strategy and executor, then backtest function for the interaction of the outermost strategy and
-    executor in the nested decision execution
-
-    Parameters
-    ----------
-    start_time : Union[pd.Timestamp, str]
-        closed start time for backtest
-        **NOTE**: This will be applied to the outmost executor's calendar.
-    end_time : Union[pd.Timestamp, str]
-        closed end time for backtest
-        **NOTE**: This will be applied to the outmost executor's calendar.
-        E.g. Executor[day](Executor[1min]),   setting `end_time == 20XX0301` will include all the minutes on 20XX0301
-    strategy : Union[str, dict, object, Path]
-        for initializing outermost portfolio strategy. Please refer to the docs of init_instance_by_config for more
-        information.
-    executor : Union[str, dict, object, Path]
-        for initializing the outermost executor.
-    benchmark: str
-        the benchmark for reporting.
-    account : Union[float, int, Position]
-        information for describing how to create the account
-        For `float` or `int`:
-            Using Account with only initial cash
-        For `Position`:
-            Using Account with a Position
-    exchange_kwargs : dict
-        the kwargs for initializing Exchange
-    pos_type : str
-        the type of Position.
-
-    Returns
-    -------
-    portfolio_dict: PORT_METRIC
-        it records the trading portfolio_metrics information
-    indicator_dict: INDICATOR_METRIC
-        it computes the trading indicator
-        It is organized in a dict format
-
-    """
-    trade_strategy, trade_executor = get_strategy_executor(
-        start_time,
-        end_time,
-        strategy,
-        executor,
-        benchmark,
-        account,
-        exchange_kwargs,
-        pos_type=pos_type,
-    )
-    return backtest_loop(start_time, end_time, trade_strategy, trade_executor)
-
-
-def collect_data(
-    start_time: Union[pd.Timestamp, str],
-    end_time: Union[pd.Timestamp, str],
-    strategy: Union[str, dict, object, Path],
-    executor: Union[str, dict, object, Path],
-    benchmark: str = "SH000300",
-    account: Union[float, int, dict] = 1e9,
-    exchange_kwargs: dict = {},
-    pos_type: str = "Position",
-    return_value: dict | None = None,
-) -> Generator[object, None, None]:
-    """initialize the strategy and executor, then collect the trade decision data for rl training
-
-    please refer to the docs of the backtest for the explanation of the parameters
-
-    Yields
-    -------
-    object
-        trade decision
-    """
-    trade_strategy, trade_executor = get_strategy_executor(
-        start_time,
-        end_time,
-        strategy,
-        executor,
-        benchmark,
-        account,
-        exchange_kwargs,
-        pos_type=pos_type,
-    )
-    yield from collect_data_loop(start_time, end_time, trade_strategy, trade_executor, return_value=return_value)
-
-
-def format_decisions(
-    decisions: List[BaseTradeDecision],
-) -> Optional[Tuple[str, List[Tuple[BaseTradeDecision, Union[Tuple, None]]]]]:
-    """
-    format the decisions collected by `qlib.backtest.collect_data`
-    The decisions will be organized into a tree-like structure.
-
-    Parameters
-    ----------
-    decisions : List[BaseTradeDecision]
-        decisions collected by `qlib.backtest.collect_data`
-
-    Returns
-    -------
-    Tuple[str, List[Tuple[BaseTradeDecision, Union[Tuple, None]]]]:
-
-        reformat the list of decisions into a more user-friendly format
-        <decisions> :=  Tuple[<freq>, List[Tuple[<decision>, <sub decisions>]]]
-        - <sub decisions> := `<decisions> in lower level` | None
-        - <freq> := "day" | "30min" | "1min" | ...
-        - <decision> := <instance of BaseTradeDecision>
-    """
-    if len(decisions) == 0:
-        return None
-
-    cur_freq = decisions[0].strategy.trade_calendar.get_freq()
-
-    res: Tuple[str, list] = (cur_freq, [])
-    last_dec_idx = 0
-    for i, dec in enumerate(decisions[1:], 1):
-        if dec.strategy.trade_calendar.get_freq() == cur_freq:
-            res[1].append((decisions[last_dec_idx], format_decisions(decisions[last_dec_idx + 1 : i])))
-            last_dec_idx = i
-    res[1].append((decisions[last_dec_idx], format_decisions(decisions[last_dec_idx + 1 :])))
-    return res
-
-
-__all__ = ["Order", "backtest", "get_strategy_executor"]
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+from __future__ import annotations
+
+import copy
+from pathlib import Path
+from typing import TYPE_CHECKING, Any, Generator, List, Optional, Tuple, Union
+
+import pandas as pd
+
+from .account import Account
+
+if TYPE_CHECKING:
+    from ..strategy.base import BaseStrategy
+    from .executor import BaseExecutor
+    from .decision import BaseTradeDecision
+
+from ..config import C
+from ..log import get_module_logger
+from ..utils import init_instance_by_config
+from .backtest import INDICATOR_METRIC, PORT_METRIC, backtest_loop, collect_data_loop
+from .decision import Order
+from .exchange import Exchange
+from .utils import CommonInfrastructure
+
+# make import more user-friendly by adding `from qlib.backtest import STH`
+
+
+logger = get_module_logger("backtest caller")
+
+
+def get_exchange(
+    exchange: Union[str, dict, object, Path] = None,
+    freq: str = "day",
+    start_time: Union[pd.Timestamp, str] = None,
+    end_time: Union[pd.Timestamp, str] = None,
+    codes: Union[list, str] = "all",
+    subscribe_fields: list = [],
+    open_cost: float = 0.0015,
+    close_cost: float = 0.0025,
+    min_cost: float = 5.0,
+    limit_threshold: Union[Tuple[str, str], float, None] | None = None,
+    deal_price: Union[str, Tuple[str, str], List[str]] | None = None,
+    **kwargs: Any,
+) -> Exchange:
+    """get_exchange
+
+    Parameters
+    ----------
+
+    # exchange related arguments
+    exchange: Exchange
+        It could be None or any types that are acceptable by `init_instance_by_config`.
+    freq: str
+        frequency of data.
+    start_time: Union[pd.Timestamp, str]
+        closed start time for backtest.
+    end_time: Union[pd.Timestamp, str]
+        closed end time for backtest.
+    codes: Union[list, str]
+        list stock_id list or a string of instruments (i.e. all, csi500, sse50)
+    subscribe_fields: list
+        subscribe fields.
+    open_cost : float
+        open transaction cost. It is a ratio. The cost is proportional to your order's deal amount.
+    close_cost : float
+        close transaction cost. It is a ratio. The cost is proportional to your order's deal amount.
+    min_cost : float
+        min transaction cost.  It is an absolute amount of cost instead of a ratio of your order's deal amount.
+        e.g. You must pay at least 5 yuan of commission regardless of your order's deal amount.
+    deal_price: Union[str, Tuple[str, str], List[str]]
+                The `deal_price` supports following two types of input
+                - <deal_price> : str
+                - (<buy_price>, <sell_price>): Tuple[str, str] or List[str]
+
+                <deal_price>, <buy_price> or <sell_price> := <price>
+                <price> := str
+                - for example '$close', '$open', '$vwap' ("close" is OK. `Exchange` will help to prepend
+                  "$" to the expression)
+    limit_threshold : float
+        limit move 0.1 (10%) for example, long and short with same limit.
+
+    Returns
+    -------
+    :class: Exchange
+    an initialized Exchange object
+    """
+
+    if limit_threshold is None:
+        limit_threshold = C.limit_threshold
+    if exchange is None:
+        logger.info("Create new exchange")
+
+        exchange = Exchange(
+            freq=freq,
+            start_time=start_time,
+            end_time=end_time,
+            codes=codes,
+            deal_price=deal_price,
+            subscribe_fields=subscribe_fields,
+            limit_threshold=limit_threshold,
+            open_cost=open_cost,
+            close_cost=close_cost,
+            min_cost=min_cost,
+            **kwargs,
+        )
+        return exchange
+    else:
+        return init_instance_by_config(exchange, accept_types=Exchange)
+
+
+def create_account_instance(
+    start_time: Union[pd.Timestamp, str],
+    end_time: Union[pd.Timestamp, str],
+    benchmark: Optional[str],
+    account: Union[float, int, dict],
+    pos_type: str = "Position",
+) -> Account:
+    """
+    # TODO: is very strange pass benchmark_config in the account (maybe for report)
+    # There should be a post-step to process the report.
+
+    Parameters
+    ----------
+    start_time
+        start time of the benchmark
+    end_time
+        end time of the benchmark
+    benchmark : str
+        the benchmark for reporting
+    account :   Union[
+                    float,
+                    {
+                        "cash": float,
+                        "stock1": Union[
+                                        int,    # it is equal to {"amount": int}
+                                        {"amount": int, "price"(optional): float},
+                                  ]
+                    },
+                ]
+        information for describing how to creating the account
+        For `float`:
+            Using Account with only initial cash
+        For `dict`:
+            key "cash" means initial cash.
+            key "stock1" means the information of first stock with amount and price(optional).
+            ...
+    pos_type: str
+        Postion type.
+    """
+    if isinstance(account, (int, float)):
+        init_cash = account
+        position_dict = {}
+    elif isinstance(account, dict):
+        init_cash = account.pop("cash")
+        position_dict = account
+    else:
+        raise ValueError("account must be in (int, float, dict)")
+
+    return Account(
+        init_cash=init_cash,
+        position_dict=position_dict,
+        pos_type=pos_type,
+        benchmark_config={}
+        if benchmark is None
+        else {
+            "benchmark": benchmark,
+            "start_time": start_time,
+            "end_time": end_time,
+        },
+    )
+
+
+def get_strategy_executor(
+    start_time: Union[pd.Timestamp, str],
+    end_time: Union[pd.Timestamp, str],
+    strategy: Union[str, dict, object, Path],
+    executor: Union[str, dict, object, Path],
+    benchmark: Optional[str] = "SH000300",
+    account: Union[float, int, dict] = 1e9,
+    exchange_kwargs: dict = {},
+    pos_type: str = "Position",
+) -> Tuple[BaseStrategy, BaseExecutor]:
+    # NOTE:
+    # - for avoiding recursive import
+    # - typing annotations is not reliable
+    from ..strategy.base import BaseStrategy  # pylint: disable=C0415
+    from .executor import BaseExecutor  # pylint: disable=C0415
+
+    trade_account = create_account_instance(
+        start_time=start_time,
+        end_time=end_time,
+        benchmark=benchmark,
+        account=account,
+        pos_type=pos_type,
+    )
+
+    exchange_kwargs = copy.copy(exchange_kwargs)
+    if "start_time" not in exchange_kwargs:
+        exchange_kwargs["start_time"] = start_time
+    if "end_time" not in exchange_kwargs:
+        exchange_kwargs["end_time"] = end_time
+    trade_exchange = get_exchange(**exchange_kwargs)
+
+    common_infra = CommonInfrastructure(trade_account=trade_account, trade_exchange=trade_exchange)
+    trade_strategy = init_instance_by_config(strategy, accept_types=BaseStrategy)
+    trade_strategy.reset_common_infra(common_infra)
+    trade_executor = init_instance_by_config(executor, accept_types=BaseExecutor)
+    trade_executor.reset_common_infra(common_infra)
+
+    return trade_strategy, trade_executor
+
+
+def backtest(
+    start_time: Union[pd.Timestamp, str],
+    end_time: Union[pd.Timestamp, str],
+    strategy: Union[str, dict, object, Path],
+    executor: Union[str, dict, object, Path],
+    benchmark: str = "SH000300",
+    account: Union[float, int, dict] = 1e9,
+    exchange_kwargs: dict = {},
+    pos_type: str = "Position",
+) -> Tuple[PORT_METRIC, INDICATOR_METRIC]:
+    """initialize the strategy and executor, then backtest function for the interaction of the outermost strategy and
+    executor in the nested decision execution
+
+    Parameters
+    ----------
+    start_time : Union[pd.Timestamp, str]
+        closed start time for backtest
+        **NOTE**: This will be applied to the outmost executor's calendar.
+    end_time : Union[pd.Timestamp, str]
+        closed end time for backtest
+        **NOTE**: This will be applied to the outmost executor's calendar.
+        E.g. Executor[day](Executor[1min]),   setting `end_time == 20XX0301` will include all the minutes on 20XX0301
+    strategy : Union[str, dict, object, Path]
+        for initializing outermost portfolio strategy. Please refer to the docs of init_instance_by_config for more
+        information.
+    executor : Union[str, dict, object, Path]
+        for initializing the outermost executor.
+    benchmark: str
+        the benchmark for reporting.
+    account : Union[float, int, Position]
+        information for describing how to create the account
+        For `float` or `int`:
+            Using Account with only initial cash
+        For `Position`:
+            Using Account with a Position
+    exchange_kwargs : dict
+        the kwargs for initializing Exchange
+    pos_type : str
+        the type of Position.
+
+    Returns
+    -------
+    portfolio_dict: PORT_METRIC
+        it records the trading portfolio_metrics information
+    indicator_dict: INDICATOR_METRIC
+        it computes the trading indicator
+        It is organized in a dict format
+
+    """
+    trade_strategy, trade_executor = get_strategy_executor(
+        start_time,
+        end_time,
+        strategy,
+        executor,
+        benchmark,
+        account,
+        exchange_kwargs,
+        pos_type=pos_type,
+    )
+    return backtest_loop(start_time, end_time, trade_strategy, trade_executor)
+
+
+def collect_data(
+    start_time: Union[pd.Timestamp, str],
+    end_time: Union[pd.Timestamp, str],
+    strategy: Union[str, dict, object, Path],
+    executor: Union[str, dict, object, Path],
+    benchmark: str = "SH000300",
+    account: Union[float, int, dict] = 1e9,
+    exchange_kwargs: dict = {},
+    pos_type: str = "Position",
+    return_value: dict | None = None,
+) -> Generator[object, None, None]:
+    """initialize the strategy and executor, then collect the trade decision data for rl training
+
+    please refer to the docs of the backtest for the explanation of the parameters
+
+    Yields
+    -------
+    object
+        trade decision
+    """
+    trade_strategy, trade_executor = get_strategy_executor(
+        start_time,
+        end_time,
+        strategy,
+        executor,
+        benchmark,
+        account,
+        exchange_kwargs,
+        pos_type=pos_type,
+    )
+    yield from collect_data_loop(start_time, end_time, trade_strategy, trade_executor, return_value=return_value)
+
+
+def format_decisions(
+    decisions: List[BaseTradeDecision],
+) -> Optional[Tuple[str, List[Tuple[BaseTradeDecision, Union[Tuple, None]]]]]:
+    """
+    format the decisions collected by `qlib.backtest.collect_data`
+    The decisions will be organized into a tree-like structure.
+
+    Parameters
+    ----------
+    decisions : List[BaseTradeDecision]
+        decisions collected by `qlib.backtest.collect_data`
+
+    Returns
+    -------
+    Tuple[str, List[Tuple[BaseTradeDecision, Union[Tuple, None]]]]:
+
+        reformat the list of decisions into a more user-friendly format
+        <decisions> :=  Tuple[<freq>, List[Tuple[<decision>, <sub decisions>]]]
+        - <sub decisions> := `<decisions> in lower level` | None
+        - <freq> := "day" | "30min" | "1min" | ...
+        - <decision> := <instance of BaseTradeDecision>
+    """
+    if len(decisions) == 0:
+        return None
+
+    cur_freq = decisions[0].strategy.trade_calendar.get_freq()
+
+    res: Tuple[str, list] = (cur_freq, [])
+    last_dec_idx = 0
+    for i, dec in enumerate(decisions[1:], 1):
+        if dec.strategy.trade_calendar.get_freq() == cur_freq:
+            res[1].append((decisions[last_dec_idx], format_decisions(decisions[last_dec_idx + 1 : i])))
+            last_dec_idx = i
+    res[1].append((decisions[last_dec_idx], format_decisions(decisions[last_dec_idx + 1 :])))
+    return res
+
+
+__all__ = ["Order", "backtest", "get_strategy_executor"]
```

## qlib/backtest/account.py

 * *Ordering differences only*

```diff
@@ -1,417 +1,417 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-from __future__ import annotations
-
-import copy
-from typing import Dict, List, Optional, Tuple, cast
-
-import pandas as pd
-
-from qlib.utils import init_instance_by_config
-
-from .decision import BaseTradeDecision, Order
-from .exchange import Exchange
-from .high_performance_ds import BaseOrderIndicator
-from .position import BasePosition
-from .report import Indicator, PortfolioMetrics
-
-"""
-rtn & earning in the Account
-    rtn:
-        from order's view
-        1.change if any order is executed, sell order or buy order
-        2.change at the end of today,   (today_close - stock_price) * amount
-    earning
-        from value of current position
-        earning will be updated at the end of trade date
-        earning = today_value - pre_value
-    **is consider cost**
-        while earning is the difference of two position value, so it considers cost, it is the true return rate
-        in the specific accomplishment for rtn, it does not consider cost, in other words, rtn - cost = earning
-
-"""
-
-
-class AccumulatedInfo:
-    """
-    accumulated trading info, including accumulated return/cost/turnover
-    AccumulatedInfo should be shared across different levels
-    """
-
-    def __init__(self) -> None:
-        self.reset()
-
-    def reset(self) -> None:
-        self.rtn: float = 0.0  # accumulated return, do not consider cost
-        self.cost: float = 0.0  # accumulated cost
-        self.to: float = 0.0  # accumulated turnover
-
-    def add_return_value(self, value: float) -> None:
-        self.rtn += value
-
-    def add_cost(self, value: float) -> None:
-        self.cost += value
-
-    def add_turnover(self, value: float) -> None:
-        self.to += value
-
-    @property
-    def get_return(self) -> float:
-        return self.rtn
-
-    @property
-    def get_cost(self) -> float:
-        return self.cost
-
-    @property
-    def get_turnover(self) -> float:
-        return self.to
-
-
-class Account:
-    """
-    The correctness of the metrics of Account in nested execution depends on the shallow copy of `trade_account` in
-    qlib/backtest/executor.py:NestedExecutor
-    Different level of executor has different Account object when calculating metrics. But the position object is
-    shared cross all the Account object.
-    """
-
-    def __init__(
-        self,
-        init_cash: float = 1e9,
-        position_dict: dict = {},
-        freq: str = "day",
-        benchmark_config: dict = {},
-        pos_type: str = "Position",
-        port_metr_enabled: bool = True,
-    ) -> None:
-        """the trade account of backtest.
-
-        Parameters
-        ----------
-        init_cash : float, optional
-            initial cash, by default 1e9
-        position_dict : Dict[
-                            stock_id,
-                            Union[
-                                int,  # it is equal to {"amount": int}
-                                {"amount": int, "price"(optional): float},
-                            ]
-                        ]
-            initial stocks with parameters amount and price,
-            if there is no price key in the dict of stocks, it will be filled by _fill_stock_value.
-            by default {}.
-        """
-
-        self._pos_type = pos_type
-        self._port_metr_enabled = port_metr_enabled
-        self.benchmark_config: dict = {}  # avoid no attribute error
-        self.init_vars(init_cash, position_dict, freq, benchmark_config)
-
-    def init_vars(self, init_cash: float, position_dict: dict, freq: str, benchmark_config: dict) -> None:
-        # 1) the following variables are shared by multiple layers
-        # - you will see a shallow copy instead of deepcopy in the NestedExecutor;
-        self.init_cash = init_cash
-        self.current_position: BasePosition = init_instance_by_config(
-            {
-                "class": self._pos_type,
-                "kwargs": {
-                    "cash": init_cash,
-                    "position_dict": position_dict,
-                },
-                "module_path": "qlib.backtest.position",
-            },
-        )
-        self.accum_info = AccumulatedInfo()
-
-        # 2) following variables are not shared between layers
-        self.portfolio_metrics: Optional[PortfolioMetrics] = None
-        self.hist_positions: Dict[pd.Timestamp, BasePosition] = {}
-        self.reset(freq=freq, benchmark_config=benchmark_config)
-
-    def is_port_metr_enabled(self) -> bool:
-        """
-        Is portfolio-based metrics enabled.
-        """
-        return self._port_metr_enabled and not self.current_position.skip_update()
-
-    def reset_report(self, freq: str, benchmark_config: dict) -> None:
-        # portfolio related metrics
-        if self.is_port_metr_enabled():
-            # NOTE:
-            # `accum_info` and `current_position` are shared here
-            self.portfolio_metrics = PortfolioMetrics(freq, benchmark_config)
-            self.hist_positions = {}
-
-            # fill stock value
-            # The frequency of account may not align with the trading frequency.
-            # This may result in obscure bugs when data quality is low.
-            if isinstance(self.benchmark_config, dict) and "start_time" in self.benchmark_config:
-                self.current_position.fill_stock_value(self.benchmark_config["start_time"], self.freq)
-
-        # trading related metrics(e.g. high-frequency trading)
-        self.indicator = Indicator()
-
-    def reset(
-        self, freq: str | None = None, benchmark_config: dict | None = None, port_metr_enabled: bool | None = None
-    ) -> None:
-        """reset freq and report of account
-
-        Parameters
-        ----------
-        freq : str, optional
-            frequency of account & report, by default None
-        benchmark_config : {}, optional
-            benchmark config of report, by default None
-        port_metr_enabled: bool
-        """
-        if freq is not None:
-            self.freq = freq
-        if benchmark_config is not None:
-            self.benchmark_config = benchmark_config
-        if port_metr_enabled is not None:
-            self._port_metr_enabled = port_metr_enabled
-
-        self.reset_report(self.freq, self.benchmark_config)
-
-    def get_hist_positions(self) -> Dict[pd.Timestamp, BasePosition]:
-        return self.hist_positions
-
-    def get_cash(self) -> float:
-        return self.current_position.get_cash()
-
-    def _update_state_from_order(self, order: Order, trade_val: float, cost: float, trade_price: float) -> None:
-        if self.is_port_metr_enabled():
-            # update turnover
-            self.accum_info.add_turnover(trade_val)
-            # update cost
-            self.accum_info.add_cost(cost)
-
-            # update return from order
-            trade_amount = trade_val / trade_price
-            if order.direction == Order.SELL:  # 0 for sell
-                # when sell stock, get profit from price change
-                profit = trade_val - self.current_position.get_stock_price(order.stock_id) * trade_amount
-                self.accum_info.add_return_value(profit)  # note here do not consider cost
-
-            elif order.direction == Order.BUY:  # 1 for buy
-                # when buy stock, we get return for the rtn computing method
-                # profit in buy order is to make rtn is consistent with earning at the end of bar
-                profit = self.current_position.get_stock_price(order.stock_id) * trade_amount - trade_val
-                self.accum_info.add_return_value(profit)  # note here do not consider cost
-
-    def update_order(self, order: Order, trade_val: float, cost: float, trade_price: float) -> None:
-        if self.current_position.skip_update():
-            # TODO: supporting polymorphism for account
-            # updating order for infinite position is meaningless
-            return
-
-        # if stock is sold out, no stock price information in Position, then we should update account first,
-        # then update current position
-        # if stock is bought, there is no stock in current position, update current, then update account
-        # The cost will be subtracted from the cash at last. So the trading logic can ignore the cost calculation
-        if order.direction == Order.SELL:
-            # sell stock
-            self._update_state_from_order(order, trade_val, cost, trade_price)
-            # update current position
-            # for may sell all of stock_id
-            self.current_position.update_order(order, trade_val, cost, trade_price)
-        else:
-            # buy stock
-            # deal order, then update state
-            self.current_position.update_order(order, trade_val, cost, trade_price)
-            self._update_state_from_order(order, trade_val, cost, trade_price)
-
-    def update_current_position(
-        self,
-        trade_start_time: pd.Timestamp,
-        trade_end_time: pd.Timestamp,
-        trade_exchange: Exchange,
-    ) -> None:
-        """
-        Update current to make rtn consistent with earning at the end of bar, and update holding bar count of stock
-        """
-        # update price for stock in the position and the profit from changed_price
-        # NOTE: updating position does not only serve portfolio metrics, it also serve the strategy
-        assert self.current_position is not None
-
-        if not self.current_position.skip_update():
-            stock_list = self.current_position.get_stock_list()
-            for code in stock_list:
-                # if suspended, no new price to be updated, profit is 0
-                if trade_exchange.check_stock_suspended(code, trade_start_time, trade_end_time):
-                    continue
-                bar_close = cast(float, trade_exchange.get_close(code, trade_start_time, trade_end_time))
-                self.current_position.update_stock_price(stock_id=code, price=bar_close)
-            # update holding day count
-            # NOTE: updating bar_count does not only serve portfolio metrics, it also serve the strategy
-            self.current_position.add_count_all(bar=self.freq)
-
-    def update_portfolio_metrics(self, trade_start_time: pd.Timestamp, trade_end_time: pd.Timestamp) -> None:
-        """update portfolio_metrics"""
-        # calculate earning
-        # account_value - last_account_value
-        # for the first trade date, account_value - init_cash
-        # self.portfolio_metrics.is_empty() to judge is_first_trade_date
-        # get last_account_value, last_total_cost, last_total_turnover
-        assert self.portfolio_metrics is not None
-
-        if self.portfolio_metrics.is_empty():
-            last_account_value = self.init_cash
-            last_total_cost = 0
-            last_total_turnover = 0
-        else:
-            last_account_value = self.portfolio_metrics.get_latest_account_value()
-            last_total_cost = self.portfolio_metrics.get_latest_total_cost()
-            last_total_turnover = self.portfolio_metrics.get_latest_total_turnover()
-
-        # get now_account_value, now_stock_value, now_earning, now_cost, now_turnover
-        now_account_value = self.current_position.calculate_value()
-        now_stock_value = self.current_position.calculate_stock_value()
-        now_earning = now_account_value - last_account_value
-        now_cost = self.accum_info.get_cost - last_total_cost
-        now_turnover = self.accum_info.get_turnover - last_total_turnover
-
-        # update portfolio_metrics for today
-        # judge whether the trading is begin.
-        # and don't add init account state into portfolio_metrics, due to we don't have excess return in those days.
-        self.portfolio_metrics.update_portfolio_metrics_record(
-            trade_start_time=trade_start_time,
-            trade_end_time=trade_end_time,
-            account_value=now_account_value,
-            cash=self.current_position.position["cash"],
-            return_rate=(now_earning + now_cost) / last_account_value,
-            # here use earning to calculate return, position's view, earning consider cost, true return
-            # in order to make same definition with original backtest in evaluate.py
-            total_turnover=self.accum_info.get_turnover,
-            turnover_rate=now_turnover / last_account_value,
-            total_cost=self.accum_info.get_cost,
-            cost_rate=now_cost / last_account_value,
-            stock_value=now_stock_value,
-        )
-
-    def update_hist_positions(self, trade_start_time: pd.Timestamp) -> None:
-        """update history position"""
-        now_account_value = self.current_position.calculate_value()
-        # set now_account_value to position
-        self.current_position.position["now_account_value"] = now_account_value
-        self.current_position.update_weight_all()
-        # update hist_positions
-        # note use deepcopy
-        self.hist_positions[trade_start_time] = copy.deepcopy(self.current_position)
-
-    def update_indicator(
-        self,
-        trade_start_time: pd.Timestamp,
-        trade_exchange: Exchange,
-        atomic: bool,
-        outer_trade_decision: BaseTradeDecision,
-        trade_info: list = [],
-        inner_order_indicators: List[BaseOrderIndicator] = [],
-        decision_list: List[Tuple[BaseTradeDecision, pd.Timestamp, pd.Timestamp]] = [],
-        indicator_config: dict = {},
-    ) -> None:
-        """update trade indicators and order indicators in each bar end"""
-        # TODO: will skip empty decisions make it faster?  `outer_trade_decision.empty():`
-
-        # indicator is trading (e.g. high-frequency order execution) related analysis
-        self.indicator.reset()
-
-        # aggregate the information for each order
-        if atomic:
-            self.indicator.update_order_indicators(trade_info)
-        else:
-            self.indicator.agg_order_indicators(
-                inner_order_indicators,
-                decision_list=decision_list,
-                outer_trade_decision=outer_trade_decision,
-                trade_exchange=trade_exchange,
-                indicator_config=indicator_config,
-            )
-
-        # aggregate all the order metrics a single step
-        self.indicator.cal_trade_indicators(trade_start_time, self.freq, indicator_config)
-
-        # record the metrics
-        self.indicator.record(trade_start_time)
-
-    def update_bar_end(
-        self,
-        trade_start_time: pd.Timestamp,
-        trade_end_time: pd.Timestamp,
-        trade_exchange: Exchange,
-        atomic: bool,
-        outer_trade_decision: BaseTradeDecision,
-        trade_info: list = [],
-        inner_order_indicators: List[BaseOrderIndicator] = [],
-        decision_list: List[Tuple[BaseTradeDecision, pd.Timestamp, pd.Timestamp]] = [],
-        indicator_config: dict = {},
-    ) -> None:
-        """update account at each trading bar step
-
-        Parameters
-        ----------
-        trade_start_time : pd.Timestamp
-            closed start time of step
-        trade_end_time : pd.Timestamp
-            closed end time of step
-        trade_exchange : Exchange
-            trading exchange, used to update current
-        atomic : bool
-            whether the trading executor is atomic, which means there is no higher-frequency trading executor inside it
-            - if atomic is True, calculate the indicators with trade_info
-            - else, aggregate indicators with inner indicators
-        outer_trade_decision: BaseTradeDecision
-            external trade decision
-        trade_info : List[(Order, float, float, float)], optional
-            trading information, by default None
-            - necessary if atomic is True
-            - list of tuple(order, trade_val, trade_cost, trade_price)
-        inner_order_indicators : Indicator, optional
-            indicators of inner executor, by default None
-            - necessary if atomic is False
-            - used to aggregate outer indicators
-        decision_list: List[Tuple[BaseTradeDecision, pd.Timestamp, pd.Timestamp]] = None,
-            The decision list of the inner level: List[Tuple[<decision>, <start_time>, <end_time>]]
-            The inner level
-        indicator_config : dict, optional
-            config of calculating indicators, by default {}
-        """
-        if atomic is True and trade_info is None:
-            raise ValueError("trade_info is necessary in atomic executor")
-        elif atomic is False and inner_order_indicators is None:
-            raise ValueError("inner_order_indicators is necessary in un-atomic executor")
-
-        # update current position and hold bar count in each bar end
-        self.update_current_position(trade_start_time, trade_end_time, trade_exchange)
-
-        if self.is_port_metr_enabled():
-            # portfolio_metrics is portfolio related analysis
-            self.update_portfolio_metrics(trade_start_time, trade_end_time)
-            self.update_hist_positions(trade_start_time)
-
-        # update indicator in each bar end
-        self.update_indicator(
-            trade_start_time=trade_start_time,
-            trade_exchange=trade_exchange,
-            atomic=atomic,
-            outer_trade_decision=outer_trade_decision,
-            trade_info=trade_info,
-            inner_order_indicators=inner_order_indicators,
-            decision_list=decision_list,
-            indicator_config=indicator_config,
-        )
-
-    def get_portfolio_metrics(self) -> Tuple[pd.DataFrame, dict]:
-        """get the history portfolio_metrics and positions instance"""
-        if self.is_port_metr_enabled():
-            assert self.portfolio_metrics is not None
-            _portfolio_metrics = self.portfolio_metrics.generate_portfolio_metrics_dataframe()
-            _positions = self.get_hist_positions()
-            return _portfolio_metrics, _positions
-        else:
-            raise ValueError("generate_portfolio_metrics should be True if you want to generate portfolio_metrics")
-
-    def get_trade_indicator(self) -> Indicator:
-        """get the trade indicator instance, which has pa/pos/ffr info."""
-        return self.indicator
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+from __future__ import annotations
+
+import copy
+from typing import Dict, List, Optional, Tuple, cast
+
+import pandas as pd
+
+from qlib.utils import init_instance_by_config
+
+from .decision import BaseTradeDecision, Order
+from .exchange import Exchange
+from .high_performance_ds import BaseOrderIndicator
+from .position import BasePosition
+from .report import Indicator, PortfolioMetrics
+
+"""
+rtn & earning in the Account
+    rtn:
+        from order's view
+        1.change if any order is executed, sell order or buy order
+        2.change at the end of today,   (today_close - stock_price) * amount
+    earning
+        from value of current position
+        earning will be updated at the end of trade date
+        earning = today_value - pre_value
+    **is consider cost**
+        while earning is the difference of two position value, so it considers cost, it is the true return rate
+        in the specific accomplishment for rtn, it does not consider cost, in other words, rtn - cost = earning
+
+"""
+
+
+class AccumulatedInfo:
+    """
+    accumulated trading info, including accumulated return/cost/turnover
+    AccumulatedInfo should be shared across different levels
+    """
+
+    def __init__(self) -> None:
+        self.reset()
+
+    def reset(self) -> None:
+        self.rtn: float = 0.0  # accumulated return, do not consider cost
+        self.cost: float = 0.0  # accumulated cost
+        self.to: float = 0.0  # accumulated turnover
+
+    def add_return_value(self, value: float) -> None:
+        self.rtn += value
+
+    def add_cost(self, value: float) -> None:
+        self.cost += value
+
+    def add_turnover(self, value: float) -> None:
+        self.to += value
+
+    @property
+    def get_return(self) -> float:
+        return self.rtn
+
+    @property
+    def get_cost(self) -> float:
+        return self.cost
+
+    @property
+    def get_turnover(self) -> float:
+        return self.to
+
+
+class Account:
+    """
+    The correctness of the metrics of Account in nested execution depends on the shallow copy of `trade_account` in
+    qlib/backtest/executor.py:NestedExecutor
+    Different level of executor has different Account object when calculating metrics. But the position object is
+    shared cross all the Account object.
+    """
+
+    def __init__(
+        self,
+        init_cash: float = 1e9,
+        position_dict: dict = {},
+        freq: str = "day",
+        benchmark_config: dict = {},
+        pos_type: str = "Position",
+        port_metr_enabled: bool = True,
+    ) -> None:
+        """the trade account of backtest.
+
+        Parameters
+        ----------
+        init_cash : float, optional
+            initial cash, by default 1e9
+        position_dict : Dict[
+                            stock_id,
+                            Union[
+                                int,  # it is equal to {"amount": int}
+                                {"amount": int, "price"(optional): float},
+                            ]
+                        ]
+            initial stocks with parameters amount and price,
+            if there is no price key in the dict of stocks, it will be filled by _fill_stock_value.
+            by default {}.
+        """
+
+        self._pos_type = pos_type
+        self._port_metr_enabled = port_metr_enabled
+        self.benchmark_config: dict = {}  # avoid no attribute error
+        self.init_vars(init_cash, position_dict, freq, benchmark_config)
+
+    def init_vars(self, init_cash: float, position_dict: dict, freq: str, benchmark_config: dict) -> None:
+        # 1) the following variables are shared by multiple layers
+        # - you will see a shallow copy instead of deepcopy in the NestedExecutor;
+        self.init_cash = init_cash
+        self.current_position: BasePosition = init_instance_by_config(
+            {
+                "class": self._pos_type,
+                "kwargs": {
+                    "cash": init_cash,
+                    "position_dict": position_dict,
+                },
+                "module_path": "qlib.backtest.position",
+            },
+        )
+        self.accum_info = AccumulatedInfo()
+
+        # 2) following variables are not shared between layers
+        self.portfolio_metrics: Optional[PortfolioMetrics] = None
+        self.hist_positions: Dict[pd.Timestamp, BasePosition] = {}
+        self.reset(freq=freq, benchmark_config=benchmark_config)
+
+    def is_port_metr_enabled(self) -> bool:
+        """
+        Is portfolio-based metrics enabled.
+        """
+        return self._port_metr_enabled and not self.current_position.skip_update()
+
+    def reset_report(self, freq: str, benchmark_config: dict) -> None:
+        # portfolio related metrics
+        if self.is_port_metr_enabled():
+            # NOTE:
+            # `accum_info` and `current_position` are shared here
+            self.portfolio_metrics = PortfolioMetrics(freq, benchmark_config)
+            self.hist_positions = {}
+
+            # fill stock value
+            # The frequency of account may not align with the trading frequency.
+            # This may result in obscure bugs when data quality is low.
+            if isinstance(self.benchmark_config, dict) and "start_time" in self.benchmark_config:
+                self.current_position.fill_stock_value(self.benchmark_config["start_time"], self.freq)
+
+        # trading related metrics(e.g. high-frequency trading)
+        self.indicator = Indicator()
+
+    def reset(
+        self, freq: str | None = None, benchmark_config: dict | None = None, port_metr_enabled: bool | None = None
+    ) -> None:
+        """reset freq and report of account
+
+        Parameters
+        ----------
+        freq : str, optional
+            frequency of account & report, by default None
+        benchmark_config : {}, optional
+            benchmark config of report, by default None
+        port_metr_enabled: bool
+        """
+        if freq is not None:
+            self.freq = freq
+        if benchmark_config is not None:
+            self.benchmark_config = benchmark_config
+        if port_metr_enabled is not None:
+            self._port_metr_enabled = port_metr_enabled
+
+        self.reset_report(self.freq, self.benchmark_config)
+
+    def get_hist_positions(self) -> Dict[pd.Timestamp, BasePosition]:
+        return self.hist_positions
+
+    def get_cash(self) -> float:
+        return self.current_position.get_cash()
+
+    def _update_state_from_order(self, order: Order, trade_val: float, cost: float, trade_price: float) -> None:
+        if self.is_port_metr_enabled():
+            # update turnover
+            self.accum_info.add_turnover(trade_val)
+            # update cost
+            self.accum_info.add_cost(cost)
+
+            # update return from order
+            trade_amount = trade_val / trade_price
+            if order.direction == Order.SELL:  # 0 for sell
+                # when sell stock, get profit from price change
+                profit = trade_val - self.current_position.get_stock_price(order.stock_id) * trade_amount
+                self.accum_info.add_return_value(profit)  # note here do not consider cost
+
+            elif order.direction == Order.BUY:  # 1 for buy
+                # when buy stock, we get return for the rtn computing method
+                # profit in buy order is to make rtn is consistent with earning at the end of bar
+                profit = self.current_position.get_stock_price(order.stock_id) * trade_amount - trade_val
+                self.accum_info.add_return_value(profit)  # note here do not consider cost
+
+    def update_order(self, order: Order, trade_val: float, cost: float, trade_price: float) -> None:
+        if self.current_position.skip_update():
+            # TODO: supporting polymorphism for account
+            # updating order for infinite position is meaningless
+            return
+
+        # if stock is sold out, no stock price information in Position, then we should update account first,
+        # then update current position
+        # if stock is bought, there is no stock in current position, update current, then update account
+        # The cost will be subtracted from the cash at last. So the trading logic can ignore the cost calculation
+        if order.direction == Order.SELL:
+            # sell stock
+            self._update_state_from_order(order, trade_val, cost, trade_price)
+            # update current position
+            # for may sell all of stock_id
+            self.current_position.update_order(order, trade_val, cost, trade_price)
+        else:
+            # buy stock
+            # deal order, then update state
+            self.current_position.update_order(order, trade_val, cost, trade_price)
+            self._update_state_from_order(order, trade_val, cost, trade_price)
+
+    def update_current_position(
+        self,
+        trade_start_time: pd.Timestamp,
+        trade_end_time: pd.Timestamp,
+        trade_exchange: Exchange,
+    ) -> None:
+        """
+        Update current to make rtn consistent with earning at the end of bar, and update holding bar count of stock
+        """
+        # update price for stock in the position and the profit from changed_price
+        # NOTE: updating position does not only serve portfolio metrics, it also serve the strategy
+        assert self.current_position is not None
+
+        if not self.current_position.skip_update():
+            stock_list = self.current_position.get_stock_list()
+            for code in stock_list:
+                # if suspended, no new price to be updated, profit is 0
+                if trade_exchange.check_stock_suspended(code, trade_start_time, trade_end_time):
+                    continue
+                bar_close = cast(float, trade_exchange.get_close(code, trade_start_time, trade_end_time))
+                self.current_position.update_stock_price(stock_id=code, price=bar_close)
+            # update holding day count
+            # NOTE: updating bar_count does not only serve portfolio metrics, it also serve the strategy
+            self.current_position.add_count_all(bar=self.freq)
+
+    def update_portfolio_metrics(self, trade_start_time: pd.Timestamp, trade_end_time: pd.Timestamp) -> None:
+        """update portfolio_metrics"""
+        # calculate earning
+        # account_value - last_account_value
+        # for the first trade date, account_value - init_cash
+        # self.portfolio_metrics.is_empty() to judge is_first_trade_date
+        # get last_account_value, last_total_cost, last_total_turnover
+        assert self.portfolio_metrics is not None
+
+        if self.portfolio_metrics.is_empty():
+            last_account_value = self.init_cash
+            last_total_cost = 0
+            last_total_turnover = 0
+        else:
+            last_account_value = self.portfolio_metrics.get_latest_account_value()
+            last_total_cost = self.portfolio_metrics.get_latest_total_cost()
+            last_total_turnover = self.portfolio_metrics.get_latest_total_turnover()
+
+        # get now_account_value, now_stock_value, now_earning, now_cost, now_turnover
+        now_account_value = self.current_position.calculate_value()
+        now_stock_value = self.current_position.calculate_stock_value()
+        now_earning = now_account_value - last_account_value
+        now_cost = self.accum_info.get_cost - last_total_cost
+        now_turnover = self.accum_info.get_turnover - last_total_turnover
+
+        # update portfolio_metrics for today
+        # judge whether the trading is begin.
+        # and don't add init account state into portfolio_metrics, due to we don't have excess return in those days.
+        self.portfolio_metrics.update_portfolio_metrics_record(
+            trade_start_time=trade_start_time,
+            trade_end_time=trade_end_time,
+            account_value=now_account_value,
+            cash=self.current_position.position["cash"],
+            return_rate=(now_earning + now_cost) / last_account_value,
+            # here use earning to calculate return, position's view, earning consider cost, true return
+            # in order to make same definition with original backtest in evaluate.py
+            total_turnover=self.accum_info.get_turnover,
+            turnover_rate=now_turnover / last_account_value,
+            total_cost=self.accum_info.get_cost,
+            cost_rate=now_cost / last_account_value,
+            stock_value=now_stock_value,
+        )
+
+    def update_hist_positions(self, trade_start_time: pd.Timestamp) -> None:
+        """update history position"""
+        now_account_value = self.current_position.calculate_value()
+        # set now_account_value to position
+        self.current_position.position["now_account_value"] = now_account_value
+        self.current_position.update_weight_all()
+        # update hist_positions
+        # note use deepcopy
+        self.hist_positions[trade_start_time] = copy.deepcopy(self.current_position)
+
+    def update_indicator(
+        self,
+        trade_start_time: pd.Timestamp,
+        trade_exchange: Exchange,
+        atomic: bool,
+        outer_trade_decision: BaseTradeDecision,
+        trade_info: list = [],
+        inner_order_indicators: List[BaseOrderIndicator] = [],
+        decision_list: List[Tuple[BaseTradeDecision, pd.Timestamp, pd.Timestamp]] = [],
+        indicator_config: dict = {},
+    ) -> None:
+        """update trade indicators and order indicators in each bar end"""
+        # TODO: will skip empty decisions make it faster?  `outer_trade_decision.empty():`
+
+        # indicator is trading (e.g. high-frequency order execution) related analysis
+        self.indicator.reset()
+
+        # aggregate the information for each order
+        if atomic:
+            self.indicator.update_order_indicators(trade_info)
+        else:
+            self.indicator.agg_order_indicators(
+                inner_order_indicators,
+                decision_list=decision_list,
+                outer_trade_decision=outer_trade_decision,
+                trade_exchange=trade_exchange,
+                indicator_config=indicator_config,
+            )
+
+        # aggregate all the order metrics a single step
+        self.indicator.cal_trade_indicators(trade_start_time, self.freq, indicator_config)
+
+        # record the metrics
+        self.indicator.record(trade_start_time)
+
+    def update_bar_end(
+        self,
+        trade_start_time: pd.Timestamp,
+        trade_end_time: pd.Timestamp,
+        trade_exchange: Exchange,
+        atomic: bool,
+        outer_trade_decision: BaseTradeDecision,
+        trade_info: list = [],
+        inner_order_indicators: List[BaseOrderIndicator] = [],
+        decision_list: List[Tuple[BaseTradeDecision, pd.Timestamp, pd.Timestamp]] = [],
+        indicator_config: dict = {},
+    ) -> None:
+        """update account at each trading bar step
+
+        Parameters
+        ----------
+        trade_start_time : pd.Timestamp
+            closed start time of step
+        trade_end_time : pd.Timestamp
+            closed end time of step
+        trade_exchange : Exchange
+            trading exchange, used to update current
+        atomic : bool
+            whether the trading executor is atomic, which means there is no higher-frequency trading executor inside it
+            - if atomic is True, calculate the indicators with trade_info
+            - else, aggregate indicators with inner indicators
+        outer_trade_decision: BaseTradeDecision
+            external trade decision
+        trade_info : List[(Order, float, float, float)], optional
+            trading information, by default None
+            - necessary if atomic is True
+            - list of tuple(order, trade_val, trade_cost, trade_price)
+        inner_order_indicators : Indicator, optional
+            indicators of inner executor, by default None
+            - necessary if atomic is False
+            - used to aggregate outer indicators
+        decision_list: List[Tuple[BaseTradeDecision, pd.Timestamp, pd.Timestamp]] = None,
+            The decision list of the inner level: List[Tuple[<decision>, <start_time>, <end_time>]]
+            The inner level
+        indicator_config : dict, optional
+            config of calculating indicators, by default {}
+        """
+        if atomic is True and trade_info is None:
+            raise ValueError("trade_info is necessary in atomic executor")
+        elif atomic is False and inner_order_indicators is None:
+            raise ValueError("inner_order_indicators is necessary in un-atomic executor")
+
+        # update current position and hold bar count in each bar end
+        self.update_current_position(trade_start_time, trade_end_time, trade_exchange)
+
+        if self.is_port_metr_enabled():
+            # portfolio_metrics is portfolio related analysis
+            self.update_portfolio_metrics(trade_start_time, trade_end_time)
+            self.update_hist_positions(trade_start_time)
+
+        # update indicator in each bar end
+        self.update_indicator(
+            trade_start_time=trade_start_time,
+            trade_exchange=trade_exchange,
+            atomic=atomic,
+            outer_trade_decision=outer_trade_decision,
+            trade_info=trade_info,
+            inner_order_indicators=inner_order_indicators,
+            decision_list=decision_list,
+            indicator_config=indicator_config,
+        )
+
+    def get_portfolio_metrics(self) -> Tuple[pd.DataFrame, dict]:
+        """get the history portfolio_metrics and positions instance"""
+        if self.is_port_metr_enabled():
+            assert self.portfolio_metrics is not None
+            _portfolio_metrics = self.portfolio_metrics.generate_portfolio_metrics_dataframe()
+            _positions = self.get_hist_positions()
+            return _portfolio_metrics, _positions
+        else:
+            raise ValueError("generate_portfolio_metrics should be True if you want to generate portfolio_metrics")
+
+    def get_trade_indicator(self) -> Indicator:
+        """get the trade indicator instance, which has pa/pos/ffr info."""
+        return self.indicator
```

## qlib/backtest/backtest.py

 * *Ordering differences only*

```diff
@@ -1,110 +1,110 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-from __future__ import annotations
-
-from typing import Dict, TYPE_CHECKING, Generator, Optional, Tuple, Union, cast
-
-import pandas as pd
-
-from qlib.backtest.decision import BaseTradeDecision
-from qlib.backtest.report import Indicator
-
-if TYPE_CHECKING:
-    from qlib.strategy.base import BaseStrategy
-    from qlib.backtest.executor import BaseExecutor
-
-from tqdm.auto import tqdm
-
-from ..utils.time import Freq
-
-
-PORT_METRIC = Dict[str, Tuple[pd.DataFrame, dict]]
-INDICATOR_METRIC = Dict[str, Tuple[pd.DataFrame, Indicator]]
-
-
-def backtest_loop(
-    start_time: Union[pd.Timestamp, str],
-    end_time: Union[pd.Timestamp, str],
-    trade_strategy: BaseStrategy,
-    trade_executor: BaseExecutor,
-) -> Tuple[PORT_METRIC, INDICATOR_METRIC]:
-    """backtest function for the interaction of the outermost strategy and executor in the nested decision execution
-
-    please refer to the docs of `collect_data_loop`
-
-    Returns
-    -------
-    portfolio_dict: PORT_METRIC
-        it records the trading portfolio_metrics information
-    indicator_dict: INDICATOR_METRIC
-        it computes the trading indicator
-    """
-    return_value: dict = {}
-    for _decision in collect_data_loop(start_time, end_time, trade_strategy, trade_executor, return_value):
-        pass
-
-    portfolio_dict = cast(PORT_METRIC, return_value.get("portfolio_dict"))
-    indicator_dict = cast(INDICATOR_METRIC, return_value.get("indicator_dict"))
-
-    return portfolio_dict, indicator_dict
-
-
-def collect_data_loop(
-    start_time: Union[pd.Timestamp, str],
-    end_time: Union[pd.Timestamp, str],
-    trade_strategy: BaseStrategy,
-    trade_executor: BaseExecutor,
-    return_value: dict | None = None,
-) -> Generator[BaseTradeDecision, Optional[BaseTradeDecision], None]:
-    """Generator for collecting the trade decision data for rl training
-
-    Parameters
-    ----------
-    start_time : Union[pd.Timestamp, str]
-        closed start time for backtest
-        **NOTE**: This will be applied to the outmost executor's calendar.
-    end_time : Union[pd.Timestamp, str]
-        closed end time for backtest
-        **NOTE**: This will be applied to the outmost executor's calendar.
-        E.g. Executor[day](Executor[1min]), setting `end_time == 20XX0301` will include all the minutes on 20XX0301
-    trade_strategy : BaseStrategy
-        the outermost portfolio strategy
-    trade_executor : BaseExecutor
-        the outermost executor
-    return_value : dict
-        used for backtest_loop
-
-    Yields
-    -------
-    object
-        trade decision
-    """
-    trade_executor.reset(start_time=start_time, end_time=end_time)
-    trade_strategy.reset(level_infra=trade_executor.get_level_infra())
-
-    with tqdm(total=trade_executor.trade_calendar.get_trade_len(), desc="backtest loop") as bar:
-        _execute_result = None
-        while not trade_executor.finished():
-            _trade_decision: BaseTradeDecision = trade_strategy.generate_trade_decision(_execute_result)
-            _execute_result = yield from trade_executor.collect_data(_trade_decision, level=0)
-            trade_strategy.post_exe_step(_execute_result)
-            bar.update(1)
-        trade_strategy.post_upper_level_exe_step()
-
-    if return_value is not None:
-        all_executors = trade_executor.get_all_executors()
-
-        portfolio_dict: PORT_METRIC = {}
-        indicator_dict: INDICATOR_METRIC = {}
-
-        for executor in all_executors:
-            key = "{}{}".format(*Freq.parse(executor.time_per_step))
-            if executor.trade_account.is_port_metr_enabled():
-                portfolio_dict[key] = executor.trade_account.get_portfolio_metrics()
-
-            indicator_df = executor.trade_account.get_trade_indicator().generate_trade_indicators_dataframe()
-            indicator_obj = executor.trade_account.get_trade_indicator()
-            indicator_dict[key] = (indicator_df, indicator_obj)
-
-        return_value.update({"portfolio_dict": portfolio_dict, "indicator_dict": indicator_dict})
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+from __future__ import annotations
+
+from typing import Dict, TYPE_CHECKING, Generator, Optional, Tuple, Union, cast
+
+import pandas as pd
+
+from qlib.backtest.decision import BaseTradeDecision
+from qlib.backtest.report import Indicator
+
+if TYPE_CHECKING:
+    from qlib.strategy.base import BaseStrategy
+    from qlib.backtest.executor import BaseExecutor
+
+from tqdm.auto import tqdm
+
+from ..utils.time import Freq
+
+
+PORT_METRIC = Dict[str, Tuple[pd.DataFrame, dict]]
+INDICATOR_METRIC = Dict[str, Tuple[pd.DataFrame, Indicator]]
+
+
+def backtest_loop(
+    start_time: Union[pd.Timestamp, str],
+    end_time: Union[pd.Timestamp, str],
+    trade_strategy: BaseStrategy,
+    trade_executor: BaseExecutor,
+) -> Tuple[PORT_METRIC, INDICATOR_METRIC]:
+    """backtest function for the interaction of the outermost strategy and executor in the nested decision execution
+
+    please refer to the docs of `collect_data_loop`
+
+    Returns
+    -------
+    portfolio_dict: PORT_METRIC
+        it records the trading portfolio_metrics information
+    indicator_dict: INDICATOR_METRIC
+        it computes the trading indicator
+    """
+    return_value: dict = {}
+    for _decision in collect_data_loop(start_time, end_time, trade_strategy, trade_executor, return_value):
+        pass
+
+    portfolio_dict = cast(PORT_METRIC, return_value.get("portfolio_dict"))
+    indicator_dict = cast(INDICATOR_METRIC, return_value.get("indicator_dict"))
+
+    return portfolio_dict, indicator_dict
+
+
+def collect_data_loop(
+    start_time: Union[pd.Timestamp, str],
+    end_time: Union[pd.Timestamp, str],
+    trade_strategy: BaseStrategy,
+    trade_executor: BaseExecutor,
+    return_value: dict | None = None,
+) -> Generator[BaseTradeDecision, Optional[BaseTradeDecision], None]:
+    """Generator for collecting the trade decision data for rl training
+
+    Parameters
+    ----------
+    start_time : Union[pd.Timestamp, str]
+        closed start time for backtest
+        **NOTE**: This will be applied to the outmost executor's calendar.
+    end_time : Union[pd.Timestamp, str]
+        closed end time for backtest
+        **NOTE**: This will be applied to the outmost executor's calendar.
+        E.g. Executor[day](Executor[1min]), setting `end_time == 20XX0301` will include all the minutes on 20XX0301
+    trade_strategy : BaseStrategy
+        the outermost portfolio strategy
+    trade_executor : BaseExecutor
+        the outermost executor
+    return_value : dict
+        used for backtest_loop
+
+    Yields
+    -------
+    object
+        trade decision
+    """
+    trade_executor.reset(start_time=start_time, end_time=end_time)
+    trade_strategy.reset(level_infra=trade_executor.get_level_infra())
+
+    with tqdm(total=trade_executor.trade_calendar.get_trade_len(), desc="backtest loop") as bar:
+        _execute_result = None
+        while not trade_executor.finished():
+            _trade_decision: BaseTradeDecision = trade_strategy.generate_trade_decision(_execute_result)
+            _execute_result = yield from trade_executor.collect_data(_trade_decision, level=0)
+            trade_strategy.post_exe_step(_execute_result)
+            bar.update(1)
+        trade_strategy.post_upper_level_exe_step()
+
+    if return_value is not None:
+        all_executors = trade_executor.get_all_executors()
+
+        portfolio_dict: PORT_METRIC = {}
+        indicator_dict: INDICATOR_METRIC = {}
+
+        for executor in all_executors:
+            key = "{}{}".format(*Freq.parse(executor.time_per_step))
+            if executor.trade_account.is_port_metr_enabled():
+                portfolio_dict[key] = executor.trade_account.get_portfolio_metrics()
+
+            indicator_df = executor.trade_account.get_trade_indicator().generate_trade_indicators_dataframe()
+            indicator_obj = executor.trade_account.get_trade_indicator()
+            indicator_dict[key] = (indicator_df, indicator_obj)
+
+        return_value.update({"portfolio_dict": portfolio_dict, "indicator_dict": indicator_dict})
```

## qlib/backtest/decision.py

 * *Ordering differences only*

```diff
@@ -1,596 +1,596 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-from __future__ import annotations
-
-from abc import abstractmethod
-from datetime import time
-from enum import IntEnum
-
-# try to fix circular imports when enabling type hints
-from typing import TYPE_CHECKING, Any, ClassVar, Generic, List, Optional, Tuple, TypeVar, Union, cast
-
-from qlib.backtest.utils import TradeCalendarManager
-from qlib.data.data import Cal
-from qlib.log import get_module_logger
-from qlib.utils.time import concat_date_time, epsilon_change
-
-if TYPE_CHECKING:
-    from qlib.strategy.base import BaseStrategy
-    from qlib.backtest.exchange import Exchange
-
-from dataclasses import dataclass
-
-import numpy as np
-import pandas as pd
-
-DecisionType = TypeVar("DecisionType")
-
-
-class OrderDir(IntEnum):
-    # Order direction
-    SELL = 0
-    BUY = 1
-
-
-@dataclass
-class Order:
-    """
-    stock_id : str
-    amount : float
-    start_time : pd.Timestamp
-        closed start time for order trading
-    end_time : pd.Timestamp
-        closed end time for order trading
-    direction : int
-        Order.SELL for sell; Order.BUY for buy
-    factor : float
-            presents the weight factor assigned in Exchange()
-    """
-
-    # 1) time invariant values
-    # - they are set by users and is time-invariant.
-    stock_id: str
-    amount: float  # `amount` is a non-negative and adjusted value
-    direction: OrderDir
-
-    # 2) time variant values:
-    # - Users may want to set these values when using lower level APIs
-    # - If users don't, TradeDecisionWO will help users to set them
-    # The interval of the order which belongs to (NOTE: this is not the expected order dealing range time)
-    start_time: pd.Timestamp
-    end_time: pd.Timestamp
-
-    # 3) results
-    # - users should not care about these values
-    # - they are set by the backtest system after finishing the results.
-    # What the value should be about in all kinds of cases
-    # - not tradable: the deal_amount == 0 , factor is None
-    #    - the stock is suspended and the entire order fails. No cost for this order
-    # - dealt or partially dealt: deal_amount >= 0 and factor is not None
-    deal_amount: float = 0.0  # `deal_amount` is a non-negative value
-    factor: Optional[float] = None
-
-    # TODO:
-    # a status field to indicate the dealing result of the order
-
-    # FIXME:
-    # for compatible now.
-    # Please remove them in the future
-    SELL: ClassVar[OrderDir] = OrderDir.SELL
-    BUY: ClassVar[OrderDir] = OrderDir.BUY
-
-    def __post_init__(self) -> None:
-        if self.direction not in {Order.SELL, Order.BUY}:
-            raise NotImplementedError("direction not supported, `Order.SELL` for sell, `Order.BUY` for buy")
-        self.deal_amount = 0.0
-        self.factor = None
-
-    @property
-    def amount_delta(self) -> float:
-        """
-        return the delta of amount.
-        - Positive value indicates buying `amount` of share
-        - Negative value indicates selling `amount` of share
-        """
-        return self.amount * self.sign
-
-    @property
-    def deal_amount_delta(self) -> float:
-        """
-        return the delta of deal_amount.
-        - Positive value indicates buying `deal_amount` of share
-        - Negative value indicates selling `deal_amount` of share
-        """
-        return self.deal_amount * self.sign
-
-    @property
-    def sign(self) -> int:
-        """
-        return the sign of trading
-        - `+1` indicates buying
-        - `-1` value indicates selling
-        """
-        return self.direction * 2 - 1
-
-    @staticmethod
-    def parse_dir(direction: Union[str, int, np.integer, OrderDir, np.ndarray]) -> Union[OrderDir, np.ndarray]:
-        if isinstance(direction, OrderDir):
-            return direction
-        elif isinstance(direction, (int, float, np.integer, np.floating)):
-            return Order.BUY if direction > 0 else Order.SELL
-        elif isinstance(direction, str):
-            dl = direction.lower().strip()
-            if dl == "sell":
-                return OrderDir.SELL
-            elif dl == "buy":
-                return OrderDir.BUY
-            else:
-                raise NotImplementedError(f"This type of input is not supported")
-        elif isinstance(direction, np.ndarray):
-            direction_array = direction.copy()
-            direction_array[direction_array > 0] = Order.BUY
-            direction_array[direction_array <= 0] = Order.SELL
-            return direction_array
-        else:
-            raise NotImplementedError(f"This type of input is not supported")
-
-    @property
-    def key_by_day(self) -> tuple:
-        """A hashable & unique key to identify this order, under the granularity in day."""
-        return self.stock_id, self.date, self.direction
-
-    @property
-    def key(self) -> tuple:
-        """A hashable & unique key to identify this order."""
-        return self.stock_id, self.start_time, self.end_time, self.direction
-
-    @property
-    def date(self) -> pd.Timestamp:
-        """Date of the order."""
-        return pd.Timestamp(self.start_time.replace(hour=0, minute=0, second=0))
-
-
-class OrderHelper:
-    """
-    Motivation
-    - Make generating order easier
-        - User may have no knowledge about the adjust-factor information about the system.
-        - It involves too much interaction with the exchange when generating orders.
-    """
-
-    def __init__(self, exchange: Exchange) -> None:
-        self.exchange = exchange
-
-    @staticmethod
-    def create(
-        code: str,
-        amount: float,
-        direction: OrderDir,
-        start_time: Union[str, pd.Timestamp] = None,
-        end_time: Union[str, pd.Timestamp] = None,
-    ) -> Order:
-        """
-        help to create a order
-
-        # TODO: create order for unadjusted amount order
-
-        Parameters
-        ----------
-        code : str
-            the id of the instrument
-        amount : float
-            **adjusted trading amount**
-        direction : OrderDir
-            trading  direction
-        start_time : Union[str, pd.Timestamp] (optional)
-            The interval of the order which belongs to
-        end_time : Union[str, pd.Timestamp] (optional)
-            The interval of the order which belongs to
-
-        Returns
-        -------
-        Order:
-            The created order
-        """
-        # NOTE: factor is a value belongs to the results section. User don't have to care about it when creating orders
-        return Order(
-            stock_id=code,
-            amount=amount,
-            start_time=None if start_time is None else pd.Timestamp(start_time),
-            end_time=None if end_time is None else pd.Timestamp(end_time),
-            direction=direction,
-        )
-
-
-class TradeRange:
-    @abstractmethod
-    def __call__(self, trade_calendar: TradeCalendarManager) -> Tuple[int, int]:
-        """
-        This method will be call with following way
-
-        The outer strategy give a decision with with `TradeRange`
-        The decision will be checked by the inner decision.
-        inner decision will pass its trade_calendar as parameter when getting the trading range
-        - The framework's step is integer-index based.
-
-        Parameters
-        ----------
-        trade_calendar : TradeCalendarManager
-            the trade_calendar is from inner strategy
-
-        Returns
-        -------
-        Tuple[int, int]:
-            the start index and end index which are tradable
-
-        Raises
-        ------
-        NotImplementedError:
-            Exceptions are raised when no range limitation
-        """
-        raise NotImplementedError(f"Please implement the `__call__` method")
-
-    @abstractmethod
-    def clip_time_range(self, start_time: pd.Timestamp, end_time: pd.Timestamp) -> Tuple[pd.Timestamp, pd.Timestamp]:
-        """
-        Parameters
-        ----------
-        start_time : pd.Timestamp
-        end_time : pd.Timestamp
-            Both sides (start_time, end_time) are closed
-
-        Returns
-        -------
-        Tuple[pd.Timestamp, pd.Timestamp]:
-            The tradable time range.
-            - It is intersection of [start_time, end_time] and the rule of TradeRange itself
-        """
-        raise NotImplementedError(f"Please implement the `clip_time_range` method")
-
-
-class IdxTradeRange(TradeRange):
-    def __init__(self, start_idx: int, end_idx: int) -> None:
-        self._start_idx = start_idx
-        self._end_idx = end_idx
-
-    def __call__(self, trade_calendar: TradeCalendarManager | None = None) -> Tuple[int, int]:
-        return self._start_idx, self._end_idx
-
-    def clip_time_range(self, start_time: pd.Timestamp, end_time: pd.Timestamp) -> Tuple[pd.Timestamp, pd.Timestamp]:
-        raise NotImplementedError
-
-
-class TradeRangeByTime(TradeRange):
-    """This is a helper function for make decisions"""
-
-    def __init__(self, start_time: str | time, end_time: str | time) -> None:
-        """
-        This is a callable class.
-
-        **NOTE**:
-        - It is designed for minute-bar for intra-day trading!!!!!
-        - Both start_time and end_time are **closed** in the range
-
-        Parameters
-        ----------
-        start_time : str | time
-            e.g. "9:30"
-        end_time : str | time
-            e.g. "14:30"
-        """
-        self.start_time = pd.Timestamp(start_time).time() if isinstance(start_time, str) else start_time
-        self.end_time = pd.Timestamp(end_time).time() if isinstance(end_time, str) else end_time
-        assert self.start_time < self.end_time
-
-    def __call__(self, trade_calendar: TradeCalendarManager) -> Tuple[int, int]:
-        if trade_calendar is None:
-            raise NotImplementedError("trade_calendar is necessary for getting TradeRangeByTime.")
-
-        start_date = trade_calendar.start_time.date()
-        val_start, val_end = concat_date_time(start_date, self.start_time), concat_date_time(start_date, self.end_time)
-        return trade_calendar.get_range_idx(val_start, val_end)
-
-    def clip_time_range(self, start_time: pd.Timestamp, end_time: pd.Timestamp) -> Tuple[pd.Timestamp, pd.Timestamp]:
-        start_date = start_time.date()
-        val_start, val_end = concat_date_time(start_date, self.start_time), concat_date_time(start_date, self.end_time)
-        # NOTE: `end_date` should not be used. Because the `end_date` is for slicing. It may be in the next day
-        # Assumption: start_time and end_time is for intra-day trading. So it is OK for only using start_date
-        return max(val_start, start_time), min(val_end, end_time)
-
-
-class BaseTradeDecision(Generic[DecisionType]):
-    """
-    Trade decisions are made by strategy and executed by executor
-
-    Motivation:
-        Here are several typical scenarios for `BaseTradeDecision`
-
-        Case 1:
-        1. Outer strategy makes a decision. The decision is not available at the start of current interval
-        2. After a period of time, the decision are updated and become available
-        3. The inner strategy try to get the decision and start to execute the decision according to `get_range_limit`
-        Case 2:
-        1. The outer strategy's decision is available at the start of the interval
-        2. Same as `case 1.3`
-    """
-
-    def __init__(self, strategy: BaseStrategy, trade_range: Union[Tuple[int, int], TradeRange, None] = None) -> None:
-        """
-        Parameters
-        ----------
-        strategy : BaseStrategy
-            The strategy who make the decision
-        trade_range: Union[Tuple[int, int], Callable] (optional)
-            The index range for underlying strategy.
-
-            Here are two examples of trade_range for each type
-
-            1) Tuple[int, int]
-            start_index and end_index of the underlying strategy(both sides are closed)
-
-            2) TradeRange
-
-        """
-        self.strategy = strategy
-        self.start_time, self.end_time = strategy.trade_calendar.get_step_time()
-        # upper strategy has no knowledge about the sub executor before `_init_sub_trading`
-        self.total_step: Optional[int] = None
-        if isinstance(trade_range, tuple):
-            # for Tuple[int, int]
-            trade_range = IdxTradeRange(*trade_range)
-        self.trade_range: Optional[TradeRange] = trade_range
-
-    def get_decision(self) -> List[DecisionType]:
-        """
-        get the **concrete decision**  (e.g. execution orders)
-        This will be called by the inner strategy
-
-        Returns
-        -------
-        List[DecisionType:
-            The decision result. Typically it is some orders
-            Example:
-                []:
-                    Decision not available
-                [concrete_decision]:
-                    available
-        """
-        raise NotImplementedError(f"This type of input is not supported")
-
-    def update(self, trade_calendar: TradeCalendarManager) -> Optional[BaseTradeDecision]:
-        """
-        Be called at the **start** of each step.
-
-        This function is design for following purpose
-        1) Leave a hook for the strategy who make `self` decision to update the decision itself
-        2) Update some information from the inner executor calendar
-
-        Parameters
-        ----------
-        trade_calendar : TradeCalendarManager
-            The calendar of the **inner strategy**!!!!!
-
-        Returns
-        -------
-        BaseTradeDecision:
-            New update, use new decision. If no updates, return None (use previous decision (or unavailable))
-        """
-        # purpose 1)
-        self.total_step = trade_calendar.get_trade_len()
-
-        # purpose 2)
-        return self.strategy.update_trade_decision(self, trade_calendar)
-
-    def _get_range_limit(self, **kwargs: Any) -> Tuple[int, int]:
-        if self.trade_range is not None:
-            return self.trade_range(trade_calendar=cast(TradeCalendarManager, kwargs.get("inner_calendar")))
-        else:
-            raise NotImplementedError("The decision didn't provide an index range")
-
-    def get_range_limit(self, **kwargs: Any) -> Tuple[int, int]:
-        """
-        return the expected step range for limiting the decision execution time
-        Both left and right are **closed**
-
-        if no available trade_range, `default_value` will be returned
-
-        It is only used in `NestedExecutor`
-        - The outmost strategy will not follow any range limit (but it may give range_limit)
-        - The inner most strategy's range_limit will be useless due to atomic executors don't have such
-          features.
-
-        **NOTE**:
-        1) This function must be called after `self.update` in following cases(ensured by NestedExecutor):
-        - user relies on the auto-clip feature of `self.update`
-
-        2) This function will be called after _init_sub_trading in NestedExecutor.
-
-        Parameters
-        ----------
-        **kwargs:
-            {
-                "default_value": <default_value>, # using dict is for distinguish no value provided or None provided
-                "inner_calendar": <trade calendar of inner strategy>
-                # because the range limit  will control the step range of inner strategy, inner calendar will be a
-                # important parameter when trade_range is callable
-            }
-
-        Returns
-        -------
-        Tuple[int, int]:
-
-        Raises
-        ------
-        NotImplementedError:
-            If the following criteria meet
-            1) the decision can't provide a unified start and end
-            2) default_value is not provided
-        """
-        try:
-            _start_idx, _end_idx = self._get_range_limit(**kwargs)
-        except NotImplementedError as e:
-            if "default_value" in kwargs:
-                return kwargs["default_value"]
-            else:
-                # Default to get full index
-                raise NotImplementedError(f"The decision didn't provide an index range") from e
-
-        # clip index
-        if getattr(self, "total_step", None) is not None:
-            # if `self.update` is called.
-            # Then the _start_idx, _end_idx should be clipped
-            assert self.total_step is not None
-            if _start_idx < 0 or _end_idx >= self.total_step:
-                logger = get_module_logger("decision")
-                logger.warning(
-                    f"[{_start_idx},{_end_idx}] go beyond the total_step({self.total_step}), it will be clipped.",
-                )
-                _start_idx, _end_idx = max(0, _start_idx), min(self.total_step - 1, _end_idx)
-        return _start_idx, _end_idx
-
-    def get_data_cal_range_limit(self, rtype: str = "full", raise_error: bool = False) -> Tuple[int, int]:
-        """
-        get the range limit based on data calendar
-
-        NOTE: it is **total** range limit instead of a single step
-
-        The following assumptions are made
-        1) The frequency of the exchange in common_infra is the same as the data calendar
-        2) Users want the index mod by **day** (i.e. 240 min)
-
-        Parameters
-        ----------
-        rtype: str
-            - "full": return the full limitation of the decision in the day
-            - "step": return the limitation of current step
-
-        raise_error: bool
-            True: raise error if no trade_range is set
-            False: return full trade calendar.
-
-            It is useful in following cases
-            - users want to follow the order specific trading time range when decision level trade range is not
-              available. Raising NotImplementedError to indicates that range limit is not available
-
-        Returns
-        -------
-        Tuple[int, int]:
-            the range limit in data calendar
-
-        Raises
-        ------
-        NotImplementedError:
-            If the following criteria meet
-            1) the decision can't provide a unified start and end
-            2) raise_error is True
-        """
-        # potential performance issue
-        day_start = pd.Timestamp(self.start_time.date())
-        day_end = epsilon_change(day_start + pd.Timedelta(days=1))
-        freq = self.strategy.trade_exchange.freq
-        _, _, day_start_idx, day_end_idx = Cal.locate_index(day_start, day_end, freq=freq)
-        if self.trade_range is None:
-            if raise_error:
-                raise NotImplementedError(f"There is no trade_range in this case")
-            else:
-                return 0, day_end_idx - day_start_idx
-        else:
-            if rtype == "full":
-                val_start, val_end = self.trade_range.clip_time_range(day_start, day_end)
-            elif rtype == "step":
-                val_start, val_end = self.trade_range.clip_time_range(self.start_time, self.end_time)
-            else:
-                raise ValueError(f"This type of input {rtype} is not supported")
-            _, _, start_idx, end_index = Cal.locate_index(val_start, val_end, freq=freq)
-            return start_idx - day_start_idx, end_index - day_start_idx
-
-    def empty(self) -> bool:
-        for obj in self.get_decision():
-            if isinstance(obj, Order):
-                # Zero amount order will be treated as empty
-                if obj.amount > 1e-6:
-                    return False
-            else:
-                return True
-        return True
-
-    def mod_inner_decision(self, inner_trade_decision: BaseTradeDecision) -> None:
-        """
-        This method will be called on the inner_trade_decision after it is generated.
-        `inner_trade_decision` will be changed **inplace**.
-
-        Motivation of the `mod_inner_decision`
-        - Leave a hook for outer decision to affect the decision generated by the inner strategy
-            - e.g. the outmost strategy generate a time range for trading. But the upper layer can only affect the
-              nearest layer in the original design.  With `mod_inner_decision`, the decision can passed through multiple
-              layers
-
-        Parameters
-        ----------
-        inner_trade_decision : BaseTradeDecision
-        """
-        # base class provide a default behaviour to modify inner_trade_decision
-        # trade_range should be propagated when inner trade_range is not set
-        if inner_trade_decision.trade_range is None:
-            inner_trade_decision.trade_range = self.trade_range
-
-
-class EmptyTradeDecision(BaseTradeDecision[object]):
-    def get_decision(self) -> List[object]:
-        return []
-
-    def empty(self) -> bool:
-        return True
-
-
-class TradeDecisionWO(BaseTradeDecision[Order]):
-    """
-    Trade Decision (W)ith (O)rder.
-    Besides, the time_range is also included.
-    """
-
-    def __init__(
-        self,
-        order_list: List[Order],
-        strategy: BaseStrategy,
-        trade_range: Union[Tuple[int, int], TradeRange, None] = None,
-    ) -> None:
-        super().__init__(strategy, trade_range=trade_range)
-        self.order_list = cast(List[Order], order_list)
-        start, end = strategy.trade_calendar.get_step_time()
-        for o in order_list:
-            assert isinstance(o, Order)
-            if o.start_time is None:
-                o.start_time = start
-            if o.end_time is None:
-                o.end_time = end
-
-    def get_decision(self) -> List[Order]:
-        return self.order_list
-
-    def __repr__(self) -> str:
-        return (
-            f"class: {self.__class__.__name__}; "
-            f"strategy: {self.strategy}; "
-            f"trade_range: {self.trade_range}; "
-            f"order_list[{len(self.order_list)}]"
-        )
-
-
-class TradeDecisionWithDetails(TradeDecisionWO):
-    """
-    Decision with detail information.
-    Detail information is used to generate execution reports.
-    """
-
-    def __init__(
-        self,
-        order_list: List[Order],
-        strategy: BaseStrategy,
-        trade_range: Optional[Tuple[int, int]] = None,
-        details: Optional[Any] = None,
-    ) -> None:
-        super().__init__(order_list, strategy, trade_range)
-
-        self.details = details
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+from __future__ import annotations
+
+from abc import abstractmethod
+from datetime import time
+from enum import IntEnum
+
+# try to fix circular imports when enabling type hints
+from typing import TYPE_CHECKING, Any, ClassVar, Generic, List, Optional, Tuple, TypeVar, Union, cast
+
+from qlib.backtest.utils import TradeCalendarManager
+from qlib.data.data import Cal
+from qlib.log import get_module_logger
+from qlib.utils.time import concat_date_time, epsilon_change
+
+if TYPE_CHECKING:
+    from qlib.strategy.base import BaseStrategy
+    from qlib.backtest.exchange import Exchange
+
+from dataclasses import dataclass
+
+import numpy as np
+import pandas as pd
+
+DecisionType = TypeVar("DecisionType")
+
+
+class OrderDir(IntEnum):
+    # Order direction
+    SELL = 0
+    BUY = 1
+
+
+@dataclass
+class Order:
+    """
+    stock_id : str
+    amount : float
+    start_time : pd.Timestamp
+        closed start time for order trading
+    end_time : pd.Timestamp
+        closed end time for order trading
+    direction : int
+        Order.SELL for sell; Order.BUY for buy
+    factor : float
+            presents the weight factor assigned in Exchange()
+    """
+
+    # 1) time invariant values
+    # - they are set by users and is time-invariant.
+    stock_id: str
+    amount: float  # `amount` is a non-negative and adjusted value
+    direction: OrderDir
+
+    # 2) time variant values:
+    # - Users may want to set these values when using lower level APIs
+    # - If users don't, TradeDecisionWO will help users to set them
+    # The interval of the order which belongs to (NOTE: this is not the expected order dealing range time)
+    start_time: pd.Timestamp
+    end_time: pd.Timestamp
+
+    # 3) results
+    # - users should not care about these values
+    # - they are set by the backtest system after finishing the results.
+    # What the value should be about in all kinds of cases
+    # - not tradable: the deal_amount == 0 , factor is None
+    #    - the stock is suspended and the entire order fails. No cost for this order
+    # - dealt or partially dealt: deal_amount >= 0 and factor is not None
+    deal_amount: float = 0.0  # `deal_amount` is a non-negative value
+    factor: Optional[float] = None
+
+    # TODO:
+    # a status field to indicate the dealing result of the order
+
+    # FIXME:
+    # for compatible now.
+    # Please remove them in the future
+    SELL: ClassVar[OrderDir] = OrderDir.SELL
+    BUY: ClassVar[OrderDir] = OrderDir.BUY
+
+    def __post_init__(self) -> None:
+        if self.direction not in {Order.SELL, Order.BUY}:
+            raise NotImplementedError("direction not supported, `Order.SELL` for sell, `Order.BUY` for buy")
+        self.deal_amount = 0.0
+        self.factor = None
+
+    @property
+    def amount_delta(self) -> float:
+        """
+        return the delta of amount.
+        - Positive value indicates buying `amount` of share
+        - Negative value indicates selling `amount` of share
+        """
+        return self.amount * self.sign
+
+    @property
+    def deal_amount_delta(self) -> float:
+        """
+        return the delta of deal_amount.
+        - Positive value indicates buying `deal_amount` of share
+        - Negative value indicates selling `deal_amount` of share
+        """
+        return self.deal_amount * self.sign
+
+    @property
+    def sign(self) -> int:
+        """
+        return the sign of trading
+        - `+1` indicates buying
+        - `-1` value indicates selling
+        """
+        return self.direction * 2 - 1
+
+    @staticmethod
+    def parse_dir(direction: Union[str, int, np.integer, OrderDir, np.ndarray]) -> Union[OrderDir, np.ndarray]:
+        if isinstance(direction, OrderDir):
+            return direction
+        elif isinstance(direction, (int, float, np.integer, np.floating)):
+            return Order.BUY if direction > 0 else Order.SELL
+        elif isinstance(direction, str):
+            dl = direction.lower().strip()
+            if dl == "sell":
+                return OrderDir.SELL
+            elif dl == "buy":
+                return OrderDir.BUY
+            else:
+                raise NotImplementedError(f"This type of input is not supported")
+        elif isinstance(direction, np.ndarray):
+            direction_array = direction.copy()
+            direction_array[direction_array > 0] = Order.BUY
+            direction_array[direction_array <= 0] = Order.SELL
+            return direction_array
+        else:
+            raise NotImplementedError(f"This type of input is not supported")
+
+    @property
+    def key_by_day(self) -> tuple:
+        """A hashable & unique key to identify this order, under the granularity in day."""
+        return self.stock_id, self.date, self.direction
+
+    @property
+    def key(self) -> tuple:
+        """A hashable & unique key to identify this order."""
+        return self.stock_id, self.start_time, self.end_time, self.direction
+
+    @property
+    def date(self) -> pd.Timestamp:
+        """Date of the order."""
+        return pd.Timestamp(self.start_time.replace(hour=0, minute=0, second=0))
+
+
+class OrderHelper:
+    """
+    Motivation
+    - Make generating order easier
+        - User may have no knowledge about the adjust-factor information about the system.
+        - It involves too much interaction with the exchange when generating orders.
+    """
+
+    def __init__(self, exchange: Exchange) -> None:
+        self.exchange = exchange
+
+    @staticmethod
+    def create(
+        code: str,
+        amount: float,
+        direction: OrderDir,
+        start_time: Union[str, pd.Timestamp] = None,
+        end_time: Union[str, pd.Timestamp] = None,
+    ) -> Order:
+        """
+        help to create a order
+
+        # TODO: create order for unadjusted amount order
+
+        Parameters
+        ----------
+        code : str
+            the id of the instrument
+        amount : float
+            **adjusted trading amount**
+        direction : OrderDir
+            trading  direction
+        start_time : Union[str, pd.Timestamp] (optional)
+            The interval of the order which belongs to
+        end_time : Union[str, pd.Timestamp] (optional)
+            The interval of the order which belongs to
+
+        Returns
+        -------
+        Order:
+            The created order
+        """
+        # NOTE: factor is a value belongs to the results section. User don't have to care about it when creating orders
+        return Order(
+            stock_id=code,
+            amount=amount,
+            start_time=None if start_time is None else pd.Timestamp(start_time),
+            end_time=None if end_time is None else pd.Timestamp(end_time),
+            direction=direction,
+        )
+
+
+class TradeRange:
+    @abstractmethod
+    def __call__(self, trade_calendar: TradeCalendarManager) -> Tuple[int, int]:
+        """
+        This method will be call with following way
+
+        The outer strategy give a decision with with `TradeRange`
+        The decision will be checked by the inner decision.
+        inner decision will pass its trade_calendar as parameter when getting the trading range
+        - The framework's step is integer-index based.
+
+        Parameters
+        ----------
+        trade_calendar : TradeCalendarManager
+            the trade_calendar is from inner strategy
+
+        Returns
+        -------
+        Tuple[int, int]:
+            the start index and end index which are tradable
+
+        Raises
+        ------
+        NotImplementedError:
+            Exceptions are raised when no range limitation
+        """
+        raise NotImplementedError(f"Please implement the `__call__` method")
+
+    @abstractmethod
+    def clip_time_range(self, start_time: pd.Timestamp, end_time: pd.Timestamp) -> Tuple[pd.Timestamp, pd.Timestamp]:
+        """
+        Parameters
+        ----------
+        start_time : pd.Timestamp
+        end_time : pd.Timestamp
+            Both sides (start_time, end_time) are closed
+
+        Returns
+        -------
+        Tuple[pd.Timestamp, pd.Timestamp]:
+            The tradable time range.
+            - It is intersection of [start_time, end_time] and the rule of TradeRange itself
+        """
+        raise NotImplementedError(f"Please implement the `clip_time_range` method")
+
+
+class IdxTradeRange(TradeRange):
+    def __init__(self, start_idx: int, end_idx: int) -> None:
+        self._start_idx = start_idx
+        self._end_idx = end_idx
+
+    def __call__(self, trade_calendar: TradeCalendarManager | None = None) -> Tuple[int, int]:
+        return self._start_idx, self._end_idx
+
+    def clip_time_range(self, start_time: pd.Timestamp, end_time: pd.Timestamp) -> Tuple[pd.Timestamp, pd.Timestamp]:
+        raise NotImplementedError
+
+
+class TradeRangeByTime(TradeRange):
+    """This is a helper function for make decisions"""
+
+    def __init__(self, start_time: str | time, end_time: str | time) -> None:
+        """
+        This is a callable class.
+
+        **NOTE**:
+        - It is designed for minute-bar for intra-day trading!!!!!
+        - Both start_time and end_time are **closed** in the range
+
+        Parameters
+        ----------
+        start_time : str | time
+            e.g. "9:30"
+        end_time : str | time
+            e.g. "14:30"
+        """
+        self.start_time = pd.Timestamp(start_time).time() if isinstance(start_time, str) else start_time
+        self.end_time = pd.Timestamp(end_time).time() if isinstance(end_time, str) else end_time
+        assert self.start_time < self.end_time
+
+    def __call__(self, trade_calendar: TradeCalendarManager) -> Tuple[int, int]:
+        if trade_calendar is None:
+            raise NotImplementedError("trade_calendar is necessary for getting TradeRangeByTime.")
+
+        start_date = trade_calendar.start_time.date()
+        val_start, val_end = concat_date_time(start_date, self.start_time), concat_date_time(start_date, self.end_time)
+        return trade_calendar.get_range_idx(val_start, val_end)
+
+    def clip_time_range(self, start_time: pd.Timestamp, end_time: pd.Timestamp) -> Tuple[pd.Timestamp, pd.Timestamp]:
+        start_date = start_time.date()
+        val_start, val_end = concat_date_time(start_date, self.start_time), concat_date_time(start_date, self.end_time)
+        # NOTE: `end_date` should not be used. Because the `end_date` is for slicing. It may be in the next day
+        # Assumption: start_time and end_time is for intra-day trading. So it is OK for only using start_date
+        return max(val_start, start_time), min(val_end, end_time)
+
+
+class BaseTradeDecision(Generic[DecisionType]):
+    """
+    Trade decisions are made by strategy and executed by executor
+
+    Motivation:
+        Here are several typical scenarios for `BaseTradeDecision`
+
+        Case 1:
+        1. Outer strategy makes a decision. The decision is not available at the start of current interval
+        2. After a period of time, the decision are updated and become available
+        3. The inner strategy try to get the decision and start to execute the decision according to `get_range_limit`
+        Case 2:
+        1. The outer strategy's decision is available at the start of the interval
+        2. Same as `case 1.3`
+    """
+
+    def __init__(self, strategy: BaseStrategy, trade_range: Union[Tuple[int, int], TradeRange, None] = None) -> None:
+        """
+        Parameters
+        ----------
+        strategy : BaseStrategy
+            The strategy who make the decision
+        trade_range: Union[Tuple[int, int], Callable] (optional)
+            The index range for underlying strategy.
+
+            Here are two examples of trade_range for each type
+
+            1) Tuple[int, int]
+            start_index and end_index of the underlying strategy(both sides are closed)
+
+            2) TradeRange
+
+        """
+        self.strategy = strategy
+        self.start_time, self.end_time = strategy.trade_calendar.get_step_time()
+        # upper strategy has no knowledge about the sub executor before `_init_sub_trading`
+        self.total_step: Optional[int] = None
+        if isinstance(trade_range, tuple):
+            # for Tuple[int, int]
+            trade_range = IdxTradeRange(*trade_range)
+        self.trade_range: Optional[TradeRange] = trade_range
+
+    def get_decision(self) -> List[DecisionType]:
+        """
+        get the **concrete decision**  (e.g. execution orders)
+        This will be called by the inner strategy
+
+        Returns
+        -------
+        List[DecisionType:
+            The decision result. Typically it is some orders
+            Example:
+                []:
+                    Decision not available
+                [concrete_decision]:
+                    available
+        """
+        raise NotImplementedError(f"This type of input is not supported")
+
+    def update(self, trade_calendar: TradeCalendarManager) -> Optional[BaseTradeDecision]:
+        """
+        Be called at the **start** of each step.
+
+        This function is design for following purpose
+        1) Leave a hook for the strategy who make `self` decision to update the decision itself
+        2) Update some information from the inner executor calendar
+
+        Parameters
+        ----------
+        trade_calendar : TradeCalendarManager
+            The calendar of the **inner strategy**!!!!!
+
+        Returns
+        -------
+        BaseTradeDecision:
+            New update, use new decision. If no updates, return None (use previous decision (or unavailable))
+        """
+        # purpose 1)
+        self.total_step = trade_calendar.get_trade_len()
+
+        # purpose 2)
+        return self.strategy.update_trade_decision(self, trade_calendar)
+
+    def _get_range_limit(self, **kwargs: Any) -> Tuple[int, int]:
+        if self.trade_range is not None:
+            return self.trade_range(trade_calendar=cast(TradeCalendarManager, kwargs.get("inner_calendar")))
+        else:
+            raise NotImplementedError("The decision didn't provide an index range")
+
+    def get_range_limit(self, **kwargs: Any) -> Tuple[int, int]:
+        """
+        return the expected step range for limiting the decision execution time
+        Both left and right are **closed**
+
+        if no available trade_range, `default_value` will be returned
+
+        It is only used in `NestedExecutor`
+        - The outmost strategy will not follow any range limit (but it may give range_limit)
+        - The inner most strategy's range_limit will be useless due to atomic executors don't have such
+          features.
+
+        **NOTE**:
+        1) This function must be called after `self.update` in following cases(ensured by NestedExecutor):
+        - user relies on the auto-clip feature of `self.update`
+
+        2) This function will be called after _init_sub_trading in NestedExecutor.
+
+        Parameters
+        ----------
+        **kwargs:
+            {
+                "default_value": <default_value>, # using dict is for distinguish no value provided or None provided
+                "inner_calendar": <trade calendar of inner strategy>
+                # because the range limit  will control the step range of inner strategy, inner calendar will be a
+                # important parameter when trade_range is callable
+            }
+
+        Returns
+        -------
+        Tuple[int, int]:
+
+        Raises
+        ------
+        NotImplementedError:
+            If the following criteria meet
+            1) the decision can't provide a unified start and end
+            2) default_value is not provided
+        """
+        try:
+            _start_idx, _end_idx = self._get_range_limit(**kwargs)
+        except NotImplementedError as e:
+            if "default_value" in kwargs:
+                return kwargs["default_value"]
+            else:
+                # Default to get full index
+                raise NotImplementedError(f"The decision didn't provide an index range") from e
+
+        # clip index
+        if getattr(self, "total_step", None) is not None:
+            # if `self.update` is called.
+            # Then the _start_idx, _end_idx should be clipped
+            assert self.total_step is not None
+            if _start_idx < 0 or _end_idx >= self.total_step:
+                logger = get_module_logger("decision")
+                logger.warning(
+                    f"[{_start_idx},{_end_idx}] go beyond the total_step({self.total_step}), it will be clipped.",
+                )
+                _start_idx, _end_idx = max(0, _start_idx), min(self.total_step - 1, _end_idx)
+        return _start_idx, _end_idx
+
+    def get_data_cal_range_limit(self, rtype: str = "full", raise_error: bool = False) -> Tuple[int, int]:
+        """
+        get the range limit based on data calendar
+
+        NOTE: it is **total** range limit instead of a single step
+
+        The following assumptions are made
+        1) The frequency of the exchange in common_infra is the same as the data calendar
+        2) Users want the index mod by **day** (i.e. 240 min)
+
+        Parameters
+        ----------
+        rtype: str
+            - "full": return the full limitation of the decision in the day
+            - "step": return the limitation of current step
+
+        raise_error: bool
+            True: raise error if no trade_range is set
+            False: return full trade calendar.
+
+            It is useful in following cases
+            - users want to follow the order specific trading time range when decision level trade range is not
+              available. Raising NotImplementedError to indicates that range limit is not available
+
+        Returns
+        -------
+        Tuple[int, int]:
+            the range limit in data calendar
+
+        Raises
+        ------
+        NotImplementedError:
+            If the following criteria meet
+            1) the decision can't provide a unified start and end
+            2) raise_error is True
+        """
+        # potential performance issue
+        day_start = pd.Timestamp(self.start_time.date())
+        day_end = epsilon_change(day_start + pd.Timedelta(days=1))
+        freq = self.strategy.trade_exchange.freq
+        _, _, day_start_idx, day_end_idx = Cal.locate_index(day_start, day_end, freq=freq)
+        if self.trade_range is None:
+            if raise_error:
+                raise NotImplementedError(f"There is no trade_range in this case")
+            else:
+                return 0, day_end_idx - day_start_idx
+        else:
+            if rtype == "full":
+                val_start, val_end = self.trade_range.clip_time_range(day_start, day_end)
+            elif rtype == "step":
+                val_start, val_end = self.trade_range.clip_time_range(self.start_time, self.end_time)
+            else:
+                raise ValueError(f"This type of input {rtype} is not supported")
+            _, _, start_idx, end_index = Cal.locate_index(val_start, val_end, freq=freq)
+            return start_idx - day_start_idx, end_index - day_start_idx
+
+    def empty(self) -> bool:
+        for obj in self.get_decision():
+            if isinstance(obj, Order):
+                # Zero amount order will be treated as empty
+                if obj.amount > 1e-6:
+                    return False
+            else:
+                return True
+        return True
+
+    def mod_inner_decision(self, inner_trade_decision: BaseTradeDecision) -> None:
+        """
+        This method will be called on the inner_trade_decision after it is generated.
+        `inner_trade_decision` will be changed **inplace**.
+
+        Motivation of the `mod_inner_decision`
+        - Leave a hook for outer decision to affect the decision generated by the inner strategy
+            - e.g. the outmost strategy generate a time range for trading. But the upper layer can only affect the
+              nearest layer in the original design.  With `mod_inner_decision`, the decision can passed through multiple
+              layers
+
+        Parameters
+        ----------
+        inner_trade_decision : BaseTradeDecision
+        """
+        # base class provide a default behaviour to modify inner_trade_decision
+        # trade_range should be propagated when inner trade_range is not set
+        if inner_trade_decision.trade_range is None:
+            inner_trade_decision.trade_range = self.trade_range
+
+
+class EmptyTradeDecision(BaseTradeDecision[object]):
+    def get_decision(self) -> List[object]:
+        return []
+
+    def empty(self) -> bool:
+        return True
+
+
+class TradeDecisionWO(BaseTradeDecision[Order]):
+    """
+    Trade Decision (W)ith (O)rder.
+    Besides, the time_range is also included.
+    """
+
+    def __init__(
+        self,
+        order_list: List[Order],
+        strategy: BaseStrategy,
+        trade_range: Union[Tuple[int, int], TradeRange, None] = None,
+    ) -> None:
+        super().__init__(strategy, trade_range=trade_range)
+        self.order_list = cast(List[Order], order_list)
+        start, end = strategy.trade_calendar.get_step_time()
+        for o in order_list:
+            assert isinstance(o, Order)
+            if o.start_time is None:
+                o.start_time = start
+            if o.end_time is None:
+                o.end_time = end
+
+    def get_decision(self) -> List[Order]:
+        return self.order_list
+
+    def __repr__(self) -> str:
+        return (
+            f"class: {self.__class__.__name__}; "
+            f"strategy: {self.strategy}; "
+            f"trade_range: {self.trade_range}; "
+            f"order_list[{len(self.order_list)}]"
+        )
+
+
+class TradeDecisionWithDetails(TradeDecisionWO):
+    """
+    Decision with detail information.
+    Detail information is used to generate execution reports.
+    """
+
+    def __init__(
+        self,
+        order_list: List[Order],
+        strategy: BaseStrategy,
+        trade_range: Optional[Tuple[int, int]] = None,
+        details: Optional[Any] = None,
+    ) -> None:
+        super().__init__(order_list, strategy, trade_range)
+
+        self.details = details
```

## qlib/backtest/exchange.py

```diff
@@ -1,958 +1,957 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-from __future__ import annotations
-
-from collections import defaultdict
-from typing import TYPE_CHECKING, Any, Dict, List, Optional, Tuple, Type, Union, cast
-
-from ..utils.index_data import IndexData
-
-if TYPE_CHECKING:
-    from .account import Account
-
-import random
-
-import numpy as np
-import pandas as pd
-
-from qlib.backtest.position import BasePosition
-
-from ..config import C
-from ..constant import REG_CN, REG_TW
-from ..data.data import D
-from ..log import get_module_logger
-from .decision import Order, OrderDir, OrderHelper
-from .high_performance_ds import BaseQuote, NumpyQuote
-
-
-class Exchange:
-    # `quote_df` is a pd.DataFrame class that contains basic information for backtesting
-    # After some processing, the data will later be maintained by `quote_cls` object for faster data retrieving.
-    # Some conventions for `quote_df`
-    # - $close is for calculating the total value at end of each day.
-    #   - if $close is None, the stock on that day is regarded as suspended.
-    # - $factor is for rounding to the trading unit;
-    #   - if any $factor is missing when $close exists, trading unit rounding will be disabled
-    quote_df: pd.DataFrame
-
-    def __init__(
-        self,
-        freq: str = "day",
-        start_time: Union[pd.Timestamp, str] = None,
-        end_time: Union[pd.Timestamp, str] = None,
-        codes: Union[list, str] = "all",
-        deal_price: Union[str, Tuple[str, str], List[str], None] = None,
-        subscribe_fields: list = [],
-        limit_threshold: Union[Tuple[str, str], float, None] = None,
-        volume_threshold: Union[tuple, dict, None] = None,
-        open_cost: float = 0.0015,
-        close_cost: float = 0.0025,
-        min_cost: float = 5.0,
-        impact_cost: float = 0.0,
-        extra_quote: pd.DataFrame = None,
-        quote_cls: Type[BaseQuote] = NumpyQuote,
-        **kwargs: Any,
-    ) -> None:
-        """__init__
-        :param freq:             frequency of data
-        :param start_time:       closed start time for backtest
-        :param end_time:         closed end time for backtest
-        :param codes:            list stock_id list or a string of instruments(i.e. all, csi500, sse50)
-        :param deal_price:      Union[str, Tuple[str, str], List[str]]
-                                The `deal_price` supports following two types of input
-                                - <deal_price> : str
-                                - (<buy_price>, <sell_price>): Tuple[str] or List[str]
-                                <deal_price>, <buy_price> or <sell_price> := <price>
-                                <price> := str
-                                - for example '$close', '$open', '$vwap' ("close" is OK. `Exchange` will help to prepend
-                                  "$" to the expression)
-        :param subscribe_fields: list, subscribe fields. This expressions will be added to the query and `self.quote`.
-                                 It is useful when users want more fields to be queried
-        :param limit_threshold: Union[Tuple[str, str], float, None]
-                                1) `None`: no limitation
-                                2) float, 0.1 for example, default None
-                                3) Tuple[str, str]: (<the expression for buying stock limitation>,
-                                                     <the expression for sell stock limitation>)
-                                                    `False` value indicates the stock is tradable
-                                                    `True` value indicates the stock is limited and not tradable
-        :param volume_threshold: Union[
-                                    Dict[
-                                        "all": ("cum" or "current", limit_str),
-                                        "buy": ("cum" or "current", limit_str),
-                                        "sell":("cum" or "current", limit_str),
-                                    ],
-                                    ("cum" or "current", limit_str),
-                                 ]
-                                1) ("cum" or "current", limit_str) denotes a single volume limit.
-                                    - limit_str is qlib data expression which is allowed to define your own Operator.
-                                    Please refer to qlib/contrib/ops/high_freq.py, here are any custom operator for
-                                    high frequency, such as DayCumsum. !!!NOTE: if you want you use the custom
-                                    operator, you need to register it in qlib_init.
-                                    - "cum" means that this is a cumulative value over time, such as cumulative market
-                                    volume. So when it is used as a volume limit, it is necessary to subtract the dealt
-                                    amount.
-                                    - "current" means that this is a real-time value and will not accumulate over time,
-                                    so it can be directly used as a capacity limit.
-                                    e.g. ("cum", "0.2 * DayCumsum($volume, '9:45', '14:45')"), ("current", "$bidV1")
-                                2) "all" means the volume limits are both buying and selling.
-                                "buy" means the volume limits of buying. "sell" means the volume limits of selling.
-                                Different volume limits will be aggregated with min(). If volume_threshold is only
-                                ("cum" or "current", limit_str) instead of a dict, the volume limits are for
-                                both by default. In other words, it is same as {"all": ("cum" or "current", limit_str)}.
-                                3) e.g. "volume_threshold": {
-                                            "all": ("cum", "0.2 * DayCumsum($volume, '9:45', '14:45')"),
-                                            "buy": ("current", "$askV1"),
-                                            "sell": ("current", "$bidV1"),
-                                        }
-        :param open_cost:        cost rate for open, default 0.0015
-        :param close_cost:       cost rate for close, default 0.0025
-        :param trade_unit:       trade unit, 100 for China A market.
-                                 None for disable trade unit.
-                                 **NOTE**: `trade_unit` is included in the `kwargs`. It is necessary because we must
-                                 distinguish `not set` and `disable trade_unit`
-        :param min_cost:         min cost, default 5
-        :param impact_cost:     market impact cost rate (a.k.a. slippage). A recommended value is 0.1.
-        :param extra_quote:     pandas, dataframe consists of
-                                    columns: like ['$vwap', '$close', '$volume', '$factor', 'limit_sell', 'limit_buy'].
-                                            The limit indicates that the etf is tradable on a specific day.
-                                            Necessary fields:
-                                                $close is for calculating the total value at end of each day.
-                                            Optional fields:
-                                                $volume is only necessary when we limit the trade amount or calculate
-                                                PA(vwap) indicator
-                                                $vwap is only necessary when we use the $vwap price as the deal price
-                                                $factor is for rounding to the trading unit
-                                                limit_sell will be set to False by default (False indicates we can sell
-                                                this target on this day).
-                                                limit_buy will be set to False by default (False indicates we can buy
-                                                this target on this day).
-                                    index: MultipleIndex(instrument, pd.Datetime)
-        """
-        self.freq = freq
-        self.start_time = start_time
-        self.end_time = end_time
-
-        self.trade_unit = kwargs.pop("trade_unit", C.trade_unit)
-        if len(kwargs) > 0:
-            raise ValueError(f"Get Unexpected arguments {kwargs}")
-
-        if limit_threshold is None:
-            limit_threshold = C.limit_threshold
-        if deal_price is None:
-            deal_price = C.deal_price
-
-        # we have some verbose information here. So logging is enabled
-        self.logger = get_module_logger("online operator")
-
-        # TODO: the quote, trade_dates, codes are not necessary.
-        # It is just for performance consideration.
-        self.limit_type = self._get_limit_type(limit_threshold)
-        if limit_threshold is None:
-            if C.region in [REG_CN, REG_TW]:
-                self.logger.warning(f"limit_threshold not set. The stocks hit the limit may be bought/sold")
-        elif self.limit_type == self.LT_FLT and abs(cast(float, limit_threshold)) > 0.1:
-            if C.region in [REG_CN, REG_TW]:
-                self.logger.warning(f"limit_threshold may not be set to a reasonable value")
-
-        if isinstance(deal_price, str):
-            if deal_price[0] != "$":
-                deal_price = "$" + deal_price
-            self.buy_price = self.sell_price = deal_price
-        elif isinstance(deal_price, (tuple, list)):
-            self.buy_price, self.sell_price = cast(Tuple[str, str], deal_price)
-        else:
-            raise NotImplementedError(f"This type of input is not supported")
-
-        if isinstance(codes, str):
-            codes = D.instruments(codes)
-        self.codes = codes
-        # Necessary fields
-        # $close is for calculating the total value at end of each day.
-        # - if $close is None, the stock on that day is regarded as suspended.
-        # $factor is for rounding to the trading unit
-        # $change is for calculating the limit of the stock
-
-        # 　get volume limit from kwargs
-        self.buy_vol_limit, self.sell_vol_limit, vol_lt_fields = self._get_vol_limit(volume_threshold)
-
-        necessary_fields = {self.buy_price, self.sell_price, "$close", "$change", "$factor", "$volume"}
-        if self.limit_type == self.LT_TP_EXP:
-            assert isinstance(limit_threshold, tuple)
-            for exp in limit_threshold:
-                necessary_fields.add(exp)
-        all_fields = list(necessary_fields | set(vol_lt_fields) | set(subscribe_fields))
-
-        self.all_fields = all_fields
-
-        self.open_cost = open_cost
-        self.close_cost = close_cost
-        self.min_cost = min_cost
-        self.impact_cost = impact_cost
-
-        self.limit_threshold: Union[Tuple[str, str], float, None] = limit_threshold
-        self.volume_threshold = volume_threshold
-        self.extra_quote = extra_quote
-        self.get_quote_from_qlib()
-
-        # init quote by quote_df
-        self.quote_cls = quote_cls
-        self.quote: BaseQuote = self.quote_cls(self.quote_df, freq)
-
-    def get_quote_from_qlib(self) -> None:
-        # get stock data from qlib
-        if len(self.codes) == 0:
-            self.codes = D.instruments()
-        self.quote_df = D.features(
-            self.codes,
-            self.all_fields,
-            self.start_time,
-            self.end_time,
-            freq=self.freq,
-            disk_cache=True,
-        )
-        self.quote_df.columns = self.all_fields
-
-        # check buy_price data and sell_price data
-        for attr in ("buy_price", "sell_price"):
-            pstr = getattr(self, attr)  # price string
-            if self.quote_df[pstr].isna().any():
-                self.logger.warning("{} field data contains nan.".format(pstr))
-
-        # update trade_w_adj_price
-        if (self.quote_df["$factor"].isna() & ~self.quote_df["$close"].isna()).any():
-            # The 'factor.day.bin' file not exists, and `factor` field contains `nan`
-            # Use adjusted price
-            self.trade_w_adj_price = True
-            self.logger.warning("factor.day.bin file not exists or factor contains `nan`. Order using adjusted_price.")
-            if self.trade_unit is not None:
-                self.logger.warning(f"trade unit {self.trade_unit} is not supported in adjusted_price mode.")
-        else:
-            # The `factor.day.bin` file exists and all data `close` and `factor` are not `nan`
-            # Use normal price
-            self.trade_w_adj_price = False
-        # update limit
-        self._update_limit(self.limit_threshold)
-
-        # concat extra_quote
-        if self.extra_quote is not None:
-            # process extra_quote
-            if "$close" not in self.extra_quote:
-                raise ValueError("$close is necessray in extra_quote")
-            for attr in "buy_price", "sell_price":
-                pstr = getattr(self, attr)  # price string
-                if pstr not in self.extra_quote.columns:
-                    self.extra_quote[pstr] = self.extra_quote["$close"]
-                    self.logger.warning(f"No {pstr} set for extra_quote. Use $close as {pstr}.")
-            if "$factor" not in self.extra_quote.columns:
-                self.extra_quote["$factor"] = 1.0
-                self.logger.warning("No $factor set for extra_quote. Use 1.0 as $factor.")
-            if "limit_sell" not in self.extra_quote.columns:
-                self.extra_quote["limit_sell"] = False
-                self.logger.warning("No limit_sell set for extra_quote. All stock will be able to be sold.")
-            if "limit_buy" not in self.extra_quote.columns:
-                self.extra_quote["limit_buy"] = False
-                self.logger.warning("No limit_buy set for extra_quote. All stock will be able to be bought.")
-            assert set(self.extra_quote.columns) == set(self.quote_df.columns) - {"$change"}
-            self.quote_df = pd.concat([self.quote_df, self.extra_quote], sort=False, axis=0)
-
-    LT_TP_EXP = "(exp)"  # Tuple[str, str]:  the limitation is calculated by a Qlib expression.
-    LT_FLT = "float"  # float:  the trading limitation is based on `abs($change) < limit_threshold`
-    LT_NONE = "none"  # none:  there is no trading limitation
-
-    def _get_limit_type(self, limit_threshold: Union[tuple, float, None]) -> str:
-        """get limit type"""
-        if isinstance(limit_threshold, tuple):
-            return self.LT_TP_EXP
-        elif isinstance(limit_threshold, float):
-            return self.LT_FLT
-        elif limit_threshold is None:
-            return self.LT_NONE
-        else:
-            raise NotImplementedError(f"This type of `limit_threshold` is not supported")
-
-    def _update_limit(self, limit_threshold: Union[Tuple, float, None]) -> None:
-        # $close may contain NaN, the nan indicates that the stock is not tradable at that timestamp
-        suspended = self.quote_df["$close"].isna()
-        # check limit_threshold
-        limit_type = self._get_limit_type(limit_threshold)
-        if limit_type == self.LT_NONE:
-            self.quote_df["limit_buy"] = suspended
-            self.quote_df["limit_sell"] = suspended
-        elif limit_type == self.LT_TP_EXP:
-            # set limit
-            limit_threshold = cast(tuple, limit_threshold)
-            # astype bool is necessary, because quote_df is an expression and could be float
-            self.quote_df["limit_buy"] = self.quote_df[limit_threshold[0]].astype("bool") | suspended
-            self.quote_df["limit_sell"] = self.quote_df[limit_threshold[1]].astype("bool") | suspended
-        elif limit_type == self.LT_FLT:
-            limit_threshold = cast(float, limit_threshold)
-            self.quote_df["limit_buy"] = self.quote_df["$change"].ge(limit_threshold) | suspended
-            self.quote_df["limit_sell"] = (
-                self.quote_df["$change"].le(-limit_threshold) | suspended
-            )  # pylint: disable=E1130
-
-    @staticmethod
-    def _get_vol_limit(volume_threshold: Union[tuple, dict, None]) -> Tuple[Optional[list], Optional[list], set]:
-        """
-        preprocess the volume limit.
-        get the fields need to get from qlib.
-        get the volume limit list of buying and selling which is composed of all limits.
-        Parameters
-        ----------
-        volume_threshold :
-            please refer to the doc of exchange.
-        Returns
-        -------
-        fields: set
-            the fields need to get from qlib.
-        buy_vol_limit: List[Tuple[str]]
-            all volume limits of buying.
-        sell_vol_limit: List[Tuple[str]]
-            all volume limits of selling.
-        Raises
-        ------
-        ValueError
-            the format of volume_threshold is not supported.
-        """
-        if volume_threshold is None:
-            return None, None, set()
-
-        fields = set()
-        buy_vol_limit = []
-        sell_vol_limit = []
-        if isinstance(volume_threshold, tuple):
-            volume_threshold = {"all": volume_threshold}
-
-        assert isinstance(volume_threshold, dict)
-        for key, vol_limit in volume_threshold.items():
-            assert isinstance(vol_limit, tuple)
-            fields.add(vol_limit[1])
-
-            if key in ("buy", "all"):
-                buy_vol_limit.append(vol_limit)
-            if key in ("sell", "all"):
-                sell_vol_limit.append(vol_limit)
-
-        return buy_vol_limit, sell_vol_limit, fields
-
-    def check_stock_limit(
-        self,
-        stock_id: str,
-        start_time: pd.Timestamp,
-        end_time: pd.Timestamp,
-        direction: int | None = None,
-    ) -> bool:
-        """
-        Parameters
-        ----------
-        stock_id : str
-        start_time: pd.Timestamp
-        end_time: pd.Timestamp
-        direction : int, optional
-            trade direction, by default None
-            - if direction is None, check if tradable for buying and selling.
-            - if direction == Order.BUY, check the if tradable for buying
-            - if direction == Order.SELL, check the sell limit for selling.
-
-        Returns
-        -------
-        True: the trading of the stock is limited (maybe hit the highest/lowest price), hence the stock is not tradable
-        False: the trading of the stock is not limited, hence the stock may be tradable
-        """
-        # NOTE:
-        # **all** is used when checking limitation.
-        # For example, the stock trading is limited in a day if every minute is limited in a day if every minute is limited.
-        if direction is None:
-            # The trading limitation is related to the trading direction
-            # if the direction is not provided, then any limitation from buy or sell will result in trading limitation
-            buy_limit = self.quote.get_data(stock_id, start_time, end_time, field="limit_buy", method="all")
-            sell_limit = self.quote.get_data(stock_id, start_time, end_time, field="limit_sell", method="all")
-            return bool(buy_limit or sell_limit)
-        elif direction == Order.BUY:
-            return cast(bool, self.quote.get_data(stock_id, start_time, end_time, field="limit_buy", method="all"))
-        elif direction == Order.SELL:
-            return cast(bool, self.quote.get_data(stock_id, start_time, end_time, field="limit_sell", method="all"))
-        else:
-            raise ValueError(f"direction {direction} is not supported!")
-
-    def check_stock_suspended(
-        self,
-        stock_id: str,
-        start_time: pd.Timestamp,
-        end_time: pd.Timestamp,
-    ) -> bool:
-        """if stock is suspended(hence not tradable), True will be returned"""
-        # is suspended
-        if stock_id in self.quote.get_all_stock():
-            # suspended stocks are represented by None $close stock
-            # The $close may contain NaN,
-            close = self.quote.get_data(stock_id, start_time, end_time, "$close")
-            if close is None:
-                # if no close record exists
-                return True
-            elif isinstance(close, IndexData):
-                # **any** non-NaN $close represents trading opportunity may exist
-                #  if all returned is nan, then the stock is suspended
-                return cast(bool, cast(IndexData, close).isna().all())
-            else:
-                # it is single value, make sure is not None
-                return np.isnan(close)
-        else:
-            # if the stock is not in the stock list, then it is not tradable and regarded as suspended
-            return True
-
-    def is_stock_tradable(
-        self,
-        stock_id: str,
-        start_time: pd.Timestamp,
-        end_time: pd.Timestamp,
-        direction: int | None = None,
-    ) -> bool:
-        # check if stock can be traded
-        return not (
-            self.check_stock_suspended(stock_id, start_time, end_time)
-            or self.check_stock_limit(stock_id, start_time, end_time, direction)
-        )
-
-    def check_order(self, order: Order) -> bool:
-        # check limit and suspended
-        return self.is_stock_tradable(order.stock_id, order.start_time, order.end_time, order.direction)
-
-    def deal_order(
-        self,
-        order: Order,
-        trade_account: Account | None = None,
-        position: BasePosition | None = None,
-        dealt_order_amount: Dict[str, float] = defaultdict(float),
-    ) -> Tuple[float, float, float]:
-        """
-        Deal order when the actual transaction
-        the results section in `Order` will be changed.
-        :param order:  Deal the order.
-        :param trade_account: Trade account to be updated after dealing the order.
-        :param position: position to be updated after dealing the order.
-        :param dealt_order_amount: the dealt order amount dict with the format of {stock_id: float}
-        :return: trade_val, trade_cost, trade_price
-        """
-        # check order first.
-        if not self.check_order(order):
-            order.deal_amount = 0.0
-            # using np.nan instead of None to make it more convenient to show the value in format string
-            self.logger.debug(f"Order failed due to trading limitation: {order}")
-            return 0.0, 0.0, np.nan
-
-        if trade_account is not None and position is not None:
-            raise ValueError("trade_account and position can only choose one")
-
-        # NOTE: order will be changed in this function
-        trade_price, trade_val, trade_cost = self._calc_trade_info_by_order(
-            order,
-            trade_account.current_position if trade_account else position,
-            dealt_order_amount,
-        )
-        if trade_val > 1e-5:
-            # If the order can only be deal 0 value. Nothing to be updated
-            # Otherwise, it will result in
-            # 1) some stock with 0 value in the position
-            # 2) `trade_unit` of trade_cost will be lost in user account
-            if trade_account:
-                trade_account.update_order(order=order, trade_val=trade_val, cost=trade_cost, trade_price=trade_price)
-            elif position:
-                position.update_order(order=order, trade_val=trade_val, cost=trade_cost, trade_price=trade_price)
-
-        return trade_val, trade_cost, trade_price
-
-    def get_quote_info(
-        self,
-        stock_id: str,
-        start_time: pd.Timestamp,
-        end_time: pd.Timestamp,
-        field: str,
-        method: str = "ts_data_last",
-    ) -> Union[None, int, float, bool, IndexData]:
-        return self.quote.get_data(stock_id, start_time, end_time, field=field, method=method)
-
-    def get_close(
-        self,
-        stock_id: str,
-        start_time: pd.Timestamp,
-        end_time: pd.Timestamp,
-        method: str = "ts_data_last",
-    ) -> Union[None, int, float, bool, IndexData]:
-        return self.quote.get_data(stock_id, start_time, end_time, field="$close", method=method)
-
-    def get_volume(
-        self,
-        stock_id: str,
-        start_time: pd.Timestamp,
-        end_time: pd.Timestamp,
-        method: Optional[str] = "sum",
-    ) -> Union[None, int, float, bool, IndexData]:
-        """get the total deal volume of stock with `stock_id` between the time interval [start_time, end_time)"""
-        return self.quote.get_data(stock_id, start_time, end_time, field="$volume", method=method)
-
-    def get_deal_price(
-        self,
-        stock_id: str,
-        start_time: pd.Timestamp,
-        end_time: pd.Timestamp,
-        direction: OrderDir,
-        method: Optional[str] = "ts_data_last",
-    ) -> Union[None, int, float, bool, IndexData]:
-        if direction == OrderDir.SELL:
-            pstr = self.sell_price
-        elif direction == OrderDir.BUY:
-            pstr = self.buy_price
-        else:
-            raise NotImplementedError(f"This type of input is not supported")
-
-        deal_price = self.quote.get_data(stock_id, start_time, end_time, field=pstr, method=method)
-        if method is not None and (deal_price is None or np.isnan(deal_price) or deal_price <= 1e-08):
-            self.logger.warning(f"(stock_id:{stock_id}, trade_time:{(start_time, end_time)}, {pstr}): {deal_price}!!!")
-            self.logger.warning(f"setting deal_price to close price")
-            deal_price = self.get_close(stock_id, start_time, end_time, method)
-        return deal_price
-
-    def get_factor(
-        self,
-        stock_id: str,
-        start_time: pd.Timestamp,
-        end_time: pd.Timestamp,
-    ) -> Optional[float]:
-        """
-        Returns
-        -------
-        Optional[float]:
-            `None`: if the stock is suspended `None` may be returned
-            `float`: return factor if the factor exists
-        """
-        assert start_time is not None and end_time is not None, "the time range must be given"
-        if stock_id not in self.quote.get_all_stock():
-            return None
-        return self.quote.get_data(stock_id, start_time, end_time, field="$factor", method="ts_data_last")
-
-    def generate_amount_position_from_weight_position(
-        self,
-        weight_position: dict,
-        cash: float,
-        start_time: pd.Timestamp,
-        end_time: pd.Timestamp,
-        direction: OrderDir = OrderDir.BUY,
-    ) -> dict:
-        """
-        Generates the target position according to the weight and the cash.
-        NOTE: All the cash will be assigned to the tradable stock.
-        Parameter:
-        weight_position : dict {stock_id : weight}; allocate cash by weight_position
-            among then, weight must be in this range: 0 < weight < 1
-        cash : cash
-        start_time : the start time point of the step
-        end_time : the end time point of the step
-        direction : the direction of the deal price for estimating the amount
-                    # NOTE: this function is used for calculating target position. So the default direction is buy
-        """
-
-        # calculate the total weight of tradable value
-        tradable_weight = 0.0
-        for stock_id, wp in weight_position.items():
-            if self.is_stock_tradable(stock_id=stock_id, start_time=start_time, end_time=end_time):
-                # weight_position must be greater than 0 and less than 1
-                if wp < 0 or wp > 1:
-                    raise ValueError(
-                        "weight_position is {}, " "weight_position is not in the range of (0, 1).".format(wp),
-                    )
-                tradable_weight += wp
-
-        if tradable_weight - 1.0 >= 1e-5:
-            raise ValueError("tradable_weight is {}, can not greater than 1.".format(tradable_weight))
-
-        amount_dict = {}
-        for stock_id in weight_position:
-            if weight_position[stock_id] > 0.0 and self.is_stock_tradable(
-                stock_id=stock_id,
-                start_time=start_time,
-                end_time=end_time,
-            ):
-                amount_dict[stock_id] = (
-                    cash
-                    * weight_position[stock_id]
-                    / tradable_weight
-                    // self.get_deal_price(
-                        stock_id=stock_id,
-                        start_time=start_time,
-                        end_time=end_time,
-                        direction=direction,
-                    )
-                )
-        return amount_dict
-
-    def get_real_deal_amount(self, current_amount: float, target_amount: float, factor: float | None = None) -> float:
-        """
-        Calculate the real adjust deal amount when considering the trading unit
-        :param current_amount:
-        :param target_amount:
-        :param factor:
-        :return  real_deal_amount;  Positive deal_amount indicates buying more stock.
-        """
-        if current_amount == target_amount:
-            return 0
-        elif current_amount < target_amount:
-            deal_amount = target_amount - current_amount
-            deal_amount = self.round_amount_by_trade_unit(deal_amount, factor)
-            return deal_amount
-        else:
-            if target_amount == 0:
-                return -current_amount
-            else:
-                deal_amount = current_amount - target_amount
-                deal_amount = self.round_amount_by_trade_unit(deal_amount, factor)
-                return -deal_amount
-
-    def generate_order_for_target_amount_position(
-        self,
-        target_position: dict,
-        current_position: dict,
-        start_time: pd.Timestamp,
-        end_time: pd.Timestamp,
-    ) -> List[Order]:
-        """
-        Note: some future information is used in this function
-        Parameter:
-        target_position : dict { stock_id : amount }
-        current_position : dict { stock_id : amount}
-        trade_unit : trade_unit
-        down sample : for amount 321 and trade_unit 100, deal_amount is 300
-        deal order on trade_date
-        """
-        # split buy and sell for further use
-        buy_order_list = []
-        sell_order_list = []
-        # three parts: kept stock_id, dropped stock_id, new stock_id
-        # handle kept stock_id
-
-        # because the order of the set is not fixed, the trading order of the stock is different, so that the backtest
-        # results of the same parameter are different;
-        # so here we sort stock_id, and then randomly shuffle the order of stock_id
-        # because the same random seed is used, the final stock_id order is fixed
-        sorted_ids = sorted(set(list(current_position.keys()) + list(target_position.keys())))
-        random.seed(0)
-        random.shuffle(sorted_ids)
-        for stock_id in sorted_ids:
-
-            # Do not generate order for the non-tradable stocks
-            if not self.is_stock_tradable(stock_id=stock_id, start_time=start_time, end_time=end_time):
-                continue
-
-            target_amount = target_position.get(stock_id, 0)
-            current_amount = current_position.get(stock_id, 0)
-            factor = self.get_factor(stock_id, start_time=start_time, end_time=end_time)
-
-            deal_amount = self.get_real_deal_amount(current_amount, target_amount, factor)
-            if deal_amount == 0:
-                continue
-            if deal_amount > 0:
-                # buy stock
-                buy_order_list.append(
-                    Order(
-                        stock_id=stock_id,
-                        amount=deal_amount,
-                        direction=Order.BUY,
-                        start_time=start_time,
-                        end_time=end_time,
-                        factor=factor,
-                    ),
-                )
-            else:
-                # sell stock
-                sell_order_list.append(
-                    Order(
-                        stock_id=stock_id,
-                        amount=abs(deal_amount),
-                        direction=Order.SELL,
-                        start_time=start_time,
-                        end_time=end_time,
-                        factor=factor,
-                    ),
-                )
-        # return order_list : buy + sell
-        return sell_order_list + buy_order_list
-
-    def calculate_amount_position_value(
-        self,
-        amount_dict: dict,
-        start_time: pd.Timestamp,
-        end_time: pd.Timestamp,
-        only_tradable: bool = False,
-        direction: OrderDir = OrderDir.SELL,
-    ) -> float:
-        """Parameter
-        position : Position()
-        amount_dict : {stock_id : amount}
-        direction : the direction of the deal price for estimating the amount
-                    # NOTE:
-                    This function is used for calculating current position value.
-                    So the default direction is sell.
-        """
-        value = 0
-        for stock_id in amount_dict:
-            if not only_tradable or (
-                not self.check_stock_suspended(stock_id=stock_id, start_time=start_time, end_time=end_time)
-                and not self.check_stock_limit(stock_id=stock_id, start_time=start_time, end_time=end_time)
-            ):
-                value += (
-                    self.get_deal_price(
-                        stock_id=stock_id,
-                        start_time=start_time,
-                        end_time=end_time,
-                        direction=direction,
-                    )
-                    * amount_dict[stock_id]
-                )
-        return value
-
-    def _get_factor_or_raise_error(
-        self,
-        factor: float | None = None,
-        stock_id: str | None = None,
-        start_time: pd.Timestamp = None,
-        end_time: pd.Timestamp = None,
-    ) -> float:
-        """Please refer to the docs of get_amount_of_trade_unit"""
-        if factor is None:
-            if stock_id is not None and start_time is not None and end_time is not None:
-                factor = self.get_factor(stock_id=stock_id, start_time=start_time, end_time=end_time)
-            else:
-                raise ValueError(f"`factor` and (`stock_id`, `start_time`, `end_time`) can't both be None")
-        assert factor is not None
-        return factor
-
-    def get_amount_of_trade_unit(
-        self,
-        factor: float | None = None,
-        stock_id: str | None = None,
-        start_time: pd.Timestamp = None,
-        end_time: pd.Timestamp = None,
-    ) -> Optional[float]:
-        """
-        get the trade unit of amount based on **factor**
-        the factor can be given directly or calculated in given time range and stock id.
-        `factor` has higher priority than `stock_id`, `start_time` and `end_time`
-        Parameters
-        ----------
-        factor : float
-            the adjusted factor
-        stock_id : str
-            the id of the stock
-        start_time :
-            the start time of trading range
-        end_time :
-            the end time of trading range
-        """
-        if not self.trade_w_adj_price and self.trade_unit is not None:
-            factor = self._get_factor_or_raise_error(
-                factor=factor,
-                stock_id=stock_id,
-                start_time=start_time,
-                end_time=end_time,
-            )
-            return self.trade_unit / factor
-        else:
-            return None
-
-    def round_amount_by_trade_unit(
-        self,
-        deal_amount: float,
-        factor: float | None = None,
-        stock_id: str | None = None,
-        start_time: pd.Timestamp = None,
-        end_time: pd.Timestamp = None,
-    ) -> float:
-        """Parameter
-        Please refer to the docs of get_amount_of_trade_unit
-        deal_amount : float, adjusted amount
-        factor : float, adjusted factor
-        return : float, real amount
-        """
-        if not self.trade_w_adj_price and self.trade_unit is not None:
-            # the minimal amount is 1. Add 0.1 for solving precision problem.
-            factor = self._get_factor_or_raise_error(
-                factor=factor,
-                stock_id=stock_id,
-                start_time=start_time,
-                end_time=end_time,
-            )
-            return (deal_amount * factor + 0.1) // self.trade_unit * self.trade_unit / factor
-        return deal_amount
-
-    def _clip_amount_by_volume(self, order: Order, dealt_order_amount: dict) -> Optional[float]:
-        """parse the capacity limit string and return the actual amount of orders that can be executed.
-        NOTE:
-            this function will change the order.deal_amount **inplace**
-            - This will make the order info more accurate
-        Parameters
-        ----------
-        order : Order
-            the order to be executed.
-        dealt_order_amount : dict
-            :param dealt_order_amount: the dealt order amount dict with the format of {stock_id: float}
-        """
-        vol_limit = self.buy_vol_limit if order.direction == Order.BUY else self.sell_vol_limit
-
-        if vol_limit is None:
-            return order.deal_amount
-
-        vol_limit_num: List[float] = []
-        for limit in vol_limit:
-            assert isinstance(limit, tuple)
-            if limit[0] == "current":
-                limit_value = self.quote.get_data(
-                    order.stock_id,
-                    order.start_time,
-                    order.end_time,
-                    field=limit[1],
-                    method="sum",
-                )
-                vol_limit_num.append(cast(float, limit_value))
-            elif limit[0] == "cum":
-                limit_value = self.quote.get_data(
-                    order.stock_id,
-                    order.start_time,
-                    order.end_time,
-                    field=limit[1],
-                    method="ts_data_last",
-                )
-                vol_limit_num.append(limit_value - dealt_order_amount[order.stock_id])
-            else:
-                raise ValueError(f"{limit[0]} is not supported")
-        vol_limit_min = min(vol_limit_num)
-        orig_deal_amount = order.deal_amount
-        order.deal_amount = max(min(vol_limit_min, orig_deal_amount), 0)
-        if vol_limit_min < orig_deal_amount:
-            self.logger.debug(f"Order clipped due to volume limitation: {order}, {list(zip(vol_limit_num, vol_limit))}")
-
-        return None
-
-    def _get_buy_amount_by_cash_limit(self, trade_price: float, cash: float, cost_ratio: float) -> float:
-        """return the real order amount after cash limit for buying.
-        Parameters
-        ----------
-        trade_price : float
-        cash : float
-        cost_ratio : float
-
-        Return
-        ----------
-        float
-            the real order amount after cash limit for buying.
-        """
-        max_trade_amount = 0.0
-        if cash >= self.min_cost:
-            # critical_price means the stock transaction price when the service fee is equal to min_cost.
-            critical_price = self.min_cost / cost_ratio + self.min_cost
-            if cash >= critical_price:
-                # the service fee is equal to cost_ratio * trade_amount
-                max_trade_amount = cash / (1 + cost_ratio) / trade_price
-            else:
-                # the service fee is equal to min_cost
-                max_trade_amount = (cash - self.min_cost) / trade_price
-        return max_trade_amount
-
-    def _calc_trade_info_by_order(
-        self,
-        order: Order,
-        position: Optional[BasePosition],
-        dealt_order_amount: dict,
-    ) -> Tuple[float, float, float]:
-        """
-        Calculation of trade info
-        **NOTE**: Order will be changed in this function
-        :param order:
-        :param position: Position
-        :param dealt_order_amount: the dealt order amount dict with the format of {stock_id: float}
-        :return: trade_price, trade_val, trade_cost
-        """
-        trade_price = cast(
-            float,
-            self.get_deal_price(order.stock_id, order.start_time, order.end_time, direction=order.direction),
-        )
-        total_trade_val = cast(float, self.get_volume(order.stock_id, order.start_time, order.end_time)) * trade_price
-        order.factor = self.get_factor(order.stock_id, order.start_time, order.end_time)
-        order.deal_amount = order.amount  # set to full amount and clip it step by step
-        # Clipping amount first
-        # - It simulates that the order is rejected directly by the exchange due to large order
-        # Another choice is placing it after rounding the order
-        # - It simulates that the large order is submitted, but partial is dealt regardless of rounding by trading unit.
-        self._clip_amount_by_volume(order, dealt_order_amount)
-
-        # TODO: the adjusted cost ratio can be overestimated as deal_amount will be clipped in the next steps
-        trade_val = order.deal_amount * trade_price
-        if not total_trade_val or np.isnan(total_trade_val):
-            # TODO: assert trade_val == 0, f"trade_val != 0, total_trade_val: {total_trade_val}; order info: {order}"
-            adj_cost_ratio = self.impact_cost
-        else:
-            adj_cost_ratio = self.impact_cost * (trade_val / total_trade_val) ** 2
-
-        if order.direction == Order.SELL:
-            cost_ratio = self.close_cost + adj_cost_ratio
-            # sell
-            # if we don't know current position, we choose to sell all
-            # Otherwise, we clip the amount based on current position
-            if position is not None:
-                current_amount = (
-                    position.get_stock_amount(order.stock_id) if position.check_stock(order.stock_id) else 0
-                )
-                if not np.isclose(order.deal_amount, current_amount):
-                    # when not selling last stock. rounding is necessary
-                    order.deal_amount = self.round_amount_by_trade_unit(
-                        min(current_amount, order.deal_amount),
-                        order.factor,
-                    )
-
-                # in case of negative value of cash
-                if position.get_cash() + order.deal_amount * trade_price < max(
-                    order.deal_amount * trade_price * cost_ratio,
-                    self.min_cost,
-                ):
-                    order.deal_amount = 0
-                    self.logger.debug(f"Order clipped due to cash limitation: {order}")
-
-        elif order.direction == Order.BUY:
-            cost_ratio = self.open_cost + adj_cost_ratio
-            # buy
-            if position is not None:
-                cash = position.get_cash()
-                trade_val = order.deal_amount * trade_price
-                if cash < max(trade_val * cost_ratio, self.min_cost):
-                    # cash cannot cover cost
-                    order.deal_amount = 0
-                    self.logger.debug(f"Order clipped due to cost higher than cash: {order}")
-                elif cash < trade_val + max(trade_val * cost_ratio, self.min_cost):
-                    # The money is not enough
-                    max_buy_amount = self._get_buy_amount_by_cash_limit(trade_price, cash, cost_ratio)
-                    order.deal_amount = self.round_amount_by_trade_unit(
-                        min(max_buy_amount, order.deal_amount),
-                        order.factor,
-                    )
-                    self.logger.debug(f"Order clipped due to cash limitation: {order}")
-                else:
-                    # The money is enough
-                    order.deal_amount = self.round_amount_by_trade_unit(order.deal_amount, order.factor)
-            else:
-                # Unknown amount of money. Just round the amount
-                order.deal_amount = self.round_amount_by_trade_unit(order.deal_amount, order.factor)
-
-        else:
-            raise NotImplementedError("order direction {} error".format(order.direction))
-
-        trade_val = order.deal_amount * trade_price
-        trade_cost = max(trade_val * cost_ratio, self.min_cost)
-        if trade_val <= 1e-5:
-            # if dealing is not successful, the trade_cost should be zero.
-            trade_cost = 0
-        return trade_price, trade_val, trade_cost
-
-    def get_order_helper(self) -> OrderHelper:
-        if not hasattr(self, "_order_helper"):
-            # cache to avoid recreate the same instance
-            self._order_helper = OrderHelper(self)
-        return self._order_helper
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+from __future__ import annotations
+
+from collections import defaultdict
+from typing import TYPE_CHECKING, Any, Dict, List, Optional, Tuple, Type, Union, cast
+
+from ..utils.index_data import IndexData
+
+if TYPE_CHECKING:
+    from .account import Account
+
+import random
+
+import numpy as np
+import pandas as pd
+
+from qlib.backtest.position import BasePosition
+
+from ..config import C
+from ..constant import REG_CN, REG_TW
+from ..data.data import D
+from ..log import get_module_logger
+from .decision import Order, OrderDir, OrderHelper
+from .high_performance_ds import BaseQuote, NumpyQuote
+
+
+class Exchange:
+    # `quote_df` is a pd.DataFrame class that contains basic information for backtesting
+    # After some processing, the data will later be maintained by `quote_cls` object for faster data retrieving.
+    # Some conventions for `quote_df`
+    # - $close is for calculating the total value at end of each day.
+    #   - if $close is None, the stock on that day is regarded as suspended.
+    # - $factor is for rounding to the trading unit;
+    #   - if any $factor is missing when $close exists, trading unit rounding will be disabled
+    quote_df: pd.DataFrame
+
+    def __init__(
+        self,
+        freq: str = "day",
+        start_time: Union[pd.Timestamp, str] = None,
+        end_time: Union[pd.Timestamp, str] = None,
+        codes: Union[list, str] = "all",
+        deal_price: Union[str, Tuple[str, str], List[str], None] = None,
+        subscribe_fields: list = [],
+        limit_threshold: Union[Tuple[str, str], float, None] = None,
+        volume_threshold: Union[tuple, dict, None] = None,
+        open_cost: float = 0.0015,
+        close_cost: float = 0.0025,
+        min_cost: float = 5.0,
+        impact_cost: float = 0.0,
+        extra_quote: pd.DataFrame = None,
+        quote_cls: Type[BaseQuote] = NumpyQuote,
+        **kwargs: Any,
+    ) -> None:
+        """__init__
+        :param freq:             frequency of data
+        :param start_time:       closed start time for backtest
+        :param end_time:         closed end time for backtest
+        :param codes:            list stock_id list or a string of instruments(i.e. all, csi500, sse50)
+        :param deal_price:      Union[str, Tuple[str, str], List[str]]
+                                The `deal_price` supports following two types of input
+                                - <deal_price> : str
+                                - (<buy_price>, <sell_price>): Tuple[str] or List[str]
+                                <deal_price>, <buy_price> or <sell_price> := <price>
+                                <price> := str
+                                - for example '$close', '$open', '$vwap' ("close" is OK. `Exchange` will help to prepend
+                                  "$" to the expression)
+        :param subscribe_fields: list, subscribe fields. This expressions will be added to the query and `self.quote`.
+                                 It is useful when users want more fields to be queried
+        :param limit_threshold: Union[Tuple[str, str], float, None]
+                                1) `None`: no limitation
+                                2) float, 0.1 for example, default None
+                                3) Tuple[str, str]: (<the expression for buying stock limitation>,
+                                                     <the expression for sell stock limitation>)
+                                                    `False` value indicates the stock is tradable
+                                                    `True` value indicates the stock is limited and not tradable
+        :param volume_threshold: Union[
+                                    Dict[
+                                        "all": ("cum" or "current", limit_str),
+                                        "buy": ("cum" or "current", limit_str),
+                                        "sell":("cum" or "current", limit_str),
+                                    ],
+                                    ("cum" or "current", limit_str),
+                                 ]
+                                1) ("cum" or "current", limit_str) denotes a single volume limit.
+                                    - limit_str is qlib data expression which is allowed to define your own Operator.
+                                    Please refer to qlib/contrib/ops/high_freq.py, here are any custom operator for
+                                    high frequency, such as DayCumsum. !!!NOTE: if you want you use the custom
+                                    operator, you need to register it in qlib_init.
+                                    - "cum" means that this is a cumulative value over time, such as cumulative market
+                                    volume. So when it is used as a volume limit, it is necessary to subtract the dealt
+                                    amount.
+                                    - "current" means that this is a real-time value and will not accumulate over time,
+                                    so it can be directly used as a capacity limit.
+                                    e.g. ("cum", "0.2 * DayCumsum($volume, '9:45', '14:45')"), ("current", "$bidV1")
+                                2) "all" means the volume limits are both buying and selling.
+                                "buy" means the volume limits of buying. "sell" means the volume limits of selling.
+                                Different volume limits will be aggregated with min(). If volume_threshold is only
+                                ("cum" or "current", limit_str) instead of a dict, the volume limits are for
+                                both by default. In other words, it is same as {"all": ("cum" or "current", limit_str)}.
+                                3) e.g. "volume_threshold": {
+                                            "all": ("cum", "0.2 * DayCumsum($volume, '9:45', '14:45')"),
+                                            "buy": ("current", "$askV1"),
+                                            "sell": ("current", "$bidV1"),
+                                        }
+        :param open_cost:        cost rate for open, default 0.0015
+        :param close_cost:       cost rate for close, default 0.0025
+        :param trade_unit:       trade unit, 100 for China A market.
+                                 None for disable trade unit.
+                                 **NOTE**: `trade_unit` is included in the `kwargs`. It is necessary because we must
+                                 distinguish `not set` and `disable trade_unit`
+        :param min_cost:         min cost, default 5
+        :param impact_cost:     market impact cost rate (a.k.a. slippage). A recommended value is 0.1.
+        :param extra_quote:     pandas, dataframe consists of
+                                    columns: like ['$vwap', '$close', '$volume', '$factor', 'limit_sell', 'limit_buy'].
+                                            The limit indicates that the etf is tradable on a specific day.
+                                            Necessary fields:
+                                                $close is for calculating the total value at end of each day.
+                                            Optional fields:
+                                                $volume is only necessary when we limit the trade amount or calculate
+                                                PA(vwap) indicator
+                                                $vwap is only necessary when we use the $vwap price as the deal price
+                                                $factor is for rounding to the trading unit
+                                                limit_sell will be set to False by default (False indicates we can sell
+                                                this target on this day).
+                                                limit_buy will be set to False by default (False indicates we can buy
+                                                this target on this day).
+                                    index: MultipleIndex(instrument, pd.Datetime)
+        """
+        self.freq = freq
+        self.start_time = start_time
+        self.end_time = end_time
+
+        self.trade_unit = kwargs.pop("trade_unit", C.trade_unit)
+        if len(kwargs) > 0:
+            raise ValueError(f"Get Unexpected arguments {kwargs}")
+
+        if limit_threshold is None:
+            limit_threshold = C.limit_threshold
+        if deal_price is None:
+            deal_price = C.deal_price
+
+        # we have some verbose information here. So logging is enabled
+        self.logger = get_module_logger("online operator")
+
+        # TODO: the quote, trade_dates, codes are not necessary.
+        # It is just for performance consideration.
+        self.limit_type = self._get_limit_type(limit_threshold)
+        if limit_threshold is None:
+            if C.region in [REG_CN, REG_TW]:
+                self.logger.warning(f"limit_threshold not set. The stocks hit the limit may be bought/sold")
+        elif self.limit_type == self.LT_FLT and abs(cast(float, limit_threshold)) > 0.1:
+            if C.region in [REG_CN, REG_TW]:
+                self.logger.warning(f"limit_threshold may not be set to a reasonable value")
+
+        if isinstance(deal_price, str):
+            if deal_price[0] != "$":
+                deal_price = "$" + deal_price
+            self.buy_price = self.sell_price = deal_price
+        elif isinstance(deal_price, (tuple, list)):
+            self.buy_price, self.sell_price = cast(Tuple[str, str], deal_price)
+        else:
+            raise NotImplementedError(f"This type of input is not supported")
+
+        if isinstance(codes, str):
+            codes = D.instruments(codes)
+        self.codes = codes
+        # Necessary fields
+        # $close is for calculating the total value at end of each day.
+        # - if $close is None, the stock on that day is regarded as suspended.
+        # $factor is for rounding to the trading unit
+        # $change is for calculating the limit of the stock
+
+        # 　get volume limit from kwargs
+        self.buy_vol_limit, self.sell_vol_limit, vol_lt_fields = self._get_vol_limit(volume_threshold)
+
+        necessary_fields = {self.buy_price, self.sell_price, "$close", "$change", "$factor", "$volume"}
+        if self.limit_type == self.LT_TP_EXP:
+            assert isinstance(limit_threshold, tuple)
+            for exp in limit_threshold:
+                necessary_fields.add(exp)
+        all_fields = list(necessary_fields | set(vol_lt_fields) | set(subscribe_fields))
+
+        self.all_fields = all_fields
+
+        self.open_cost = open_cost
+        self.close_cost = close_cost
+        self.min_cost = min_cost
+        self.impact_cost = impact_cost
+
+        self.limit_threshold: Union[Tuple[str, str], float, None] = limit_threshold
+        self.volume_threshold = volume_threshold
+        self.extra_quote = extra_quote
+        self.get_quote_from_qlib()
+
+        # init quote by quote_df
+        self.quote_cls = quote_cls
+        self.quote: BaseQuote = self.quote_cls(self.quote_df, freq)
+
+    def get_quote_from_qlib(self) -> None:
+        # get stock data from qlib
+        if len(self.codes) == 0:
+            self.codes = D.instruments()
+        self.quote_df = D.features(
+            self.codes,
+            self.all_fields,
+            self.start_time,
+            self.end_time,
+            freq=self.freq,
+            disk_cache=True,
+        )
+        self.quote_df.columns = self.all_fields
+
+        # check buy_price data and sell_price data
+        for attr in ("buy_price", "sell_price"):
+            pstr = getattr(self, attr)  # price string
+            if self.quote_df[pstr].isna().any():
+                self.logger.warning("{} field data contains nan.".format(pstr))
+
+        # update trade_w_adj_price
+        if (self.quote_df["$factor"].isna() & ~self.quote_df["$close"].isna()).any():
+            # The 'factor.day.bin' file not exists, and `factor` field contains `nan`
+            # Use adjusted price
+            self.trade_w_adj_price = True
+            self.logger.warning("factor.day.bin file not exists or factor contains `nan`. Order using adjusted_price.")
+            if self.trade_unit is not None:
+                self.logger.warning(f"trade unit {self.trade_unit} is not supported in adjusted_price mode.")
+        else:
+            # The `factor.day.bin` file exists and all data `close` and `factor` are not `nan`
+            # Use normal price
+            self.trade_w_adj_price = False
+        # update limit
+        self._update_limit(self.limit_threshold)
+
+        # concat extra_quote
+        if self.extra_quote is not None:
+            # process extra_quote
+            if "$close" not in self.extra_quote:
+                raise ValueError("$close is necessray in extra_quote")
+            for attr in "buy_price", "sell_price":
+                pstr = getattr(self, attr)  # price string
+                if pstr not in self.extra_quote.columns:
+                    self.extra_quote[pstr] = self.extra_quote["$close"]
+                    self.logger.warning(f"No {pstr} set for extra_quote. Use $close as {pstr}.")
+            if "$factor" not in self.extra_quote.columns:
+                self.extra_quote["$factor"] = 1.0
+                self.logger.warning("No $factor set for extra_quote. Use 1.0 as $factor.")
+            if "limit_sell" not in self.extra_quote.columns:
+                self.extra_quote["limit_sell"] = False
+                self.logger.warning("No limit_sell set for extra_quote. All stock will be able to be sold.")
+            if "limit_buy" not in self.extra_quote.columns:
+                self.extra_quote["limit_buy"] = False
+                self.logger.warning("No limit_buy set for extra_quote. All stock will be able to be bought.")
+            assert set(self.extra_quote.columns) == set(self.quote_df.columns) - {"$change"}
+            self.quote_df = pd.concat([self.quote_df, self.extra_quote], sort=False, axis=0)
+
+    LT_TP_EXP = "(exp)"  # Tuple[str, str]:  the limitation is calculated by a Qlib expression.
+    LT_FLT = "float"  # float:  the trading limitation is based on `abs($change) < limit_threshold`
+    LT_NONE = "none"  # none:  there is no trading limitation
+
+    def _get_limit_type(self, limit_threshold: Union[tuple, float, None]) -> str:
+        """get limit type"""
+        if isinstance(limit_threshold, tuple):
+            return self.LT_TP_EXP
+        elif isinstance(limit_threshold, float):
+            return self.LT_FLT
+        elif limit_threshold is None:
+            return self.LT_NONE
+        else:
+            raise NotImplementedError(f"This type of `limit_threshold` is not supported")
+
+    def _update_limit(self, limit_threshold: Union[Tuple, float, None]) -> None:
+        # $close may contain NaN, the nan indicates that the stock is not tradable at that timestamp
+        suspended = self.quote_df["$close"].isna()
+        # check limit_threshold
+        limit_type = self._get_limit_type(limit_threshold)
+        if limit_type == self.LT_NONE:
+            self.quote_df["limit_buy"] = suspended
+            self.quote_df["limit_sell"] = suspended
+        elif limit_type == self.LT_TP_EXP:
+            # set limit
+            limit_threshold = cast(tuple, limit_threshold)
+            # astype bool is necessary, because quote_df is an expression and could be float
+            self.quote_df["limit_buy"] = self.quote_df[limit_threshold[0]].astype("bool") | suspended
+            self.quote_df["limit_sell"] = self.quote_df[limit_threshold[1]].astype("bool") | suspended
+        elif limit_type == self.LT_FLT:
+            limit_threshold = cast(float, limit_threshold)
+            self.quote_df["limit_buy"] = self.quote_df["$change"].ge(limit_threshold) | suspended
+            self.quote_df["limit_sell"] = (
+                self.quote_df["$change"].le(-limit_threshold) | suspended
+            )  # pylint: disable=E1130
+
+    @staticmethod
+    def _get_vol_limit(volume_threshold: Union[tuple, dict, None]) -> Tuple[Optional[list], Optional[list], set]:
+        """
+        preprocess the volume limit.
+        get the fields need to get from qlib.
+        get the volume limit list of buying and selling which is composed of all limits.
+        Parameters
+        ----------
+        volume_threshold :
+            please refer to the doc of exchange.
+        Returns
+        -------
+        fields: set
+            the fields need to get from qlib.
+        buy_vol_limit: List[Tuple[str]]
+            all volume limits of buying.
+        sell_vol_limit: List[Tuple[str]]
+            all volume limits of selling.
+        Raises
+        ------
+        ValueError
+            the format of volume_threshold is not supported.
+        """
+        if volume_threshold is None:
+            return None, None, set()
+
+        fields = set()
+        buy_vol_limit = []
+        sell_vol_limit = []
+        if isinstance(volume_threshold, tuple):
+            volume_threshold = {"all": volume_threshold}
+
+        assert isinstance(volume_threshold, dict)
+        for key, vol_limit in volume_threshold.items():
+            assert isinstance(vol_limit, tuple)
+            fields.add(vol_limit[1])
+
+            if key in ("buy", "all"):
+                buy_vol_limit.append(vol_limit)
+            if key in ("sell", "all"):
+                sell_vol_limit.append(vol_limit)
+
+        return buy_vol_limit, sell_vol_limit, fields
+
+    def check_stock_limit(
+        self,
+        stock_id: str,
+        start_time: pd.Timestamp,
+        end_time: pd.Timestamp,
+        direction: int | None = None,
+    ) -> bool:
+        """
+        Parameters
+        ----------
+        stock_id : str
+        start_time: pd.Timestamp
+        end_time: pd.Timestamp
+        direction : int, optional
+            trade direction, by default None
+            - if direction is None, check if tradable for buying and selling.
+            - if direction == Order.BUY, check the if tradable for buying
+            - if direction == Order.SELL, check the sell limit for selling.
+
+        Returns
+        -------
+        True: the trading of the stock is limited (maybe hit the highest/lowest price), hence the stock is not tradable
+        False: the trading of the stock is not limited, hence the stock may be tradable
+        """
+        # NOTE:
+        # **all** is used when checking limitation.
+        # For example, the stock trading is limited in a day if every minute is limited in a day if every minute is limited.
+        if direction is None:
+            # The trading limitation is related to the trading direction
+            # if the direction is not provided, then any limitation from buy or sell will result in trading limitation
+            buy_limit = self.quote.get_data(stock_id, start_time, end_time, field="limit_buy", method="all")
+            sell_limit = self.quote.get_data(stock_id, start_time, end_time, field="limit_sell", method="all")
+            return bool(buy_limit or sell_limit)
+        elif direction == Order.BUY:
+            return cast(bool, self.quote.get_data(stock_id, start_time, end_time, field="limit_buy", method="all"))
+        elif direction == Order.SELL:
+            return cast(bool, self.quote.get_data(stock_id, start_time, end_time, field="limit_sell", method="all"))
+        else:
+            raise ValueError(f"direction {direction} is not supported!")
+
+    def check_stock_suspended(
+        self,
+        stock_id: str,
+        start_time: pd.Timestamp,
+        end_time: pd.Timestamp,
+    ) -> bool:
+        """if stock is suspended(hence not tradable), True will be returned"""
+        # is suspended
+        if stock_id in self.quote.get_all_stock():
+            # suspended stocks are represented by None $close stock
+            # The $close may contain NaN,
+            close = self.quote.get_data(stock_id, start_time, end_time, "$close")
+            if close is None:
+                # if no close record exists
+                return True
+            elif isinstance(close, IndexData):
+                # **any** non-NaN $close represents trading opportunity may exist
+                #  if all returned is nan, then the stock is suspended
+                return cast(bool, cast(IndexData, close).isna().all())
+            else:
+                # it is single value, make sure is not None
+                return np.isnan(close)
+        else:
+            # if the stock is not in the stock list, then it is not tradable and regarded as suspended
+            return True
+
+    def is_stock_tradable(
+        self,
+        stock_id: str,
+        start_time: pd.Timestamp,
+        end_time: pd.Timestamp,
+        direction: int | None = None,
+    ) -> bool:
+        # check if stock can be traded
+        return not (
+            self.check_stock_suspended(stock_id, start_time, end_time)
+            or self.check_stock_limit(stock_id, start_time, end_time, direction)
+        )
+
+    def check_order(self, order: Order) -> bool:
+        # check limit and suspended
+        return self.is_stock_tradable(order.stock_id, order.start_time, order.end_time, order.direction)
+
+    def deal_order(
+        self,
+        order: Order,
+        trade_account: Account | None = None,
+        position: BasePosition | None = None,
+        dealt_order_amount: Dict[str, float] = defaultdict(float),
+    ) -> Tuple[float, float, float]:
+        """
+        Deal order when the actual transaction
+        the results section in `Order` will be changed.
+        :param order:  Deal the order.
+        :param trade_account: Trade account to be updated after dealing the order.
+        :param position: position to be updated after dealing the order.
+        :param dealt_order_amount: the dealt order amount dict with the format of {stock_id: float}
+        :return: trade_val, trade_cost, trade_price
+        """
+        # check order first.
+        if not self.check_order(order):
+            order.deal_amount = 0.0
+            # using np.nan instead of None to make it more convenient to show the value in format string
+            self.logger.debug(f"Order failed due to trading limitation: {order}")
+            return 0.0, 0.0, np.nan
+
+        if trade_account is not None and position is not None:
+            raise ValueError("trade_account and position can only choose one")
+
+        # NOTE: order will be changed in this function
+        trade_price, trade_val, trade_cost = self._calc_trade_info_by_order(
+            order,
+            trade_account.current_position if trade_account else position,
+            dealt_order_amount,
+        )
+        if trade_val > 1e-5:
+            # If the order can only be deal 0 value. Nothing to be updated
+            # Otherwise, it will result in
+            # 1) some stock with 0 value in the position
+            # 2) `trade_unit` of trade_cost will be lost in user account
+            if trade_account:
+                trade_account.update_order(order=order, trade_val=trade_val, cost=trade_cost, trade_price=trade_price)
+            elif position:
+                position.update_order(order=order, trade_val=trade_val, cost=trade_cost, trade_price=trade_price)
+
+        return trade_val, trade_cost, trade_price
+
+    def get_quote_info(
+        self,
+        stock_id: str,
+        start_time: pd.Timestamp,
+        end_time: pd.Timestamp,
+        field: str,
+        method: str = "ts_data_last",
+    ) -> Union[None, int, float, bool, IndexData]:
+        return self.quote.get_data(stock_id, start_time, end_time, field=field, method=method)
+
+    def get_close(
+        self,
+        stock_id: str,
+        start_time: pd.Timestamp,
+        end_time: pd.Timestamp,
+        method: str = "ts_data_last",
+    ) -> Union[None, int, float, bool, IndexData]:
+        return self.quote.get_data(stock_id, start_time, end_time, field="$close", method=method)
+
+    def get_volume(
+        self,
+        stock_id: str,
+        start_time: pd.Timestamp,
+        end_time: pd.Timestamp,
+        method: Optional[str] = "sum",
+    ) -> Union[None, int, float, bool, IndexData]:
+        """get the total deal volume of stock with `stock_id` between the time interval [start_time, end_time)"""
+        return self.quote.get_data(stock_id, start_time, end_time, field="$volume", method=method)
+
+    def get_deal_price(
+        self,
+        stock_id: str,
+        start_time: pd.Timestamp,
+        end_time: pd.Timestamp,
+        direction: OrderDir,
+        method: Optional[str] = "ts_data_last",
+    ) -> Union[None, int, float, bool, IndexData]:
+        if direction == OrderDir.SELL:
+            pstr = self.sell_price
+        elif direction == OrderDir.BUY:
+            pstr = self.buy_price
+        else:
+            raise NotImplementedError(f"This type of input is not supported")
+
+        deal_price = self.quote.get_data(stock_id, start_time, end_time, field=pstr, method=method)
+        if method is not None and (deal_price is None or np.isnan(deal_price) or deal_price <= 1e-08):
+            self.logger.warning(f"(stock_id:{stock_id}, trade_time:{(start_time, end_time)}, {pstr}): {deal_price}!!!")
+            self.logger.warning(f"setting deal_price to close price")
+            deal_price = self.get_close(stock_id, start_time, end_time, method)
+        return deal_price
+
+    def get_factor(
+        self,
+        stock_id: str,
+        start_time: pd.Timestamp,
+        end_time: pd.Timestamp,
+    ) -> Optional[float]:
+        """
+        Returns
+        -------
+        Optional[float]:
+            `None`: if the stock is suspended `None` may be returned
+            `float`: return factor if the factor exists
+        """
+        assert start_time is not None and end_time is not None, "the time range must be given"
+        if stock_id not in self.quote.get_all_stock():
+            return None
+        return self.quote.get_data(stock_id, start_time, end_time, field="$factor", method="ts_data_last")
+
+    def generate_amount_position_from_weight_position(
+        self,
+        weight_position: dict,
+        cash: float,
+        start_time: pd.Timestamp,
+        end_time: pd.Timestamp,
+        direction: OrderDir = OrderDir.BUY,
+    ) -> dict:
+        """
+        Generates the target position according to the weight and the cash.
+        NOTE: All the cash will be assigned to the tradable stock.
+        Parameter:
+        weight_position : dict {stock_id : weight}; allocate cash by weight_position
+            among then, weight must be in this range: 0 < weight < 1
+        cash : cash
+        start_time : the start time point of the step
+        end_time : the end time point of the step
+        direction : the direction of the deal price for estimating the amount
+                    # NOTE: this function is used for calculating target position. So the default direction is buy
+        """
+
+        # calculate the total weight of tradable value
+        tradable_weight = 0.0
+        for stock_id, wp in weight_position.items():
+            if self.is_stock_tradable(stock_id=stock_id, start_time=start_time, end_time=end_time):
+                # weight_position must be greater than 0 and less than 1
+                if wp < 0 or wp > 1:
+                    raise ValueError(
+                        "weight_position is {}, " "weight_position is not in the range of (0, 1).".format(wp),
+                    )
+                tradable_weight += wp
+
+        if tradable_weight - 1.0 >= 1e-5:
+            raise ValueError("tradable_weight is {}, can not greater than 1.".format(tradable_weight))
+
+        amount_dict = {}
+        for stock_id in weight_position:
+            if weight_position[stock_id] > 0.0 and self.is_stock_tradable(
+                stock_id=stock_id,
+                start_time=start_time,
+                end_time=end_time,
+            ):
+                amount_dict[stock_id] = (
+                    cash
+                    * weight_position[stock_id]
+                    / tradable_weight
+                    // self.get_deal_price(
+                        stock_id=stock_id,
+                        start_time=start_time,
+                        end_time=end_time,
+                        direction=direction,
+                    )
+                )
+        return amount_dict
+
+    def get_real_deal_amount(self, current_amount: float, target_amount: float, factor: float | None = None) -> float:
+        """
+        Calculate the real adjust deal amount when considering the trading unit
+        :param current_amount:
+        :param target_amount:
+        :param factor:
+        :return  real_deal_amount;  Positive deal_amount indicates buying more stock.
+        """
+        if current_amount == target_amount:
+            return 0
+        elif current_amount < target_amount:
+            deal_amount = target_amount - current_amount
+            deal_amount = self.round_amount_by_trade_unit(deal_amount, factor)
+            return deal_amount
+        else:
+            if target_amount == 0:
+                return -current_amount
+            else:
+                deal_amount = current_amount - target_amount
+                deal_amount = self.round_amount_by_trade_unit(deal_amount, factor)
+                return -deal_amount
+
+    def generate_order_for_target_amount_position(
+        self,
+        target_position: dict,
+        current_position: dict,
+        start_time: pd.Timestamp,
+        end_time: pd.Timestamp,
+    ) -> List[Order]:
+        """
+        Note: some future information is used in this function
+        Parameter:
+        target_position : dict { stock_id : amount }
+        current_position : dict { stock_id : amount}
+        trade_unit : trade_unit
+        down sample : for amount 321 and trade_unit 100, deal_amount is 300
+        deal order on trade_date
+        """
+        # split buy and sell for further use
+        buy_order_list = []
+        sell_order_list = []
+        # three parts: kept stock_id, dropped stock_id, new stock_id
+        # handle kept stock_id
+
+        # because the order of the set is not fixed, the trading order of the stock is different, so that the backtest
+        # results of the same parameter are different;
+        # so here we sort stock_id, and then randomly shuffle the order of stock_id
+        # because the same random seed is used, the final stock_id order is fixed
+        sorted_ids = sorted(set(list(current_position.keys()) + list(target_position.keys())))
+        random.seed(0)
+        random.shuffle(sorted_ids)
+        for stock_id in sorted_ids:
+            # Do not generate order for the non-tradable stocks
+            if not self.is_stock_tradable(stock_id=stock_id, start_time=start_time, end_time=end_time):
+                continue
+
+            target_amount = target_position.get(stock_id, 0)
+            current_amount = current_position.get(stock_id, 0)
+            factor = self.get_factor(stock_id, start_time=start_time, end_time=end_time)
+
+            deal_amount = self.get_real_deal_amount(current_amount, target_amount, factor)
+            if deal_amount == 0:
+                continue
+            if deal_amount > 0:
+                # buy stock
+                buy_order_list.append(
+                    Order(
+                        stock_id=stock_id,
+                        amount=deal_amount,
+                        direction=Order.BUY,
+                        start_time=start_time,
+                        end_time=end_time,
+                        factor=factor,
+                    ),
+                )
+            else:
+                # sell stock
+                sell_order_list.append(
+                    Order(
+                        stock_id=stock_id,
+                        amount=abs(deal_amount),
+                        direction=Order.SELL,
+                        start_time=start_time,
+                        end_time=end_time,
+                        factor=factor,
+                    ),
+                )
+        # return order_list : buy + sell
+        return sell_order_list + buy_order_list
+
+    def calculate_amount_position_value(
+        self,
+        amount_dict: dict,
+        start_time: pd.Timestamp,
+        end_time: pd.Timestamp,
+        only_tradable: bool = False,
+        direction: OrderDir = OrderDir.SELL,
+    ) -> float:
+        """Parameter
+        position : Position()
+        amount_dict : {stock_id : amount}
+        direction : the direction of the deal price for estimating the amount
+                    # NOTE:
+                    This function is used for calculating current position value.
+                    So the default direction is sell.
+        """
+        value = 0
+        for stock_id in amount_dict:
+            if not only_tradable or (
+                not self.check_stock_suspended(stock_id=stock_id, start_time=start_time, end_time=end_time)
+                and not self.check_stock_limit(stock_id=stock_id, start_time=start_time, end_time=end_time)
+            ):
+                value += (
+                    self.get_deal_price(
+                        stock_id=stock_id,
+                        start_time=start_time,
+                        end_time=end_time,
+                        direction=direction,
+                    )
+                    * amount_dict[stock_id]
+                )
+        return value
+
+    def _get_factor_or_raise_error(
+        self,
+        factor: float | None = None,
+        stock_id: str | None = None,
+        start_time: pd.Timestamp = None,
+        end_time: pd.Timestamp = None,
+    ) -> float:
+        """Please refer to the docs of get_amount_of_trade_unit"""
+        if factor is None:
+            if stock_id is not None and start_time is not None and end_time is not None:
+                factor = self.get_factor(stock_id=stock_id, start_time=start_time, end_time=end_time)
+            else:
+                raise ValueError(f"`factor` and (`stock_id`, `start_time`, `end_time`) can't both be None")
+        assert factor is not None
+        return factor
+
+    def get_amount_of_trade_unit(
+        self,
+        factor: float | None = None,
+        stock_id: str | None = None,
+        start_time: pd.Timestamp = None,
+        end_time: pd.Timestamp = None,
+    ) -> Optional[float]:
+        """
+        get the trade unit of amount based on **factor**
+        the factor can be given directly or calculated in given time range and stock id.
+        `factor` has higher priority than `stock_id`, `start_time` and `end_time`
+        Parameters
+        ----------
+        factor : float
+            the adjusted factor
+        stock_id : str
+            the id of the stock
+        start_time :
+            the start time of trading range
+        end_time :
+            the end time of trading range
+        """
+        if not self.trade_w_adj_price and self.trade_unit is not None:
+            factor = self._get_factor_or_raise_error(
+                factor=factor,
+                stock_id=stock_id,
+                start_time=start_time,
+                end_time=end_time,
+            )
+            return self.trade_unit / factor
+        else:
+            return None
+
+    def round_amount_by_trade_unit(
+        self,
+        deal_amount: float,
+        factor: float | None = None,
+        stock_id: str | None = None,
+        start_time: pd.Timestamp = None,
+        end_time: pd.Timestamp = None,
+    ) -> float:
+        """Parameter
+        Please refer to the docs of get_amount_of_trade_unit
+        deal_amount : float, adjusted amount
+        factor : float, adjusted factor
+        return : float, real amount
+        """
+        if not self.trade_w_adj_price and self.trade_unit is not None:
+            # the minimal amount is 1. Add 0.1 for solving precision problem.
+            factor = self._get_factor_or_raise_error(
+                factor=factor,
+                stock_id=stock_id,
+                start_time=start_time,
+                end_time=end_time,
+            )
+            return (deal_amount * factor + 0.1) // self.trade_unit * self.trade_unit / factor
+        return deal_amount
+
+    def _clip_amount_by_volume(self, order: Order, dealt_order_amount: dict) -> Optional[float]:
+        """parse the capacity limit string and return the actual amount of orders that can be executed.
+        NOTE:
+            this function will change the order.deal_amount **inplace**
+            - This will make the order info more accurate
+        Parameters
+        ----------
+        order : Order
+            the order to be executed.
+        dealt_order_amount : dict
+            :param dealt_order_amount: the dealt order amount dict with the format of {stock_id: float}
+        """
+        vol_limit = self.buy_vol_limit if order.direction == Order.BUY else self.sell_vol_limit
+
+        if vol_limit is None:
+            return order.deal_amount
+
+        vol_limit_num: List[float] = []
+        for limit in vol_limit:
+            assert isinstance(limit, tuple)
+            if limit[0] == "current":
+                limit_value = self.quote.get_data(
+                    order.stock_id,
+                    order.start_time,
+                    order.end_time,
+                    field=limit[1],
+                    method="sum",
+                )
+                vol_limit_num.append(cast(float, limit_value))
+            elif limit[0] == "cum":
+                limit_value = self.quote.get_data(
+                    order.stock_id,
+                    order.start_time,
+                    order.end_time,
+                    field=limit[1],
+                    method="ts_data_last",
+                )
+                vol_limit_num.append(limit_value - dealt_order_amount[order.stock_id])
+            else:
+                raise ValueError(f"{limit[0]} is not supported")
+        vol_limit_min = min(vol_limit_num)
+        orig_deal_amount = order.deal_amount
+        order.deal_amount = max(min(vol_limit_min, orig_deal_amount), 0)
+        if vol_limit_min < orig_deal_amount:
+            self.logger.debug(f"Order clipped due to volume limitation: {order}, {list(zip(vol_limit_num, vol_limit))}")
+
+        return None
+
+    def _get_buy_amount_by_cash_limit(self, trade_price: float, cash: float, cost_ratio: float) -> float:
+        """return the real order amount after cash limit for buying.
+        Parameters
+        ----------
+        trade_price : float
+        cash : float
+        cost_ratio : float
+
+        Return
+        ----------
+        float
+            the real order amount after cash limit for buying.
+        """
+        max_trade_amount = 0.0
+        if cash >= self.min_cost:
+            # critical_price means the stock transaction price when the service fee is equal to min_cost.
+            critical_price = self.min_cost / cost_ratio + self.min_cost
+            if cash >= critical_price:
+                # the service fee is equal to cost_ratio * trade_amount
+                max_trade_amount = cash / (1 + cost_ratio) / trade_price
+            else:
+                # the service fee is equal to min_cost
+                max_trade_amount = (cash - self.min_cost) / trade_price
+        return max_trade_amount
+
+    def _calc_trade_info_by_order(
+        self,
+        order: Order,
+        position: Optional[BasePosition],
+        dealt_order_amount: dict,
+    ) -> Tuple[float, float, float]:
+        """
+        Calculation of trade info
+        **NOTE**: Order will be changed in this function
+        :param order:
+        :param position: Position
+        :param dealt_order_amount: the dealt order amount dict with the format of {stock_id: float}
+        :return: trade_price, trade_val, trade_cost
+        """
+        trade_price = cast(
+            float,
+            self.get_deal_price(order.stock_id, order.start_time, order.end_time, direction=order.direction),
+        )
+        total_trade_val = cast(float, self.get_volume(order.stock_id, order.start_time, order.end_time)) * trade_price
+        order.factor = self.get_factor(order.stock_id, order.start_time, order.end_time)
+        order.deal_amount = order.amount  # set to full amount and clip it step by step
+        # Clipping amount first
+        # - It simulates that the order is rejected directly by the exchange due to large order
+        # Another choice is placing it after rounding the order
+        # - It simulates that the large order is submitted, but partial is dealt regardless of rounding by trading unit.
+        self._clip_amount_by_volume(order, dealt_order_amount)
+
+        # TODO: the adjusted cost ratio can be overestimated as deal_amount will be clipped in the next steps
+        trade_val = order.deal_amount * trade_price
+        if not total_trade_val or np.isnan(total_trade_val):
+            # TODO: assert trade_val == 0, f"trade_val != 0, total_trade_val: {total_trade_val}; order info: {order}"
+            adj_cost_ratio = self.impact_cost
+        else:
+            adj_cost_ratio = self.impact_cost * (trade_val / total_trade_val) ** 2
+
+        if order.direction == Order.SELL:
+            cost_ratio = self.close_cost + adj_cost_ratio
+            # sell
+            # if we don't know current position, we choose to sell all
+            # Otherwise, we clip the amount based on current position
+            if position is not None:
+                current_amount = (
+                    position.get_stock_amount(order.stock_id) if position.check_stock(order.stock_id) else 0
+                )
+                if not np.isclose(order.deal_amount, current_amount):
+                    # when not selling last stock. rounding is necessary
+                    order.deal_amount = self.round_amount_by_trade_unit(
+                        min(current_amount, order.deal_amount),
+                        order.factor,
+                    )
+
+                # in case of negative value of cash
+                if position.get_cash() + order.deal_amount * trade_price < max(
+                    order.deal_amount * trade_price * cost_ratio,
+                    self.min_cost,
+                ):
+                    order.deal_amount = 0
+                    self.logger.debug(f"Order clipped due to cash limitation: {order}")
+
+        elif order.direction == Order.BUY:
+            cost_ratio = self.open_cost + adj_cost_ratio
+            # buy
+            if position is not None:
+                cash = position.get_cash()
+                trade_val = order.deal_amount * trade_price
+                if cash < max(trade_val * cost_ratio, self.min_cost):
+                    # cash cannot cover cost
+                    order.deal_amount = 0
+                    self.logger.debug(f"Order clipped due to cost higher than cash: {order}")
+                elif cash < trade_val + max(trade_val * cost_ratio, self.min_cost):
+                    # The money is not enough
+                    max_buy_amount = self._get_buy_amount_by_cash_limit(trade_price, cash, cost_ratio)
+                    order.deal_amount = self.round_amount_by_trade_unit(
+                        min(max_buy_amount, order.deal_amount),
+                        order.factor,
+                    )
+                    self.logger.debug(f"Order clipped due to cash limitation: {order}")
+                else:
+                    # The money is enough
+                    order.deal_amount = self.round_amount_by_trade_unit(order.deal_amount, order.factor)
+            else:
+                # Unknown amount of money. Just round the amount
+                order.deal_amount = self.round_amount_by_trade_unit(order.deal_amount, order.factor)
+
+        else:
+            raise NotImplementedError("order direction {} error".format(order.direction))
+
+        trade_val = order.deal_amount * trade_price
+        trade_cost = max(trade_val * cost_ratio, self.min_cost)
+        if trade_val <= 1e-5:
+            # if dealing is not successful, the trade_cost should be zero.
+            trade_cost = 0
+        return trade_price, trade_val, trade_cost
+
+    def get_order_helper(self) -> OrderHelper:
+        if not hasattr(self, "_order_helper"):
+            # cache to avoid recreate the same instance
+            self._order_helper = OrderHelper(self)
+        return self._order_helper
```

## qlib/backtest/executor.py

 * *Ordering differences only*

```diff
@@ -1,628 +1,628 @@
-from __future__ import annotations
-
-import copy
-from abc import abstractmethod
-from collections import defaultdict
-from types import GeneratorType
-from typing import Any, Dict, Generator, List, Tuple, Union, cast
-
-import pandas as pd
-
-from qlib.backtest.account import Account
-from qlib.backtest.position import BasePosition
-from qlib.log import get_module_logger
-
-from ..strategy.base import BaseStrategy
-from ..utils import init_instance_by_config
-from .decision import BaseTradeDecision, Order
-from .exchange import Exchange
-from .utils import CommonInfrastructure, LevelInfrastructure, TradeCalendarManager, get_start_end_idx
-
-
-class BaseExecutor:
-    """Base executor for trading"""
-
-    def __init__(
-        self,
-        time_per_step: str,
-        start_time: Union[str, pd.Timestamp] = None,
-        end_time: Union[str, pd.Timestamp] = None,
-        indicator_config: dict = {},
-        generate_portfolio_metrics: bool = False,
-        verbose: bool = False,
-        track_data: bool = False,
-        trade_exchange: Exchange | None = None,
-        common_infra: CommonInfrastructure | None = None,
-        settle_type: str = BasePosition.ST_NO,
-        **kwargs: Any,
-    ) -> None:
-        """
-        Parameters
-        ----------
-        time_per_step : str
-            trade time per trading step, used for generate the trade calendar
-        show_indicator: bool, optional
-            whether to show indicators, :
-            - 'pa', the price advantage
-            - 'pos', the positive rate
-            - 'ffr', the fulfill rate
-        indicator_config: dict, optional
-            config for calculating trade indicator, including the following fields:
-            - 'show_indicator': whether to show indicators, optional, default by False. The indicators includes
-                - 'pa', the price advantage
-                - 'pos', the positive rate
-                - 'ffr', the fulfill rate
-            - 'pa_config': config for calculating price advantage(pa), optional
-                - 'base_price': the based price than which the trading price is advanced, Optional, default by 'twap'
-                    - If 'base_price' is 'twap', the based price is the time weighted average price
-                    - If 'base_price' is 'vwap', the based price is the volume weighted average price
-                - 'weight_method': weighted method when calculating total trading pa by different orders' pa in each
-                    step, optional, default by 'mean'
-                    - If 'weight_method' is 'mean', calculating mean value of different orders' pa
-                    - If 'weight_method' is 'amount_weighted', calculating amount weighted average value of different
-                        orders' pa
-                    - If 'weight_method' is 'value_weighted', calculating value weighted average value of different
-                        orders' pa
-            - 'ffr_config': config for calculating fulfill rate(ffr), optional
-                - 'weight_method': weighted method when calculating total trading ffr by different orders' ffr in each
-                    step, optional, default by 'mean'
-                    - If 'weight_method' is 'mean', calculating mean value of different orders' ffr
-                    - If 'weight_method' is 'amount_weighted', calculating amount weighted average value of different
-                        orders' ffr
-                    - If 'weight_method' is 'value_weighted', calculating value weighted average value of different
-                        orders' ffr
-            Example:
-                {
-                    'show_indicator': True,
-                    'pa_config': {
-                        "agg": "twap",  # "vwap"
-                        "price": "$close", # default to use deal price of the exchange
-                    },
-                    'ffr_config':{
-                        'weight_method': 'value_weighted',
-                    }
-                }
-        generate_portfolio_metrics : bool, optional
-            whether to generate portfolio_metrics, by default False
-        verbose : bool, optional
-            whether to print trading info, by default False
-        track_data : bool, optional
-            whether to generate trade_decision, will be used when training rl agent
-            - If `self.track_data` is true, when making data for training, the input `trade_decision` of `execute` will
-                be generated by `collect_data`
-            - Else,  `trade_decision` will not be generated
-
-        trade_exchange : Exchange
-            exchange that provides market info, used to generate portfolio_metrics
-            - If generate_portfolio_metrics is None, trade_exchange will be ignored
-            - Else If `trade_exchange` is None, self.trade_exchange will be set with common_infra
-
-        common_infra : CommonInfrastructure, optional:
-            common infrastructure for backtesting, may including:
-            - trade_account : Account, optional
-                trade account for trading
-            - trade_exchange : Exchange, optional
-                exchange that provides market info
-
-        settle_type : str
-            Please refer to the docs of BasePosition.settle_start
-        """
-        self.time_per_step = time_per_step
-        self.indicator_config = indicator_config
-        self.generate_portfolio_metrics = generate_portfolio_metrics
-        self.verbose = verbose
-        self.track_data = track_data
-        self._trade_exchange = trade_exchange
-        self.level_infra = LevelInfrastructure()
-        self.level_infra.reset_infra(common_infra=common_infra, executor=self)
-        self._settle_type = settle_type
-        self.reset(start_time=start_time, end_time=end_time, common_infra=common_infra)
-        if common_infra is None:
-            get_module_logger("BaseExecutor").warning(f"`common_infra` is not set for {self}")
-
-        # record deal order amount in one day
-        self.dealt_order_amount: Dict[str, float] = defaultdict(float)
-        self.deal_day = None
-
-    def reset_common_infra(self, common_infra: CommonInfrastructure, copy_trade_account: bool = False) -> None:
-        """
-        reset infrastructure for trading
-            - reset trade_account
-        """
-        if not hasattr(self, "common_infra"):
-            self.common_infra = common_infra
-        else:
-            self.common_infra.update(common_infra)
-
-        self.level_infra.reset_infra(common_infra=self.common_infra)
-
-        if common_infra.has("trade_account"):
-            # NOTE: there is a trick in the code.
-            # shallow copy is used instead of deepcopy.
-            # 1. So positions are shared
-            # 2. Others are not shared, so each level has it own metrics (portfolio and trading metrics)
-            self.trade_account: Account = (
-                copy.copy(common_infra.get("trade_account"))
-                if copy_trade_account
-                else common_infra.get("trade_account")
-            )
-            self.trade_account.reset(freq=self.time_per_step, port_metr_enabled=self.generate_portfolio_metrics)
-
-    @property
-    def trade_exchange(self) -> Exchange:
-        """get trade exchange in a prioritized order"""
-        return getattr(self, "_trade_exchange", None) or self.common_infra.get("trade_exchange")
-
-    @property
-    def trade_calendar(self) -> TradeCalendarManager:
-        """
-        Though trade calendar can be accessed from multiple sources, but managing in a centralized way will make the
-        code easier
-        """
-        return self.level_infra.get("trade_calendar")
-
-    def reset(self, common_infra: CommonInfrastructure | None = None, **kwargs: Any) -> None:
-        """
-        - reset `start_time` and `end_time`, used in trade calendar
-        - reset `common_infra`, used to reset `trade_account`, `trade_exchange`, .etc
-        """
-
-        if "start_time" in kwargs or "end_time" in kwargs:
-            start_time = kwargs.get("start_time")
-            end_time = kwargs.get("end_time")
-            self.level_infra.reset_cal(freq=self.time_per_step, start_time=start_time, end_time=end_time)
-        if common_infra is not None:
-            self.reset_common_infra(common_infra)
-
-    def get_level_infra(self) -> LevelInfrastructure:
-        return self.level_infra
-
-    def finished(self) -> bool:
-        return self.trade_calendar.finished()
-
-    def execute(self, trade_decision: BaseTradeDecision, level: int = 0) -> List[object]:
-        """execute the trade decision and return the executed result
-
-        NOTE: this function is never used directly in the framework. Should we delete it?
-
-        Parameters
-        ----------
-        trade_decision : BaseTradeDecision
-
-        level : int
-            the level of current executor
-
-        Returns
-        ----------
-        execute_result : List[object]
-            the executed result for trade decision
-        """
-        return_value: dict = {}
-        for _decision in self.collect_data(trade_decision, return_value=return_value, level=level):
-            pass
-        return cast(list, return_value.get("execute_result"))
-
-    @abstractmethod
-    def _collect_data(
-        self,
-        trade_decision: BaseTradeDecision,
-        level: int = 0,
-    ) -> Union[Generator[Any, Any, Tuple[List[object], dict]], Tuple[List[object], dict]]:
-        """
-        Please refer to the doc of collect_data
-        The only difference between `_collect_data` and `collect_data` is that some common steps are moved into
-        collect_data
-
-        Parameters
-        ----------
-        Please refer to the doc of collect_data
-
-
-        Returns
-        -------
-        Tuple[List[object], dict]:
-            (<the executed result for trade decision>, <the extra kwargs for `self.trade_account.update_bar_end`>)
-        """
-
-    def collect_data(
-        self,
-        trade_decision: BaseTradeDecision,
-        return_value: dict | None = None,
-        level: int = 0,
-    ) -> Generator[Any, Any, List[object]]:
-        """Generator for collecting the trade decision data for rl training
-
-        his function will make a step forward
-
-        Parameters
-        ----------
-        trade_decision : BaseTradeDecision
-
-        level : int
-            the level of current executor. 0 indicates the top level
-
-        return_value : dict
-            the mem address to return the value
-            e.g.  {"return_value": <the executed result>}
-
-        Returns
-        ----------
-        execute_result : List[object]
-            the executed result for trade decision.
-            ** NOTE!!!! **:
-            1) This is necessary,  The return value of generator will be used in NestedExecutor
-            2) Please note the executed results are not merged.
-
-        Yields
-        -------
-        object
-            trade decision
-        """
-
-        if self.track_data:
-            yield trade_decision
-
-        atomic = not issubclass(self.__class__, NestedExecutor)  # issubclass(A, A) is True
-
-        if atomic and trade_decision.get_range_limit(default_value=None) is not None:
-            raise ValueError("atomic executor doesn't support specify `range_limit`")
-
-        if self._settle_type != BasePosition.ST_NO:
-            self.trade_account.current_position.settle_start(self._settle_type)
-
-        obj = self._collect_data(trade_decision=trade_decision, level=level)
-
-        if isinstance(obj, GeneratorType):
-            yield_res = yield from obj
-            assert isinstance(yield_res, tuple) and len(yield_res) == 2
-            res, kwargs = yield_res
-        else:
-            # Some concrete executor don't have inner decisions
-            res, kwargs = obj
-
-        trade_start_time, trade_end_time = self.trade_calendar.get_step_time()
-        # Account will not be changed in this function
-        self.trade_account.update_bar_end(
-            trade_start_time,
-            trade_end_time,
-            self.trade_exchange,
-            atomic=atomic,
-            outer_trade_decision=trade_decision,
-            indicator_config=self.indicator_config,
-            **kwargs,
-        )
-
-        self.trade_calendar.step()
-
-        if self._settle_type != BasePosition.ST_NO:
-            self.trade_account.current_position.settle_commit()
-
-        if return_value is not None:
-            return_value.update({"execute_result": res})
-
-        return res
-
-    def get_all_executors(self) -> List[BaseExecutor]:
-        """get all executors"""
-        return [self]
-
-
-class NestedExecutor(BaseExecutor):
-    """
-    Nested Executor with inner strategy and executor
-    - At each time `execute` is called, it will call the inner strategy and executor to execute the `trade_decision`
-        in a higher frequency env.
-    """
-
-    def __init__(
-        self,
-        time_per_step: str,
-        inner_executor: Union[BaseExecutor, dict],
-        inner_strategy: Union[BaseStrategy, dict],
-        start_time: Union[str, pd.Timestamp] = None,
-        end_time: Union[str, pd.Timestamp] = None,
-        indicator_config: dict = {},
-        generate_portfolio_metrics: bool = False,
-        verbose: bool = False,
-        track_data: bool = False,
-        skip_empty_decision: bool = True,
-        align_range_limit: bool = True,
-        common_infra: CommonInfrastructure | None = None,
-        **kwargs: Any,
-    ) -> None:
-        """
-        Parameters
-        ----------
-        inner_executor : BaseExecutor
-            trading env in each trading bar.
-        inner_strategy : BaseStrategy
-            trading strategy in each trading bar
-        skip_empty_decision: bool
-            Will the executor skip call inner loop when the decision is empty.
-            It should be False in following cases
-            - The decisions may be updated by steps
-            - The inner executor may not follow the decisions from the outer strategy
-        align_range_limit: bool
-            force to align the trade_range decision
-            It is only for nested executor, because range_limit is given by outer strategy
-        """
-        self.inner_executor: BaseExecutor = init_instance_by_config(
-            inner_executor,
-            common_infra=common_infra,
-            accept_types=BaseExecutor,
-        )
-        self.inner_strategy: BaseStrategy = init_instance_by_config(
-            inner_strategy,
-            common_infra=common_infra,
-            accept_types=BaseStrategy,
-        )
-
-        self._skip_empty_decision = skip_empty_decision
-        self._align_range_limit = align_range_limit
-
-        super(NestedExecutor, self).__init__(
-            time_per_step=time_per_step,
-            start_time=start_time,
-            end_time=end_time,
-            indicator_config=indicator_config,
-            generate_portfolio_metrics=generate_portfolio_metrics,
-            verbose=verbose,
-            track_data=track_data,
-            common_infra=common_infra,
-            **kwargs,
-        )
-
-    def reset_common_infra(self, common_infra: CommonInfrastructure, copy_trade_account: bool = False) -> None:
-        """
-        reset infrastructure for trading
-            - reset inner_strategy and inner_executor common infra
-        """
-        # NOTE: please refer to the docs of BaseExecutor.reset_common_infra for the meaning of `copy_trade_account`
-
-        # The first level follow the `copy_trade_account` from the upper level
-        super(NestedExecutor, self).reset_common_infra(common_infra, copy_trade_account=copy_trade_account)
-
-        # The lower level have to copy the trade_account
-        self.inner_executor.reset_common_infra(common_infra, copy_trade_account=True)
-        self.inner_strategy.reset_common_infra(common_infra)
-
-    def _init_sub_trading(self, trade_decision: BaseTradeDecision) -> None:
-        trade_start_time, trade_end_time = self.trade_calendar.get_step_time()
-        self.inner_executor.reset(start_time=trade_start_time, end_time=trade_end_time)
-        sub_level_infra = self.inner_executor.get_level_infra()
-        self.level_infra.set_sub_level_infra(sub_level_infra)
-        self.inner_strategy.reset(level_infra=sub_level_infra, outer_trade_decision=trade_decision)
-
-    def _update_trade_decision(self, trade_decision: BaseTradeDecision) -> BaseTradeDecision:
-        # outer strategy have chance to update decision each iterator
-        updated_trade_decision = trade_decision.update(self.inner_executor.trade_calendar)
-        if updated_trade_decision is not None:  # TODO: always is None for now?
-            trade_decision = updated_trade_decision
-            # NEW UPDATE
-            # create a hook for inner strategy to update outer decision
-            trade_decision = self.inner_strategy.alter_outer_trade_decision(trade_decision)
-        return trade_decision
-
-    def _collect_data(
-        self,
-        trade_decision: BaseTradeDecision,
-        level: int = 0,
-    ) -> Generator[Any, Any, Tuple[List[object], dict]]:
-        execute_result = []
-        inner_order_indicators = []
-        decision_list = []
-        # NOTE:
-        # - this is necessary to calculating the steps in sub level
-        # - more detailed information will be set into trade decision
-        self._init_sub_trading(trade_decision)
-
-        _inner_execute_result = None
-        while not self.inner_executor.finished():
-            trade_decision = self._update_trade_decision(trade_decision)
-
-            if trade_decision.empty() and self._skip_empty_decision:
-                # give one chance for outer strategy to update the strategy
-                # - For updating some information in the sub executor (the strategy have no knowledge of the inner
-                #   executor when generating the decision)
-                break
-
-            sub_cal: TradeCalendarManager = self.inner_executor.trade_calendar
-
-            # NOTE: make sure get_start_end_idx is after `self._update_trade_decision`
-            start_idx, end_idx = get_start_end_idx(sub_cal, trade_decision)
-            if not self._align_range_limit or start_idx <= sub_cal.get_trade_step() <= end_idx:
-                # if force align the range limit, skip the steps outside the decision range limit
-
-                res = self.inner_strategy.generate_trade_decision(_inner_execute_result)
-
-                # NOTE: !!!!!
-                # the two lines below is for a special case in RL
-                # To solve the conflicts below
-                # - Normally, user will create a strategy and embed it into Qlib's executor and simulator interaction
-                #   loop For a _nested qlib example_, (Qlib Strategy) <=> (Qlib Executor[(inner Qlib Strategy) <=>
-                #   (inner Qlib Executor)])
-                # - However, RL-based framework has it's own script to run the loop
-                #   For an _RL learning example_, (RL Policy) <=> (RL Env[(inner Qlib Executor)])
-                # To make it possible to run  _nested qlib example_ and _RL learning example_ together, the solution
-                # below is proposed
-                # - The entry script follow the example of  _RL learning example_ to be compatible with all kinds of
-                #   RL Framework
-                # - Each step of (RL Env) will make (inner Qlib Executor) one step forward
-                #     - (inner Qlib Strategy) is a proxy strategy, it will give the program control right to (RL Env)
-                #       by `yield from` and wait for the action from the policy
-                # So the two lines below is the implementation of yielding control rights
-                if isinstance(res, GeneratorType):
-                    res = yield from res
-
-                _inner_trade_decision: BaseTradeDecision = res
-
-                trade_decision.mod_inner_decision(_inner_trade_decision)  # propagate part of decision information
-
-                # NOTE sub_cal.get_step_time() must be called before collect_data in case of step shifting
-                decision_list.append((_inner_trade_decision, *sub_cal.get_step_time()))
-
-                # NOTE: Trade Calendar will step forward in the follow line
-                _inner_execute_result = yield from self.inner_executor.collect_data(
-                    trade_decision=_inner_trade_decision,
-                    level=level + 1,
-                )
-                assert isinstance(_inner_execute_result, list)
-                self.post_inner_exe_step(_inner_execute_result)
-                execute_result.extend(_inner_execute_result)
-
-                inner_order_indicators.append(
-                    self.inner_executor.trade_account.get_trade_indicator().get_order_indicator(raw=True),
-                )
-            else:
-                # do nothing and just step forward
-                sub_cal.step()
-
-        # Let inner strategy know that the outer level execution is done.
-        self.inner_strategy.post_upper_level_exe_step()
-
-        return execute_result, {"inner_order_indicators": inner_order_indicators, "decision_list": decision_list}
-
-    def post_inner_exe_step(self, inner_exe_res: List[object]) -> None:
-        """
-        A hook for doing sth after each step of inner strategy
-
-        Parameters
-        ----------
-        inner_exe_res :
-            the execution result of inner task
-        """
-        self.inner_strategy.post_exe_step(inner_exe_res)
-
-    def get_all_executors(self) -> List[BaseExecutor]:
-        """get all executors, including self and inner_executor.get_all_executors()"""
-        return [self, *self.inner_executor.get_all_executors()]
-
-
-def _retrieve_orders_from_decision(trade_decision: BaseTradeDecision) -> List[Order]:
-    """
-    IDE-friendly helper function.
-    """
-    decisions = trade_decision.get_decision()
-    orders: List[Order] = []
-    for decision in decisions:
-        assert isinstance(decision, Order)
-        orders.append(decision)
-    return orders
-
-
-class SimulatorExecutor(BaseExecutor):
-    """Executor that simulate the true market"""
-
-    # TODO: TT_SERIAL & TT_PARAL will be replaced by feature fix_pos now.
-    # Please remove them in the future.
-
-    # available trade_types
-    TT_SERIAL = "serial"
-    # The orders will be executed serially in a sequence
-    # In each trading step, it is possible that users sell instruments first and use the money to buy new instruments
-    TT_PARAL = "parallel"
-    # The orders will be executed in parallel
-    # In each trading step, if users try to sell instruments first and buy new instruments with money, failure will
-    # occur
-
-    def __init__(
-        self,
-        time_per_step: str,
-        start_time: Union[str, pd.Timestamp] = None,
-        end_time: Union[str, pd.Timestamp] = None,
-        indicator_config: dict = {},
-        generate_portfolio_metrics: bool = False,
-        verbose: bool = False,
-        track_data: bool = False,
-        common_infra: CommonInfrastructure | None = None,
-        trade_type: str = TT_SERIAL,
-        **kwargs: Any,
-    ) -> None:
-        """
-        Parameters
-        ----------
-        trade_type: str
-            please refer to the doc of `TT_SERIAL` & `TT_PARAL`
-        """
-        super(SimulatorExecutor, self).__init__(
-            time_per_step=time_per_step,
-            start_time=start_time,
-            end_time=end_time,
-            indicator_config=indicator_config,
-            generate_portfolio_metrics=generate_portfolio_metrics,
-            verbose=verbose,
-            track_data=track_data,
-            common_infra=common_infra,
-            **kwargs,
-        )
-
-        self.trade_type = trade_type
-
-    def _get_order_iterator(self, trade_decision: BaseTradeDecision) -> List[Order]:
-        """
-
-        Parameters
-        ----------
-        trade_decision : BaseTradeDecision
-            the trade decision given by the strategy
-
-        Returns
-        -------
-        List[Order]:
-            get a list orders according to `self.trade_type`
-        """
-        orders = _retrieve_orders_from_decision(trade_decision)
-
-        if self.trade_type == self.TT_SERIAL:
-            # Orders will be traded in a parallel way
-            order_it = orders
-        elif self.trade_type == self.TT_PARAL:
-            # NOTE: !!!!!!!
-            # Assumption: there will not be orders in different trading direction in a single step of a strategy !!!!
-            # The parallel trading failure will be caused only by the conflicts of money
-            # Therefore, make the buying go first will make sure the conflicts happen.
-            # It equals to parallel trading after sorting the order by direction
-            order_it = sorted(orders, key=lambda order: -order.direction)
-        else:
-            raise NotImplementedError(f"This type of input is not supported")
-        return order_it
-
-    def _collect_data(self, trade_decision: BaseTradeDecision, level: int = 0) -> Tuple[List[object], dict]:
-        trade_start_time, _ = self.trade_calendar.get_step_time()
-        execute_result: list = []
-
-        for order in self._get_order_iterator(trade_decision):
-            # Each time we move into a new date, clear `self.dealt_order_amount` since it only maintains intraday
-            # information.
-            now_deal_day = self.trade_calendar.get_step_time()[0].floor(freq="D")
-            if self.deal_day is None or now_deal_day > self.deal_day:
-                self.dealt_order_amount = defaultdict(float)
-                self.deal_day = now_deal_day
-
-            # execute the order.
-            # NOTE: The trade_account will be changed in this function
-            trade_val, trade_cost, trade_price = self.trade_exchange.deal_order(
-                order,
-                trade_account=self.trade_account,
-                dealt_order_amount=self.dealt_order_amount,
-            )
-            execute_result.append((order, trade_val, trade_cost, trade_price))
-
-            self.dealt_order_amount[order.stock_id] += order.deal_amount
-
-            if self.verbose:
-                print(
-                    "[I {:%Y-%m-%d %H:%M:%S}]: {} {}, price {:.2f}, amount {}, deal_amount {}, factor {}, "
-                    "value {:.2f}, cash {:.2f}.".format(
-                        trade_start_time,
-                        "sell" if order.direction == Order.SELL else "buy",
-                        order.stock_id,
-                        trade_price,
-                        order.amount,
-                        order.deal_amount,
-                        order.factor,
-                        trade_val,
-                        self.trade_account.get_cash(),
-                    ),
-                )
-        return execute_result, {"trade_info": execute_result}
+from __future__ import annotations
+
+import copy
+from abc import abstractmethod
+from collections import defaultdict
+from types import GeneratorType
+from typing import Any, Dict, Generator, List, Tuple, Union, cast
+
+import pandas as pd
+
+from qlib.backtest.account import Account
+from qlib.backtest.position import BasePosition
+from qlib.log import get_module_logger
+
+from ..strategy.base import BaseStrategy
+from ..utils import init_instance_by_config
+from .decision import BaseTradeDecision, Order
+from .exchange import Exchange
+from .utils import CommonInfrastructure, LevelInfrastructure, TradeCalendarManager, get_start_end_idx
+
+
+class BaseExecutor:
+    """Base executor for trading"""
+
+    def __init__(
+        self,
+        time_per_step: str,
+        start_time: Union[str, pd.Timestamp] = None,
+        end_time: Union[str, pd.Timestamp] = None,
+        indicator_config: dict = {},
+        generate_portfolio_metrics: bool = False,
+        verbose: bool = False,
+        track_data: bool = False,
+        trade_exchange: Exchange | None = None,
+        common_infra: CommonInfrastructure | None = None,
+        settle_type: str = BasePosition.ST_NO,
+        **kwargs: Any,
+    ) -> None:
+        """
+        Parameters
+        ----------
+        time_per_step : str
+            trade time per trading step, used for generate the trade calendar
+        show_indicator: bool, optional
+            whether to show indicators, :
+            - 'pa', the price advantage
+            - 'pos', the positive rate
+            - 'ffr', the fulfill rate
+        indicator_config: dict, optional
+            config for calculating trade indicator, including the following fields:
+            - 'show_indicator': whether to show indicators, optional, default by False. The indicators includes
+                - 'pa', the price advantage
+                - 'pos', the positive rate
+                - 'ffr', the fulfill rate
+            - 'pa_config': config for calculating price advantage(pa), optional
+                - 'base_price': the based price than which the trading price is advanced, Optional, default by 'twap'
+                    - If 'base_price' is 'twap', the based price is the time weighted average price
+                    - If 'base_price' is 'vwap', the based price is the volume weighted average price
+                - 'weight_method': weighted method when calculating total trading pa by different orders' pa in each
+                    step, optional, default by 'mean'
+                    - If 'weight_method' is 'mean', calculating mean value of different orders' pa
+                    - If 'weight_method' is 'amount_weighted', calculating amount weighted average value of different
+                        orders' pa
+                    - If 'weight_method' is 'value_weighted', calculating value weighted average value of different
+                        orders' pa
+            - 'ffr_config': config for calculating fulfill rate(ffr), optional
+                - 'weight_method': weighted method when calculating total trading ffr by different orders' ffr in each
+                    step, optional, default by 'mean'
+                    - If 'weight_method' is 'mean', calculating mean value of different orders' ffr
+                    - If 'weight_method' is 'amount_weighted', calculating amount weighted average value of different
+                        orders' ffr
+                    - If 'weight_method' is 'value_weighted', calculating value weighted average value of different
+                        orders' ffr
+            Example:
+                {
+                    'show_indicator': True,
+                    'pa_config': {
+                        "agg": "twap",  # "vwap"
+                        "price": "$close", # default to use deal price of the exchange
+                    },
+                    'ffr_config':{
+                        'weight_method': 'value_weighted',
+                    }
+                }
+        generate_portfolio_metrics : bool, optional
+            whether to generate portfolio_metrics, by default False
+        verbose : bool, optional
+            whether to print trading info, by default False
+        track_data : bool, optional
+            whether to generate trade_decision, will be used when training rl agent
+            - If `self.track_data` is true, when making data for training, the input `trade_decision` of `execute` will
+                be generated by `collect_data`
+            - Else,  `trade_decision` will not be generated
+
+        trade_exchange : Exchange
+            exchange that provides market info, used to generate portfolio_metrics
+            - If generate_portfolio_metrics is None, trade_exchange will be ignored
+            - Else If `trade_exchange` is None, self.trade_exchange will be set with common_infra
+
+        common_infra : CommonInfrastructure, optional:
+            common infrastructure for backtesting, may including:
+            - trade_account : Account, optional
+                trade account for trading
+            - trade_exchange : Exchange, optional
+                exchange that provides market info
+
+        settle_type : str
+            Please refer to the docs of BasePosition.settle_start
+        """
+        self.time_per_step = time_per_step
+        self.indicator_config = indicator_config
+        self.generate_portfolio_metrics = generate_portfolio_metrics
+        self.verbose = verbose
+        self.track_data = track_data
+        self._trade_exchange = trade_exchange
+        self.level_infra = LevelInfrastructure()
+        self.level_infra.reset_infra(common_infra=common_infra, executor=self)
+        self._settle_type = settle_type
+        self.reset(start_time=start_time, end_time=end_time, common_infra=common_infra)
+        if common_infra is None:
+            get_module_logger("BaseExecutor").warning(f"`common_infra` is not set for {self}")
+
+        # record deal order amount in one day
+        self.dealt_order_amount: Dict[str, float] = defaultdict(float)
+        self.deal_day = None
+
+    def reset_common_infra(self, common_infra: CommonInfrastructure, copy_trade_account: bool = False) -> None:
+        """
+        reset infrastructure for trading
+            - reset trade_account
+        """
+        if not hasattr(self, "common_infra"):
+            self.common_infra = common_infra
+        else:
+            self.common_infra.update(common_infra)
+
+        self.level_infra.reset_infra(common_infra=self.common_infra)
+
+        if common_infra.has("trade_account"):
+            # NOTE: there is a trick in the code.
+            # shallow copy is used instead of deepcopy.
+            # 1. So positions are shared
+            # 2. Others are not shared, so each level has it own metrics (portfolio and trading metrics)
+            self.trade_account: Account = (
+                copy.copy(common_infra.get("trade_account"))
+                if copy_trade_account
+                else common_infra.get("trade_account")
+            )
+            self.trade_account.reset(freq=self.time_per_step, port_metr_enabled=self.generate_portfolio_metrics)
+
+    @property
+    def trade_exchange(self) -> Exchange:
+        """get trade exchange in a prioritized order"""
+        return getattr(self, "_trade_exchange", None) or self.common_infra.get("trade_exchange")
+
+    @property
+    def trade_calendar(self) -> TradeCalendarManager:
+        """
+        Though trade calendar can be accessed from multiple sources, but managing in a centralized way will make the
+        code easier
+        """
+        return self.level_infra.get("trade_calendar")
+
+    def reset(self, common_infra: CommonInfrastructure | None = None, **kwargs: Any) -> None:
+        """
+        - reset `start_time` and `end_time`, used in trade calendar
+        - reset `common_infra`, used to reset `trade_account`, `trade_exchange`, .etc
+        """
+
+        if "start_time" in kwargs or "end_time" in kwargs:
+            start_time = kwargs.get("start_time")
+            end_time = kwargs.get("end_time")
+            self.level_infra.reset_cal(freq=self.time_per_step, start_time=start_time, end_time=end_time)
+        if common_infra is not None:
+            self.reset_common_infra(common_infra)
+
+    def get_level_infra(self) -> LevelInfrastructure:
+        return self.level_infra
+
+    def finished(self) -> bool:
+        return self.trade_calendar.finished()
+
+    def execute(self, trade_decision: BaseTradeDecision, level: int = 0) -> List[object]:
+        """execute the trade decision and return the executed result
+
+        NOTE: this function is never used directly in the framework. Should we delete it?
+
+        Parameters
+        ----------
+        trade_decision : BaseTradeDecision
+
+        level : int
+            the level of current executor
+
+        Returns
+        ----------
+        execute_result : List[object]
+            the executed result for trade decision
+        """
+        return_value: dict = {}
+        for _decision in self.collect_data(trade_decision, return_value=return_value, level=level):
+            pass
+        return cast(list, return_value.get("execute_result"))
+
+    @abstractmethod
+    def _collect_data(
+        self,
+        trade_decision: BaseTradeDecision,
+        level: int = 0,
+    ) -> Union[Generator[Any, Any, Tuple[List[object], dict]], Tuple[List[object], dict]]:
+        """
+        Please refer to the doc of collect_data
+        The only difference between `_collect_data` and `collect_data` is that some common steps are moved into
+        collect_data
+
+        Parameters
+        ----------
+        Please refer to the doc of collect_data
+
+
+        Returns
+        -------
+        Tuple[List[object], dict]:
+            (<the executed result for trade decision>, <the extra kwargs for `self.trade_account.update_bar_end`>)
+        """
+
+    def collect_data(
+        self,
+        trade_decision: BaseTradeDecision,
+        return_value: dict | None = None,
+        level: int = 0,
+    ) -> Generator[Any, Any, List[object]]:
+        """Generator for collecting the trade decision data for rl training
+
+        his function will make a step forward
+
+        Parameters
+        ----------
+        trade_decision : BaseTradeDecision
+
+        level : int
+            the level of current executor. 0 indicates the top level
+
+        return_value : dict
+            the mem address to return the value
+            e.g.  {"return_value": <the executed result>}
+
+        Returns
+        ----------
+        execute_result : List[object]
+            the executed result for trade decision.
+            ** NOTE!!!! **:
+            1) This is necessary,  The return value of generator will be used in NestedExecutor
+            2) Please note the executed results are not merged.
+
+        Yields
+        -------
+        object
+            trade decision
+        """
+
+        if self.track_data:
+            yield trade_decision
+
+        atomic = not issubclass(self.__class__, NestedExecutor)  # issubclass(A, A) is True
+
+        if atomic and trade_decision.get_range_limit(default_value=None) is not None:
+            raise ValueError("atomic executor doesn't support specify `range_limit`")
+
+        if self._settle_type != BasePosition.ST_NO:
+            self.trade_account.current_position.settle_start(self._settle_type)
+
+        obj = self._collect_data(trade_decision=trade_decision, level=level)
+
+        if isinstance(obj, GeneratorType):
+            yield_res = yield from obj
+            assert isinstance(yield_res, tuple) and len(yield_res) == 2
+            res, kwargs = yield_res
+        else:
+            # Some concrete executor don't have inner decisions
+            res, kwargs = obj
+
+        trade_start_time, trade_end_time = self.trade_calendar.get_step_time()
+        # Account will not be changed in this function
+        self.trade_account.update_bar_end(
+            trade_start_time,
+            trade_end_time,
+            self.trade_exchange,
+            atomic=atomic,
+            outer_trade_decision=trade_decision,
+            indicator_config=self.indicator_config,
+            **kwargs,
+        )
+
+        self.trade_calendar.step()
+
+        if self._settle_type != BasePosition.ST_NO:
+            self.trade_account.current_position.settle_commit()
+
+        if return_value is not None:
+            return_value.update({"execute_result": res})
+
+        return res
+
+    def get_all_executors(self) -> List[BaseExecutor]:
+        """get all executors"""
+        return [self]
+
+
+class NestedExecutor(BaseExecutor):
+    """
+    Nested Executor with inner strategy and executor
+    - At each time `execute` is called, it will call the inner strategy and executor to execute the `trade_decision`
+        in a higher frequency env.
+    """
+
+    def __init__(
+        self,
+        time_per_step: str,
+        inner_executor: Union[BaseExecutor, dict],
+        inner_strategy: Union[BaseStrategy, dict],
+        start_time: Union[str, pd.Timestamp] = None,
+        end_time: Union[str, pd.Timestamp] = None,
+        indicator_config: dict = {},
+        generate_portfolio_metrics: bool = False,
+        verbose: bool = False,
+        track_data: bool = False,
+        skip_empty_decision: bool = True,
+        align_range_limit: bool = True,
+        common_infra: CommonInfrastructure | None = None,
+        **kwargs: Any,
+    ) -> None:
+        """
+        Parameters
+        ----------
+        inner_executor : BaseExecutor
+            trading env in each trading bar.
+        inner_strategy : BaseStrategy
+            trading strategy in each trading bar
+        skip_empty_decision: bool
+            Will the executor skip call inner loop when the decision is empty.
+            It should be False in following cases
+            - The decisions may be updated by steps
+            - The inner executor may not follow the decisions from the outer strategy
+        align_range_limit: bool
+            force to align the trade_range decision
+            It is only for nested executor, because range_limit is given by outer strategy
+        """
+        self.inner_executor: BaseExecutor = init_instance_by_config(
+            inner_executor,
+            common_infra=common_infra,
+            accept_types=BaseExecutor,
+        )
+        self.inner_strategy: BaseStrategy = init_instance_by_config(
+            inner_strategy,
+            common_infra=common_infra,
+            accept_types=BaseStrategy,
+        )
+
+        self._skip_empty_decision = skip_empty_decision
+        self._align_range_limit = align_range_limit
+
+        super(NestedExecutor, self).__init__(
+            time_per_step=time_per_step,
+            start_time=start_time,
+            end_time=end_time,
+            indicator_config=indicator_config,
+            generate_portfolio_metrics=generate_portfolio_metrics,
+            verbose=verbose,
+            track_data=track_data,
+            common_infra=common_infra,
+            **kwargs,
+        )
+
+    def reset_common_infra(self, common_infra: CommonInfrastructure, copy_trade_account: bool = False) -> None:
+        """
+        reset infrastructure for trading
+            - reset inner_strategy and inner_executor common infra
+        """
+        # NOTE: please refer to the docs of BaseExecutor.reset_common_infra for the meaning of `copy_trade_account`
+
+        # The first level follow the `copy_trade_account` from the upper level
+        super(NestedExecutor, self).reset_common_infra(common_infra, copy_trade_account=copy_trade_account)
+
+        # The lower level have to copy the trade_account
+        self.inner_executor.reset_common_infra(common_infra, copy_trade_account=True)
+        self.inner_strategy.reset_common_infra(common_infra)
+
+    def _init_sub_trading(self, trade_decision: BaseTradeDecision) -> None:
+        trade_start_time, trade_end_time = self.trade_calendar.get_step_time()
+        self.inner_executor.reset(start_time=trade_start_time, end_time=trade_end_time)
+        sub_level_infra = self.inner_executor.get_level_infra()
+        self.level_infra.set_sub_level_infra(sub_level_infra)
+        self.inner_strategy.reset(level_infra=sub_level_infra, outer_trade_decision=trade_decision)
+
+    def _update_trade_decision(self, trade_decision: BaseTradeDecision) -> BaseTradeDecision:
+        # outer strategy have chance to update decision each iterator
+        updated_trade_decision = trade_decision.update(self.inner_executor.trade_calendar)
+        if updated_trade_decision is not None:  # TODO: always is None for now?
+            trade_decision = updated_trade_decision
+            # NEW UPDATE
+            # create a hook for inner strategy to update outer decision
+            trade_decision = self.inner_strategy.alter_outer_trade_decision(trade_decision)
+        return trade_decision
+
+    def _collect_data(
+        self,
+        trade_decision: BaseTradeDecision,
+        level: int = 0,
+    ) -> Generator[Any, Any, Tuple[List[object], dict]]:
+        execute_result = []
+        inner_order_indicators = []
+        decision_list = []
+        # NOTE:
+        # - this is necessary to calculating the steps in sub level
+        # - more detailed information will be set into trade decision
+        self._init_sub_trading(trade_decision)
+
+        _inner_execute_result = None
+        while not self.inner_executor.finished():
+            trade_decision = self._update_trade_decision(trade_decision)
+
+            if trade_decision.empty() and self._skip_empty_decision:
+                # give one chance for outer strategy to update the strategy
+                # - For updating some information in the sub executor (the strategy have no knowledge of the inner
+                #   executor when generating the decision)
+                break
+
+            sub_cal: TradeCalendarManager = self.inner_executor.trade_calendar
+
+            # NOTE: make sure get_start_end_idx is after `self._update_trade_decision`
+            start_idx, end_idx = get_start_end_idx(sub_cal, trade_decision)
+            if not self._align_range_limit or start_idx <= sub_cal.get_trade_step() <= end_idx:
+                # if force align the range limit, skip the steps outside the decision range limit
+
+                res = self.inner_strategy.generate_trade_decision(_inner_execute_result)
+
+                # NOTE: !!!!!
+                # the two lines below is for a special case in RL
+                # To solve the conflicts below
+                # - Normally, user will create a strategy and embed it into Qlib's executor and simulator interaction
+                #   loop For a _nested qlib example_, (Qlib Strategy) <=> (Qlib Executor[(inner Qlib Strategy) <=>
+                #   (inner Qlib Executor)])
+                # - However, RL-based framework has it's own script to run the loop
+                #   For an _RL learning example_, (RL Policy) <=> (RL Env[(inner Qlib Executor)])
+                # To make it possible to run  _nested qlib example_ and _RL learning example_ together, the solution
+                # below is proposed
+                # - The entry script follow the example of  _RL learning example_ to be compatible with all kinds of
+                #   RL Framework
+                # - Each step of (RL Env) will make (inner Qlib Executor) one step forward
+                #     - (inner Qlib Strategy) is a proxy strategy, it will give the program control right to (RL Env)
+                #       by `yield from` and wait for the action from the policy
+                # So the two lines below is the implementation of yielding control rights
+                if isinstance(res, GeneratorType):
+                    res = yield from res
+
+                _inner_trade_decision: BaseTradeDecision = res
+
+                trade_decision.mod_inner_decision(_inner_trade_decision)  # propagate part of decision information
+
+                # NOTE sub_cal.get_step_time() must be called before collect_data in case of step shifting
+                decision_list.append((_inner_trade_decision, *sub_cal.get_step_time()))
+
+                # NOTE: Trade Calendar will step forward in the follow line
+                _inner_execute_result = yield from self.inner_executor.collect_data(
+                    trade_decision=_inner_trade_decision,
+                    level=level + 1,
+                )
+                assert isinstance(_inner_execute_result, list)
+                self.post_inner_exe_step(_inner_execute_result)
+                execute_result.extend(_inner_execute_result)
+
+                inner_order_indicators.append(
+                    self.inner_executor.trade_account.get_trade_indicator().get_order_indicator(raw=True),
+                )
+            else:
+                # do nothing and just step forward
+                sub_cal.step()
+
+        # Let inner strategy know that the outer level execution is done.
+        self.inner_strategy.post_upper_level_exe_step()
+
+        return execute_result, {"inner_order_indicators": inner_order_indicators, "decision_list": decision_list}
+
+    def post_inner_exe_step(self, inner_exe_res: List[object]) -> None:
+        """
+        A hook for doing sth after each step of inner strategy
+
+        Parameters
+        ----------
+        inner_exe_res :
+            the execution result of inner task
+        """
+        self.inner_strategy.post_exe_step(inner_exe_res)
+
+    def get_all_executors(self) -> List[BaseExecutor]:
+        """get all executors, including self and inner_executor.get_all_executors()"""
+        return [self, *self.inner_executor.get_all_executors()]
+
+
+def _retrieve_orders_from_decision(trade_decision: BaseTradeDecision) -> List[Order]:
+    """
+    IDE-friendly helper function.
+    """
+    decisions = trade_decision.get_decision()
+    orders: List[Order] = []
+    for decision in decisions:
+        assert isinstance(decision, Order)
+        orders.append(decision)
+    return orders
+
+
+class SimulatorExecutor(BaseExecutor):
+    """Executor that simulate the true market"""
+
+    # TODO: TT_SERIAL & TT_PARAL will be replaced by feature fix_pos now.
+    # Please remove them in the future.
+
+    # available trade_types
+    TT_SERIAL = "serial"
+    # The orders will be executed serially in a sequence
+    # In each trading step, it is possible that users sell instruments first and use the money to buy new instruments
+    TT_PARAL = "parallel"
+    # The orders will be executed in parallel
+    # In each trading step, if users try to sell instruments first and buy new instruments with money, failure will
+    # occur
+
+    def __init__(
+        self,
+        time_per_step: str,
+        start_time: Union[str, pd.Timestamp] = None,
+        end_time: Union[str, pd.Timestamp] = None,
+        indicator_config: dict = {},
+        generate_portfolio_metrics: bool = False,
+        verbose: bool = False,
+        track_data: bool = False,
+        common_infra: CommonInfrastructure | None = None,
+        trade_type: str = TT_SERIAL,
+        **kwargs: Any,
+    ) -> None:
+        """
+        Parameters
+        ----------
+        trade_type: str
+            please refer to the doc of `TT_SERIAL` & `TT_PARAL`
+        """
+        super(SimulatorExecutor, self).__init__(
+            time_per_step=time_per_step,
+            start_time=start_time,
+            end_time=end_time,
+            indicator_config=indicator_config,
+            generate_portfolio_metrics=generate_portfolio_metrics,
+            verbose=verbose,
+            track_data=track_data,
+            common_infra=common_infra,
+            **kwargs,
+        )
+
+        self.trade_type = trade_type
+
+    def _get_order_iterator(self, trade_decision: BaseTradeDecision) -> List[Order]:
+        """
+
+        Parameters
+        ----------
+        trade_decision : BaseTradeDecision
+            the trade decision given by the strategy
+
+        Returns
+        -------
+        List[Order]:
+            get a list orders according to `self.trade_type`
+        """
+        orders = _retrieve_orders_from_decision(trade_decision)
+
+        if self.trade_type == self.TT_SERIAL:
+            # Orders will be traded in a parallel way
+            order_it = orders
+        elif self.trade_type == self.TT_PARAL:
+            # NOTE: !!!!!!!
+            # Assumption: there will not be orders in different trading direction in a single step of a strategy !!!!
+            # The parallel trading failure will be caused only by the conflicts of money
+            # Therefore, make the buying go first will make sure the conflicts happen.
+            # It equals to parallel trading after sorting the order by direction
+            order_it = sorted(orders, key=lambda order: -order.direction)
+        else:
+            raise NotImplementedError(f"This type of input is not supported")
+        return order_it
+
+    def _collect_data(self, trade_decision: BaseTradeDecision, level: int = 0) -> Tuple[List[object], dict]:
+        trade_start_time, _ = self.trade_calendar.get_step_time()
+        execute_result: list = []
+
+        for order in self._get_order_iterator(trade_decision):
+            # Each time we move into a new date, clear `self.dealt_order_amount` since it only maintains intraday
+            # information.
+            now_deal_day = self.trade_calendar.get_step_time()[0].floor(freq="D")
+            if self.deal_day is None or now_deal_day > self.deal_day:
+                self.dealt_order_amount = defaultdict(float)
+                self.deal_day = now_deal_day
+
+            # execute the order.
+            # NOTE: The trade_account will be changed in this function
+            trade_val, trade_cost, trade_price = self.trade_exchange.deal_order(
+                order,
+                trade_account=self.trade_account,
+                dealt_order_amount=self.dealt_order_amount,
+            )
+            execute_result.append((order, trade_val, trade_cost, trade_price))
+
+            self.dealt_order_amount[order.stock_id] += order.deal_amount
+
+            if self.verbose:
+                print(
+                    "[I {:%Y-%m-%d %H:%M:%S}]: {} {}, price {:.2f}, amount {}, deal_amount {}, factor {}, "
+                    "value {:.2f}, cash {:.2f}.".format(
+                        trade_start_time,
+                        "sell" if order.direction == Order.SELL else "buy",
+                        order.stock_id,
+                        trade_price,
+                        order.amount,
+                        order.deal_amount,
+                        order.factor,
+                        trade_val,
+                        self.trade_account.get_cash(),
+                    ),
+                )
+        return execute_result, {"trade_info": execute_result}
```

## qlib/backtest/high_performance_ds.py

 * *Ordering differences only*

```diff
@@ -1,658 +1,658 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-from __future__ import annotations
-
-import inspect
-import logging
-from collections import OrderedDict
-from functools import lru_cache
-from typing import Any, Callable, Dict, Iterable, List, Optional, Text, Union, cast
-
-import numpy as np
-import pandas as pd
-
-import qlib.utils.index_data as idd
-
-from ..log import get_module_logger
-from ..utils.index_data import IndexData, SingleData
-from ..utils.resam import resam_ts_data, ts_data_last
-from ..utils.time import Freq, is_single_value
-
-
-class BaseQuote:
-    def __init__(self, quote_df: pd.DataFrame, freq: str) -> None:
-        self.logger = get_module_logger("online operator", level=logging.INFO)
-
-    def get_all_stock(self) -> Iterable:
-        """return all stock codes
-
-        Return
-        ------
-        Iterable
-            all stock codes
-        """
-
-        raise NotImplementedError(f"Please implement the `get_all_stock` method")
-
-    def get_data(
-        self,
-        stock_id: str,
-        start_time: Union[pd.Timestamp, str],
-        end_time: Union[pd.Timestamp, str],
-        field: Union[str],
-        method: Optional[str] = None,
-    ) -> Union[None, int, float, bool, IndexData]:
-        """get the specific field of stock data during start time and end_time,
-           and apply method to the data.
-
-           Example:
-            .. code-block::
-                                        $close      $volume
-                instrument  datetime
-                SH600000    2010-01-04  86.778313   16162960.0
-                            2010-01-05  87.433578   28117442.0
-                            2010-01-06  85.713585   23632884.0
-                            2010-01-07  83.788803   20813402.0
-                            2010-01-08  84.730675   16044853.0
-
-                SH600655    2010-01-04  2699.567383  158193.328125
-                            2010-01-08  2612.359619   77501.406250
-                            2010-01-11  2712.982422  160852.390625
-                            2010-01-12  2788.688232  164587.937500
-                            2010-01-13  2790.604004  145460.453125
-
-                this function is used for three case:
-
-                1. method is not None. It returns int/float/bool/None.
-                    - It will return None in one case, the method return None
-
-                    print(get_data(stock_id="SH600000", start_time="2010-01-04", end_time="2010-01-06", field="$close", method="last"))
-
-                    85.713585
-
-                2. method is None. It returns IndexData.
-                    print(get_data(stock_id="SH600000", start_time="2010-01-04", end_time="2010-01-06", field="$close", method=None))
-
-                    IndexData([86.778313, 87.433578, 85.713585], [2010-01-04, 2010-01-05, 2010-01-06])
-
-        Parameters
-        ----------
-        stock_id: str
-        start_time : Union[pd.Timestamp, str]
-            closed start time for backtest
-        end_time : Union[pd.Timestamp, str]
-            closed end time for backtest
-        field : str
-            the columns of data to fetch
-        method : Union[str, None]
-            the method apply to data.
-            e.g [None, "last", "all", "sum", "mean", "ts_data_last"]
-
-        Return
-        ----------
-        Union[None, int, float, bool, IndexData]
-            it will return None in following cases
-            - There is no stock data which meet the query criterion from data source.
-            - The `method` returns None
-        """
-
-        raise NotImplementedError(f"Please implement the `get_data` method")
-
-
-class PandasQuote(BaseQuote):
-    def __init__(self, quote_df: pd.DataFrame, freq: str) -> None:
-        super().__init__(quote_df=quote_df, freq=freq)
-        quote_dict = {}
-        for stock_id, stock_val in quote_df.groupby(level="instrument"):
-            quote_dict[stock_id] = stock_val.droplevel(level="instrument")
-        self.data = quote_dict
-
-    def get_all_stock(self):
-        return self.data.keys()
-
-    def get_data(self, stock_id, start_time, end_time, field, method=None):
-        if method == "ts_data_last":
-            method = ts_data_last
-        stock_data = resam_ts_data(self.data[stock_id][field], start_time, end_time, method=method)
-        if stock_data is None:
-            return None
-        elif isinstance(stock_data, (bool, np.bool_, int, float, np.number)):
-            return stock_data
-        elif isinstance(stock_data, pd.Series):
-            return idd.SingleData(stock_data)
-        else:
-            raise ValueError(f"stock data from resam_ts_data must be a number, pd.Series or pd.DataFrame")
-
-
-class NumpyQuote(BaseQuote):
-    def __init__(self, quote_df: pd.DataFrame, freq: str, region: str = "cn") -> None:
-        """NumpyQuote
-
-        Parameters
-        ----------
-        quote_df : pd.DataFrame
-            the init dataframe from qlib.
-        self.data : Dict(stock_id, IndexData.DataFrame)
-        """
-        super().__init__(quote_df=quote_df, freq=freq)
-        quote_dict = {}
-        for stock_id, stock_val in quote_df.groupby(level="instrument"):
-            quote_dict[stock_id] = idd.MultiData(stock_val.droplevel(level="instrument"))
-            quote_dict[stock_id].sort_index()  # To support more flexible slicing, we must sort data first
-        self.data = quote_dict
-
-        n, unit = Freq.parse(freq)
-        if unit in Freq.SUPPORT_CAL_LIST:
-            self.freq = Freq.get_timedelta(1, unit)
-        else:
-            raise ValueError(f"{freq} is not supported in NumpyQuote")
-        self.region = region
-
-    def get_all_stock(self):
-        return self.data.keys()
-
-    @lru_cache(maxsize=512)
-    def get_data(self, stock_id, start_time, end_time, field, method=None):
-        # check stock id
-        if stock_id not in self.get_all_stock():
-            return None
-
-        # single data
-        # If it don't consider the classification of single data, it will consume a lot of time.
-        if is_single_value(start_time, end_time, self.freq, self.region):
-            # this is a very special case.
-            # skip aggregating function to speed-up the query calculation
-
-            # FIXME:
-            # it will go to the else logic when it comes to the
-            # 1) the day before holiday when daily trading
-            # 2) the last minute of the day when intraday trading
-            try:
-                return self.data[stock_id].loc[start_time, field]
-            except KeyError:
-                return None
-        else:
-            data = self.data[stock_id].loc[start_time:end_time, field]
-            if data.empty:
-                return None
-            if method is not None:
-                data = self._agg_data(data, method)
-            return data
-
-    @staticmethod
-    def _agg_data(data: IndexData, method: str) -> Union[IndexData, np.ndarray, None]:
-        """Agg data by specific method."""
-        # FIXME: why not call the method of data directly?
-        if method == "sum":
-            return np.nansum(data)
-        elif method == "mean":
-            return np.nanmean(data)
-        elif method == "last":
-            # FIXME: I've never seen that this method was called.
-            # Please merge it with "ts_data_last"
-            return data[-1]
-        elif method == "all":
-            return data.all()
-        elif method == "ts_data_last":
-            valid_data = data.loc[~data.isna().data.astype(bool)]
-            if len(valid_data) == 0:
-                return None
-            else:
-                return valid_data.iloc[-1]
-        else:
-            raise ValueError(f"{method} is not supported")
-
-
-class BaseSingleMetric:
-    """
-    The data structure of the single metric.
-    The following methods are used for computing metrics in one indicator.
-    """
-
-    def __init__(self, metric: Union[dict, pd.Series]):
-        """Single data structure for each metric.
-
-        Parameters
-        ----------
-        metric : Union[dict, pd.Series]
-            keys/index is stock_id, value is the metric value.
-            for example:
-                SH600068    NaN
-                SH600079    1.0
-                SH600266    NaN
-                           ...
-                SZ300692    NaN
-                SZ300719    NaN,
-        """
-        raise NotImplementedError(f"Please implement the `__init__` method")
-
-    def __add__(self, other: Union[BaseSingleMetric, int, float]) -> BaseSingleMetric:
-        raise NotImplementedError(f"Please implement the `__add__` method")
-
-    def __radd__(self, other: Union[BaseSingleMetric, int, float]) -> BaseSingleMetric:
-        return self + other
-
-    def __sub__(self, other: Union[BaseSingleMetric, int, float]) -> BaseSingleMetric:
-        raise NotImplementedError(f"Please implement the `__sub__` method")
-
-    def __rsub__(self, other: Union[BaseSingleMetric, int, float]) -> BaseSingleMetric:
-        raise NotImplementedError(f"Please implement the `__rsub__` method")
-
-    def __mul__(self, other: Union[BaseSingleMetric, int, float]) -> BaseSingleMetric:
-        raise NotImplementedError(f"Please implement the `__mul__` method")
-
-    def __truediv__(self, other: Union[BaseSingleMetric, int, float]) -> BaseSingleMetric:
-        raise NotImplementedError(f"Please implement the `__truediv__` method")
-
-    def __eq__(self, other: object) -> BaseSingleMetric:
-        raise NotImplementedError(f"Please implement the `__eq__` method")
-
-    def __gt__(self, other: Union[BaseSingleMetric, int, float]) -> BaseSingleMetric:
-        raise NotImplementedError(f"Please implement the `__gt__` method")
-
-    def __lt__(self, other: Union[BaseSingleMetric, int, float]) -> BaseSingleMetric:
-        raise NotImplementedError(f"Please implement the `__lt__` method")
-
-    def __len__(self) -> int:
-        raise NotImplementedError(f"Please implement the `__len__` method")
-
-    def sum(self) -> float:
-        raise NotImplementedError(f"Please implement the `sum` method")
-
-    def mean(self) -> float:
-        raise NotImplementedError(f"Please implement the `mean` method")
-
-    def count(self) -> int:
-        """Return the count of the single metric, NaN is not included."""
-
-        raise NotImplementedError(f"Please implement the `count` method")
-
-    def abs(self) -> BaseSingleMetric:
-        raise NotImplementedError(f"Please implement the `abs` method")
-
-    @property
-    def empty(self) -> bool:
-        """If metric is empty, return True."""
-
-        raise NotImplementedError(f"Please implement the `empty` method")
-
-    def add(self, other: BaseSingleMetric, fill_value: float = None) -> BaseSingleMetric:
-        """Replace np.NaN with fill_value in two metrics and add them."""
-
-        raise NotImplementedError(f"Please implement the `add` method")
-
-    def replace(self, replace_dict: dict) -> BaseSingleMetric:
-        """Replace the value of metric according to replace_dict."""
-
-        raise NotImplementedError(f"Please implement the `replace` method")
-
-    def apply(self, func: Callable) -> BaseSingleMetric:
-        """Replace the value of metric with func (metric).
-        Currently, the func is only qlib/backtest/order/Order.parse_dir.
-        """
-
-        raise NotImplementedError(f"Please implement the 'apply' method")
-
-
-class BaseOrderIndicator:
-    """
-    The data structure of order indicator.
-    !!!NOTE: There are two ways to organize the data structure. Please choose a better way.
-        1. One way is using BaseSingleMetric to represent each metric. For example, the data
-        structure of PandasOrderIndicator is Dict[str, PandasSingleMetric]. It uses
-        PandasSingleMetric based on pd.Series to represent each metric.
-        2. The another way doesn't use BaseSingleMetric to represent each metric. The data
-        structure of PandasOrderIndicator is a whole matrix. It means you are not necessary
-        to inherit the BaseSingleMetric.
-    """
-
-    def __init__(self):
-        self.data = {}  # will be created in the subclass
-        self.logger = get_module_logger("online operator")
-
-    def assign(self, col: str, metric: Union[dict, pd.Series]) -> None:
-        """assign one metric.
-
-        Parameters
-        ----------
-        col : str
-            the metric name of one metric.
-        metric : Union[dict, pd.Series]
-            one metric with stock_id index, such as deal_amount, ffr, etc.
-            for example:
-                SH600068    NaN
-                SH600079    1.0
-                SH600266    NaN
-                           ...
-                SZ300692    NaN
-                SZ300719    NaN,
-        """
-
-        raise NotImplementedError(f"Please implement the 'assign' method")
-
-    def transfer(self, func: Callable, new_col: str = None) -> Optional[BaseSingleMetric]:
-        """compute new metric with existing metrics.
-
-        Parameters
-        ----------
-        func : Callable
-            the func of computing new metric.
-            the kwargs of func will be replaced with metric data by name in this function.
-            e.g.
-                def func(pa):
-                    return (pa > 0).sum() / pa.count()
-        new_col : str, optional
-            New metric will be assigned in the data if new_col is not None, by default None.
-
-        Return
-        ----------
-        BaseSingleMetric
-            new metric.
-        """
-        func_sig = inspect.signature(func).parameters.keys()
-        func_kwargs = {sig: self.data[sig] for sig in func_sig}
-        tmp_metric = func(**func_kwargs)
-        if new_col is not None:
-            self.data[new_col] = tmp_metric
-            return None
-        else:
-            return tmp_metric
-
-    def get_metric_series(self, metric: str) -> pd.Series:
-        """return the single metric with pd.Series format.
-
-        Parameters
-        ----------
-        metric : str
-            the metric name.
-
-        Return
-        ----------
-        pd.Series
-            the single metric.
-            If there is no metric name in the data, return pd.Series().
-        """
-
-        raise NotImplementedError(f"Please implement the 'get_metric_series' method")
-
-    def get_index_data(self, metric: str) -> SingleData:
-        """get one metric with the format of SingleData
-
-        Parameters
-        ----------
-        metric : str
-            the metric name.
-
-        Return
-        ------
-        IndexData.Series
-            one metric with the format of SingleData
-        """
-
-        raise NotImplementedError(f"Please implement the 'get_index_data' method")
-
-    @staticmethod
-    def sum_all_indicators(
-        order_indicator: BaseOrderIndicator,
-        indicators: List[BaseOrderIndicator],
-        metrics: Union[str, List[str]],
-        fill_value: float = 0,
-    ) -> None:
-        """sum indicators with the same metrics.
-        and assign to the order_indicator(BaseOrderIndicator).
-        NOTE: indicators could be a empty list when orders in lower level all fail.
-
-        Parameters
-        ----------
-        order_indicator : BaseOrderIndicator
-            the order indicator to assign.
-        indicators : List[BaseOrderIndicator]
-            the list of all inner indicators.
-        metrics : Union[str, List[str]]
-            all metrics needs to be sumed.
-        fill_value : float, optional
-            fill np.NaN with value. By default None.
-        """
-
-        raise NotImplementedError(f"Please implement the 'sum_all_indicators' method")
-
-    def to_series(self) -> Dict[Text, pd.Series]:
-        """return the metrics as pandas series
-
-        for example: { "ffr":
-                SH600068    NaN
-                SH600079    1.0
-                SH600266    NaN
-                           ...
-                SZ300692    NaN
-                SZ300719    NaN,
-                ...
-         }
-        """
-        raise NotImplementedError(f"Please implement the `to_series` method")
-
-
-class SingleMetric(BaseSingleMetric):
-    def __init__(self, metric):
-        self.metric = metric
-
-    def __add__(self, other):
-        if isinstance(other, (int, float)):
-            return self.__class__(self.metric + other)
-        elif isinstance(other, self.__class__):
-            return self.__class__(self.metric + other.metric)
-        else:
-            return NotImplemented
-
-    def __sub__(self, other):
-        if isinstance(other, (int, float)):
-            return self.__class__(self.metric - other)
-        elif isinstance(other, self.__class__):
-            return self.__class__(self.metric - other.metric)
-        else:
-            return NotImplemented
-
-    def __rsub__(self, other):
-        if isinstance(other, (int, float)):
-            return self.__class__(other - self.metric)
-        elif isinstance(other, self.__class__):
-            return self.__class__(other.metric - self.metric)
-        else:
-            return NotImplemented
-
-    def __mul__(self, other):
-        if isinstance(other, (int, float)):
-            return self.__class__(self.metric * other)
-        elif isinstance(other, self.__class__):
-            return self.__class__(self.metric * other.metric)
-        else:
-            return NotImplemented
-
-    def __truediv__(self, other):
-        if isinstance(other, (int, float)):
-            return self.__class__(self.metric / other)
-        elif isinstance(other, self.__class__):
-            return self.__class__(self.metric / other.metric)
-        else:
-            return NotImplemented
-
-    def __eq__(self, other):
-        if isinstance(other, (int, float)):
-            return self.__class__(self.metric == other)
-        elif isinstance(other, self.__class__):
-            return self.__class__(self.metric == other.metric)
-        else:
-            return NotImplemented
-
-    def __gt__(self, other):
-        if isinstance(other, (int, float)):
-            return self.__class__(self.metric > other)
-        elif isinstance(other, self.__class__):
-            return self.__class__(self.metric > other.metric)
-        else:
-            return NotImplemented
-
-    def __lt__(self, other):
-        if isinstance(other, (int, float)):
-            return self.__class__(self.metric < other)
-        elif isinstance(other, self.__class__):
-            return self.__class__(self.metric < other.metric)
-        else:
-            return NotImplemented
-
-    def __len__(self):
-        return len(self.metric)
-
-
-class PandasSingleMetric(SingleMetric):
-    """Each SingleMetric is based on pd.Series."""
-
-    def __init__(self, metric: Union[dict, pd.Series] = {}):
-        if isinstance(metric, dict):
-            self.metric = pd.Series(metric)
-        elif isinstance(metric, pd.Series):
-            self.metric = metric
-        else:
-            raise ValueError(f"metric must be dict or pd.Series")
-
-    def sum(self):
-        return self.metric.sum()
-
-    def mean(self):
-        return self.metric.mean()
-
-    def count(self):
-        return self.metric.count()
-
-    def abs(self):
-        return self.__class__(self.metric.abs())
-
-    @property
-    def empty(self):
-        return self.metric.empty
-
-    @property
-    def index(self):
-        return list(self.metric.index)
-
-    def add(self, other: BaseSingleMetric, fill_value: float = None) -> PandasSingleMetric:
-        other = cast(PandasSingleMetric, other)
-        return self.__class__(self.metric.add(other.metric, fill_value=fill_value))
-
-    def replace(self, replace_dict: dict) -> PandasSingleMetric:
-        return self.__class__(self.metric.replace(replace_dict))
-
-    def apply(self, func: Callable) -> PandasSingleMetric:
-        return self.__class__(self.metric.apply(func))
-
-    def reindex(self, index: Any, fill_value: float) -> PandasSingleMetric:
-        return self.__class__(self.metric.reindex(index, fill_value=fill_value))
-
-    def __repr__(self):
-        return repr(self.metric)
-
-
-class PandasOrderIndicator(BaseOrderIndicator):
-    """
-    The data structure is OrderedDict(str: PandasSingleMetric).
-    Each PandasSingleMetric based on pd.Series is one metric.
-    Str is the name of metric.
-    """
-
-    def __init__(self) -> None:
-        super(PandasOrderIndicator, self).__init__()
-        self.data: Dict[str, PandasSingleMetric] = OrderedDict()
-
-    def assign(self, col: str, metric: Union[dict, pd.Series]) -> None:
-        self.data[col] = PandasSingleMetric(metric)
-
-    def get_index_data(self, metric: str) -> SingleData:
-        if metric in self.data:
-            return idd.SingleData(self.data[metric].metric)
-        else:
-            return idd.SingleData()
-
-    def get_metric_series(self, metric: str) -> Union[pd.Series]:
-        if metric in self.data:
-            return self.data[metric].metric
-        else:
-            return pd.Series()
-
-    def to_series(self):
-        return {k: v.metric for k, v in self.data.items()}
-
-    @staticmethod
-    def sum_all_indicators(
-        order_indicator: BaseOrderIndicator,
-        indicators: List[BaseOrderIndicator],
-        metrics: Union[str, List[str]],
-        fill_value: float = 0,
-    ) -> None:
-        if isinstance(metrics, str):
-            metrics = [metrics]
-        for metric in metrics:
-            tmp_metric = PandasSingleMetric({})
-            for indicator in indicators:
-                tmp_metric = tmp_metric.add(indicator.data[metric], fill_value)
-            order_indicator.assign(metric, tmp_metric.metric)
-
-    def __repr__(self):
-        return repr(self.data)
-
-
-class NumpyOrderIndicator(BaseOrderIndicator):
-    """
-    The data structure is OrderedDict(str: SingleData).
-    Each idd.SingleData is one metric.
-    Str is the name of metric.
-    """
-
-    def __init__(self) -> None:
-        super(NumpyOrderIndicator, self).__init__()
-        self.data: Dict[str, SingleData] = OrderedDict()
-
-    def assign(self, col: str, metric: dict) -> None:
-        self.data[col] = idd.SingleData(metric)
-
-    def get_index_data(self, metric: str) -> SingleData:
-        if metric in self.data:
-            return self.data[metric]
-        else:
-            return idd.SingleData()
-
-    def get_metric_series(self, metric: str) -> Union[pd.Series]:
-        return self.data[metric].to_series()
-
-    def to_series(self) -> Dict[str, pd.Series]:
-        tmp_metric_dict = {}
-        for metric in self.data:
-            tmp_metric_dict[metric] = self.get_metric_series(metric)
-        return tmp_metric_dict
-
-    @staticmethod
-    def sum_all_indicators(
-        order_indicator: BaseOrderIndicator,
-        indicators: List[BaseOrderIndicator],
-        metrics: Union[str, List[str]],
-        fill_value: float = 0,
-    ) -> None:
-        # get all index(stock_id)
-        stock_set: set = set()
-        for indicator in indicators:
-            # set(np.ndarray.tolist()) is faster than set(np.ndarray)
-            stock_set = stock_set | set(indicator.data[metrics[0]].index.tolist())
-        stocks = sorted(list(stock_set))
-
-        # add metric by index
-        if isinstance(metrics, str):
-            metrics = [metrics]
-        for metric in metrics:
-            order_indicator.data[metric] = idd.sum_by_index(
-                [indicator.data[metric] for indicator in indicators],
-                stocks,
-                fill_value,
-            )
-
-    def __repr__(self):
-        return repr(self.data)
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+from __future__ import annotations
+
+import inspect
+import logging
+from collections import OrderedDict
+from functools import lru_cache
+from typing import Any, Callable, Dict, Iterable, List, Optional, Text, Union, cast
+
+import numpy as np
+import pandas as pd
+
+import qlib.utils.index_data as idd
+
+from ..log import get_module_logger
+from ..utils.index_data import IndexData, SingleData
+from ..utils.resam import resam_ts_data, ts_data_last
+from ..utils.time import Freq, is_single_value
+
+
+class BaseQuote:
+    def __init__(self, quote_df: pd.DataFrame, freq: str) -> None:
+        self.logger = get_module_logger("online operator", level=logging.INFO)
+
+    def get_all_stock(self) -> Iterable:
+        """return all stock codes
+
+        Return
+        ------
+        Iterable
+            all stock codes
+        """
+
+        raise NotImplementedError(f"Please implement the `get_all_stock` method")
+
+    def get_data(
+        self,
+        stock_id: str,
+        start_time: Union[pd.Timestamp, str],
+        end_time: Union[pd.Timestamp, str],
+        field: Union[str],
+        method: Optional[str] = None,
+    ) -> Union[None, int, float, bool, IndexData]:
+        """get the specific field of stock data during start time and end_time,
+           and apply method to the data.
+
+           Example:
+            .. code-block::
+                                        $close      $volume
+                instrument  datetime
+                SH600000    2010-01-04  86.778313   16162960.0
+                            2010-01-05  87.433578   28117442.0
+                            2010-01-06  85.713585   23632884.0
+                            2010-01-07  83.788803   20813402.0
+                            2010-01-08  84.730675   16044853.0
+
+                SH600655    2010-01-04  2699.567383  158193.328125
+                            2010-01-08  2612.359619   77501.406250
+                            2010-01-11  2712.982422  160852.390625
+                            2010-01-12  2788.688232  164587.937500
+                            2010-01-13  2790.604004  145460.453125
+
+                this function is used for three case:
+
+                1. method is not None. It returns int/float/bool/None.
+                    - It will return None in one case, the method return None
+
+                    print(get_data(stock_id="SH600000", start_time="2010-01-04", end_time="2010-01-06", field="$close", method="last"))
+
+                    85.713585
+
+                2. method is None. It returns IndexData.
+                    print(get_data(stock_id="SH600000", start_time="2010-01-04", end_time="2010-01-06", field="$close", method=None))
+
+                    IndexData([86.778313, 87.433578, 85.713585], [2010-01-04, 2010-01-05, 2010-01-06])
+
+        Parameters
+        ----------
+        stock_id: str
+        start_time : Union[pd.Timestamp, str]
+            closed start time for backtest
+        end_time : Union[pd.Timestamp, str]
+            closed end time for backtest
+        field : str
+            the columns of data to fetch
+        method : Union[str, None]
+            the method apply to data.
+            e.g [None, "last", "all", "sum", "mean", "ts_data_last"]
+
+        Return
+        ----------
+        Union[None, int, float, bool, IndexData]
+            it will return None in following cases
+            - There is no stock data which meet the query criterion from data source.
+            - The `method` returns None
+        """
+
+        raise NotImplementedError(f"Please implement the `get_data` method")
+
+
+class PandasQuote(BaseQuote):
+    def __init__(self, quote_df: pd.DataFrame, freq: str) -> None:
+        super().__init__(quote_df=quote_df, freq=freq)
+        quote_dict = {}
+        for stock_id, stock_val in quote_df.groupby(level="instrument"):
+            quote_dict[stock_id] = stock_val.droplevel(level="instrument")
+        self.data = quote_dict
+
+    def get_all_stock(self):
+        return self.data.keys()
+
+    def get_data(self, stock_id, start_time, end_time, field, method=None):
+        if method == "ts_data_last":
+            method = ts_data_last
+        stock_data = resam_ts_data(self.data[stock_id][field], start_time, end_time, method=method)
+        if stock_data is None:
+            return None
+        elif isinstance(stock_data, (bool, np.bool_, int, float, np.number)):
+            return stock_data
+        elif isinstance(stock_data, pd.Series):
+            return idd.SingleData(stock_data)
+        else:
+            raise ValueError(f"stock data from resam_ts_data must be a number, pd.Series or pd.DataFrame")
+
+
+class NumpyQuote(BaseQuote):
+    def __init__(self, quote_df: pd.DataFrame, freq: str, region: str = "cn") -> None:
+        """NumpyQuote
+
+        Parameters
+        ----------
+        quote_df : pd.DataFrame
+            the init dataframe from qlib.
+        self.data : Dict(stock_id, IndexData.DataFrame)
+        """
+        super().__init__(quote_df=quote_df, freq=freq)
+        quote_dict = {}
+        for stock_id, stock_val in quote_df.groupby(level="instrument"):
+            quote_dict[stock_id] = idd.MultiData(stock_val.droplevel(level="instrument"))
+            quote_dict[stock_id].sort_index()  # To support more flexible slicing, we must sort data first
+        self.data = quote_dict
+
+        n, unit = Freq.parse(freq)
+        if unit in Freq.SUPPORT_CAL_LIST:
+            self.freq = Freq.get_timedelta(1, unit)
+        else:
+            raise ValueError(f"{freq} is not supported in NumpyQuote")
+        self.region = region
+
+    def get_all_stock(self):
+        return self.data.keys()
+
+    @lru_cache(maxsize=512)
+    def get_data(self, stock_id, start_time, end_time, field, method=None):
+        # check stock id
+        if stock_id not in self.get_all_stock():
+            return None
+
+        # single data
+        # If it don't consider the classification of single data, it will consume a lot of time.
+        if is_single_value(start_time, end_time, self.freq, self.region):
+            # this is a very special case.
+            # skip aggregating function to speed-up the query calculation
+
+            # FIXME:
+            # it will go to the else logic when it comes to the
+            # 1) the day before holiday when daily trading
+            # 2) the last minute of the day when intraday trading
+            try:
+                return self.data[stock_id].loc[start_time, field]
+            except KeyError:
+                return None
+        else:
+            data = self.data[stock_id].loc[start_time:end_time, field]
+            if data.empty:
+                return None
+            if method is not None:
+                data = self._agg_data(data, method)
+            return data
+
+    @staticmethod
+    def _agg_data(data: IndexData, method: str) -> Union[IndexData, np.ndarray, None]:
+        """Agg data by specific method."""
+        # FIXME: why not call the method of data directly?
+        if method == "sum":
+            return np.nansum(data)
+        elif method == "mean":
+            return np.nanmean(data)
+        elif method == "last":
+            # FIXME: I've never seen that this method was called.
+            # Please merge it with "ts_data_last"
+            return data[-1]
+        elif method == "all":
+            return data.all()
+        elif method == "ts_data_last":
+            valid_data = data.loc[~data.isna().data.astype(bool)]
+            if len(valid_data) == 0:
+                return None
+            else:
+                return valid_data.iloc[-1]
+        else:
+            raise ValueError(f"{method} is not supported")
+
+
+class BaseSingleMetric:
+    """
+    The data structure of the single metric.
+    The following methods are used for computing metrics in one indicator.
+    """
+
+    def __init__(self, metric: Union[dict, pd.Series]):
+        """Single data structure for each metric.
+
+        Parameters
+        ----------
+        metric : Union[dict, pd.Series]
+            keys/index is stock_id, value is the metric value.
+            for example:
+                SH600068    NaN
+                SH600079    1.0
+                SH600266    NaN
+                           ...
+                SZ300692    NaN
+                SZ300719    NaN,
+        """
+        raise NotImplementedError(f"Please implement the `__init__` method")
+
+    def __add__(self, other: Union[BaseSingleMetric, int, float]) -> BaseSingleMetric:
+        raise NotImplementedError(f"Please implement the `__add__` method")
+
+    def __radd__(self, other: Union[BaseSingleMetric, int, float]) -> BaseSingleMetric:
+        return self + other
+
+    def __sub__(self, other: Union[BaseSingleMetric, int, float]) -> BaseSingleMetric:
+        raise NotImplementedError(f"Please implement the `__sub__` method")
+
+    def __rsub__(self, other: Union[BaseSingleMetric, int, float]) -> BaseSingleMetric:
+        raise NotImplementedError(f"Please implement the `__rsub__` method")
+
+    def __mul__(self, other: Union[BaseSingleMetric, int, float]) -> BaseSingleMetric:
+        raise NotImplementedError(f"Please implement the `__mul__` method")
+
+    def __truediv__(self, other: Union[BaseSingleMetric, int, float]) -> BaseSingleMetric:
+        raise NotImplementedError(f"Please implement the `__truediv__` method")
+
+    def __eq__(self, other: object) -> BaseSingleMetric:
+        raise NotImplementedError(f"Please implement the `__eq__` method")
+
+    def __gt__(self, other: Union[BaseSingleMetric, int, float]) -> BaseSingleMetric:
+        raise NotImplementedError(f"Please implement the `__gt__` method")
+
+    def __lt__(self, other: Union[BaseSingleMetric, int, float]) -> BaseSingleMetric:
+        raise NotImplementedError(f"Please implement the `__lt__` method")
+
+    def __len__(self) -> int:
+        raise NotImplementedError(f"Please implement the `__len__` method")
+
+    def sum(self) -> float:
+        raise NotImplementedError(f"Please implement the `sum` method")
+
+    def mean(self) -> float:
+        raise NotImplementedError(f"Please implement the `mean` method")
+
+    def count(self) -> int:
+        """Return the count of the single metric, NaN is not included."""
+
+        raise NotImplementedError(f"Please implement the `count` method")
+
+    def abs(self) -> BaseSingleMetric:
+        raise NotImplementedError(f"Please implement the `abs` method")
+
+    @property
+    def empty(self) -> bool:
+        """If metric is empty, return True."""
+
+        raise NotImplementedError(f"Please implement the `empty` method")
+
+    def add(self, other: BaseSingleMetric, fill_value: float = None) -> BaseSingleMetric:
+        """Replace np.NaN with fill_value in two metrics and add them."""
+
+        raise NotImplementedError(f"Please implement the `add` method")
+
+    def replace(self, replace_dict: dict) -> BaseSingleMetric:
+        """Replace the value of metric according to replace_dict."""
+
+        raise NotImplementedError(f"Please implement the `replace` method")
+
+    def apply(self, func: Callable) -> BaseSingleMetric:
+        """Replace the value of metric with func (metric).
+        Currently, the func is only qlib/backtest/order/Order.parse_dir.
+        """
+
+        raise NotImplementedError(f"Please implement the 'apply' method")
+
+
+class BaseOrderIndicator:
+    """
+    The data structure of order indicator.
+    !!!NOTE: There are two ways to organize the data structure. Please choose a better way.
+        1. One way is using BaseSingleMetric to represent each metric. For example, the data
+        structure of PandasOrderIndicator is Dict[str, PandasSingleMetric]. It uses
+        PandasSingleMetric based on pd.Series to represent each metric.
+        2. The another way doesn't use BaseSingleMetric to represent each metric. The data
+        structure of PandasOrderIndicator is a whole matrix. It means you are not necessary
+        to inherit the BaseSingleMetric.
+    """
+
+    def __init__(self):
+        self.data = {}  # will be created in the subclass
+        self.logger = get_module_logger("online operator")
+
+    def assign(self, col: str, metric: Union[dict, pd.Series]) -> None:
+        """assign one metric.
+
+        Parameters
+        ----------
+        col : str
+            the metric name of one metric.
+        metric : Union[dict, pd.Series]
+            one metric with stock_id index, such as deal_amount, ffr, etc.
+            for example:
+                SH600068    NaN
+                SH600079    1.0
+                SH600266    NaN
+                           ...
+                SZ300692    NaN
+                SZ300719    NaN,
+        """
+
+        raise NotImplementedError(f"Please implement the 'assign' method")
+
+    def transfer(self, func: Callable, new_col: str = None) -> Optional[BaseSingleMetric]:
+        """compute new metric with existing metrics.
+
+        Parameters
+        ----------
+        func : Callable
+            the func of computing new metric.
+            the kwargs of func will be replaced with metric data by name in this function.
+            e.g.
+                def func(pa):
+                    return (pa > 0).sum() / pa.count()
+        new_col : str, optional
+            New metric will be assigned in the data if new_col is not None, by default None.
+
+        Return
+        ----------
+        BaseSingleMetric
+            new metric.
+        """
+        func_sig = inspect.signature(func).parameters.keys()
+        func_kwargs = {sig: self.data[sig] for sig in func_sig}
+        tmp_metric = func(**func_kwargs)
+        if new_col is not None:
+            self.data[new_col] = tmp_metric
+            return None
+        else:
+            return tmp_metric
+
+    def get_metric_series(self, metric: str) -> pd.Series:
+        """return the single metric with pd.Series format.
+
+        Parameters
+        ----------
+        metric : str
+            the metric name.
+
+        Return
+        ----------
+        pd.Series
+            the single metric.
+            If there is no metric name in the data, return pd.Series().
+        """
+
+        raise NotImplementedError(f"Please implement the 'get_metric_series' method")
+
+    def get_index_data(self, metric: str) -> SingleData:
+        """get one metric with the format of SingleData
+
+        Parameters
+        ----------
+        metric : str
+            the metric name.
+
+        Return
+        ------
+        IndexData.Series
+            one metric with the format of SingleData
+        """
+
+        raise NotImplementedError(f"Please implement the 'get_index_data' method")
+
+    @staticmethod
+    def sum_all_indicators(
+        order_indicator: BaseOrderIndicator,
+        indicators: List[BaseOrderIndicator],
+        metrics: Union[str, List[str]],
+        fill_value: float = 0,
+    ) -> None:
+        """sum indicators with the same metrics.
+        and assign to the order_indicator(BaseOrderIndicator).
+        NOTE: indicators could be a empty list when orders in lower level all fail.
+
+        Parameters
+        ----------
+        order_indicator : BaseOrderIndicator
+            the order indicator to assign.
+        indicators : List[BaseOrderIndicator]
+            the list of all inner indicators.
+        metrics : Union[str, List[str]]
+            all metrics needs to be sumed.
+        fill_value : float, optional
+            fill np.NaN with value. By default None.
+        """
+
+        raise NotImplementedError(f"Please implement the 'sum_all_indicators' method")
+
+    def to_series(self) -> Dict[Text, pd.Series]:
+        """return the metrics as pandas series
+
+        for example: { "ffr":
+                SH600068    NaN
+                SH600079    1.0
+                SH600266    NaN
+                           ...
+                SZ300692    NaN
+                SZ300719    NaN,
+                ...
+         }
+        """
+        raise NotImplementedError(f"Please implement the `to_series` method")
+
+
+class SingleMetric(BaseSingleMetric):
+    def __init__(self, metric):
+        self.metric = metric
+
+    def __add__(self, other):
+        if isinstance(other, (int, float)):
+            return self.__class__(self.metric + other)
+        elif isinstance(other, self.__class__):
+            return self.__class__(self.metric + other.metric)
+        else:
+            return NotImplemented
+
+    def __sub__(self, other):
+        if isinstance(other, (int, float)):
+            return self.__class__(self.metric - other)
+        elif isinstance(other, self.__class__):
+            return self.__class__(self.metric - other.metric)
+        else:
+            return NotImplemented
+
+    def __rsub__(self, other):
+        if isinstance(other, (int, float)):
+            return self.__class__(other - self.metric)
+        elif isinstance(other, self.__class__):
+            return self.__class__(other.metric - self.metric)
+        else:
+            return NotImplemented
+
+    def __mul__(self, other):
+        if isinstance(other, (int, float)):
+            return self.__class__(self.metric * other)
+        elif isinstance(other, self.__class__):
+            return self.__class__(self.metric * other.metric)
+        else:
+            return NotImplemented
+
+    def __truediv__(self, other):
+        if isinstance(other, (int, float)):
+            return self.__class__(self.metric / other)
+        elif isinstance(other, self.__class__):
+            return self.__class__(self.metric / other.metric)
+        else:
+            return NotImplemented
+
+    def __eq__(self, other):
+        if isinstance(other, (int, float)):
+            return self.__class__(self.metric == other)
+        elif isinstance(other, self.__class__):
+            return self.__class__(self.metric == other.metric)
+        else:
+            return NotImplemented
+
+    def __gt__(self, other):
+        if isinstance(other, (int, float)):
+            return self.__class__(self.metric > other)
+        elif isinstance(other, self.__class__):
+            return self.__class__(self.metric > other.metric)
+        else:
+            return NotImplemented
+
+    def __lt__(self, other):
+        if isinstance(other, (int, float)):
+            return self.__class__(self.metric < other)
+        elif isinstance(other, self.__class__):
+            return self.__class__(self.metric < other.metric)
+        else:
+            return NotImplemented
+
+    def __len__(self):
+        return len(self.metric)
+
+
+class PandasSingleMetric(SingleMetric):
+    """Each SingleMetric is based on pd.Series."""
+
+    def __init__(self, metric: Union[dict, pd.Series] = {}):
+        if isinstance(metric, dict):
+            self.metric = pd.Series(metric)
+        elif isinstance(metric, pd.Series):
+            self.metric = metric
+        else:
+            raise ValueError(f"metric must be dict or pd.Series")
+
+    def sum(self):
+        return self.metric.sum()
+
+    def mean(self):
+        return self.metric.mean()
+
+    def count(self):
+        return self.metric.count()
+
+    def abs(self):
+        return self.__class__(self.metric.abs())
+
+    @property
+    def empty(self):
+        return self.metric.empty
+
+    @property
+    def index(self):
+        return list(self.metric.index)
+
+    def add(self, other: BaseSingleMetric, fill_value: float = None) -> PandasSingleMetric:
+        other = cast(PandasSingleMetric, other)
+        return self.__class__(self.metric.add(other.metric, fill_value=fill_value))
+
+    def replace(self, replace_dict: dict) -> PandasSingleMetric:
+        return self.__class__(self.metric.replace(replace_dict))
+
+    def apply(self, func: Callable) -> PandasSingleMetric:
+        return self.__class__(self.metric.apply(func))
+
+    def reindex(self, index: Any, fill_value: float) -> PandasSingleMetric:
+        return self.__class__(self.metric.reindex(index, fill_value=fill_value))
+
+    def __repr__(self):
+        return repr(self.metric)
+
+
+class PandasOrderIndicator(BaseOrderIndicator):
+    """
+    The data structure is OrderedDict(str: PandasSingleMetric).
+    Each PandasSingleMetric based on pd.Series is one metric.
+    Str is the name of metric.
+    """
+
+    def __init__(self) -> None:
+        super(PandasOrderIndicator, self).__init__()
+        self.data: Dict[str, PandasSingleMetric] = OrderedDict()
+
+    def assign(self, col: str, metric: Union[dict, pd.Series]) -> None:
+        self.data[col] = PandasSingleMetric(metric)
+
+    def get_index_data(self, metric: str) -> SingleData:
+        if metric in self.data:
+            return idd.SingleData(self.data[metric].metric)
+        else:
+            return idd.SingleData()
+
+    def get_metric_series(self, metric: str) -> Union[pd.Series]:
+        if metric in self.data:
+            return self.data[metric].metric
+        else:
+            return pd.Series()
+
+    def to_series(self):
+        return {k: v.metric for k, v in self.data.items()}
+
+    @staticmethod
+    def sum_all_indicators(
+        order_indicator: BaseOrderIndicator,
+        indicators: List[BaseOrderIndicator],
+        metrics: Union[str, List[str]],
+        fill_value: float = 0,
+    ) -> None:
+        if isinstance(metrics, str):
+            metrics = [metrics]
+        for metric in metrics:
+            tmp_metric = PandasSingleMetric({})
+            for indicator in indicators:
+                tmp_metric = tmp_metric.add(indicator.data[metric], fill_value)
+            order_indicator.assign(metric, tmp_metric.metric)
+
+    def __repr__(self):
+        return repr(self.data)
+
+
+class NumpyOrderIndicator(BaseOrderIndicator):
+    """
+    The data structure is OrderedDict(str: SingleData).
+    Each idd.SingleData is one metric.
+    Str is the name of metric.
+    """
+
+    def __init__(self) -> None:
+        super(NumpyOrderIndicator, self).__init__()
+        self.data: Dict[str, SingleData] = OrderedDict()
+
+    def assign(self, col: str, metric: dict) -> None:
+        self.data[col] = idd.SingleData(metric)
+
+    def get_index_data(self, metric: str) -> SingleData:
+        if metric in self.data:
+            return self.data[metric]
+        else:
+            return idd.SingleData()
+
+    def get_metric_series(self, metric: str) -> Union[pd.Series]:
+        return self.data[metric].to_series()
+
+    def to_series(self) -> Dict[str, pd.Series]:
+        tmp_metric_dict = {}
+        for metric in self.data:
+            tmp_metric_dict[metric] = self.get_metric_series(metric)
+        return tmp_metric_dict
+
+    @staticmethod
+    def sum_all_indicators(
+        order_indicator: BaseOrderIndicator,
+        indicators: List[BaseOrderIndicator],
+        metrics: Union[str, List[str]],
+        fill_value: float = 0,
+    ) -> None:
+        # get all index(stock_id)
+        stock_set: set = set()
+        for indicator in indicators:
+            # set(np.ndarray.tolist()) is faster than set(np.ndarray)
+            stock_set = stock_set | set(indicator.data[metrics[0]].index.tolist())
+        stocks = sorted(list(stock_set))
+
+        # add metric by index
+        if isinstance(metrics, str):
+            metrics = [metrics]
+        for metric in metrics:
+            order_indicator.data[metric] = idd.sum_by_index(
+                [indicator.data[metric] for indicator in indicators],
+                stocks,
+                fill_value,
+            )
+
+    def __repr__(self):
+        return repr(self.data)
```

## qlib/backtest/position.py

 * *Ordering differences only*

```diff
@@ -1,565 +1,565 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-from __future__ import annotations
-
-from datetime import timedelta
-from typing import Any, Dict, List, Union
-
-import numpy as np
-import pandas as pd
-
-from ..data.data import D
-from .decision import Order
-
-
-class BasePosition:
-    """
-    The Position wants to maintain the position like a dictionary
-    Please refer to the `Position` class for the position
-    """
-
-    def __init__(self, *args: Any, cash: float = 0.0, **kwargs: Any) -> None:
-        self._settle_type = self.ST_NO
-        self.position: dict = {}
-
-    def fill_stock_value(self, start_time: Union[str, pd.Timestamp], freq: str, last_days: int = 30) -> None:
-        pass
-
-    def skip_update(self) -> bool:
-        """
-        Should we skip updating operation for this position
-        For example, updating is meaningless for InfPosition
-
-        Returns
-        -------
-        bool:
-            should we skip the updating operator
-        """
-        return False
-
-    def check_stock(self, stock_id: str) -> bool:
-        """
-        check if is the stock in the position
-
-        Parameters
-        ----------
-        stock_id : str
-            the id of the stock
-
-        Returns
-        -------
-        bool:
-            if is the stock in the position
-        """
-        raise NotImplementedError(f"Please implement the `check_stock` method")
-
-    def update_order(self, order: Order, trade_val: float, cost: float, trade_price: float) -> None:
-        """
-        Parameters
-        ----------
-        order : Order
-            the order to update the position
-        trade_val : float
-            the trade value(money) of dealing results
-        cost : float
-            the trade cost of the dealing results
-        trade_price : float
-            the trade price of the dealing results
-        """
-        raise NotImplementedError(f"Please implement the `update_order` method")
-
-    def update_stock_price(self, stock_id: str, price: float) -> None:
-        """
-        Updating the latest price of the order
-        The useful when clearing balance at each bar end
-
-        Parameters
-        ----------
-        stock_id :
-            the id of the stock
-        price : float
-            the price to be updated
-        """
-        raise NotImplementedError(f"Please implement the `update stock price` method")
-
-    def calculate_stock_value(self) -> float:
-        """
-        calculate the value of the all assets except cash in the position
-
-        Returns
-        -------
-        float:
-            the value(money) of all the stock
-        """
-        raise NotImplementedError(f"Please implement the `calculate_stock_value` method")
-
-    def calculate_value(self) -> float:
-        raise NotImplementedError(f"Please implement the `calculate_value` method")
-
-    def get_stock_list(self) -> List[str]:
-        """
-        Get the list of stocks in the position.
-        """
-        raise NotImplementedError(f"Please implement the `get_stock_list` method")
-
-    def get_stock_price(self, code: str) -> float:
-        """
-        get the latest price of the stock
-
-        Parameters
-        ----------
-        code :
-            the code of the stock
-        """
-        raise NotImplementedError(f"Please implement the `get_stock_price` method")
-
-    def get_stock_amount(self, code: str) -> float:
-        """
-        get the amount of the stock
-
-        Parameters
-        ----------
-        code :
-            the code of the stock
-
-        Returns
-        -------
-        float:
-            the amount of the stock
-        """
-        raise NotImplementedError(f"Please implement the `get_stock_amount` method")
-
-    def get_cash(self, include_settle: bool = False) -> float:
-        """
-        Parameters
-        ----------
-        include_settle:
-            will the unsettled(delayed) cash included
-            Default: not include those unavailable cash
-
-        Returns
-        -------
-        float:
-            the available(tradable) cash in position
-        """
-        raise NotImplementedError(f"Please implement the `get_cash` method")
-
-    def get_stock_amount_dict(self) -> dict:
-        """
-        generate stock amount dict {stock_id : amount of stock}
-
-        Returns
-        -------
-        Dict:
-            {stock_id : amount of stock}
-        """
-        raise NotImplementedError(f"Please implement the `get_stock_amount_dict` method")
-
-    def get_stock_weight_dict(self, only_stock: bool = False) -> dict:
-        """
-        generate stock weight dict {stock_id : value weight of stock in the position}
-        it is meaningful in the beginning or the end of each trade step
-        - During execution of each trading step, the weight may be not consistent with the portfolio value
-
-        Parameters
-        ----------
-        only_stock : bool
-            If only_stock=True, the weight of each stock in total stock will be returned
-            If only_stock=False, the weight of each stock in total assets(stock + cash) will be returned
-
-        Returns
-        -------
-        Dict:
-            {stock_id : value weight of stock in the position}
-        """
-        raise NotImplementedError(f"Please implement the `get_stock_weight_dict` method")
-
-    def add_count_all(self, bar: str) -> None:
-        """
-        Will be called at the end of each bar on each level
-
-        Parameters
-        ----------
-        bar :
-            The level to be updated
-        """
-        raise NotImplementedError(f"Please implement the `add_count_all` method")
-
-    def update_weight_all(self) -> None:
-        """
-        Updating the position weight;
-
-        # TODO: this function is a little weird. The weight data in the position is in a wrong state after dealing order
-        # and before updating weight.
-        """
-        raise NotImplementedError(f"Please implement the `add_count_all` method")
-
-    ST_CASH = "cash"
-    ST_NO = "None"  # String is more typehint friendly than None
-
-    def settle_start(self, settle_type: str) -> None:
-        """
-        settlement start
-        It will act like start and commit a transaction
-
-        Parameters
-        ----------
-        settle_type : str
-            Should we make delay the settlement in each execution (each execution will make the executor a step forward)
-            - "cash": make the cash settlement delayed.
-                - The cash you get can't be used in current step (e.g. you can't sell a stock to get cash to buy another
-                        stock)
-            - None: not settlement mechanism
-            - TODO: other assets will be supported in the future.
-        """
-        raise NotImplementedError(f"Please implement the `settle_conf` method")
-
-    def settle_commit(self) -> None:
-        """
-        settlement commit
-        """
-        raise NotImplementedError(f"Please implement the `settle_commit` method")
-
-    def __str__(self) -> str:
-        return self.__dict__.__str__()
-
-    def __repr__(self) -> str:
-        return self.__dict__.__repr__()
-
-
-class Position(BasePosition):
-    """Position
-
-    current state of position
-    a typical example is :{
-      <instrument_id>: {
-        'count': <how many days the security has been hold>,
-        'amount': <the amount of the security>,
-        'price': <the close price of security in the last trading day>,
-        'weight': <the security weight of total position value>,
-      },
-    }
-    """
-
-    def __init__(self, cash: float = 0, position_dict: Dict[str, Union[Dict[str, float], float]] = {}) -> None:
-        """Init position by cash and position_dict.
-
-        Parameters
-        ----------
-        cash : float, optional
-            initial cash in account, by default 0
-        position_dict : Dict[
-                            stock_id,
-                            Union[
-                                int,  # it is equal to {"amount": int}
-                                {"amount": int, "price"(optional): float},
-                            ]
-                        ]
-            initial stocks with parameters amount and price,
-            if there is no price key in the dict of stocks, it will be filled by _fill_stock_value.
-            by default {}.
-        """
-        super().__init__()
-
-        # NOTE: The position dict must be copied!!!
-        # Otherwise the initial value
-        self.init_cash = cash
-        self.position = position_dict.copy()
-        for stock, value in self.position.items():
-            if isinstance(value, int):
-                self.position[stock] = {"amount": value}
-        self.position["cash"] = cash
-
-        # If the stock price information is missing, the account value will not be calculated temporarily
-        try:
-            self.position["now_account_value"] = self.calculate_value()
-        except KeyError:
-            pass
-
-    def fill_stock_value(self, start_time: Union[str, pd.Timestamp], freq: str, last_days: int = 30) -> None:
-        """fill the stock value by the close price of latest last_days from qlib.
-
-        Parameters
-        ----------
-        start_time :
-            the start time of backtest.
-        freq : str
-            Frequency
-        last_days : int, optional
-            the days to get the latest close price, by default 30.
-        """
-        stock_list = []
-        for stock, value in self.position.items():
-            if not isinstance(value, dict):
-                continue
-            if value.get("price", None) is None:
-                stock_list.append(stock)
-
-        if len(stock_list) == 0:
-            return
-
-        start_time = pd.Timestamp(start_time)
-        # note that start time is 2020-01-01 00:00:00 if raw start time is "2020-01-01"
-        price_end_time = start_time
-        price_start_time = start_time - timedelta(days=last_days)
-        price_df = D.features(
-            stock_list,
-            ["$close"],
-            price_start_time,
-            price_end_time,
-            freq=freq,
-            disk_cache=True,
-        ).dropna()
-        price_dict = price_df.groupby(["instrument"]).tail(1).reset_index(level=1, drop=True)["$close"].to_dict()
-
-        if len(price_dict) < len(stock_list):
-            lack_stock = set(stock_list) - set(price_dict)
-            raise ValueError(f"{lack_stock} doesn't have close price in qlib in the latest {last_days} days")
-
-        for stock in stock_list:
-            self.position[stock]["price"] = price_dict[stock]
-        self.position["now_account_value"] = self.calculate_value()
-
-    def _init_stock(self, stock_id: str, amount: float, price: float | None = None) -> None:
-        """
-        initialization the stock in current position
-
-        Parameters
-        ----------
-        stock_id :
-            the id of the stock
-        amount : float
-            the amount of the stock
-        price :
-             the price when buying the init stock
-        """
-        self.position[stock_id] = {}
-        self.position[stock_id]["amount"] = amount
-        self.position[stock_id]["price"] = price
-        self.position[stock_id]["weight"] = 0  # update the weight in the end of the trade date
-
-    def _buy_stock(self, stock_id: str, trade_val: float, cost: float, trade_price: float) -> None:
-        trade_amount = trade_val / trade_price
-        if stock_id not in self.position:
-            self._init_stock(stock_id=stock_id, amount=trade_amount, price=trade_price)
-        else:
-            # exist, add amount
-            self.position[stock_id]["amount"] += trade_amount
-
-        self.position["cash"] -= trade_val + cost
-
-    def _sell_stock(self, stock_id: str, trade_val: float, cost: float, trade_price: float) -> None:
-        trade_amount = trade_val / trade_price
-        if stock_id not in self.position:
-            raise KeyError("{} not in current position".format(stock_id))
-        else:
-            if np.isclose(self.position[stock_id]["amount"], trade_amount):
-                # Selling all the stocks
-                # we use np.isclose instead of abs(<the final amount>) <= 1e-5  because `np.isclose` consider both
-                # relative amount and absolute amount
-                # Using abs(<the final amount>) <= 1e-5 will result in error when the amount is large
-                self._del_stock(stock_id)
-            else:
-                # decrease the amount of stock
-                self.position[stock_id]["amount"] -= trade_amount
-                # check if to delete
-                if self.position[stock_id]["amount"] < -1e-5:
-                    raise ValueError(
-                        "only have {} {}, require {}".format(
-                            self.position[stock_id]["amount"] + trade_amount,
-                            stock_id,
-                            trade_amount,
-                        ),
-                    )
-
-        new_cash = trade_val - cost
-        if self._settle_type == self.ST_CASH:
-            self.position["cash_delay"] += new_cash
-        elif self._settle_type == self.ST_NO:
-            self.position["cash"] += new_cash
-        else:
-            raise NotImplementedError(f"This type of input is not supported")
-
-    def _del_stock(self, stock_id: str) -> None:
-        del self.position[stock_id]
-
-    def check_stock(self, stock_id: str) -> bool:
-        return stock_id in self.position
-
-    def update_order(self, order: Order, trade_val: float, cost: float, trade_price: float) -> None:
-        # handle order, order is a order class, defined in exchange.py
-        if order.direction == Order.BUY:
-            # BUY
-            self._buy_stock(order.stock_id, trade_val, cost, trade_price)
-        elif order.direction == Order.SELL:
-            # SELL
-            self._sell_stock(order.stock_id, trade_val, cost, trade_price)
-        else:
-            raise NotImplementedError("do not support order direction {}".format(order.direction))
-
-    def update_stock_price(self, stock_id: str, price: float) -> None:
-        self.position[stock_id]["price"] = price
-
-    def update_stock_count(self, stock_id: str, bar: str, count: float) -> None:  # TODO: check type of `bar`
-        self.position[stock_id][f"count_{bar}"] = count
-
-    def update_stock_weight(self, stock_id: str, weight: float) -> None:
-        self.position[stock_id]["weight"] = weight
-
-    def calculate_stock_value(self) -> float:
-        stock_list = self.get_stock_list()
-        value = 0
-        for stock_id in stock_list:
-            value += self.position[stock_id]["amount"] * self.position[stock_id]["price"]
-        return value
-
-    def calculate_value(self) -> float:
-        value = self.calculate_stock_value()
-        value += self.position["cash"] + self.position.get("cash_delay", 0.0)
-        return value
-
-    def get_stock_list(self) -> List[str]:
-        stock_list = list(set(self.position.keys()) - {"cash", "now_account_value", "cash_delay"})
-        return stock_list
-
-    def get_stock_price(self, code: str) -> float:
-        return self.position[code]["price"]
-
-    def get_stock_amount(self, code: str) -> float:
-        return self.position[code]["amount"] if code in self.position else 0
-
-    def get_stock_count(self, code: str, bar: str) -> float:
-        """the days the account has been hold, it may be used in some special strategies"""
-        if f"count_{bar}" in self.position[code]:
-            return self.position[code][f"count_{bar}"]
-        else:
-            return 0
-
-    def get_stock_weight(self, code: str) -> float:
-        return self.position[code]["weight"]
-
-    def get_cash(self, include_settle: bool = False) -> float:
-        cash = self.position["cash"]
-        if include_settle:
-            cash += self.position.get("cash_delay", 0.0)
-        return cash
-
-    def get_stock_amount_dict(self) -> dict:
-        """generate stock amount dict {stock_id : amount of stock}"""
-        d = {}
-        stock_list = self.get_stock_list()
-        for stock_code in stock_list:
-            d[stock_code] = self.get_stock_amount(code=stock_code)
-        return d
-
-    def get_stock_weight_dict(self, only_stock: bool = False) -> dict:
-        """get_stock_weight_dict
-        generate stock weight dict {stock_id : value weight of stock in the position}
-        it is meaningful in the beginning or the end of each trade date
-
-        :param only_stock: If only_stock=True, the weight of each stock in total stock will be returned
-                           If only_stock=False, the weight of each stock in total assets(stock + cash) will be returned
-        """
-        if only_stock:
-            position_value = self.calculate_stock_value()
-        else:
-            position_value = self.calculate_value()
-        d = {}
-        stock_list = self.get_stock_list()
-        for stock_code in stock_list:
-            d[stock_code] = self.position[stock_code]["amount"] * self.position[stock_code]["price"] / position_value
-        return d
-
-    def add_count_all(self, bar: str) -> None:
-        stock_list = self.get_stock_list()
-        for code in stock_list:
-            if f"count_{bar}" in self.position[code]:
-                self.position[code][f"count_{bar}"] += 1
-            else:
-                self.position[code][f"count_{bar}"] = 1
-
-    def update_weight_all(self) -> None:
-        weight_dict = self.get_stock_weight_dict()
-        for stock_code, weight in weight_dict.items():
-            self.update_stock_weight(stock_code, weight)
-
-    def settle_start(self, settle_type: str) -> None:
-        assert self._settle_type == self.ST_NO, "Currently, settlement can't be nested!!!!!"
-        self._settle_type = settle_type
-        if settle_type == self.ST_CASH:
-            self.position["cash_delay"] = 0.0
-
-    def settle_commit(self) -> None:
-        if self._settle_type != self.ST_NO:
-            if self._settle_type == self.ST_CASH:
-                self.position["cash"] += self.position["cash_delay"]
-                del self.position["cash_delay"]
-            else:
-                raise NotImplementedError(f"This type of input is not supported")
-            self._settle_type = self.ST_NO
-
-
-class InfPosition(BasePosition):
-    """
-    Position with infinite cash and amount.
-
-    This is useful for generating random orders.
-    """
-
-    def skip_update(self) -> bool:
-        """Updating state is meaningless for InfPosition"""
-        return True
-
-    def check_stock(self, stock_id: str) -> bool:
-        # InfPosition always have any stocks
-        return True
-
-    def update_order(self, order: Order, trade_val: float, cost: float, trade_price: float) -> None:
-        pass
-
-    def update_stock_price(self, stock_id: str, price: float) -> None:
-        pass
-
-    def calculate_stock_value(self) -> float:
-        """
-        Returns
-        -------
-        float:
-            infinity stock value
-        """
-        return np.inf
-
-    def calculate_value(self) -> float:
-        raise NotImplementedError(f"InfPosition doesn't support calculating value")
-
-    def get_stock_list(self) -> List[str]:
-        raise NotImplementedError(f"InfPosition doesn't support stock list position")
-
-    def get_stock_price(self, code: str) -> float:
-        """the price of the inf position is meaningless"""
-        return np.nan
-
-    def get_stock_amount(self, code: str) -> float:
-        return np.inf
-
-    def get_cash(self, include_settle: bool = False) -> float:
-        return np.inf
-
-    def get_stock_amount_dict(self) -> dict:
-        raise NotImplementedError(f"InfPosition doesn't support get_stock_amount_dict")
-
-    def get_stock_weight_dict(self, only_stock: bool = False) -> dict:
-        raise NotImplementedError(f"InfPosition doesn't support get_stock_weight_dict")
-
-    def add_count_all(self, bar: str) -> None:
-        raise NotImplementedError(f"InfPosition doesn't support add_count_all")
-
-    def update_weight_all(self) -> None:
-        raise NotImplementedError(f"InfPosition doesn't support update_weight_all")
-
-    def settle_start(self, settle_type: str) -> None:
-        pass
-
-    def settle_commit(self) -> None:
-        pass
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+from __future__ import annotations
+
+from datetime import timedelta
+from typing import Any, Dict, List, Union
+
+import numpy as np
+import pandas as pd
+
+from ..data.data import D
+from .decision import Order
+
+
+class BasePosition:
+    """
+    The Position wants to maintain the position like a dictionary
+    Please refer to the `Position` class for the position
+    """
+
+    def __init__(self, *args: Any, cash: float = 0.0, **kwargs: Any) -> None:
+        self._settle_type = self.ST_NO
+        self.position: dict = {}
+
+    def fill_stock_value(self, start_time: Union[str, pd.Timestamp], freq: str, last_days: int = 30) -> None:
+        pass
+
+    def skip_update(self) -> bool:
+        """
+        Should we skip updating operation for this position
+        For example, updating is meaningless for InfPosition
+
+        Returns
+        -------
+        bool:
+            should we skip the updating operator
+        """
+        return False
+
+    def check_stock(self, stock_id: str) -> bool:
+        """
+        check if is the stock in the position
+
+        Parameters
+        ----------
+        stock_id : str
+            the id of the stock
+
+        Returns
+        -------
+        bool:
+            if is the stock in the position
+        """
+        raise NotImplementedError(f"Please implement the `check_stock` method")
+
+    def update_order(self, order: Order, trade_val: float, cost: float, trade_price: float) -> None:
+        """
+        Parameters
+        ----------
+        order : Order
+            the order to update the position
+        trade_val : float
+            the trade value(money) of dealing results
+        cost : float
+            the trade cost of the dealing results
+        trade_price : float
+            the trade price of the dealing results
+        """
+        raise NotImplementedError(f"Please implement the `update_order` method")
+
+    def update_stock_price(self, stock_id: str, price: float) -> None:
+        """
+        Updating the latest price of the order
+        The useful when clearing balance at each bar end
+
+        Parameters
+        ----------
+        stock_id :
+            the id of the stock
+        price : float
+            the price to be updated
+        """
+        raise NotImplementedError(f"Please implement the `update stock price` method")
+
+    def calculate_stock_value(self) -> float:
+        """
+        calculate the value of the all assets except cash in the position
+
+        Returns
+        -------
+        float:
+            the value(money) of all the stock
+        """
+        raise NotImplementedError(f"Please implement the `calculate_stock_value` method")
+
+    def calculate_value(self) -> float:
+        raise NotImplementedError(f"Please implement the `calculate_value` method")
+
+    def get_stock_list(self) -> List[str]:
+        """
+        Get the list of stocks in the position.
+        """
+        raise NotImplementedError(f"Please implement the `get_stock_list` method")
+
+    def get_stock_price(self, code: str) -> float:
+        """
+        get the latest price of the stock
+
+        Parameters
+        ----------
+        code :
+            the code of the stock
+        """
+        raise NotImplementedError(f"Please implement the `get_stock_price` method")
+
+    def get_stock_amount(self, code: str) -> float:
+        """
+        get the amount of the stock
+
+        Parameters
+        ----------
+        code :
+            the code of the stock
+
+        Returns
+        -------
+        float:
+            the amount of the stock
+        """
+        raise NotImplementedError(f"Please implement the `get_stock_amount` method")
+
+    def get_cash(self, include_settle: bool = False) -> float:
+        """
+        Parameters
+        ----------
+        include_settle:
+            will the unsettled(delayed) cash included
+            Default: not include those unavailable cash
+
+        Returns
+        -------
+        float:
+            the available(tradable) cash in position
+        """
+        raise NotImplementedError(f"Please implement the `get_cash` method")
+
+    def get_stock_amount_dict(self) -> dict:
+        """
+        generate stock amount dict {stock_id : amount of stock}
+
+        Returns
+        -------
+        Dict:
+            {stock_id : amount of stock}
+        """
+        raise NotImplementedError(f"Please implement the `get_stock_amount_dict` method")
+
+    def get_stock_weight_dict(self, only_stock: bool = False) -> dict:
+        """
+        generate stock weight dict {stock_id : value weight of stock in the position}
+        it is meaningful in the beginning or the end of each trade step
+        - During execution of each trading step, the weight may be not consistent with the portfolio value
+
+        Parameters
+        ----------
+        only_stock : bool
+            If only_stock=True, the weight of each stock in total stock will be returned
+            If only_stock=False, the weight of each stock in total assets(stock + cash) will be returned
+
+        Returns
+        -------
+        Dict:
+            {stock_id : value weight of stock in the position}
+        """
+        raise NotImplementedError(f"Please implement the `get_stock_weight_dict` method")
+
+    def add_count_all(self, bar: str) -> None:
+        """
+        Will be called at the end of each bar on each level
+
+        Parameters
+        ----------
+        bar :
+            The level to be updated
+        """
+        raise NotImplementedError(f"Please implement the `add_count_all` method")
+
+    def update_weight_all(self) -> None:
+        """
+        Updating the position weight;
+
+        # TODO: this function is a little weird. The weight data in the position is in a wrong state after dealing order
+        # and before updating weight.
+        """
+        raise NotImplementedError(f"Please implement the `add_count_all` method")
+
+    ST_CASH = "cash"
+    ST_NO = "None"  # String is more typehint friendly than None
+
+    def settle_start(self, settle_type: str) -> None:
+        """
+        settlement start
+        It will act like start and commit a transaction
+
+        Parameters
+        ----------
+        settle_type : str
+            Should we make delay the settlement in each execution (each execution will make the executor a step forward)
+            - "cash": make the cash settlement delayed.
+                - The cash you get can't be used in current step (e.g. you can't sell a stock to get cash to buy another
+                        stock)
+            - None: not settlement mechanism
+            - TODO: other assets will be supported in the future.
+        """
+        raise NotImplementedError(f"Please implement the `settle_conf` method")
+
+    def settle_commit(self) -> None:
+        """
+        settlement commit
+        """
+        raise NotImplementedError(f"Please implement the `settle_commit` method")
+
+    def __str__(self) -> str:
+        return self.__dict__.__str__()
+
+    def __repr__(self) -> str:
+        return self.__dict__.__repr__()
+
+
+class Position(BasePosition):
+    """Position
+
+    current state of position
+    a typical example is :{
+      <instrument_id>: {
+        'count': <how many days the security has been hold>,
+        'amount': <the amount of the security>,
+        'price': <the close price of security in the last trading day>,
+        'weight': <the security weight of total position value>,
+      },
+    }
+    """
+
+    def __init__(self, cash: float = 0, position_dict: Dict[str, Union[Dict[str, float], float]] = {}) -> None:
+        """Init position by cash and position_dict.
+
+        Parameters
+        ----------
+        cash : float, optional
+            initial cash in account, by default 0
+        position_dict : Dict[
+                            stock_id,
+                            Union[
+                                int,  # it is equal to {"amount": int}
+                                {"amount": int, "price"(optional): float},
+                            ]
+                        ]
+            initial stocks with parameters amount and price,
+            if there is no price key in the dict of stocks, it will be filled by _fill_stock_value.
+            by default {}.
+        """
+        super().__init__()
+
+        # NOTE: The position dict must be copied!!!
+        # Otherwise the initial value
+        self.init_cash = cash
+        self.position = position_dict.copy()
+        for stock, value in self.position.items():
+            if isinstance(value, int):
+                self.position[stock] = {"amount": value}
+        self.position["cash"] = cash
+
+        # If the stock price information is missing, the account value will not be calculated temporarily
+        try:
+            self.position["now_account_value"] = self.calculate_value()
+        except KeyError:
+            pass
+
+    def fill_stock_value(self, start_time: Union[str, pd.Timestamp], freq: str, last_days: int = 30) -> None:
+        """fill the stock value by the close price of latest last_days from qlib.
+
+        Parameters
+        ----------
+        start_time :
+            the start time of backtest.
+        freq : str
+            Frequency
+        last_days : int, optional
+            the days to get the latest close price, by default 30.
+        """
+        stock_list = []
+        for stock, value in self.position.items():
+            if not isinstance(value, dict):
+                continue
+            if value.get("price", None) is None:
+                stock_list.append(stock)
+
+        if len(stock_list) == 0:
+            return
+
+        start_time = pd.Timestamp(start_time)
+        # note that start time is 2020-01-01 00:00:00 if raw start time is "2020-01-01"
+        price_end_time = start_time
+        price_start_time = start_time - timedelta(days=last_days)
+        price_df = D.features(
+            stock_list,
+            ["$close"],
+            price_start_time,
+            price_end_time,
+            freq=freq,
+            disk_cache=True,
+        ).dropna()
+        price_dict = price_df.groupby(["instrument"]).tail(1).reset_index(level=1, drop=True)["$close"].to_dict()
+
+        if len(price_dict) < len(stock_list):
+            lack_stock = set(stock_list) - set(price_dict)
+            raise ValueError(f"{lack_stock} doesn't have close price in qlib in the latest {last_days} days")
+
+        for stock in stock_list:
+            self.position[stock]["price"] = price_dict[stock]
+        self.position["now_account_value"] = self.calculate_value()
+
+    def _init_stock(self, stock_id: str, amount: float, price: float | None = None) -> None:
+        """
+        initialization the stock in current position
+
+        Parameters
+        ----------
+        stock_id :
+            the id of the stock
+        amount : float
+            the amount of the stock
+        price :
+             the price when buying the init stock
+        """
+        self.position[stock_id] = {}
+        self.position[stock_id]["amount"] = amount
+        self.position[stock_id]["price"] = price
+        self.position[stock_id]["weight"] = 0  # update the weight in the end of the trade date
+
+    def _buy_stock(self, stock_id: str, trade_val: float, cost: float, trade_price: float) -> None:
+        trade_amount = trade_val / trade_price
+        if stock_id not in self.position:
+            self._init_stock(stock_id=stock_id, amount=trade_amount, price=trade_price)
+        else:
+            # exist, add amount
+            self.position[stock_id]["amount"] += trade_amount
+
+        self.position["cash"] -= trade_val + cost
+
+    def _sell_stock(self, stock_id: str, trade_val: float, cost: float, trade_price: float) -> None:
+        trade_amount = trade_val / trade_price
+        if stock_id not in self.position:
+            raise KeyError("{} not in current position".format(stock_id))
+        else:
+            if np.isclose(self.position[stock_id]["amount"], trade_amount):
+                # Selling all the stocks
+                # we use np.isclose instead of abs(<the final amount>) <= 1e-5  because `np.isclose` consider both
+                # relative amount and absolute amount
+                # Using abs(<the final amount>) <= 1e-5 will result in error when the amount is large
+                self._del_stock(stock_id)
+            else:
+                # decrease the amount of stock
+                self.position[stock_id]["amount"] -= trade_amount
+                # check if to delete
+                if self.position[stock_id]["amount"] < -1e-5:
+                    raise ValueError(
+                        "only have {} {}, require {}".format(
+                            self.position[stock_id]["amount"] + trade_amount,
+                            stock_id,
+                            trade_amount,
+                        ),
+                    )
+
+        new_cash = trade_val - cost
+        if self._settle_type == self.ST_CASH:
+            self.position["cash_delay"] += new_cash
+        elif self._settle_type == self.ST_NO:
+            self.position["cash"] += new_cash
+        else:
+            raise NotImplementedError(f"This type of input is not supported")
+
+    def _del_stock(self, stock_id: str) -> None:
+        del self.position[stock_id]
+
+    def check_stock(self, stock_id: str) -> bool:
+        return stock_id in self.position
+
+    def update_order(self, order: Order, trade_val: float, cost: float, trade_price: float) -> None:
+        # handle order, order is a order class, defined in exchange.py
+        if order.direction == Order.BUY:
+            # BUY
+            self._buy_stock(order.stock_id, trade_val, cost, trade_price)
+        elif order.direction == Order.SELL:
+            # SELL
+            self._sell_stock(order.stock_id, trade_val, cost, trade_price)
+        else:
+            raise NotImplementedError("do not support order direction {}".format(order.direction))
+
+    def update_stock_price(self, stock_id: str, price: float) -> None:
+        self.position[stock_id]["price"] = price
+
+    def update_stock_count(self, stock_id: str, bar: str, count: float) -> None:  # TODO: check type of `bar`
+        self.position[stock_id][f"count_{bar}"] = count
+
+    def update_stock_weight(self, stock_id: str, weight: float) -> None:
+        self.position[stock_id]["weight"] = weight
+
+    def calculate_stock_value(self) -> float:
+        stock_list = self.get_stock_list()
+        value = 0
+        for stock_id in stock_list:
+            value += self.position[stock_id]["amount"] * self.position[stock_id]["price"]
+        return value
+
+    def calculate_value(self) -> float:
+        value = self.calculate_stock_value()
+        value += self.position["cash"] + self.position.get("cash_delay", 0.0)
+        return value
+
+    def get_stock_list(self) -> List[str]:
+        stock_list = list(set(self.position.keys()) - {"cash", "now_account_value", "cash_delay"})
+        return stock_list
+
+    def get_stock_price(self, code: str) -> float:
+        return self.position[code]["price"]
+
+    def get_stock_amount(self, code: str) -> float:
+        return self.position[code]["amount"] if code in self.position else 0
+
+    def get_stock_count(self, code: str, bar: str) -> float:
+        """the days the account has been hold, it may be used in some special strategies"""
+        if f"count_{bar}" in self.position[code]:
+            return self.position[code][f"count_{bar}"]
+        else:
+            return 0
+
+    def get_stock_weight(self, code: str) -> float:
+        return self.position[code]["weight"]
+
+    def get_cash(self, include_settle: bool = False) -> float:
+        cash = self.position["cash"]
+        if include_settle:
+            cash += self.position.get("cash_delay", 0.0)
+        return cash
+
+    def get_stock_amount_dict(self) -> dict:
+        """generate stock amount dict {stock_id : amount of stock}"""
+        d = {}
+        stock_list = self.get_stock_list()
+        for stock_code in stock_list:
+            d[stock_code] = self.get_stock_amount(code=stock_code)
+        return d
+
+    def get_stock_weight_dict(self, only_stock: bool = False) -> dict:
+        """get_stock_weight_dict
+        generate stock weight dict {stock_id : value weight of stock in the position}
+        it is meaningful in the beginning or the end of each trade date
+
+        :param only_stock: If only_stock=True, the weight of each stock in total stock will be returned
+                           If only_stock=False, the weight of each stock in total assets(stock + cash) will be returned
+        """
+        if only_stock:
+            position_value = self.calculate_stock_value()
+        else:
+            position_value = self.calculate_value()
+        d = {}
+        stock_list = self.get_stock_list()
+        for stock_code in stock_list:
+            d[stock_code] = self.position[stock_code]["amount"] * self.position[stock_code]["price"] / position_value
+        return d
+
+    def add_count_all(self, bar: str) -> None:
+        stock_list = self.get_stock_list()
+        for code in stock_list:
+            if f"count_{bar}" in self.position[code]:
+                self.position[code][f"count_{bar}"] += 1
+            else:
+                self.position[code][f"count_{bar}"] = 1
+
+    def update_weight_all(self) -> None:
+        weight_dict = self.get_stock_weight_dict()
+        for stock_code, weight in weight_dict.items():
+            self.update_stock_weight(stock_code, weight)
+
+    def settle_start(self, settle_type: str) -> None:
+        assert self._settle_type == self.ST_NO, "Currently, settlement can't be nested!!!!!"
+        self._settle_type = settle_type
+        if settle_type == self.ST_CASH:
+            self.position["cash_delay"] = 0.0
+
+    def settle_commit(self) -> None:
+        if self._settle_type != self.ST_NO:
+            if self._settle_type == self.ST_CASH:
+                self.position["cash"] += self.position["cash_delay"]
+                del self.position["cash_delay"]
+            else:
+                raise NotImplementedError(f"This type of input is not supported")
+            self._settle_type = self.ST_NO
+
+
+class InfPosition(BasePosition):
+    """
+    Position with infinite cash and amount.
+
+    This is useful for generating random orders.
+    """
+
+    def skip_update(self) -> bool:
+        """Updating state is meaningless for InfPosition"""
+        return True
+
+    def check_stock(self, stock_id: str) -> bool:
+        # InfPosition always have any stocks
+        return True
+
+    def update_order(self, order: Order, trade_val: float, cost: float, trade_price: float) -> None:
+        pass
+
+    def update_stock_price(self, stock_id: str, price: float) -> None:
+        pass
+
+    def calculate_stock_value(self) -> float:
+        """
+        Returns
+        -------
+        float:
+            infinity stock value
+        """
+        return np.inf
+
+    def calculate_value(self) -> float:
+        raise NotImplementedError(f"InfPosition doesn't support calculating value")
+
+    def get_stock_list(self) -> List[str]:
+        raise NotImplementedError(f"InfPosition doesn't support stock list position")
+
+    def get_stock_price(self, code: str) -> float:
+        """the price of the inf position is meaningless"""
+        return np.nan
+
+    def get_stock_amount(self, code: str) -> float:
+        return np.inf
+
+    def get_cash(self, include_settle: bool = False) -> float:
+        return np.inf
+
+    def get_stock_amount_dict(self) -> dict:
+        raise NotImplementedError(f"InfPosition doesn't support get_stock_amount_dict")
+
+    def get_stock_weight_dict(self, only_stock: bool = False) -> dict:
+        raise NotImplementedError(f"InfPosition doesn't support get_stock_weight_dict")
+
+    def add_count_all(self, bar: str) -> None:
+        raise NotImplementedError(f"InfPosition doesn't support add_count_all")
+
+    def update_weight_all(self) -> None:
+        raise NotImplementedError(f"InfPosition doesn't support update_weight_all")
+
+    def settle_start(self, settle_type: str) -> None:
+        pass
+
+    def settle_commit(self) -> None:
+        pass
```

## qlib/backtest/profit_attribution.py

 * *Ordering differences only*

```diff
@@ -1,334 +1,334 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-"""
-This module is not well maintained.
-"""
-
-import datetime
-from pathlib import Path
-
-import numpy as np
-import pandas as pd
-
-from ..config import C
-from ..data import D
-from .position import Position
-
-
-def get_benchmark_weight(
-    bench,
-    start_date=None,
-    end_date=None,
-    path=None,
-    freq="day",
-):
-    """get_benchmark_weight
-
-    get the stock weight distribution of the benchmark
-
-    :param bench:
-    :param start_date:
-    :param end_date:
-    :param path:
-    :param freq:
-
-    :return: The weight distribution of the the benchmark described by a pandas dataframe
-             Every row corresponds to a trading day.
-             Every column corresponds to a stock.
-             Every cell represents the strategy.
-
-    """
-    if not path:
-        path = Path(C.dpm.get_data_uri(freq)).expanduser() / "raw" / "AIndexMembers" / "weights.csv"
-    # TODO: the storage of weights should be implemented in a more elegent way
-    # TODO: The benchmark is not consistent with the filename in instruments.
-    bench_weight_df = pd.read_csv(path, usecols=["code", "date", "index", "weight"])
-    bench_weight_df = bench_weight_df[bench_weight_df["index"] == bench]
-    bench_weight_df["date"] = pd.to_datetime(bench_weight_df["date"])
-    if start_date is not None:
-        bench_weight_df = bench_weight_df[bench_weight_df.date >= start_date]
-    if end_date is not None:
-        bench_weight_df = bench_weight_df[bench_weight_df.date <= end_date]
-    bench_stock_weight = bench_weight_df.pivot_table(index="date", columns="code", values="weight") / 100.0
-    return bench_stock_weight
-
-
-def get_stock_weight_df(positions):
-    """get_stock_weight_df
-    :param positions: Given a positions from backtest result.
-    :return:          A weight distribution for the position
-    """
-    stock_weight = []
-    index = []
-    for date in sorted(positions.keys()):
-        pos = positions[date]
-        if isinstance(pos, dict):
-            pos = Position(position_dict=pos)
-        index.append(date)
-        stock_weight.append(pos.get_stock_weight_dict(only_stock=True))
-    return pd.DataFrame(stock_weight, index=index)
-
-
-def decompose_portofolio_weight(stock_weight_df, stock_group_df):
-    """decompose_portofolio_weight
-
-    '''
-    :param stock_weight_df: a pandas dataframe to describe the portofolio by weight.
-                    every row corresponds to a  day
-                    every column corresponds to a stock.
-                    Here is an example below.
-                    code        SH600004  SH600006  SH600017  SH600022  SH600026  SH600037  \
-                    date
-                    2016-01-05  0.001543  0.001570  0.002732  0.001320  0.003000       NaN
-                    2016-01-06  0.001538  0.001569  0.002770  0.001417  0.002945       NaN
-                    ....
-    :param stock_group_df: a pandas dataframe to describe  the stock group.
-                    every row corresponds to a  day
-                    every column corresponds to a stock.
-                    the value in the cell repreponds the group id.
-                    Here is a example by for stock_group_df for industry. The value is the industry code
-                    instrument  SH600000  SH600004  SH600005  SH600006  SH600007  SH600008  \
-                    datetime
-                    2016-01-05  801780.0  801170.0  801040.0  801880.0  801180.0  801160.0
-                    2016-01-06  801780.0  801170.0  801040.0  801880.0  801180.0  801160.0
-                    ...
-    :return:        Two dict will be returned.  The group_weight and the stock_weight_in_group.
-                    The key is the group. The value is a Series or Dataframe to describe the weight of group or weight of stock
-    """
-    all_group = np.unique(stock_group_df.values.flatten())
-    all_group = all_group[~np.isnan(all_group)]
-
-    group_weight = {}
-    stock_weight_in_group = {}
-    for group_key in all_group:
-        group_mask = stock_group_df == group_key
-        group_weight[group_key] = stock_weight_df[group_mask].sum(axis=1)
-        stock_weight_in_group[group_key] = stock_weight_df[group_mask].divide(group_weight[group_key], axis=0)
-    return group_weight, stock_weight_in_group
-
-
-def decompose_portofolio(stock_weight_df, stock_group_df, stock_ret_df):
-    """
-    :param stock_weight_df: a pandas dataframe to describe the portofolio by weight.
-                    every row corresponds to a  day
-                    every column corresponds to a stock.
-                    Here is an example below.
-                    code        SH600004  SH600006  SH600017  SH600022  SH600026  SH600037  \
-                    date
-                    2016-01-05  0.001543  0.001570  0.002732  0.001320  0.003000       NaN
-                    2016-01-06  0.001538  0.001569  0.002770  0.001417  0.002945       NaN
-                    2016-01-07  0.001555  0.001546  0.002772  0.001393  0.002904       NaN
-                    2016-01-08  0.001564  0.001527  0.002791  0.001506  0.002948       NaN
-                    2016-01-11  0.001597  0.001476  0.002738  0.001493  0.003043       NaN
-                    ....
-
-    :param stock_group_df: a pandas dataframe to describe  the stock group.
-                    every row corresponds to a  day
-                    every column corresponds to a stock.
-                    the value in the cell repreponds the group id.
-                    Here is a example by for stock_group_df for industry. The value is the industry code
-                    instrument  SH600000  SH600004  SH600005  SH600006  SH600007  SH600008  \
-                    datetime
-                    2016-01-05  801780.0  801170.0  801040.0  801880.0  801180.0  801160.0
-                    2016-01-06  801780.0  801170.0  801040.0  801880.0  801180.0  801160.0
-                    2016-01-07  801780.0  801170.0  801040.0  801880.0  801180.0  801160.0
-                    2016-01-08  801780.0  801170.0  801040.0  801880.0  801180.0  801160.0
-                    2016-01-11  801780.0  801170.0  801040.0  801880.0  801180.0  801160.0
-                    ...
-
-    :param stock_ret_df:   a pandas dataframe to describe the stock return.
-                    every row corresponds to a day
-                    every column corresponds to a stock.
-                    the value in the cell repreponds the return of the group.
-                    Here is a example by for stock_ret_df.
-                    instrument  SH600000  SH600004  SH600005  SH600006  SH600007  SH600008  \
-                    datetime
-                    2016-01-05  0.007795  0.022070  0.099099  0.024707  0.009473  0.016216
-                    2016-01-06 -0.032597 -0.075205 -0.098361 -0.098985 -0.099707 -0.098936
-                    2016-01-07 -0.001142  0.022544  0.100000  0.004225  0.000651  0.047226
-                    2016-01-08 -0.025157 -0.047244 -0.038567 -0.098177 -0.099609 -0.074408
-                    2016-01-11  0.023460  0.004959 -0.034384  0.018663  0.014461  0.010962
-                    ...
-
-    :return: It will decompose the portofolio to the group weight and group return.
-    """
-    all_group = np.unique(stock_group_df.values.flatten())
-    all_group = all_group[~np.isnan(all_group)]
-
-    group_weight, stock_weight_in_group = decompose_portofolio_weight(stock_weight_df, stock_group_df)
-
-    group_ret = {}
-    for group_key, val in stock_weight_in_group.items():
-        stock_weight_in_group_start_date = min(val.index)
-        stock_weight_in_group_end_date = max(val.index)
-
-        temp_stock_ret_df = stock_ret_df[
-            (stock_ret_df.index >= stock_weight_in_group_start_date)
-            & (stock_ret_df.index <= stock_weight_in_group_end_date)
-        ]
-
-        group_ret[group_key] = (temp_stock_ret_df * val).sum(axis=1)
-        # If no weight is assigned, then the return of group will be np.nan
-        group_ret[group_key][group_weight[group_key] == 0.0] = np.nan
-
-    group_weight_df = pd.DataFrame(group_weight)
-    group_ret_df = pd.DataFrame(group_ret)
-    return group_weight_df, group_ret_df
-
-
-def get_daily_bin_group(bench_values, stock_values, group_n):
-    """get_daily_bin_group
-    Group the values of the stocks of benchmark into several bins in a day.
-    Put the stocks into these bins.
-
-    :param bench_values: A series contains the value of stocks in benchmark.
-                         The index is the stock code.
-    :param stock_values: A series contains the value of stocks of your portofolio
-                         The index is the stock code.
-    :param group_n:      Bins will be produced
-
-    :return:             A series with the same size and index as the stock_value.
-                         The value in the series is the group id of the bins.
-                         The No.1 bin contains the biggest values.
-    """
-    stock_group = stock_values.copy()
-
-    # get the bin split points based on the daily proportion of benchmark
-    split_points = np.percentile(bench_values[~bench_values.isna()], np.linspace(0, 100, group_n + 1))
-    # Modify the biggest uppper bound and smallest lowerbound
-    split_points[0], split_points[-1] = -np.inf, np.inf
-    for i, (lb, up) in enumerate(zip(split_points, split_points[1:])):
-        stock_group.loc[stock_values[(stock_values >= lb) & (stock_values < up)].index] = group_n - i
-    return stock_group
-
-
-def get_stock_group(stock_group_field_df, bench_stock_weight_df, group_method, group_n=None):
-    if group_method == "category":
-        # use the value of the benchmark as the category
-        return stock_group_field_df
-    elif group_method == "bins":
-        assert group_n is not None
-        # place the values into `group_n` fields.
-        # Each bin corresponds to a category.
-        new_stock_group_df = stock_group_field_df.copy().loc[
-            bench_stock_weight_df.index.min() : bench_stock_weight_df.index.max()
-        ]
-        for idx, row in (~bench_stock_weight_df.isna()).iterrows():
-            bench_values = stock_group_field_df.loc[idx, row[row].index]
-            new_stock_group_df.loc[idx] = get_daily_bin_group(
-                bench_values,
-                stock_group_field_df.loc[idx],
-                group_n=group_n,
-            )
-        return new_stock_group_df
-
-
-def brinson_pa(
-    positions,
-    bench="SH000905",
-    group_field="industry",
-    group_method="category",
-    group_n=None,
-    deal_price="vwap",
-    freq="day",
-):
-    """brinson profit attribution
-
-    :param positions: The position produced by the backtest class
-    :param bench: The benchmark for comparing. TODO: if no benchmark is set, the equal-weighted is used.
-    :param group_field: The field used to set the group for assets allocation.
-                        `industry` and `market_value` is often used.
-    :param group_method: 'category' or 'bins'. The method used to set the group for asstes allocation
-                         `bin` will split the value into `group_n` bins and each bins represents a group
-    :param group_n: . Only used when group_method == 'bins'.
-
-    :return:
-        A dataframe with three columns: RAA(excess Return of Assets Allocation),  RSS(excess Return of Stock Selectino),  RTotal(Total excess Return)
-                                        Every row corresponds to a trading day, the value corresponds to the next return for this trading day
-        The middle info of brinson profit attribution
-    """
-    # group_method will decide how to group the group_field.
-    dates = sorted(positions.keys())
-
-    start_date, end_date = min(dates), max(dates)
-
-    bench_stock_weight = get_benchmark_weight(bench, start_date, end_date, freq)
-
-    # The attributes for allocation will not
-    if not group_field.startswith("$"):
-        group_field = "$" + group_field
-    if not deal_price.startswith("$"):
-        deal_price = "$" + deal_price
-
-    # FIXME: In current version.  Some attributes(such as market_value) of some
-    # suspend stock is NAN. So we have to get more date to forward fill the NAN
-    shift_start_date = start_date - datetime.timedelta(days=250)
-    instruments = D.list_instruments(
-        D.instruments(market="all"),
-        start_time=shift_start_date,
-        end_time=end_date,
-        as_list=True,
-        freq=freq,
-    )
-    stock_df = D.features(
-        instruments,
-        [group_field, deal_price],
-        start_time=shift_start_date,
-        end_time=end_date,
-        freq=freq,
-    )
-    stock_df.columns = [group_field, "deal_price"]
-
-    stock_group_field = stock_df[group_field].unstack().T
-    # FIXME: some attributes of some suspend stock is NAN.
-    stock_group_field = stock_group_field.fillna(method="ffill")
-    stock_group_field = stock_group_field.loc[start_date:end_date]
-
-    stock_group = get_stock_group(stock_group_field, bench_stock_weight, group_method, group_n)
-
-    deal_price_df = stock_df["deal_price"].unstack().T
-    deal_price_df = deal_price_df.fillna(method="ffill")
-
-    # NOTE:
-    # The return will be slightly different from the of the return in the report.
-    # Here the position are adjusted at the end of the trading day with close
-    stock_ret = (deal_price_df - deal_price_df.shift(1)) / deal_price_df.shift(1)
-    stock_ret = stock_ret.shift(-1).loc[start_date:end_date]
-
-    port_stock_weight_df = get_stock_weight_df(positions)
-
-    # decomposing the portofolio
-    port_group_weight_df, port_group_ret_df = decompose_portofolio(port_stock_weight_df, stock_group, stock_ret)
-    bench_group_weight_df, bench_group_ret_df = decompose_portofolio(bench_stock_weight, stock_group, stock_ret)
-
-    # if the group return of the portofolio is NaN, replace it with the market
-    # value
-    mod_port_group_ret_df = port_group_ret_df.copy()
-    mod_port_group_ret_df[mod_port_group_ret_df.isna()] = bench_group_ret_df
-
-    Q1 = (bench_group_weight_df * bench_group_ret_df).sum(axis=1)
-    Q2 = (port_group_weight_df * bench_group_ret_df).sum(axis=1)
-    Q3 = (bench_group_weight_df * mod_port_group_ret_df).sum(axis=1)
-    Q4 = (port_group_weight_df * mod_port_group_ret_df).sum(axis=1)
-
-    return (
-        pd.DataFrame(
-            {
-                "RAA": Q2 - Q1,  # The excess profit from the assets allocation
-                "RSS": Q3 - Q1,  # The excess profit from the stocks selection
-                # The excess profit from the interaction of assets allocation and stocks selection
-                "RIN": Q4 - Q3 - Q2 + Q1,
-                "RTotal": Q4 - Q1,  # The totoal excess profit
-            },
-        ),
-        {
-            "port_group_ret": port_group_ret_df,
-            "port_group_weight": port_group_weight_df,
-            "bench_group_ret": bench_group_ret_df,
-            "bench_group_weight": bench_group_weight_df,
-            "stock_group": stock_group,
-            "bench_stock_weight": bench_stock_weight,
-            "port_stock_weight": port_stock_weight_df,
-            "stock_ret": stock_ret,
-        },
-    )
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+"""
+This module is not well maintained.
+"""
+
+import datetime
+from pathlib import Path
+
+import numpy as np
+import pandas as pd
+
+from ..config import C
+from ..data import D
+from .position import Position
+
+
+def get_benchmark_weight(
+    bench,
+    start_date=None,
+    end_date=None,
+    path=None,
+    freq="day",
+):
+    """get_benchmark_weight
+
+    get the stock weight distribution of the benchmark
+
+    :param bench:
+    :param start_date:
+    :param end_date:
+    :param path:
+    :param freq:
+
+    :return: The weight distribution of the the benchmark described by a pandas dataframe
+             Every row corresponds to a trading day.
+             Every column corresponds to a stock.
+             Every cell represents the strategy.
+
+    """
+    if not path:
+        path = Path(C.dpm.get_data_uri(freq)).expanduser() / "raw" / "AIndexMembers" / "weights.csv"
+    # TODO: the storage of weights should be implemented in a more elegent way
+    # TODO: The benchmark is not consistent with the filename in instruments.
+    bench_weight_df = pd.read_csv(path, usecols=["code", "date", "index", "weight"])
+    bench_weight_df = bench_weight_df[bench_weight_df["index"] == bench]
+    bench_weight_df["date"] = pd.to_datetime(bench_weight_df["date"])
+    if start_date is not None:
+        bench_weight_df = bench_weight_df[bench_weight_df.date >= start_date]
+    if end_date is not None:
+        bench_weight_df = bench_weight_df[bench_weight_df.date <= end_date]
+    bench_stock_weight = bench_weight_df.pivot_table(index="date", columns="code", values="weight") / 100.0
+    return bench_stock_weight
+
+
+def get_stock_weight_df(positions):
+    """get_stock_weight_df
+    :param positions: Given a positions from backtest result.
+    :return:          A weight distribution for the position
+    """
+    stock_weight = []
+    index = []
+    for date in sorted(positions.keys()):
+        pos = positions[date]
+        if isinstance(pos, dict):
+            pos = Position(position_dict=pos)
+        index.append(date)
+        stock_weight.append(pos.get_stock_weight_dict(only_stock=True))
+    return pd.DataFrame(stock_weight, index=index)
+
+
+def decompose_portofolio_weight(stock_weight_df, stock_group_df):
+    """decompose_portofolio_weight
+
+    '''
+    :param stock_weight_df: a pandas dataframe to describe the portofolio by weight.
+                    every row corresponds to a  day
+                    every column corresponds to a stock.
+                    Here is an example below.
+                    code        SH600004  SH600006  SH600017  SH600022  SH600026  SH600037  \
+                    date
+                    2016-01-05  0.001543  0.001570  0.002732  0.001320  0.003000       NaN
+                    2016-01-06  0.001538  0.001569  0.002770  0.001417  0.002945       NaN
+                    ....
+    :param stock_group_df: a pandas dataframe to describe  the stock group.
+                    every row corresponds to a  day
+                    every column corresponds to a stock.
+                    the value in the cell repreponds the group id.
+                    Here is a example by for stock_group_df for industry. The value is the industry code
+                    instrument  SH600000  SH600004  SH600005  SH600006  SH600007  SH600008  \
+                    datetime
+                    2016-01-05  801780.0  801170.0  801040.0  801880.0  801180.0  801160.0
+                    2016-01-06  801780.0  801170.0  801040.0  801880.0  801180.0  801160.0
+                    ...
+    :return:        Two dict will be returned.  The group_weight and the stock_weight_in_group.
+                    The key is the group. The value is a Series or Dataframe to describe the weight of group or weight of stock
+    """
+    all_group = np.unique(stock_group_df.values.flatten())
+    all_group = all_group[~np.isnan(all_group)]
+
+    group_weight = {}
+    stock_weight_in_group = {}
+    for group_key in all_group:
+        group_mask = stock_group_df == group_key
+        group_weight[group_key] = stock_weight_df[group_mask].sum(axis=1)
+        stock_weight_in_group[group_key] = stock_weight_df[group_mask].divide(group_weight[group_key], axis=0)
+    return group_weight, stock_weight_in_group
+
+
+def decompose_portofolio(stock_weight_df, stock_group_df, stock_ret_df):
+    """
+    :param stock_weight_df: a pandas dataframe to describe the portofolio by weight.
+                    every row corresponds to a  day
+                    every column corresponds to a stock.
+                    Here is an example below.
+                    code        SH600004  SH600006  SH600017  SH600022  SH600026  SH600037  \
+                    date
+                    2016-01-05  0.001543  0.001570  0.002732  0.001320  0.003000       NaN
+                    2016-01-06  0.001538  0.001569  0.002770  0.001417  0.002945       NaN
+                    2016-01-07  0.001555  0.001546  0.002772  0.001393  0.002904       NaN
+                    2016-01-08  0.001564  0.001527  0.002791  0.001506  0.002948       NaN
+                    2016-01-11  0.001597  0.001476  0.002738  0.001493  0.003043       NaN
+                    ....
+
+    :param stock_group_df: a pandas dataframe to describe  the stock group.
+                    every row corresponds to a  day
+                    every column corresponds to a stock.
+                    the value in the cell repreponds the group id.
+                    Here is a example by for stock_group_df for industry. The value is the industry code
+                    instrument  SH600000  SH600004  SH600005  SH600006  SH600007  SH600008  \
+                    datetime
+                    2016-01-05  801780.0  801170.0  801040.0  801880.0  801180.0  801160.0
+                    2016-01-06  801780.0  801170.0  801040.0  801880.0  801180.0  801160.0
+                    2016-01-07  801780.0  801170.0  801040.0  801880.0  801180.0  801160.0
+                    2016-01-08  801780.0  801170.0  801040.0  801880.0  801180.0  801160.0
+                    2016-01-11  801780.0  801170.0  801040.0  801880.0  801180.0  801160.0
+                    ...
+
+    :param stock_ret_df:   a pandas dataframe to describe the stock return.
+                    every row corresponds to a day
+                    every column corresponds to a stock.
+                    the value in the cell repreponds the return of the group.
+                    Here is a example by for stock_ret_df.
+                    instrument  SH600000  SH600004  SH600005  SH600006  SH600007  SH600008  \
+                    datetime
+                    2016-01-05  0.007795  0.022070  0.099099  0.024707  0.009473  0.016216
+                    2016-01-06 -0.032597 -0.075205 -0.098361 -0.098985 -0.099707 -0.098936
+                    2016-01-07 -0.001142  0.022544  0.100000  0.004225  0.000651  0.047226
+                    2016-01-08 -0.025157 -0.047244 -0.038567 -0.098177 -0.099609 -0.074408
+                    2016-01-11  0.023460  0.004959 -0.034384  0.018663  0.014461  0.010962
+                    ...
+
+    :return: It will decompose the portofolio to the group weight and group return.
+    """
+    all_group = np.unique(stock_group_df.values.flatten())
+    all_group = all_group[~np.isnan(all_group)]
+
+    group_weight, stock_weight_in_group = decompose_portofolio_weight(stock_weight_df, stock_group_df)
+
+    group_ret = {}
+    for group_key, val in stock_weight_in_group.items():
+        stock_weight_in_group_start_date = min(val.index)
+        stock_weight_in_group_end_date = max(val.index)
+
+        temp_stock_ret_df = stock_ret_df[
+            (stock_ret_df.index >= stock_weight_in_group_start_date)
+            & (stock_ret_df.index <= stock_weight_in_group_end_date)
+        ]
+
+        group_ret[group_key] = (temp_stock_ret_df * val).sum(axis=1)
+        # If no weight is assigned, then the return of group will be np.nan
+        group_ret[group_key][group_weight[group_key] == 0.0] = np.nan
+
+    group_weight_df = pd.DataFrame(group_weight)
+    group_ret_df = pd.DataFrame(group_ret)
+    return group_weight_df, group_ret_df
+
+
+def get_daily_bin_group(bench_values, stock_values, group_n):
+    """get_daily_bin_group
+    Group the values of the stocks of benchmark into several bins in a day.
+    Put the stocks into these bins.
+
+    :param bench_values: A series contains the value of stocks in benchmark.
+                         The index is the stock code.
+    :param stock_values: A series contains the value of stocks of your portofolio
+                         The index is the stock code.
+    :param group_n:      Bins will be produced
+
+    :return:             A series with the same size and index as the stock_value.
+                         The value in the series is the group id of the bins.
+                         The No.1 bin contains the biggest values.
+    """
+    stock_group = stock_values.copy()
+
+    # get the bin split points based on the daily proportion of benchmark
+    split_points = np.percentile(bench_values[~bench_values.isna()], np.linspace(0, 100, group_n + 1))
+    # Modify the biggest uppper bound and smallest lowerbound
+    split_points[0], split_points[-1] = -np.inf, np.inf
+    for i, (lb, up) in enumerate(zip(split_points, split_points[1:])):
+        stock_group.loc[stock_values[(stock_values >= lb) & (stock_values < up)].index] = group_n - i
+    return stock_group
+
+
+def get_stock_group(stock_group_field_df, bench_stock_weight_df, group_method, group_n=None):
+    if group_method == "category":
+        # use the value of the benchmark as the category
+        return stock_group_field_df
+    elif group_method == "bins":
+        assert group_n is not None
+        # place the values into `group_n` fields.
+        # Each bin corresponds to a category.
+        new_stock_group_df = stock_group_field_df.copy().loc[
+            bench_stock_weight_df.index.min() : bench_stock_weight_df.index.max()
+        ]
+        for idx, row in (~bench_stock_weight_df.isna()).iterrows():
+            bench_values = stock_group_field_df.loc[idx, row[row].index]
+            new_stock_group_df.loc[idx] = get_daily_bin_group(
+                bench_values,
+                stock_group_field_df.loc[idx],
+                group_n=group_n,
+            )
+        return new_stock_group_df
+
+
+def brinson_pa(
+    positions,
+    bench="SH000905",
+    group_field="industry",
+    group_method="category",
+    group_n=None,
+    deal_price="vwap",
+    freq="day",
+):
+    """brinson profit attribution
+
+    :param positions: The position produced by the backtest class
+    :param bench: The benchmark for comparing. TODO: if no benchmark is set, the equal-weighted is used.
+    :param group_field: The field used to set the group for assets allocation.
+                        `industry` and `market_value` is often used.
+    :param group_method: 'category' or 'bins'. The method used to set the group for asstes allocation
+                         `bin` will split the value into `group_n` bins and each bins represents a group
+    :param group_n: . Only used when group_method == 'bins'.
+
+    :return:
+        A dataframe with three columns: RAA(excess Return of Assets Allocation),  RSS(excess Return of Stock Selectino),  RTotal(Total excess Return)
+                                        Every row corresponds to a trading day, the value corresponds to the next return for this trading day
+        The middle info of brinson profit attribution
+    """
+    # group_method will decide how to group the group_field.
+    dates = sorted(positions.keys())
+
+    start_date, end_date = min(dates), max(dates)
+
+    bench_stock_weight = get_benchmark_weight(bench, start_date, end_date, freq)
+
+    # The attributes for allocation will not
+    if not group_field.startswith("$"):
+        group_field = "$" + group_field
+    if not deal_price.startswith("$"):
+        deal_price = "$" + deal_price
+
+    # FIXME: In current version.  Some attributes(such as market_value) of some
+    # suspend stock is NAN. So we have to get more date to forward fill the NAN
+    shift_start_date = start_date - datetime.timedelta(days=250)
+    instruments = D.list_instruments(
+        D.instruments(market="all"),
+        start_time=shift_start_date,
+        end_time=end_date,
+        as_list=True,
+        freq=freq,
+    )
+    stock_df = D.features(
+        instruments,
+        [group_field, deal_price],
+        start_time=shift_start_date,
+        end_time=end_date,
+        freq=freq,
+    )
+    stock_df.columns = [group_field, "deal_price"]
+
+    stock_group_field = stock_df[group_field].unstack().T
+    # FIXME: some attributes of some suspend stock is NAN.
+    stock_group_field = stock_group_field.fillna(method="ffill")
+    stock_group_field = stock_group_field.loc[start_date:end_date]
+
+    stock_group = get_stock_group(stock_group_field, bench_stock_weight, group_method, group_n)
+
+    deal_price_df = stock_df["deal_price"].unstack().T
+    deal_price_df = deal_price_df.fillna(method="ffill")
+
+    # NOTE:
+    # The return will be slightly different from the of the return in the report.
+    # Here the position are adjusted at the end of the trading day with close
+    stock_ret = (deal_price_df - deal_price_df.shift(1)) / deal_price_df.shift(1)
+    stock_ret = stock_ret.shift(-1).loc[start_date:end_date]
+
+    port_stock_weight_df = get_stock_weight_df(positions)
+
+    # decomposing the portofolio
+    port_group_weight_df, port_group_ret_df = decompose_portofolio(port_stock_weight_df, stock_group, stock_ret)
+    bench_group_weight_df, bench_group_ret_df = decompose_portofolio(bench_stock_weight, stock_group, stock_ret)
+
+    # if the group return of the portofolio is NaN, replace it with the market
+    # value
+    mod_port_group_ret_df = port_group_ret_df.copy()
+    mod_port_group_ret_df[mod_port_group_ret_df.isna()] = bench_group_ret_df
+
+    Q1 = (bench_group_weight_df * bench_group_ret_df).sum(axis=1)
+    Q2 = (port_group_weight_df * bench_group_ret_df).sum(axis=1)
+    Q3 = (bench_group_weight_df * mod_port_group_ret_df).sum(axis=1)
+    Q4 = (port_group_weight_df * mod_port_group_ret_df).sum(axis=1)
+
+    return (
+        pd.DataFrame(
+            {
+                "RAA": Q2 - Q1,  # The excess profit from the assets allocation
+                "RSS": Q3 - Q1,  # The excess profit from the stocks selection
+                # The excess profit from the interaction of assets allocation and stocks selection
+                "RIN": Q4 - Q3 - Q2 + Q1,
+                "RTotal": Q4 - Q1,  # The totoal excess profit
+            },
+        ),
+        {
+            "port_group_ret": port_group_ret_df,
+            "port_group_weight": port_group_weight_df,
+            "bench_group_ret": bench_group_ret_df,
+            "bench_group_weight": bench_group_weight_df,
+            "stock_group": stock_group,
+            "bench_stock_weight": bench_stock_weight,
+            "port_stock_weight": port_stock_weight_df,
+            "stock_ret": stock_ret,
+        },
+    )
```

## qlib/backtest/report.py

 * *Ordering differences only*

```diff
@@ -1,641 +1,641 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-from __future__ import annotations
-
-import pathlib
-from collections import OrderedDict
-from typing import Any, Dict, List, Optional, Text, Tuple, Type, Union, cast
-
-import numpy as np
-import pandas as pd
-
-import qlib.utils.index_data as idd
-from qlib.backtest.decision import BaseTradeDecision, Order, OrderDir
-from qlib.backtest.exchange import Exchange
-
-from ..tests.config import CSI300_BENCH
-from ..utils.resam import get_higher_eq_freq_feature, resam_ts_data
-from .high_performance_ds import BaseOrderIndicator, BaseSingleMetric, NumpyOrderIndicator
-
-
-class PortfolioMetrics:
-    """
-    Motivation:
-        PortfolioMetrics is for supporting portfolio related metrics.
-
-    Implementation:
-
-        daily portfolio metrics of the account
-        contain those followings: return, cost, turnover, account, cash, bench, value
-        For each step(bar/day/minute), each column represents
-        - return: the return of the portfolio generated by strategy **without transaction fee**.
-        - cost: the transaction fee and slippage.
-        - account: the total value of assets(cash and securities are both included) in user account based on the close price of each step.
-        - cash: the amount of cash in user's account.
-        - bench: the return of the benchmark
-        - value: the total value of securities/stocks/instruments (cash is excluded).
-
-        update report
-    """
-
-    def __init__(self, freq: str = "day", benchmark_config: dict = {}) -> None:
-        """
-        Parameters
-        ----------
-        freq : str
-            frequency of trading bar, used for updating hold count of trading bar
-        benchmark_config : dict
-            config of benchmark, may including the following arguments:
-            - benchmark : Union[str, list, pd.Series]
-                - If `benchmark` is pd.Series, `index` is trading date; the value T is the change from T-1 to T.
-                    example:
-                        print(
-                            D.features(D.instruments('csi500'),
-                            ['$close/Ref($close, 1)-1'])['$close/Ref($close, 1)-1'].head()
-                        )
-                            2017-01-04    0.011693
-                            2017-01-05    0.000721
-                            2017-01-06   -0.004322
-                            2017-01-09    0.006874
-                            2017-01-10   -0.003350
-                - If `benchmark` is list, will use the daily average change of the stock pool in the list as the
-                    'bench'.
-                - If `benchmark` is str, will use the daily change as the 'bench'.
-                benchmark code, default is SH000300 CSI300
-            - start_time : Union[str, pd.Timestamp], optional
-                - If `benchmark` is pd.Series, it will be ignored
-                - Else, it represent start time of benchmark, by default None
-            - end_time : Union[str, pd.Timestamp], optional
-                - If `benchmark` is pd.Series, it will be ignored
-                - Else, it represent end time of benchmark, by default None
-
-        """
-
-        self.init_vars()
-        self.init_bench(freq=freq, benchmark_config=benchmark_config)
-
-    def init_vars(self) -> None:
-        self.accounts: dict = OrderedDict()  # account position value for each trade time
-        self.returns: dict = OrderedDict()  # daily return rate for each trade time
-        self.total_turnovers: dict = OrderedDict()  # total turnover for each trade time
-        self.turnovers: dict = OrderedDict()  # turnover for each trade time
-        self.total_costs: dict = OrderedDict()  # total trade cost for each trade time
-        self.costs: dict = OrderedDict()  # trade cost rate for each trade time
-        self.values: dict = OrderedDict()  # value for each trade time
-        self.cashes: dict = OrderedDict()
-        self.benches: dict = OrderedDict()
-        self.latest_pm_time: Optional[pd.TimeStamp] = None
-
-    def init_bench(self, freq: str | None = None, benchmark_config: dict | None = None) -> None:
-        if freq is not None:
-            self.freq = freq
-        self.benchmark_config = benchmark_config
-        self.bench = self._cal_benchmark(self.benchmark_config, self.freq)
-
-    @staticmethod
-    def _cal_benchmark(benchmark_config: Optional[dict], freq: str) -> Optional[pd.Series]:
-        if benchmark_config is None:
-            return None
-        benchmark = benchmark_config.get("benchmark", CSI300_BENCH)
-        if benchmark is None:
-            return None
-
-        if isinstance(benchmark, pd.Series):
-            return benchmark
-        else:
-            start_time = benchmark_config.get("start_time", None)
-            end_time = benchmark_config.get("end_time", None)
-
-            if freq is None:
-                raise ValueError("benchmark freq can't be None!")
-            _codes = benchmark if isinstance(benchmark, (list, dict)) else [benchmark]
-            fields = ["$close/Ref($close,1)-1"]
-            _temp_result, _ = get_higher_eq_freq_feature(_codes, fields, start_time, end_time, freq=freq)
-            if len(_temp_result) == 0:
-                raise ValueError(f"The benchmark {_codes} does not exist. Please provide the right benchmark")
-            return _temp_result.groupby(level="datetime")[_temp_result.columns.tolist()[0]].mean().fillna(0)
-
-    def _sample_benchmark(
-        self,
-        bench: pd.Series,
-        trade_start_time: Union[str, pd.Timestamp],
-        trade_end_time: Union[str, pd.Timestamp],
-    ) -> Optional[float]:
-        if self.bench is None:
-            return None
-
-        def cal_change(x):
-            return (x + 1).prod()
-
-        _ret = resam_ts_data(bench, trade_start_time, trade_end_time, method=cal_change)
-        return 0.0 if _ret is None else _ret - 1
-
-    def is_empty(self) -> bool:
-        return len(self.accounts) == 0
-
-    def get_latest_date(self) -> pd.Timestamp:
-        return self.latest_pm_time
-
-    def get_latest_account_value(self) -> float:
-        return self.accounts[self.latest_pm_time]
-
-    def get_latest_total_cost(self) -> Any:
-        return self.total_costs[self.latest_pm_time]
-
-    def get_latest_total_turnover(self) -> Any:
-        return self.total_turnovers[self.latest_pm_time]
-
-    def update_portfolio_metrics_record(
-        self,
-        trade_start_time: Union[str, pd.Timestamp] = None,
-        trade_end_time: Union[str, pd.Timestamp] = None,
-        account_value: float | None = None,
-        cash: float | None = None,
-        return_rate: float | None = None,
-        total_turnover: float | None = None,
-        turnover_rate: float | None = None,
-        total_cost: float | None = None,
-        cost_rate: float | None = None,
-        stock_value: float | None = None,
-        bench_value: float | None = None,
-    ) -> None:
-        # check data
-        if None in [
-            trade_start_time,
-            account_value,
-            cash,
-            return_rate,
-            total_turnover,
-            turnover_rate,
-            total_cost,
-            cost_rate,
-            stock_value,
-        ]:
-            raise ValueError(
-                "None in [trade_start_time, account_value, cash, return_rate, total_turnover, turnover_rate, "
-                "total_cost, cost_rate, stock_value]",
-            )
-
-        if trade_end_time is None and bench_value is None:
-            raise ValueError("Both trade_end_time and bench_value is None, benchmark is not usable.")
-        elif bench_value is None:
-            bench_value = self._sample_benchmark(self.bench, trade_start_time, trade_end_time)
-
-        # update pm data
-        self.accounts[trade_start_time] = account_value
-        self.returns[trade_start_time] = return_rate
-        self.total_turnovers[trade_start_time] = total_turnover
-        self.turnovers[trade_start_time] = turnover_rate
-        self.total_costs[trade_start_time] = total_cost
-        self.costs[trade_start_time] = cost_rate
-        self.values[trade_start_time] = stock_value
-        self.cashes[trade_start_time] = cash
-        self.benches[trade_start_time] = bench_value
-        # update pm
-        self.latest_pm_time = trade_start_time
-        # finish pm update in each step
-
-    def generate_portfolio_metrics_dataframe(self) -> pd.DataFrame:
-        pm = pd.DataFrame()
-        pm["account"] = pd.Series(self.accounts)
-        pm["return"] = pd.Series(self.returns)
-        pm["total_turnover"] = pd.Series(self.total_turnovers)
-        pm["turnover"] = pd.Series(self.turnovers)
-        pm["total_cost"] = pd.Series(self.total_costs)
-        pm["cost"] = pd.Series(self.costs)
-        pm["value"] = pd.Series(self.values)
-        pm["cash"] = pd.Series(self.cashes)
-        pm["bench"] = pd.Series(self.benches)
-        pm.index.name = "datetime"
-        return pm
-
-    def save_portfolio_metrics(self, path: str) -> None:
-        r = self.generate_portfolio_metrics_dataframe()
-        r.to_csv(path)
-
-    def load_portfolio_metrics(self, path: str) -> None:
-        """load pm from a file
-        should have format like
-        columns = ['account', 'return', 'total_turnover', 'turnover', 'cost', 'total_cost', 'value', 'cash', 'bench']
-            :param
-                path: str/ pathlib.Path()
-        """
-        with pathlib.Path(path).open("rb") as f:
-            r = pd.read_csv(f, index_col=0)
-        r.index = pd.DatetimeIndex(r.index)
-
-        index = r.index
-        self.init_vars()
-        for trade_start_time in index:
-            self.update_portfolio_metrics_record(
-                trade_start_time=trade_start_time,
-                account_value=r.loc[trade_start_time]["account"],
-                cash=r.loc[trade_start_time]["cash"],
-                return_rate=r.loc[trade_start_time]["return"],
-                total_turnover=r.loc[trade_start_time]["total_turnover"],
-                turnover_rate=r.loc[trade_start_time]["turnover"],
-                total_cost=r.loc[trade_start_time]["total_cost"],
-                cost_rate=r.loc[trade_start_time]["cost"],
-                stock_value=r.loc[trade_start_time]["value"],
-                bench_value=r.loc[trade_start_time]["bench"],
-            )
-
-
-class Indicator:
-    """
-    `Indicator` is implemented in a aggregate way.
-    All the metrics are calculated aggregately.
-    All the metrics are calculated for a separated stock and in a specific step on a specific level.
-
-    | indicator    | desc.                                                        |
-    |--------------+--------------------------------------------------------------|
-    | amount       | the *target* amount given by the outer strategy              |
-    | deal_amount  | the real deal amount                                         |
-    | inner_amount | the total *target* amount of inner strategy                  |
-    | trade_price  | the average deal price                                       |
-    | trade_value  | the total trade value                                        |
-    | trade_cost   | the total trade cost  (base price need drection)             |
-    | trade_dir    | the trading direction                                        |
-    | ffr          | full fill rate                                               |
-    | pa           | price advantage                                              |
-    | pos          | win rate                                                     |
-    | base_price   | the price of baseline                                        |
-    | base_volume  | the volume of baseline (for weighted aggregating base_price) |
-
-    **NOTE**:
-    The `base_price` and `base_volume` can't be NaN when there are not trading on that step. Otherwise
-    aggregating get wrong results.
-
-    So `base_price` will not be calculated in a aggregate way!!
-
-    """
-
-    def __init__(self, order_indicator_cls: Type[BaseOrderIndicator] = NumpyOrderIndicator) -> None:
-        self.order_indicator_cls = order_indicator_cls
-
-        # order indicator is metrics for a single order for a specific step
-        self.order_indicator_his: dict = OrderedDict()
-        self.order_indicator: BaseOrderIndicator = self.order_indicator_cls()
-
-        # trade indicator is metrics for all orders for a specific step
-        self.trade_indicator_his: dict = OrderedDict()
-        self.trade_indicator: Dict[str, Optional[BaseSingleMetric]] = OrderedDict()
-
-        self._trade_calendar = None
-
-    # def reset(self, trade_calendar: TradeCalendarManager):
-    def reset(self) -> None:
-        self.order_indicator = self.order_indicator_cls()
-        self.trade_indicator = OrderedDict()
-        # self._trade_calendar = trade_calendar
-
-    def record(self, trade_start_time: Union[str, pd.Timestamp]) -> None:
-        self.order_indicator_his[trade_start_time] = self.get_order_indicator()
-        self.trade_indicator_his[trade_start_time] = self.get_trade_indicator()
-
-    def _update_order_trade_info(self, trade_info: List[Tuple[Order, float, float, float]]) -> None:
-        amount = dict()
-        deal_amount = dict()
-        trade_price = dict()
-        trade_value = dict()
-        trade_cost = dict()
-        trade_dir = dict()
-        pa = dict()
-
-        for order, _trade_val, _trade_cost, _trade_price in trade_info:
-            amount[order.stock_id] = order.amount_delta
-            deal_amount[order.stock_id] = order.deal_amount_delta
-            trade_price[order.stock_id] = _trade_price
-            trade_value[order.stock_id] = _trade_val * order.sign
-            trade_cost[order.stock_id] = _trade_cost
-            trade_dir[order.stock_id] = order.direction
-            # The PA in the innermost layer is meanless
-            pa[order.stock_id] = 0
-
-        self.order_indicator.assign("amount", amount)
-        self.order_indicator.assign("inner_amount", amount)
-        self.order_indicator.assign("deal_amount", deal_amount)
-        # NOTE: trade_price and baseline price will be same on the lowest-level
-        self.order_indicator.assign("trade_price", trade_price)
-        self.order_indicator.assign("trade_value", trade_value)
-        self.order_indicator.assign("trade_cost", trade_cost)
-        self.order_indicator.assign("trade_dir", trade_dir)
-        self.order_indicator.assign("pa", pa)
-
-    def _update_order_fulfill_rate(self) -> None:
-        def func(deal_amount, amount):
-            # deal_amount is np.NaN or None when there is no inner decision. So full fill rate is 0.
-            tmp_deal_amount = deal_amount.reindex(amount.index, 0)
-            tmp_deal_amount = tmp_deal_amount.replace({np.NaN: 0})
-            return tmp_deal_amount / amount
-
-        self.order_indicator.transfer(func, "ffr")
-
-    def update_order_indicators(self, trade_info: List[Tuple[Order, float, float, float]]) -> None:
-        self._update_order_trade_info(trade_info=trade_info)
-        self._update_order_fulfill_rate()
-
-    def _agg_order_trade_info(self, inner_order_indicators: List[BaseOrderIndicator]) -> None:
-        # calculate total trade amount with each inner order indicator.
-        def trade_amount_func(deal_amount, trade_price):
-            return deal_amount * trade_price
-
-        for indicator in inner_order_indicators:
-            indicator.transfer(trade_amount_func, "trade_price")
-
-        # sum inner order indicators with same metric.
-        all_metric = ["inner_amount", "deal_amount", "trade_price", "trade_value", "trade_cost", "trade_dir"]
-        self.order_indicator_cls.sum_all_indicators(
-            self.order_indicator,
-            inner_order_indicators,
-            all_metric,
-            fill_value=0,
-        )
-
-        def func(trade_price, deal_amount):
-            # trade_price is np.NaN instead of inf when deal_amount is zero.
-            tmp_deal_amount = deal_amount.replace({0: np.NaN})
-            return trade_price / tmp_deal_amount
-
-        self.order_indicator.transfer(func, "trade_price")
-
-        def func_apply(trade_dir):
-            return trade_dir.apply(Order.parse_dir)
-
-        self.order_indicator.transfer(func_apply, "trade_dir")
-
-    def _update_trade_amount(self, outer_trade_decision: BaseTradeDecision) -> None:
-        # NOTE: these indicator is designed for order execution, so the
-        decision: List[Order] = cast(List[Order], outer_trade_decision.get_decision())
-        if len(decision) == 0:
-            self.order_indicator.assign("amount", {})
-        else:
-            self.order_indicator.assign("amount", {order.stock_id: order.amount_delta for order in decision})
-
-    def _get_base_vol_pri(
-        self,
-        inst: str,
-        trade_start_time: pd.Timestamp,
-        trade_end_time: pd.Timestamp,
-        direction: OrderDir,
-        decision: BaseTradeDecision,
-        trade_exchange: Exchange,
-        pa_config: dict = {},
-    ) -> Tuple[Optional[float], Optional[float]]:
-        """
-        Get the base volume and price information
-        All the base price values are rooted from this function
-        """
-
-        agg = pa_config.get("agg", "twap").lower()
-        price = pa_config.get("price", "deal_price").lower()
-
-        if decision.trade_range is not None:
-            trade_start_time, trade_end_time = decision.trade_range.clip_time_range(
-                start_time=trade_start_time,
-                end_time=trade_end_time,
-            )
-
-        if price == "deal_price":
-            price_s = trade_exchange.get_deal_price(
-                inst,
-                trade_start_time,
-                trade_end_time,
-                direction=direction,
-                method=None,
-            )
-        else:
-            raise NotImplementedError(f"This type of input is not supported")
-
-        # if there is no stock data during the time period
-        if price_s is None:
-            return None, None
-
-        if isinstance(price_s, (int, float, np.number)):
-            price_s = idd.SingleData(price_s, [trade_start_time])
-        elif isinstance(price_s, idd.SingleData):
-            pass
-        else:
-            raise NotImplementedError(f"This type of input is not supported")
-
-        # NOTE: there are some zeros in the trading price. These cases are known meaningless
-        # for aligning the previous logic, remove it.
-        # remove zero and negative values.
-        assert isinstance(price_s, idd.SingleData)
-        price_s = price_s.loc[(price_s > 1e-08).data.astype(bool)]
-        # NOTE ~(price_s < 1e-08) is different from price_s >= 1e-8
-        #   ~(np.NaN < 1e-8) -> ~(False)  -> True
-
-        assert isinstance(price_s, idd.SingleData)
-        if agg == "vwap":
-            volume_s = trade_exchange.get_volume(inst, trade_start_time, trade_end_time, method=None)
-            if isinstance(volume_s, (int, float, np.number)):
-                volume_s = idd.SingleData(volume_s, [trade_start_time])
-            assert isinstance(volume_s, idd.SingleData)
-            volume_s = volume_s.reindex(price_s.index)
-        elif agg == "twap":
-            volume_s = idd.SingleData(1, price_s.index)
-        else:
-            raise NotImplementedError(f"This type of input is not supported")
-
-        assert isinstance(volume_s, idd.SingleData)
-        base_volume = volume_s.sum()
-        base_price = (price_s * volume_s).sum() / base_volume
-        return base_price, base_volume
-
-    def _agg_base_price(
-        self,
-        inner_order_indicators: List[BaseOrderIndicator],
-        decision_list: List[Tuple[BaseTradeDecision, pd.Timestamp, pd.Timestamp]],
-        trade_exchange: Exchange,
-        pa_config: dict = {},
-    ) -> None:
-        """
-        # NOTE:!!!!
-        # Strong assumption!!!!!!
-        # the correctness of the base_price relies on that the **same** exchange is used
-
-        Parameters
-        ----------
-        inner_order_indicators : List[BaseOrderIndicator]
-            the indicators of account of inner executor
-        decision_list: List[Tuple[BaseTradeDecision, pd.Timestamp, pd.Timestamp]],
-            a list of decisions according to inner_order_indicators
-        trade_exchange : Exchange
-            for retrieving trading price
-        pa_config : dict
-            For example
-            {
-                "agg": "twap",  # "vwap"
-                "price": "$close",  # TODO: this is not supported now!!!!!
-                                    # default to use deal price of the exchange
-            }
-        """
-
-        # TODO: I think there are potentials to be optimized
-        trade_dir = self.order_indicator.get_index_data("trade_dir")
-        if len(trade_dir) > 0:
-            bp_all, bv_all = [], []
-            # <step, inst, (base_volume | base_price)>
-            for oi, (dec, start, end) in zip(inner_order_indicators, decision_list):
-                bp_s = oi.get_index_data("base_price").reindex(trade_dir.index)
-                bv_s = oi.get_index_data("base_volume").reindex(trade_dir.index)
-
-                bp_new, bv_new = {}, {}
-                for pr, v, (inst, direction) in zip(bp_s.data, bv_s.data, zip(trade_dir.index, trade_dir.data)):
-                    if np.isnan(pr):
-                        bp_tmp, bv_tmp = self._get_base_vol_pri(
-                            inst,
-                            start,
-                            end,
-                            decision=dec,
-                            direction=direction,
-                            trade_exchange=trade_exchange,
-                            pa_config=pa_config,
-                        )
-                        if (bp_tmp is not None) and (bv_tmp is not None):
-                            bp_new[inst], bv_new[inst] = bp_tmp, bv_tmp
-                    else:
-                        bp_new[inst], bv_new[inst] = pr, v
-
-                bp_new = idd.SingleData(bp_new)
-                bv_new = idd.SingleData(bv_new)
-                bp_all.append(bp_new)
-                bv_all.append(bv_new)
-            bp_all_multi_data = idd.concat(bp_all, axis=1)
-            bv_all_multi_data = idd.concat(bv_all, axis=1)
-
-            base_volume = bv_all_multi_data.sum(axis=1)
-            self.order_indicator.assign("base_volume", base_volume.to_dict())
-            self.order_indicator.assign(
-                "base_price",
-                ((bp_all_multi_data * bv_all_multi_data).sum(axis=1) / base_volume).to_dict(),
-            )
-
-    def _agg_order_price_advantage(self) -> None:
-        def if_empty_func(trade_price):
-            return trade_price.empty
-
-        if_empty = self.order_indicator.transfer(if_empty_func)
-        if not if_empty:
-
-            def func(trade_dir, trade_price, base_price):
-                sign = 1 - trade_dir * 2
-                return sign * (trade_price / base_price - 1)
-
-            self.order_indicator.transfer(func, "pa")
-        else:
-            self.order_indicator.assign("pa", {})
-
-    def agg_order_indicators(
-        self,
-        inner_order_indicators: List[BaseOrderIndicator],
-        decision_list: List[Tuple[BaseTradeDecision, pd.Timestamp, pd.Timestamp]],
-        outer_trade_decision: BaseTradeDecision,
-        trade_exchange: Exchange,
-        indicator_config: dict = {},
-    ) -> None:
-        self._agg_order_trade_info(inner_order_indicators)
-        self._update_trade_amount(outer_trade_decision)
-        self._update_order_fulfill_rate()
-        pa_config = indicator_config.get("pa_config", {})
-        self._agg_base_price(inner_order_indicators, decision_list, trade_exchange, pa_config=pa_config)  # TODO
-        self._agg_order_price_advantage()
-
-    def _cal_trade_fulfill_rate(self, method: str = "mean") -> Optional[BaseSingleMetric]:
-        if method == "mean":
-            return self.order_indicator.transfer(
-                lambda ffr: ffr.mean(),
-            )
-        elif method == "amount_weighted":
-            return self.order_indicator.transfer(
-                lambda ffr, deal_amount: (ffr * deal_amount.abs()).sum() / (deal_amount.abs().sum()),
-            )
-        elif method == "value_weighted":
-            return self.order_indicator.transfer(
-                lambda ffr, trade_value: (ffr * trade_value.abs()).sum() / (trade_value.abs().sum()),
-            )
-        else:
-            raise ValueError(f"method {method} is not supported!")
-
-    def _cal_trade_price_advantage(self, method: str = "mean") -> Optional[BaseSingleMetric]:
-        if method == "mean":
-            return self.order_indicator.transfer(lambda pa: pa.mean())
-        elif method == "amount_weighted":
-            return self.order_indicator.transfer(
-                lambda pa, deal_amount: (pa * deal_amount.abs()).sum() / (deal_amount.abs().sum()),
-            )
-        elif method == "value_weighted":
-            return self.order_indicator.transfer(
-                lambda pa, trade_value: (pa * trade_value.abs()).sum() / (trade_value.abs().sum()),
-            )
-        else:
-            raise ValueError(f"method {method} is not supported!")
-
-    def _cal_trade_positive_rate(self) -> Optional[BaseSingleMetric]:
-        def func(pa):
-            return (pa > 0).sum() / pa.count()
-
-        return self.order_indicator.transfer(func)
-
-    def _cal_deal_amount(self) -> Optional[BaseSingleMetric]:
-        def func(deal_amount):
-            return deal_amount.abs().sum()
-
-        return self.order_indicator.transfer(func)
-
-    def _cal_trade_value(self) -> Optional[BaseSingleMetric]:
-        def func(trade_value):
-            return trade_value.abs().sum()
-
-        return self.order_indicator.transfer(func)
-
-    def _cal_trade_order_count(self) -> Optional[BaseSingleMetric]:
-        def func(amount):
-            return amount.count()
-
-        return self.order_indicator.transfer(func)
-
-    def cal_trade_indicators(
-        self,
-        trade_start_time: Union[str, pd.Timestamp],
-        freq: str,
-        indicator_config: dict = {},
-    ) -> None:
-        show_indicator = indicator_config.get("show_indicator", False)
-        ffr_config = indicator_config.get("ffr_config", {})
-        pa_config = indicator_config.get("pa_config", {})
-        fulfill_rate = self._cal_trade_fulfill_rate(method=ffr_config.get("weight_method", "mean"))
-        price_advantage = self._cal_trade_price_advantage(method=pa_config.get("weight_method", "mean"))
-        positive_rate = self._cal_trade_positive_rate()
-        deal_amount = self._cal_deal_amount()
-        trade_value = self._cal_trade_value()
-        order_count = self._cal_trade_order_count()
-        self.trade_indicator["ffr"] = fulfill_rate
-        self.trade_indicator["pa"] = price_advantage
-        self.trade_indicator["pos"] = positive_rate
-        self.trade_indicator["deal_amount"] = deal_amount
-        self.trade_indicator["value"] = trade_value
-        self.trade_indicator["count"] = order_count
-        if show_indicator:
-            print(
-                "[Indicator({}) {}]: FFR: {}, PA: {}, POS: {}".format(
-                    freq,
-                    trade_start_time
-                    if isinstance(trade_start_time, str)
-                    else trade_start_time.strftime("%Y-%m-%d %H:%M:%S"),
-                    fulfill_rate,
-                    price_advantage,
-                    positive_rate,
-                ),
-            )
-
-    def get_order_indicator(self, raw: bool = True) -> Union[BaseOrderIndicator, Dict[Text, pd.Series]]:
-        return self.order_indicator if raw else self.order_indicator.to_series()
-
-    def get_trade_indicator(self) -> Dict[str, Optional[BaseSingleMetric]]:
-        return self.trade_indicator
-
-    def generate_trade_indicators_dataframe(self) -> pd.DataFrame:
-        return pd.DataFrame.from_dict(self.trade_indicator_his, orient="index")
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+from __future__ import annotations
+
+import pathlib
+from collections import OrderedDict
+from typing import Any, Dict, List, Optional, Text, Tuple, Type, Union, cast
+
+import numpy as np
+import pandas as pd
+
+import qlib.utils.index_data as idd
+from qlib.backtest.decision import BaseTradeDecision, Order, OrderDir
+from qlib.backtest.exchange import Exchange
+
+from ..tests.config import CSI300_BENCH
+from ..utils.resam import get_higher_eq_freq_feature, resam_ts_data
+from .high_performance_ds import BaseOrderIndicator, BaseSingleMetric, NumpyOrderIndicator
+
+
+class PortfolioMetrics:
+    """
+    Motivation:
+        PortfolioMetrics is for supporting portfolio related metrics.
+
+    Implementation:
+
+        daily portfolio metrics of the account
+        contain those followings: return, cost, turnover, account, cash, bench, value
+        For each step(bar/day/minute), each column represents
+        - return: the return of the portfolio generated by strategy **without transaction fee**.
+        - cost: the transaction fee and slippage.
+        - account: the total value of assets(cash and securities are both included) in user account based on the close price of each step.
+        - cash: the amount of cash in user's account.
+        - bench: the return of the benchmark
+        - value: the total value of securities/stocks/instruments (cash is excluded).
+
+        update report
+    """
+
+    def __init__(self, freq: str = "day", benchmark_config: dict = {}) -> None:
+        """
+        Parameters
+        ----------
+        freq : str
+            frequency of trading bar, used for updating hold count of trading bar
+        benchmark_config : dict
+            config of benchmark, may including the following arguments:
+            - benchmark : Union[str, list, pd.Series]
+                - If `benchmark` is pd.Series, `index` is trading date; the value T is the change from T-1 to T.
+                    example:
+                        print(
+                            D.features(D.instruments('csi500'),
+                            ['$close/Ref($close, 1)-1'])['$close/Ref($close, 1)-1'].head()
+                        )
+                            2017-01-04    0.011693
+                            2017-01-05    0.000721
+                            2017-01-06   -0.004322
+                            2017-01-09    0.006874
+                            2017-01-10   -0.003350
+                - If `benchmark` is list, will use the daily average change of the stock pool in the list as the
+                    'bench'.
+                - If `benchmark` is str, will use the daily change as the 'bench'.
+                benchmark code, default is SH000300 CSI300
+            - start_time : Union[str, pd.Timestamp], optional
+                - If `benchmark` is pd.Series, it will be ignored
+                - Else, it represent start time of benchmark, by default None
+            - end_time : Union[str, pd.Timestamp], optional
+                - If `benchmark` is pd.Series, it will be ignored
+                - Else, it represent end time of benchmark, by default None
+
+        """
+
+        self.init_vars()
+        self.init_bench(freq=freq, benchmark_config=benchmark_config)
+
+    def init_vars(self) -> None:
+        self.accounts: dict = OrderedDict()  # account position value for each trade time
+        self.returns: dict = OrderedDict()  # daily return rate for each trade time
+        self.total_turnovers: dict = OrderedDict()  # total turnover for each trade time
+        self.turnovers: dict = OrderedDict()  # turnover for each trade time
+        self.total_costs: dict = OrderedDict()  # total trade cost for each trade time
+        self.costs: dict = OrderedDict()  # trade cost rate for each trade time
+        self.values: dict = OrderedDict()  # value for each trade time
+        self.cashes: dict = OrderedDict()
+        self.benches: dict = OrderedDict()
+        self.latest_pm_time: Optional[pd.TimeStamp] = None
+
+    def init_bench(self, freq: str | None = None, benchmark_config: dict | None = None) -> None:
+        if freq is not None:
+            self.freq = freq
+        self.benchmark_config = benchmark_config
+        self.bench = self._cal_benchmark(self.benchmark_config, self.freq)
+
+    @staticmethod
+    def _cal_benchmark(benchmark_config: Optional[dict], freq: str) -> Optional[pd.Series]:
+        if benchmark_config is None:
+            return None
+        benchmark = benchmark_config.get("benchmark", CSI300_BENCH)
+        if benchmark is None:
+            return None
+
+        if isinstance(benchmark, pd.Series):
+            return benchmark
+        else:
+            start_time = benchmark_config.get("start_time", None)
+            end_time = benchmark_config.get("end_time", None)
+
+            if freq is None:
+                raise ValueError("benchmark freq can't be None!")
+            _codes = benchmark if isinstance(benchmark, (list, dict)) else [benchmark]
+            fields = ["$close/Ref($close,1)-1"]
+            _temp_result, _ = get_higher_eq_freq_feature(_codes, fields, start_time, end_time, freq=freq)
+            if len(_temp_result) == 0:
+                raise ValueError(f"The benchmark {_codes} does not exist. Please provide the right benchmark")
+            return _temp_result.groupby(level="datetime")[_temp_result.columns.tolist()[0]].mean().fillna(0)
+
+    def _sample_benchmark(
+        self,
+        bench: pd.Series,
+        trade_start_time: Union[str, pd.Timestamp],
+        trade_end_time: Union[str, pd.Timestamp],
+    ) -> Optional[float]:
+        if self.bench is None:
+            return None
+
+        def cal_change(x):
+            return (x + 1).prod()
+
+        _ret = resam_ts_data(bench, trade_start_time, trade_end_time, method=cal_change)
+        return 0.0 if _ret is None else _ret - 1
+
+    def is_empty(self) -> bool:
+        return len(self.accounts) == 0
+
+    def get_latest_date(self) -> pd.Timestamp:
+        return self.latest_pm_time
+
+    def get_latest_account_value(self) -> float:
+        return self.accounts[self.latest_pm_time]
+
+    def get_latest_total_cost(self) -> Any:
+        return self.total_costs[self.latest_pm_time]
+
+    def get_latest_total_turnover(self) -> Any:
+        return self.total_turnovers[self.latest_pm_time]
+
+    def update_portfolio_metrics_record(
+        self,
+        trade_start_time: Union[str, pd.Timestamp] = None,
+        trade_end_time: Union[str, pd.Timestamp] = None,
+        account_value: float | None = None,
+        cash: float | None = None,
+        return_rate: float | None = None,
+        total_turnover: float | None = None,
+        turnover_rate: float | None = None,
+        total_cost: float | None = None,
+        cost_rate: float | None = None,
+        stock_value: float | None = None,
+        bench_value: float | None = None,
+    ) -> None:
+        # check data
+        if None in [
+            trade_start_time,
+            account_value,
+            cash,
+            return_rate,
+            total_turnover,
+            turnover_rate,
+            total_cost,
+            cost_rate,
+            stock_value,
+        ]:
+            raise ValueError(
+                "None in [trade_start_time, account_value, cash, return_rate, total_turnover, turnover_rate, "
+                "total_cost, cost_rate, stock_value]",
+            )
+
+        if trade_end_time is None and bench_value is None:
+            raise ValueError("Both trade_end_time and bench_value is None, benchmark is not usable.")
+        elif bench_value is None:
+            bench_value = self._sample_benchmark(self.bench, trade_start_time, trade_end_time)
+
+        # update pm data
+        self.accounts[trade_start_time] = account_value
+        self.returns[trade_start_time] = return_rate
+        self.total_turnovers[trade_start_time] = total_turnover
+        self.turnovers[trade_start_time] = turnover_rate
+        self.total_costs[trade_start_time] = total_cost
+        self.costs[trade_start_time] = cost_rate
+        self.values[trade_start_time] = stock_value
+        self.cashes[trade_start_time] = cash
+        self.benches[trade_start_time] = bench_value
+        # update pm
+        self.latest_pm_time = trade_start_time
+        # finish pm update in each step
+
+    def generate_portfolio_metrics_dataframe(self) -> pd.DataFrame:
+        pm = pd.DataFrame()
+        pm["account"] = pd.Series(self.accounts)
+        pm["return"] = pd.Series(self.returns)
+        pm["total_turnover"] = pd.Series(self.total_turnovers)
+        pm["turnover"] = pd.Series(self.turnovers)
+        pm["total_cost"] = pd.Series(self.total_costs)
+        pm["cost"] = pd.Series(self.costs)
+        pm["value"] = pd.Series(self.values)
+        pm["cash"] = pd.Series(self.cashes)
+        pm["bench"] = pd.Series(self.benches)
+        pm.index.name = "datetime"
+        return pm
+
+    def save_portfolio_metrics(self, path: str) -> None:
+        r = self.generate_portfolio_metrics_dataframe()
+        r.to_csv(path)
+
+    def load_portfolio_metrics(self, path: str) -> None:
+        """load pm from a file
+        should have format like
+        columns = ['account', 'return', 'total_turnover', 'turnover', 'cost', 'total_cost', 'value', 'cash', 'bench']
+            :param
+                path: str/ pathlib.Path()
+        """
+        with pathlib.Path(path).open("rb") as f:
+            r = pd.read_csv(f, index_col=0)
+        r.index = pd.DatetimeIndex(r.index)
+
+        index = r.index
+        self.init_vars()
+        for trade_start_time in index:
+            self.update_portfolio_metrics_record(
+                trade_start_time=trade_start_time,
+                account_value=r.loc[trade_start_time]["account"],
+                cash=r.loc[trade_start_time]["cash"],
+                return_rate=r.loc[trade_start_time]["return"],
+                total_turnover=r.loc[trade_start_time]["total_turnover"],
+                turnover_rate=r.loc[trade_start_time]["turnover"],
+                total_cost=r.loc[trade_start_time]["total_cost"],
+                cost_rate=r.loc[trade_start_time]["cost"],
+                stock_value=r.loc[trade_start_time]["value"],
+                bench_value=r.loc[trade_start_time]["bench"],
+            )
+
+
+class Indicator:
+    """
+    `Indicator` is implemented in a aggregate way.
+    All the metrics are calculated aggregately.
+    All the metrics are calculated for a separated stock and in a specific step on a specific level.
+
+    | indicator    | desc.                                                        |
+    |--------------+--------------------------------------------------------------|
+    | amount       | the *target* amount given by the outer strategy              |
+    | deal_amount  | the real deal amount                                         |
+    | inner_amount | the total *target* amount of inner strategy                  |
+    | trade_price  | the average deal price                                       |
+    | trade_value  | the total trade value                                        |
+    | trade_cost   | the total trade cost  (base price need drection)             |
+    | trade_dir    | the trading direction                                        |
+    | ffr          | full fill rate                                               |
+    | pa           | price advantage                                              |
+    | pos          | win rate                                                     |
+    | base_price   | the price of baseline                                        |
+    | base_volume  | the volume of baseline (for weighted aggregating base_price) |
+
+    **NOTE**:
+    The `base_price` and `base_volume` can't be NaN when there are not trading on that step. Otherwise
+    aggregating get wrong results.
+
+    So `base_price` will not be calculated in a aggregate way!!
+
+    """
+
+    def __init__(self, order_indicator_cls: Type[BaseOrderIndicator] = NumpyOrderIndicator) -> None:
+        self.order_indicator_cls = order_indicator_cls
+
+        # order indicator is metrics for a single order for a specific step
+        self.order_indicator_his: dict = OrderedDict()
+        self.order_indicator: BaseOrderIndicator = self.order_indicator_cls()
+
+        # trade indicator is metrics for all orders for a specific step
+        self.trade_indicator_his: dict = OrderedDict()
+        self.trade_indicator: Dict[str, Optional[BaseSingleMetric]] = OrderedDict()
+
+        self._trade_calendar = None
+
+    # def reset(self, trade_calendar: TradeCalendarManager):
+    def reset(self) -> None:
+        self.order_indicator = self.order_indicator_cls()
+        self.trade_indicator = OrderedDict()
+        # self._trade_calendar = trade_calendar
+
+    def record(self, trade_start_time: Union[str, pd.Timestamp]) -> None:
+        self.order_indicator_his[trade_start_time] = self.get_order_indicator()
+        self.trade_indicator_his[trade_start_time] = self.get_trade_indicator()
+
+    def _update_order_trade_info(self, trade_info: List[Tuple[Order, float, float, float]]) -> None:
+        amount = dict()
+        deal_amount = dict()
+        trade_price = dict()
+        trade_value = dict()
+        trade_cost = dict()
+        trade_dir = dict()
+        pa = dict()
+
+        for order, _trade_val, _trade_cost, _trade_price in trade_info:
+            amount[order.stock_id] = order.amount_delta
+            deal_amount[order.stock_id] = order.deal_amount_delta
+            trade_price[order.stock_id] = _trade_price
+            trade_value[order.stock_id] = _trade_val * order.sign
+            trade_cost[order.stock_id] = _trade_cost
+            trade_dir[order.stock_id] = order.direction
+            # The PA in the innermost layer is meanless
+            pa[order.stock_id] = 0
+
+        self.order_indicator.assign("amount", amount)
+        self.order_indicator.assign("inner_amount", amount)
+        self.order_indicator.assign("deal_amount", deal_amount)
+        # NOTE: trade_price and baseline price will be same on the lowest-level
+        self.order_indicator.assign("trade_price", trade_price)
+        self.order_indicator.assign("trade_value", trade_value)
+        self.order_indicator.assign("trade_cost", trade_cost)
+        self.order_indicator.assign("trade_dir", trade_dir)
+        self.order_indicator.assign("pa", pa)
+
+    def _update_order_fulfill_rate(self) -> None:
+        def func(deal_amount, amount):
+            # deal_amount is np.NaN or None when there is no inner decision. So full fill rate is 0.
+            tmp_deal_amount = deal_amount.reindex(amount.index, 0)
+            tmp_deal_amount = tmp_deal_amount.replace({np.NaN: 0})
+            return tmp_deal_amount / amount
+
+        self.order_indicator.transfer(func, "ffr")
+
+    def update_order_indicators(self, trade_info: List[Tuple[Order, float, float, float]]) -> None:
+        self._update_order_trade_info(trade_info=trade_info)
+        self._update_order_fulfill_rate()
+
+    def _agg_order_trade_info(self, inner_order_indicators: List[BaseOrderIndicator]) -> None:
+        # calculate total trade amount with each inner order indicator.
+        def trade_amount_func(deal_amount, trade_price):
+            return deal_amount * trade_price
+
+        for indicator in inner_order_indicators:
+            indicator.transfer(trade_amount_func, "trade_price")
+
+        # sum inner order indicators with same metric.
+        all_metric = ["inner_amount", "deal_amount", "trade_price", "trade_value", "trade_cost", "trade_dir"]
+        self.order_indicator_cls.sum_all_indicators(
+            self.order_indicator,
+            inner_order_indicators,
+            all_metric,
+            fill_value=0,
+        )
+
+        def func(trade_price, deal_amount):
+            # trade_price is np.NaN instead of inf when deal_amount is zero.
+            tmp_deal_amount = deal_amount.replace({0: np.NaN})
+            return trade_price / tmp_deal_amount
+
+        self.order_indicator.transfer(func, "trade_price")
+
+        def func_apply(trade_dir):
+            return trade_dir.apply(Order.parse_dir)
+
+        self.order_indicator.transfer(func_apply, "trade_dir")
+
+    def _update_trade_amount(self, outer_trade_decision: BaseTradeDecision) -> None:
+        # NOTE: these indicator is designed for order execution, so the
+        decision: List[Order] = cast(List[Order], outer_trade_decision.get_decision())
+        if len(decision) == 0:
+            self.order_indicator.assign("amount", {})
+        else:
+            self.order_indicator.assign("amount", {order.stock_id: order.amount_delta for order in decision})
+
+    def _get_base_vol_pri(
+        self,
+        inst: str,
+        trade_start_time: pd.Timestamp,
+        trade_end_time: pd.Timestamp,
+        direction: OrderDir,
+        decision: BaseTradeDecision,
+        trade_exchange: Exchange,
+        pa_config: dict = {},
+    ) -> Tuple[Optional[float], Optional[float]]:
+        """
+        Get the base volume and price information
+        All the base price values are rooted from this function
+        """
+
+        agg = pa_config.get("agg", "twap").lower()
+        price = pa_config.get("price", "deal_price").lower()
+
+        if decision.trade_range is not None:
+            trade_start_time, trade_end_time = decision.trade_range.clip_time_range(
+                start_time=trade_start_time,
+                end_time=trade_end_time,
+            )
+
+        if price == "deal_price":
+            price_s = trade_exchange.get_deal_price(
+                inst,
+                trade_start_time,
+                trade_end_time,
+                direction=direction,
+                method=None,
+            )
+        else:
+            raise NotImplementedError(f"This type of input is not supported")
+
+        # if there is no stock data during the time period
+        if price_s is None:
+            return None, None
+
+        if isinstance(price_s, (int, float, np.number)):
+            price_s = idd.SingleData(price_s, [trade_start_time])
+        elif isinstance(price_s, idd.SingleData):
+            pass
+        else:
+            raise NotImplementedError(f"This type of input is not supported")
+
+        # NOTE: there are some zeros in the trading price. These cases are known meaningless
+        # for aligning the previous logic, remove it.
+        # remove zero and negative values.
+        assert isinstance(price_s, idd.SingleData)
+        price_s = price_s.loc[(price_s > 1e-08).data.astype(bool)]
+        # NOTE ~(price_s < 1e-08) is different from price_s >= 1e-8
+        #   ~(np.NaN < 1e-8) -> ~(False)  -> True
+
+        assert isinstance(price_s, idd.SingleData)
+        if agg == "vwap":
+            volume_s = trade_exchange.get_volume(inst, trade_start_time, trade_end_time, method=None)
+            if isinstance(volume_s, (int, float, np.number)):
+                volume_s = idd.SingleData(volume_s, [trade_start_time])
+            assert isinstance(volume_s, idd.SingleData)
+            volume_s = volume_s.reindex(price_s.index)
+        elif agg == "twap":
+            volume_s = idd.SingleData(1, price_s.index)
+        else:
+            raise NotImplementedError(f"This type of input is not supported")
+
+        assert isinstance(volume_s, idd.SingleData)
+        base_volume = volume_s.sum()
+        base_price = (price_s * volume_s).sum() / base_volume
+        return base_price, base_volume
+
+    def _agg_base_price(
+        self,
+        inner_order_indicators: List[BaseOrderIndicator],
+        decision_list: List[Tuple[BaseTradeDecision, pd.Timestamp, pd.Timestamp]],
+        trade_exchange: Exchange,
+        pa_config: dict = {},
+    ) -> None:
+        """
+        # NOTE:!!!!
+        # Strong assumption!!!!!!
+        # the correctness of the base_price relies on that the **same** exchange is used
+
+        Parameters
+        ----------
+        inner_order_indicators : List[BaseOrderIndicator]
+            the indicators of account of inner executor
+        decision_list: List[Tuple[BaseTradeDecision, pd.Timestamp, pd.Timestamp]],
+            a list of decisions according to inner_order_indicators
+        trade_exchange : Exchange
+            for retrieving trading price
+        pa_config : dict
+            For example
+            {
+                "agg": "twap",  # "vwap"
+                "price": "$close",  # TODO: this is not supported now!!!!!
+                                    # default to use deal price of the exchange
+            }
+        """
+
+        # TODO: I think there are potentials to be optimized
+        trade_dir = self.order_indicator.get_index_data("trade_dir")
+        if len(trade_dir) > 0:
+            bp_all, bv_all = [], []
+            # <step, inst, (base_volume | base_price)>
+            for oi, (dec, start, end) in zip(inner_order_indicators, decision_list):
+                bp_s = oi.get_index_data("base_price").reindex(trade_dir.index)
+                bv_s = oi.get_index_data("base_volume").reindex(trade_dir.index)
+
+                bp_new, bv_new = {}, {}
+                for pr, v, (inst, direction) in zip(bp_s.data, bv_s.data, zip(trade_dir.index, trade_dir.data)):
+                    if np.isnan(pr):
+                        bp_tmp, bv_tmp = self._get_base_vol_pri(
+                            inst,
+                            start,
+                            end,
+                            decision=dec,
+                            direction=direction,
+                            trade_exchange=trade_exchange,
+                            pa_config=pa_config,
+                        )
+                        if (bp_tmp is not None) and (bv_tmp is not None):
+                            bp_new[inst], bv_new[inst] = bp_tmp, bv_tmp
+                    else:
+                        bp_new[inst], bv_new[inst] = pr, v
+
+                bp_new = idd.SingleData(bp_new)
+                bv_new = idd.SingleData(bv_new)
+                bp_all.append(bp_new)
+                bv_all.append(bv_new)
+            bp_all_multi_data = idd.concat(bp_all, axis=1)
+            bv_all_multi_data = idd.concat(bv_all, axis=1)
+
+            base_volume = bv_all_multi_data.sum(axis=1)
+            self.order_indicator.assign("base_volume", base_volume.to_dict())
+            self.order_indicator.assign(
+                "base_price",
+                ((bp_all_multi_data * bv_all_multi_data).sum(axis=1) / base_volume).to_dict(),
+            )
+
+    def _agg_order_price_advantage(self) -> None:
+        def if_empty_func(trade_price):
+            return trade_price.empty
+
+        if_empty = self.order_indicator.transfer(if_empty_func)
+        if not if_empty:
+
+            def func(trade_dir, trade_price, base_price):
+                sign = 1 - trade_dir * 2
+                return sign * (trade_price / base_price - 1)
+
+            self.order_indicator.transfer(func, "pa")
+        else:
+            self.order_indicator.assign("pa", {})
+
+    def agg_order_indicators(
+        self,
+        inner_order_indicators: List[BaseOrderIndicator],
+        decision_list: List[Tuple[BaseTradeDecision, pd.Timestamp, pd.Timestamp]],
+        outer_trade_decision: BaseTradeDecision,
+        trade_exchange: Exchange,
+        indicator_config: dict = {},
+    ) -> None:
+        self._agg_order_trade_info(inner_order_indicators)
+        self._update_trade_amount(outer_trade_decision)
+        self._update_order_fulfill_rate()
+        pa_config = indicator_config.get("pa_config", {})
+        self._agg_base_price(inner_order_indicators, decision_list, trade_exchange, pa_config=pa_config)  # TODO
+        self._agg_order_price_advantage()
+
+    def _cal_trade_fulfill_rate(self, method: str = "mean") -> Optional[BaseSingleMetric]:
+        if method == "mean":
+            return self.order_indicator.transfer(
+                lambda ffr: ffr.mean(),
+            )
+        elif method == "amount_weighted":
+            return self.order_indicator.transfer(
+                lambda ffr, deal_amount: (ffr * deal_amount.abs()).sum() / (deal_amount.abs().sum()),
+            )
+        elif method == "value_weighted":
+            return self.order_indicator.transfer(
+                lambda ffr, trade_value: (ffr * trade_value.abs()).sum() / (trade_value.abs().sum()),
+            )
+        else:
+            raise ValueError(f"method {method} is not supported!")
+
+    def _cal_trade_price_advantage(self, method: str = "mean") -> Optional[BaseSingleMetric]:
+        if method == "mean":
+            return self.order_indicator.transfer(lambda pa: pa.mean())
+        elif method == "amount_weighted":
+            return self.order_indicator.transfer(
+                lambda pa, deal_amount: (pa * deal_amount.abs()).sum() / (deal_amount.abs().sum()),
+            )
+        elif method == "value_weighted":
+            return self.order_indicator.transfer(
+                lambda pa, trade_value: (pa * trade_value.abs()).sum() / (trade_value.abs().sum()),
+            )
+        else:
+            raise ValueError(f"method {method} is not supported!")
+
+    def _cal_trade_positive_rate(self) -> Optional[BaseSingleMetric]:
+        def func(pa):
+            return (pa > 0).sum() / pa.count()
+
+        return self.order_indicator.transfer(func)
+
+    def _cal_deal_amount(self) -> Optional[BaseSingleMetric]:
+        def func(deal_amount):
+            return deal_amount.abs().sum()
+
+        return self.order_indicator.transfer(func)
+
+    def _cal_trade_value(self) -> Optional[BaseSingleMetric]:
+        def func(trade_value):
+            return trade_value.abs().sum()
+
+        return self.order_indicator.transfer(func)
+
+    def _cal_trade_order_count(self) -> Optional[BaseSingleMetric]:
+        def func(amount):
+            return amount.count()
+
+        return self.order_indicator.transfer(func)
+
+    def cal_trade_indicators(
+        self,
+        trade_start_time: Union[str, pd.Timestamp],
+        freq: str,
+        indicator_config: dict = {},
+    ) -> None:
+        show_indicator = indicator_config.get("show_indicator", False)
+        ffr_config = indicator_config.get("ffr_config", {})
+        pa_config = indicator_config.get("pa_config", {})
+        fulfill_rate = self._cal_trade_fulfill_rate(method=ffr_config.get("weight_method", "mean"))
+        price_advantage = self._cal_trade_price_advantage(method=pa_config.get("weight_method", "mean"))
+        positive_rate = self._cal_trade_positive_rate()
+        deal_amount = self._cal_deal_amount()
+        trade_value = self._cal_trade_value()
+        order_count = self._cal_trade_order_count()
+        self.trade_indicator["ffr"] = fulfill_rate
+        self.trade_indicator["pa"] = price_advantage
+        self.trade_indicator["pos"] = positive_rate
+        self.trade_indicator["deal_amount"] = deal_amount
+        self.trade_indicator["value"] = trade_value
+        self.trade_indicator["count"] = order_count
+        if show_indicator:
+            print(
+                "[Indicator({}) {}]: FFR: {}, PA: {}, POS: {}".format(
+                    freq,
+                    trade_start_time
+                    if isinstance(trade_start_time, str)
+                    else trade_start_time.strftime("%Y-%m-%d %H:%M:%S"),
+                    fulfill_rate,
+                    price_advantage,
+                    positive_rate,
+                ),
+            )
+
+    def get_order_indicator(self, raw: bool = True) -> Union[BaseOrderIndicator, Dict[Text, pd.Series]]:
+        return self.order_indicator if raw else self.order_indicator.to_series()
+
+    def get_trade_indicator(self) -> Dict[str, Optional[BaseSingleMetric]]:
+        return self.trade_indicator
+
+    def generate_trade_indicators_dataframe(self) -> pd.DataFrame:
+        return pd.DataFrame.from_dict(self.trade_indicator_his, orient="index")
```

## qlib/backtest/signal.py

 * *Ordering differences only*

```diff
@@ -1,105 +1,105 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-import abc
-from typing import Dict, List, Text, Tuple, Union
-
-import pandas as pd
-
-from qlib.utils import init_instance_by_config
-
-from ..data.dataset import Dataset
-from ..data.dataset.utils import convert_index_format
-from ..model.base import BaseModel
-from ..utils.resam import resam_ts_data
-
-
-class Signal(metaclass=abc.ABCMeta):
-    """
-    Some trading strategy make decisions based on other prediction signals
-    The signals may comes from different sources(e.g. prepared data, online prediction from model and dataset)
-
-    This interface is tries to provide unified interface for those different sources
-    """
-
-    @abc.abstractmethod
-    def get_signal(self, start_time: pd.Timestamp, end_time: pd.Timestamp) -> Union[pd.Series, pd.DataFrame, None]:
-        """
-        get the signal at the end of the decision step(from `start_time` to `end_time`)
-
-        Returns
-        -------
-        Union[pd.Series, pd.DataFrame, None]:
-            returns None if no signal in the specific day
-        """
-
-
-class SignalWCache(Signal):
-    """
-    Signal With pandas with based Cache
-    SignalWCache will store the prepared signal as a attribute and give the according signal based on input query
-    """
-
-    def __init__(self, signal: Union[pd.Series, pd.DataFrame]) -> None:
-        """
-
-        Parameters
-        ----------
-        signal : Union[pd.Series, pd.DataFrame]
-            The expected format of the signal is like the data below (the order of index is not important and can be
-            automatically adjusted)
-
-                instrument datetime
-                SH600000   2008-01-02  0.079704
-                           2008-01-03  0.120125
-                           2008-01-04  0.878860
-                           2008-01-07  0.505539
-                           2008-01-08  0.395004
-        """
-        self.signal_cache = convert_index_format(signal, level="datetime")
-
-    def get_signal(self, start_time: pd.Timestamp, end_time: pd.Timestamp) -> Union[pd.Series, pd.DataFrame]:
-        # the frequency of the signal may not align with the decision frequency of strategy
-        # so resampling from the data is necessary
-        # the latest signal leverage more recent data and therefore is used in trading.
-        signal = resam_ts_data(self.signal_cache, start_time=start_time, end_time=end_time, method="last")
-        return signal
-
-
-class ModelSignal(SignalWCache):
-    def __init__(self, model: BaseModel, dataset: Dataset) -> None:
-        self.model = model
-        self.dataset = dataset
-        pred_scores = self.model.predict(dataset)
-        if isinstance(pred_scores, pd.DataFrame):
-            pred_scores = pred_scores.iloc[:, 0]
-        super().__init__(pred_scores)
-
-    def _update_model(self) -> None:
-        """
-        When using online data, update model in each bar as the following steps:
-            - update dataset with online data, the dataset should support online update
-            - make the latest prediction scores of the new bar
-            - update the pred score into the latest prediction
-        """
-        # TODO: this method is not included in the framework and could be refactor later
-        raise NotImplementedError("_update_model is not implemented!")
-
-
-def create_signal_from(
-    obj: Union[Signal, Tuple[BaseModel, Dataset], List, Dict, Text, pd.Series, pd.DataFrame],
-) -> Signal:
-    """
-    create signal from diverse information
-    This method will choose the right method to create a signal based on `obj`
-    Please refer to the code below.
-    """
-    if isinstance(obj, Signal):
-        return obj
-    elif isinstance(obj, (tuple, list)):
-        return ModelSignal(*obj)
-    elif isinstance(obj, (dict, str)):
-        return init_instance_by_config(obj)
-    elif isinstance(obj, (pd.DataFrame, pd.Series)):
-        return SignalWCache(signal=obj)
-    else:
-        raise NotImplementedError(f"This type of signal is not supported")
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+import abc
+from typing import Dict, List, Text, Tuple, Union
+
+import pandas as pd
+
+from qlib.utils import init_instance_by_config
+
+from ..data.dataset import Dataset
+from ..data.dataset.utils import convert_index_format
+from ..model.base import BaseModel
+from ..utils.resam import resam_ts_data
+
+
+class Signal(metaclass=abc.ABCMeta):
+    """
+    Some trading strategy make decisions based on other prediction signals
+    The signals may comes from different sources(e.g. prepared data, online prediction from model and dataset)
+
+    This interface is tries to provide unified interface for those different sources
+    """
+
+    @abc.abstractmethod
+    def get_signal(self, start_time: pd.Timestamp, end_time: pd.Timestamp) -> Union[pd.Series, pd.DataFrame, None]:
+        """
+        get the signal at the end of the decision step(from `start_time` to `end_time`)
+
+        Returns
+        -------
+        Union[pd.Series, pd.DataFrame, None]:
+            returns None if no signal in the specific day
+        """
+
+
+class SignalWCache(Signal):
+    """
+    Signal With pandas with based Cache
+    SignalWCache will store the prepared signal as a attribute and give the according signal based on input query
+    """
+
+    def __init__(self, signal: Union[pd.Series, pd.DataFrame]) -> None:
+        """
+
+        Parameters
+        ----------
+        signal : Union[pd.Series, pd.DataFrame]
+            The expected format of the signal is like the data below (the order of index is not important and can be
+            automatically adjusted)
+
+                instrument datetime
+                SH600000   2008-01-02  0.079704
+                           2008-01-03  0.120125
+                           2008-01-04  0.878860
+                           2008-01-07  0.505539
+                           2008-01-08  0.395004
+        """
+        self.signal_cache = convert_index_format(signal, level="datetime")
+
+    def get_signal(self, start_time: pd.Timestamp, end_time: pd.Timestamp) -> Union[pd.Series, pd.DataFrame]:
+        # the frequency of the signal may not align with the decision frequency of strategy
+        # so resampling from the data is necessary
+        # the latest signal leverage more recent data and therefore is used in trading.
+        signal = resam_ts_data(self.signal_cache, start_time=start_time, end_time=end_time, method="last")
+        return signal
+
+
+class ModelSignal(SignalWCache):
+    def __init__(self, model: BaseModel, dataset: Dataset) -> None:
+        self.model = model
+        self.dataset = dataset
+        pred_scores = self.model.predict(dataset)
+        if isinstance(pred_scores, pd.DataFrame):
+            pred_scores = pred_scores.iloc[:, 0]
+        super().__init__(pred_scores)
+
+    def _update_model(self) -> None:
+        """
+        When using online data, update model in each bar as the following steps:
+            - update dataset with online data, the dataset should support online update
+            - make the latest prediction scores of the new bar
+            - update the pred score into the latest prediction
+        """
+        # TODO: this method is not included in the framework and could be refactor later
+        raise NotImplementedError("_update_model is not implemented!")
+
+
+def create_signal_from(
+    obj: Union[Signal, Tuple[BaseModel, Dataset], List, Dict, Text, pd.Series, pd.DataFrame],
+) -> Signal:
+    """
+    create signal from diverse information
+    This method will choose the right method to create a signal based on `obj`
+    Please refer to the code below.
+    """
+    if isinstance(obj, Signal):
+        return obj
+    elif isinstance(obj, (tuple, list)):
+        return ModelSignal(*obj)
+    elif isinstance(obj, (dict, str)):
+        return init_instance_by_config(obj)
+    elif isinstance(obj, (pd.DataFrame, pd.Series)):
+        return SignalWCache(signal=obj)
+    else:
+        raise NotImplementedError(f"This type of signal is not supported")
```

## qlib/backtest/utils.py

 * *Ordering differences only*

```diff
@@ -1,290 +1,290 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-from __future__ import annotations
-
-from abc import abstractmethod
-from typing import Any, Set, Tuple, TYPE_CHECKING, Union
-
-import numpy as np
-
-from qlib.utils.time import epsilon_change
-
-if TYPE_CHECKING:
-    from qlib.backtest.decision import BaseTradeDecision
-
-import warnings
-
-import pandas as pd
-
-from ..data.data import Cal
-
-
-class TradeCalendarManager:
-    """
-    Manager for trading calendar
-        - BaseStrategy and BaseExecutor will use it
-    """
-
-    def __init__(
-        self,
-        freq: str,
-        start_time: Union[str, pd.Timestamp] = None,
-        end_time: Union[str, pd.Timestamp] = None,
-        level_infra: LevelInfrastructure | None = None,
-    ) -> None:
-        """
-        Parameters
-        ----------
-        freq : str
-            frequency of trading calendar, also trade time per trading step
-        start_time : Union[str, pd.Timestamp], optional
-            closed start of the trading calendar, by default None
-            If `start_time` is None, it must be reset before trading.
-        end_time : Union[str, pd.Timestamp], optional
-            closed end of the trade time range, by default None
-            If `end_time` is None, it must be reset before trading.
-        """
-        self.level_infra = level_infra
-        self.reset(freq=freq, start_time=start_time, end_time=end_time)
-
-    def reset(
-        self,
-        freq: str,
-        start_time: Union[str, pd.Timestamp] = None,
-        end_time: Union[str, pd.Timestamp] = None,
-    ) -> None:
-        """
-        Please refer to the docs of `__init__`
-
-        Reset the trade calendar
-        - self.trade_len : The total count for trading step
-        - self.trade_step : The number of trading step finished, self.trade_step can be
-            [0, 1, 2, ..., self.trade_len - 1]
-        """
-        self.freq = freq
-        self.start_time = pd.Timestamp(start_time) if start_time else None
-        self.end_time = pd.Timestamp(end_time) if end_time else None
-
-        _calendar = Cal.calendar(freq=freq, future=True)
-        assert isinstance(_calendar, np.ndarray)
-        self._calendar = _calendar
-        _, _, _start_index, _end_index = Cal.locate_index(start_time, end_time, freq=freq, future=True)
-        self.start_index = _start_index
-        self.end_index = _end_index
-        self.trade_len = _end_index - _start_index + 1
-        self.trade_step = 0
-
-    def finished(self) -> bool:
-        """
-        Check if the trading finished
-        - Should check before calling strategy.generate_decisions and executor.execute
-        - If self.trade_step >= self.self.trade_len, it means the trading is finished
-        - If self.trade_step < self.self.trade_len, it means the number of trading step finished is self.trade_step
-        """
-        return self.trade_step >= self.trade_len
-
-    def step(self) -> None:
-        if self.finished():
-            raise RuntimeError(f"The calendar is finished, please reset it if you want to call it!")
-        self.trade_step += 1
-
-    def get_freq(self) -> str:
-        return self.freq
-
-    def get_trade_len(self) -> int:
-        """get the total step length"""
-        return self.trade_len
-
-    def get_trade_step(self) -> int:
-        return self.trade_step
-
-    def get_step_time(self, trade_step: int | None = None, shift: int = 0) -> Tuple[pd.Timestamp, pd.Timestamp]:
-        """
-        Get the left and right endpoints of the trade_step'th trading interval
-
-        About the endpoints:
-            - Qlib uses the closed interval in time-series data selection, which has the same performance as
-            pandas.Series.loc
-            # - The returned right endpoints should minus 1 seconds because of the closed interval representation in
-            #   Qlib.
-            # Note: Qlib supports up to minutely decision execution, so 1 seconds is less than any trading time
-            #   interval.
-
-        Parameters
-        ----------
-        trade_step : int, optional
-            the number of trading step finished, by default None to indicate current step
-        shift : int, optional
-            shift bars , by default 0
-
-        Returns
-        -------
-        Tuple[pd.Timestamp, pd.Timestamp]
-            - If shift == 0, return the trading time range
-            - If shift > 0, return the trading time range of the earlier shift bars
-            - If shift < 0, return the trading time range of the later shift bar
-        """
-        if trade_step is None:
-            trade_step = self.get_trade_step()
-        calendar_index = self.start_index + trade_step - shift
-        return self._calendar[calendar_index], epsilon_change(self._calendar[calendar_index + 1])
-
-    def get_data_cal_range(self, rtype: str = "full") -> Tuple[int, int]:
-        """
-        get the calendar range
-        The following assumptions are made
-        1) The frequency of the exchange in common_infra is the same as the data calendar
-        2) Users want the **data index** mod by **day** (i.e. 240 min)
-
-        Parameters
-        ----------
-        rtype: str
-            - "full": return the full limitation of the decision in the day
-            - "step": return the limitation of current step
-
-        Returns
-        -------
-        Tuple[int, int]:
-        """
-        # potential performance issue
-        assert self.level_infra is not None
-
-        day_start = pd.Timestamp(self.start_time.date())
-        day_end = epsilon_change(day_start + pd.Timedelta(days=1))
-        freq = self.level_infra.get("common_infra").get("trade_exchange").freq
-        _, _, day_start_idx, _ = Cal.locate_index(day_start, day_end, freq=freq)
-
-        if rtype == "full":
-            _, _, start_idx, end_index = Cal.locate_index(self.start_time, self.end_time, freq=freq)
-        elif rtype == "step":
-            _, _, start_idx, end_index = Cal.locate_index(*self.get_step_time(), freq=freq)
-        else:
-            raise ValueError(f"This type of input {rtype} is not supported")
-
-        return start_idx - day_start_idx, end_index - day_start_idx
-
-    def get_all_time(self) -> Tuple[pd.Timestamp, pd.Timestamp]:
-        """Get the start_time and end_time for trading"""
-        return self.start_time, self.end_time
-
-    # helper functions
-    def get_range_idx(self, start_time: pd.Timestamp, end_time: pd.Timestamp) -> Tuple[int, int]:
-        """
-        get the range index which involve start_time~end_time  (both sides are closed)
-
-        Parameters
-        ----------
-        start_time : pd.Timestamp
-        end_time : pd.Timestamp
-
-        Returns
-        -------
-        Tuple[int, int]:
-            the index of the range.  **the left and right are closed**
-        """
-        left = int(np.searchsorted(self._calendar, start_time, side="right") - 1)
-        right = int(np.searchsorted(self._calendar, end_time, side="right") - 1)
-        left -= self.start_index
-        right -= self.start_index
-
-        def clip(idx: int) -> int:
-            return min(max(0, idx), self.trade_len - 1)
-
-        return clip(left), clip(right)
-
-    def __repr__(self) -> str:
-        return (
-            f"class: {self.__class__.__name__}; "
-            f"{self.start_time}[{self.start_index}]~{self.end_time}[{self.end_index}]: "
-            f"[{self.trade_step}/{self.trade_len}]"
-        )
-
-
-class BaseInfrastructure:
-    def __init__(self, **kwargs: Any) -> None:
-        self.reset_infra(**kwargs)
-
-    @abstractmethod
-    def get_support_infra(self) -> Set[str]:
-        raise NotImplementedError("`get_support_infra` is not implemented!")
-
-    def reset_infra(self, **kwargs: Any) -> None:
-        support_infra = self.get_support_infra()
-        for k, v in kwargs.items():
-            if k in support_infra:
-                setattr(self, k, v)
-            else:
-                warnings.warn(f"{k} is ignored in `reset_infra`!")
-
-    def get(self, infra_name: str) -> Any:
-        if hasattr(self, infra_name):
-            return getattr(self, infra_name)
-        else:
-            warnings.warn(f"infra {infra_name} is not found!")
-
-    def has(self, infra_name: str) -> bool:
-        return infra_name in self.get_support_infra() and hasattr(self, infra_name)
-
-    def update(self, other: BaseInfrastructure) -> None:
-        support_infra = other.get_support_infra()
-        infra_dict = {_infra: getattr(other, _infra) for _infra in support_infra if hasattr(other, _infra)}
-        self.reset_infra(**infra_dict)
-
-
-class CommonInfrastructure(BaseInfrastructure):
-    def get_support_infra(self) -> Set[str]:
-        return {"trade_account", "trade_exchange"}
-
-
-class LevelInfrastructure(BaseInfrastructure):
-    """level infrastructure is created by executor, and then shared to strategies on the same level"""
-
-    def get_support_infra(self) -> Set[str]:
-        """
-        Descriptions about the infrastructure
-
-        sub_level_infra:
-        - **NOTE**: this will only work after _init_sub_trading !!!
-        """
-        return {"trade_calendar", "sub_level_infra", "common_infra", "executor"}
-
-    def reset_cal(
-        self,
-        freq: str,
-        start_time: Union[str, pd.Timestamp, None],
-        end_time: Union[str, pd.Timestamp, None],
-    ) -> None:
-        """reset trade calendar manager"""
-        if self.has("trade_calendar"):
-            self.get("trade_calendar").reset(freq, start_time=start_time, end_time=end_time)
-        else:
-            self.reset_infra(
-                trade_calendar=TradeCalendarManager(freq, start_time=start_time, end_time=end_time, level_infra=self),
-            )
-
-    def set_sub_level_infra(self, sub_level_infra: LevelInfrastructure) -> None:
-        """this will make the calendar access easier when crossing multi-levels"""
-        self.reset_infra(sub_level_infra=sub_level_infra)
-
-
-def get_start_end_idx(trade_calendar: TradeCalendarManager, outer_trade_decision: BaseTradeDecision) -> Tuple[int, int]:
-    """
-    A helper function for getting the decision-level index range limitation for inner strategy
-    - NOTE: this function is not applicable to order-level
-
-    Parameters
-    ----------
-    trade_calendar : TradeCalendarManager
-    outer_trade_decision : BaseTradeDecision
-        the trade decision made by outer strategy
-
-    Returns
-    -------
-    Union[int, int]:
-        start index and end index
-    """
-    try:
-        return outer_trade_decision.get_range_limit(inner_calendar=trade_calendar)
-    except NotImplementedError:
-        return 0, trade_calendar.get_trade_len() - 1
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+from __future__ import annotations
+
+from abc import abstractmethod
+from typing import Any, Set, Tuple, TYPE_CHECKING, Union
+
+import numpy as np
+
+from qlib.utils.time import epsilon_change
+
+if TYPE_CHECKING:
+    from qlib.backtest.decision import BaseTradeDecision
+
+import warnings
+
+import pandas as pd
+
+from ..data.data import Cal
+
+
+class TradeCalendarManager:
+    """
+    Manager for trading calendar
+        - BaseStrategy and BaseExecutor will use it
+    """
+
+    def __init__(
+        self,
+        freq: str,
+        start_time: Union[str, pd.Timestamp] = None,
+        end_time: Union[str, pd.Timestamp] = None,
+        level_infra: LevelInfrastructure | None = None,
+    ) -> None:
+        """
+        Parameters
+        ----------
+        freq : str
+            frequency of trading calendar, also trade time per trading step
+        start_time : Union[str, pd.Timestamp], optional
+            closed start of the trading calendar, by default None
+            If `start_time` is None, it must be reset before trading.
+        end_time : Union[str, pd.Timestamp], optional
+            closed end of the trade time range, by default None
+            If `end_time` is None, it must be reset before trading.
+        """
+        self.level_infra = level_infra
+        self.reset(freq=freq, start_time=start_time, end_time=end_time)
+
+    def reset(
+        self,
+        freq: str,
+        start_time: Union[str, pd.Timestamp] = None,
+        end_time: Union[str, pd.Timestamp] = None,
+    ) -> None:
+        """
+        Please refer to the docs of `__init__`
+
+        Reset the trade calendar
+        - self.trade_len : The total count for trading step
+        - self.trade_step : The number of trading step finished, self.trade_step can be
+            [0, 1, 2, ..., self.trade_len - 1]
+        """
+        self.freq = freq
+        self.start_time = pd.Timestamp(start_time) if start_time else None
+        self.end_time = pd.Timestamp(end_time) if end_time else None
+
+        _calendar = Cal.calendar(freq=freq, future=True)
+        assert isinstance(_calendar, np.ndarray)
+        self._calendar = _calendar
+        _, _, _start_index, _end_index = Cal.locate_index(start_time, end_time, freq=freq, future=True)
+        self.start_index = _start_index
+        self.end_index = _end_index
+        self.trade_len = _end_index - _start_index + 1
+        self.trade_step = 0
+
+    def finished(self) -> bool:
+        """
+        Check if the trading finished
+        - Should check before calling strategy.generate_decisions and executor.execute
+        - If self.trade_step >= self.self.trade_len, it means the trading is finished
+        - If self.trade_step < self.self.trade_len, it means the number of trading step finished is self.trade_step
+        """
+        return self.trade_step >= self.trade_len
+
+    def step(self) -> None:
+        if self.finished():
+            raise RuntimeError(f"The calendar is finished, please reset it if you want to call it!")
+        self.trade_step += 1
+
+    def get_freq(self) -> str:
+        return self.freq
+
+    def get_trade_len(self) -> int:
+        """get the total step length"""
+        return self.trade_len
+
+    def get_trade_step(self) -> int:
+        return self.trade_step
+
+    def get_step_time(self, trade_step: int | None = None, shift: int = 0) -> Tuple[pd.Timestamp, pd.Timestamp]:
+        """
+        Get the left and right endpoints of the trade_step'th trading interval
+
+        About the endpoints:
+            - Qlib uses the closed interval in time-series data selection, which has the same performance as
+            pandas.Series.loc
+            # - The returned right endpoints should minus 1 seconds because of the closed interval representation in
+            #   Qlib.
+            # Note: Qlib supports up to minutely decision execution, so 1 seconds is less than any trading time
+            #   interval.
+
+        Parameters
+        ----------
+        trade_step : int, optional
+            the number of trading step finished, by default None to indicate current step
+        shift : int, optional
+            shift bars , by default 0
+
+        Returns
+        -------
+        Tuple[pd.Timestamp, pd.Timestamp]
+            - If shift == 0, return the trading time range
+            - If shift > 0, return the trading time range of the earlier shift bars
+            - If shift < 0, return the trading time range of the later shift bar
+        """
+        if trade_step is None:
+            trade_step = self.get_trade_step()
+        calendar_index = self.start_index + trade_step - shift
+        return self._calendar[calendar_index], epsilon_change(self._calendar[calendar_index + 1])
+
+    def get_data_cal_range(self, rtype: str = "full") -> Tuple[int, int]:
+        """
+        get the calendar range
+        The following assumptions are made
+        1) The frequency of the exchange in common_infra is the same as the data calendar
+        2) Users want the **data index** mod by **day** (i.e. 240 min)
+
+        Parameters
+        ----------
+        rtype: str
+            - "full": return the full limitation of the decision in the day
+            - "step": return the limitation of current step
+
+        Returns
+        -------
+        Tuple[int, int]:
+        """
+        # potential performance issue
+        assert self.level_infra is not None
+
+        day_start = pd.Timestamp(self.start_time.date())
+        day_end = epsilon_change(day_start + pd.Timedelta(days=1))
+        freq = self.level_infra.get("common_infra").get("trade_exchange").freq
+        _, _, day_start_idx, _ = Cal.locate_index(day_start, day_end, freq=freq)
+
+        if rtype == "full":
+            _, _, start_idx, end_index = Cal.locate_index(self.start_time, self.end_time, freq=freq)
+        elif rtype == "step":
+            _, _, start_idx, end_index = Cal.locate_index(*self.get_step_time(), freq=freq)
+        else:
+            raise ValueError(f"This type of input {rtype} is not supported")
+
+        return start_idx - day_start_idx, end_index - day_start_idx
+
+    def get_all_time(self) -> Tuple[pd.Timestamp, pd.Timestamp]:
+        """Get the start_time and end_time for trading"""
+        return self.start_time, self.end_time
+
+    # helper functions
+    def get_range_idx(self, start_time: pd.Timestamp, end_time: pd.Timestamp) -> Tuple[int, int]:
+        """
+        get the range index which involve start_time~end_time  (both sides are closed)
+
+        Parameters
+        ----------
+        start_time : pd.Timestamp
+        end_time : pd.Timestamp
+
+        Returns
+        -------
+        Tuple[int, int]:
+            the index of the range.  **the left and right are closed**
+        """
+        left = int(np.searchsorted(self._calendar, start_time, side="right") - 1)
+        right = int(np.searchsorted(self._calendar, end_time, side="right") - 1)
+        left -= self.start_index
+        right -= self.start_index
+
+        def clip(idx: int) -> int:
+            return min(max(0, idx), self.trade_len - 1)
+
+        return clip(left), clip(right)
+
+    def __repr__(self) -> str:
+        return (
+            f"class: {self.__class__.__name__}; "
+            f"{self.start_time}[{self.start_index}]~{self.end_time}[{self.end_index}]: "
+            f"[{self.trade_step}/{self.trade_len}]"
+        )
+
+
+class BaseInfrastructure:
+    def __init__(self, **kwargs: Any) -> None:
+        self.reset_infra(**kwargs)
+
+    @abstractmethod
+    def get_support_infra(self) -> Set[str]:
+        raise NotImplementedError("`get_support_infra` is not implemented!")
+
+    def reset_infra(self, **kwargs: Any) -> None:
+        support_infra = self.get_support_infra()
+        for k, v in kwargs.items():
+            if k in support_infra:
+                setattr(self, k, v)
+            else:
+                warnings.warn(f"{k} is ignored in `reset_infra`!")
+
+    def get(self, infra_name: str) -> Any:
+        if hasattr(self, infra_name):
+            return getattr(self, infra_name)
+        else:
+            warnings.warn(f"infra {infra_name} is not found!")
+
+    def has(self, infra_name: str) -> bool:
+        return infra_name in self.get_support_infra() and hasattr(self, infra_name)
+
+    def update(self, other: BaseInfrastructure) -> None:
+        support_infra = other.get_support_infra()
+        infra_dict = {_infra: getattr(other, _infra) for _infra in support_infra if hasattr(other, _infra)}
+        self.reset_infra(**infra_dict)
+
+
+class CommonInfrastructure(BaseInfrastructure):
+    def get_support_infra(self) -> Set[str]:
+        return {"trade_account", "trade_exchange"}
+
+
+class LevelInfrastructure(BaseInfrastructure):
+    """level infrastructure is created by executor, and then shared to strategies on the same level"""
+
+    def get_support_infra(self) -> Set[str]:
+        """
+        Descriptions about the infrastructure
+
+        sub_level_infra:
+        - **NOTE**: this will only work after _init_sub_trading !!!
+        """
+        return {"trade_calendar", "sub_level_infra", "common_infra", "executor"}
+
+    def reset_cal(
+        self,
+        freq: str,
+        start_time: Union[str, pd.Timestamp, None],
+        end_time: Union[str, pd.Timestamp, None],
+    ) -> None:
+        """reset trade calendar manager"""
+        if self.has("trade_calendar"):
+            self.get("trade_calendar").reset(freq, start_time=start_time, end_time=end_time)
+        else:
+            self.reset_infra(
+                trade_calendar=TradeCalendarManager(freq, start_time=start_time, end_time=end_time, level_infra=self),
+            )
+
+    def set_sub_level_infra(self, sub_level_infra: LevelInfrastructure) -> None:
+        """this will make the calendar access easier when crossing multi-levels"""
+        self.reset_infra(sub_level_infra=sub_level_infra)
+
+
+def get_start_end_idx(trade_calendar: TradeCalendarManager, outer_trade_decision: BaseTradeDecision) -> Tuple[int, int]:
+    """
+    A helper function for getting the decision-level index range limitation for inner strategy
+    - NOTE: this function is not applicable to order-level
+
+    Parameters
+    ----------
+    trade_calendar : TradeCalendarManager
+    outer_trade_decision : BaseTradeDecision
+        the trade decision made by outer strategy
+
+    Returns
+    -------
+    Union[int, int]:
+        start index and end index
+    """
+    try:
+        return outer_trade_decision.get_range_limit(inner_calendar=trade_calendar)
+    except NotImplementedError:
+        return 0, trade_calendar.get_trade_len() - 1
```

## qlib/contrib/evaluate.py

 * *Ordering differences only*

```diff
@@ -1,404 +1,404 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-from __future__ import division
-from __future__ import print_function
-
-import numpy as np
-import pandas as pd
-import warnings
-from typing import Union
-
-from ..log import get_module_logger
-from ..utils import get_date_range
-from ..utils.resam import Freq
-from ..strategy.base import BaseStrategy
-from ..backtest import get_exchange, position, backtest as backtest_func, executor as _executor
-
-
-from ..data import D
-from ..config import C
-from ..data.dataset.utils import get_level_index
-
-
-logger = get_module_logger("Evaluate")
-
-
-def risk_analysis(r, N: int = None, freq: str = "day"):
-    """Risk Analysis
-    NOTE:
-    The calculation of annulaized return is different from the definition of annualized return.
-    It is implemented by design.
-    Qlib tries to cumulated returns by summation instead of production to avoid the cumulated curve being skewed exponentially.
-    All the calculation of annualized returns follows this principle in Qlib.
-
-    TODO: add a parameter to enable calculating metrics with production accumulation of return.
-
-    Parameters
-    ----------
-    r : pandas.Series
-        daily return series.
-    N: int
-        scaler for annualizing information_ratio (day: 252, week: 50, month: 12), at least one of `N` and `freq` should exist
-    freq: str
-        analysis frequency used for calculating the scaler, at least one of `N` and `freq` should exist
-    """
-
-    def cal_risk_analysis_scaler(freq):
-        _count, _freq = Freq.parse(freq)
-        # len(D.calendar(start_time='2010-01-01', end_time='2019-12-31', freq='day')) = 2384
-        _freq_scaler = {
-            Freq.NORM_FREQ_MINUTE: 240 * 238,
-            Freq.NORM_FREQ_DAY: 238,
-            Freq.NORM_FREQ_WEEK: 50,
-            Freq.NORM_FREQ_MONTH: 12,
-        }
-        return _freq_scaler[_freq] / _count
-
-    if N is None and freq is None:
-        raise ValueError("at least one of `N` and `freq` should exist")
-    if N is not None and freq is not None:
-        warnings.warn("risk_analysis freq will be ignored")
-    if N is None:
-        N = cal_risk_analysis_scaler(freq)
-
-    mean = r.mean()
-    std = r.std(ddof=1)
-    annualized_return = mean * N
-    information_ratio = mean / std * np.sqrt(N)
-    max_drawdown = (r.cumsum() - r.cumsum().cummax()).min()
-    data = {
-        "mean": mean,
-        "std": std,
-        "annualized_return": annualized_return,
-        "information_ratio": information_ratio,
-        "max_drawdown": max_drawdown,
-    }
-    res = pd.Series(data).to_frame("risk")
-    return res
-
-
-def indicator_analysis(df, method="mean"):
-    """analyze statistical time-series indicators of trading
-
-    Parameters
-    ----------
-    df : pandas.DataFrame
-        columns: like ['pa', 'pos', 'ffr', 'deal_amount', 'value'].
-            Necessary fields:
-                - 'pa' is the price advantage in trade indicators
-                - 'pos' is the positive rate in trade indicators
-                - 'ffr' is the fulfill rate in trade indicators
-            Optional fields:
-                - 'deal_amount' is the total deal deal_amount, only necessary when method is 'amount_weighted'
-                - 'value' is the total trade value, only necessary when method is 'value_weighted'
-
-        index: Index(datetime)
-    method : str, optional
-        statistics method of pa/ffr, by default "mean"
-
-        - if method is 'mean', count the mean statistical value of each trade indicator
-        - if method is 'amount_weighted', count the deal_amount weighted mean statistical value of each trade indicator
-        - if method is 'value_weighted', count the value weighted mean statistical value of each trade indicator
-
-        Note: statistics method of pos is always "mean"
-
-    Returns
-    -------
-    pd.DataFrame
-        statistical value of each trade indicators
-    """
-    weights_dict = {
-        "mean": df["count"],
-        "amount_weighted": df["deal_amount"].abs(),
-        "value_weighted": df["value"].abs(),
-    }
-    if method not in weights_dict:
-        raise ValueError(f"indicator_analysis method {method} is not supported!")
-
-    # statistic pa/ffr indicator
-    indicators_df = df[["ffr", "pa"]]
-    weights = weights_dict.get(method)
-    res = indicators_df.mul(weights, axis=0).sum() / weights.sum()
-
-    # statistic pos
-    weights = weights_dict.get("mean")
-    res.loc["pos"] = df["pos"].mul(weights).sum() / weights.sum()
-    res = res.to_frame("value")
-    return res
-
-
-# This is the API for compatibility for legacy code
-def backtest_daily(
-    start_time: Union[str, pd.Timestamp],
-    end_time: Union[str, pd.Timestamp],
-    strategy: Union[str, dict, BaseStrategy],
-    executor: Union[str, dict, _executor.BaseExecutor] = None,
-    account: Union[float, int, position.Position] = 1e8,
-    benchmark: str = "SH000300",
-    exchange_kwargs: dict = None,
-    pos_type: str = "Position",
-):
-    """initialize the strategy and executor, then executor the backtest of daily frequency
-
-    Parameters
-    ----------
-    start_time : Union[str, pd.Timestamp]
-        closed start time for backtest
-        **NOTE**: This will be applied to the outmost executor's calendar.
-    end_time : Union[str, pd.Timestamp]
-        closed end time for backtest
-        **NOTE**: This will be applied to the outmost executor's calendar.
-        E.g. Executor[day](Executor[1min]),   setting `end_time == 20XX0301` will include all the minutes on 20XX0301
-    strategy : Union[str, dict, BaseStrategy]
-        for initializing outermost portfolio strategy. Please refer to the docs of init_instance_by_config for more information.
-
-        E.g.
-
-        .. code-block:: python
-
-            # dict
-            strategy = {
-                "class": "TopkDropoutStrategy",
-                "module_path": "qlib.contrib.strategy.signal_strategy",
-                "kwargs": {
-                    "signal": (model, dataset),
-                    "topk": 50,
-                    "n_drop": 5,
-                },
-            }
-            # BaseStrategy
-            pred_score = pd.read_pickle("score.pkl")["score"]
-            STRATEGY_CONFIG = {
-                "topk": 50,
-                "n_drop": 5,
-                "signal": pred_score,
-            }
-            strategy = TopkDropoutStrategy(**STRATEGY_CONFIG)
-            # str example.
-            # 1) specify a pickle object
-            #     - path like 'file:///<path to pickle file>/obj.pkl'
-            # 2) specify a class name
-            #     - "ClassName":  getattr(module, "ClassName")() will be used.
-            # 3) specify module path with class name
-            #     - "a.b.c.ClassName" getattr(<a.b.c.module>, "ClassName")() will be used.
-
-    executor : Union[str, dict, BaseExecutor]
-        for initializing the outermost executor.
-    benchmark: str
-        the benchmark for reporting.
-    account : Union[float, int, Position]
-        information for describing how to creating the account
-
-        For `float` or `int`:
-
-            Using Account with only initial cash
-
-        For `Position`:
-
-            Using Account with a Position
-    exchange_kwargs : dict
-        the kwargs for initializing Exchange
-        E.g.
-
-        .. code-block:: python
-
-            exchange_kwargs = {
-                "freq": freq,
-                "limit_threshold": None, # limit_threshold is None, using C.limit_threshold
-                "deal_price": None, # deal_price is None, using C.deal_price
-                "open_cost": 0.0005,
-                "close_cost": 0.0015,
-                "min_cost": 5,
-            }
-
-    pos_type : str
-        the type of Position.
-
-    Returns
-    -------
-    report_normal: pd.DataFrame
-        backtest report
-    positions_normal: pd.DataFrame
-        backtest positions
-
-    """
-    freq = "day"
-    if executor is None:
-        executor_config = {
-            "time_per_step": freq,
-            "generate_portfolio_metrics": True,
-        }
-        executor = _executor.SimulatorExecutor(**executor_config)
-    _exchange_kwargs = {
-        "freq": freq,
-        "limit_threshold": None,
-        "deal_price": None,
-        "open_cost": 0.0005,
-        "close_cost": 0.0015,
-        "min_cost": 5,
-    }
-    if exchange_kwargs is not None:
-        _exchange_kwargs.update(exchange_kwargs)
-
-    portfolio_metric_dict, indicator_dict = backtest_func(
-        start_time=start_time,
-        end_time=end_time,
-        strategy=strategy,
-        executor=executor,
-        account=account,
-        benchmark=benchmark,
-        exchange_kwargs=_exchange_kwargs,
-        pos_type=pos_type,
-    )
-    analysis_freq = "{0}{1}".format(*Freq.parse(freq))
-
-    report_normal, positions_normal = portfolio_metric_dict.get(analysis_freq)
-
-    return report_normal, positions_normal
-
-
-def long_short_backtest(
-    pred,
-    topk=50,
-    deal_price=None,
-    shift=1,
-    open_cost=0,
-    close_cost=0,
-    trade_unit=None,
-    limit_threshold=None,
-    min_cost=5,
-    subscribe_fields=[],
-    extract_codes=False,
-):
-    """
-    A backtest for long-short strategy
-
-    :param pred:        The trading signal produced on day `T`.
-    :param topk:       The short topk securities and long topk securities.
-    :param deal_price:  The price to deal the trading.
-    :param shift:       Whether to shift prediction by one day.  The trading day will be T+1 if shift==1.
-    :param open_cost:   open transaction cost.
-    :param close_cost:  close transaction cost.
-    :param trade_unit:  100 for China A.
-    :param limit_threshold: limit move 0.1 (10%) for example, long and short with same limit.
-    :param min_cost:    min transaction cost.
-    :param subscribe_fields: subscribe fields.
-    :param extract_codes:  bool.
-                       will we pass the codes extracted from the pred to the exchange.
-                       NOTE: This will be faster with offline qlib.
-    :return:            The result of backtest, it is represented by a dict.
-                        { "long": long_returns(excess),
-                        "short": short_returns(excess),
-                        "long_short": long_short_returns}
-    """
-    if get_level_index(pred, level="datetime") == 1:
-        pred = pred.swaplevel().sort_index()
-
-    if trade_unit is None:
-        trade_unit = C.trade_unit
-    if limit_threshold is None:
-        limit_threshold = C.limit_threshold
-    if deal_price is None:
-        deal_price = C.deal_price
-    if deal_price[0] != "$":
-        deal_price = "$" + deal_price
-
-    subscribe_fields = subscribe_fields.copy()
-    profit_str = f"Ref({deal_price}, -1)/{deal_price} - 1"
-    subscribe_fields.append(profit_str)
-
-    trade_exchange = get_exchange(
-        pred=pred,
-        deal_price=deal_price,
-        subscribe_fields=subscribe_fields,
-        limit_threshold=limit_threshold,
-        open_cost=open_cost,
-        close_cost=close_cost,
-        min_cost=min_cost,
-        trade_unit=trade_unit,
-        extract_codes=extract_codes,
-        shift=shift,
-    )
-
-    _pred_dates = pred.index.get_level_values(level="datetime")
-    predict_dates = D.calendar(start_time=_pred_dates.min(), end_time=_pred_dates.max())
-    trade_dates = np.append(predict_dates[shift:], get_date_range(predict_dates[-1], left_shift=1, right_shift=shift))
-
-    long_returns = {}
-    short_returns = {}
-    ls_returns = {}
-
-    for pdate, date in zip(predict_dates, trade_dates):
-        score = pred.loc(axis=0)[pdate, :]
-        score = score.reset_index().sort_values(by="score", ascending=False)
-
-        long_stocks = list(score.iloc[:topk]["instrument"])
-        short_stocks = list(score.iloc[-topk:]["instrument"])
-
-        score = score.set_index(["datetime", "instrument"]).sort_index()
-
-        long_profit = []
-        short_profit = []
-        all_profit = []
-
-        for stock in long_stocks:
-            if not trade_exchange.is_stock_tradable(stock_id=stock, trade_date=date):
-                continue
-            profit = trade_exchange.get_quote_info(stock_id=stock, start_time=date, end_time=date, field=profit_str)
-            if np.isnan(profit):
-                long_profit.append(0)
-            else:
-                long_profit.append(profit)
-
-        for stock in short_stocks:
-            if not trade_exchange.is_stock_tradable(stock_id=stock, trade_date=date):
-                continue
-            profit = trade_exchange.get_quote_info(stock_id=stock, start_time=date, end_time=date, field=profit_str)
-            if np.isnan(profit):
-                short_profit.append(0)
-            else:
-                short_profit.append(profit * -1)
-
-        for stock in list(score.loc(axis=0)[pdate, :].index.get_level_values(level=0)):
-            # exclude the suspend stock
-            if trade_exchange.check_stock_suspended(stock_id=stock, trade_date=date):
-                continue
-            profit = trade_exchange.get_quote_info(stock_id=stock, start_time=date, end_time=date, field=profit_str)
-            if np.isnan(profit):
-                all_profit.append(0)
-            else:
-                all_profit.append(profit)
-
-        long_returns[date] = np.mean(long_profit) - np.mean(all_profit)
-        short_returns[date] = np.mean(short_profit) + np.mean(all_profit)
-        ls_returns[date] = np.mean(short_profit) + np.mean(long_profit)
-
-    return dict(
-        zip(
-            ["long", "short", "long_short"],
-            map(pd.Series, [long_returns, short_returns, ls_returns]),
-        )
-    )
-
-
-def t_run():
-    pred_FN = "./check_pred.csv"
-    pred: pd.DataFrame = pd.read_csv(pred_FN)
-    pred["datetime"] = pd.to_datetime(pred["datetime"])
-    pred = pred.set_index([pred.columns[0], pred.columns[1]])
-    pred = pred.iloc[:9000]
-    strategy_config = {
-        "topk": 50,
-        "n_drop": 5,
-        "signal": pred,
-    }
-    report_df, positions = backtest_daily(start_time="2017-01-01", end_time="2020-08-01", strategy=strategy_config)
-    print(report_df.head())
-    print(positions.keys())
-    print(positions[list(positions.keys())[0]])
-    return 0
-
-
-if __name__ == "__main__":
-    t_run()
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+from __future__ import division
+from __future__ import print_function
+
+import numpy as np
+import pandas as pd
+import warnings
+from typing import Union
+
+from ..log import get_module_logger
+from ..utils import get_date_range
+from ..utils.resam import Freq
+from ..strategy.base import BaseStrategy
+from ..backtest import get_exchange, position, backtest as backtest_func, executor as _executor
+
+
+from ..data import D
+from ..config import C
+from ..data.dataset.utils import get_level_index
+
+
+logger = get_module_logger("Evaluate")
+
+
+def risk_analysis(r, N: int = None, freq: str = "day"):
+    """Risk Analysis
+    NOTE:
+    The calculation of annulaized return is different from the definition of annualized return.
+    It is implemented by design.
+    Qlib tries to cumulated returns by summation instead of production to avoid the cumulated curve being skewed exponentially.
+    All the calculation of annualized returns follows this principle in Qlib.
+
+    TODO: add a parameter to enable calculating metrics with production accumulation of return.
+
+    Parameters
+    ----------
+    r : pandas.Series
+        daily return series.
+    N: int
+        scaler for annualizing information_ratio (day: 252, week: 50, month: 12), at least one of `N` and `freq` should exist
+    freq: str
+        analysis frequency used for calculating the scaler, at least one of `N` and `freq` should exist
+    """
+
+    def cal_risk_analysis_scaler(freq):
+        _count, _freq = Freq.parse(freq)
+        # len(D.calendar(start_time='2010-01-01', end_time='2019-12-31', freq='day')) = 2384
+        _freq_scaler = {
+            Freq.NORM_FREQ_MINUTE: 240 * 238,
+            Freq.NORM_FREQ_DAY: 238,
+            Freq.NORM_FREQ_WEEK: 50,
+            Freq.NORM_FREQ_MONTH: 12,
+        }
+        return _freq_scaler[_freq] / _count
+
+    if N is None and freq is None:
+        raise ValueError("at least one of `N` and `freq` should exist")
+    if N is not None and freq is not None:
+        warnings.warn("risk_analysis freq will be ignored")
+    if N is None:
+        N = cal_risk_analysis_scaler(freq)
+
+    mean = r.mean()
+    std = r.std(ddof=1)
+    annualized_return = mean * N
+    information_ratio = mean / std * np.sqrt(N)
+    max_drawdown = (r.cumsum() - r.cumsum().cummax()).min()
+    data = {
+        "mean": mean,
+        "std": std,
+        "annualized_return": annualized_return,
+        "information_ratio": information_ratio,
+        "max_drawdown": max_drawdown,
+    }
+    res = pd.Series(data).to_frame("risk")
+    return res
+
+
+def indicator_analysis(df, method="mean"):
+    """analyze statistical time-series indicators of trading
+
+    Parameters
+    ----------
+    df : pandas.DataFrame
+        columns: like ['pa', 'pos', 'ffr', 'deal_amount', 'value'].
+            Necessary fields:
+                - 'pa' is the price advantage in trade indicators
+                - 'pos' is the positive rate in trade indicators
+                - 'ffr' is the fulfill rate in trade indicators
+            Optional fields:
+                - 'deal_amount' is the total deal deal_amount, only necessary when method is 'amount_weighted'
+                - 'value' is the total trade value, only necessary when method is 'value_weighted'
+
+        index: Index(datetime)
+    method : str, optional
+        statistics method of pa/ffr, by default "mean"
+
+        - if method is 'mean', count the mean statistical value of each trade indicator
+        - if method is 'amount_weighted', count the deal_amount weighted mean statistical value of each trade indicator
+        - if method is 'value_weighted', count the value weighted mean statistical value of each trade indicator
+
+        Note: statistics method of pos is always "mean"
+
+    Returns
+    -------
+    pd.DataFrame
+        statistical value of each trade indicators
+    """
+    weights_dict = {
+        "mean": df["count"],
+        "amount_weighted": df["deal_amount"].abs(),
+        "value_weighted": df["value"].abs(),
+    }
+    if method not in weights_dict:
+        raise ValueError(f"indicator_analysis method {method} is not supported!")
+
+    # statistic pa/ffr indicator
+    indicators_df = df[["ffr", "pa"]]
+    weights = weights_dict.get(method)
+    res = indicators_df.mul(weights, axis=0).sum() / weights.sum()
+
+    # statistic pos
+    weights = weights_dict.get("mean")
+    res.loc["pos"] = df["pos"].mul(weights).sum() / weights.sum()
+    res = res.to_frame("value")
+    return res
+
+
+# This is the API for compatibility for legacy code
+def backtest_daily(
+    start_time: Union[str, pd.Timestamp],
+    end_time: Union[str, pd.Timestamp],
+    strategy: Union[str, dict, BaseStrategy],
+    executor: Union[str, dict, _executor.BaseExecutor] = None,
+    account: Union[float, int, position.Position] = 1e8,
+    benchmark: str = "SH000300",
+    exchange_kwargs: dict = None,
+    pos_type: str = "Position",
+):
+    """initialize the strategy and executor, then executor the backtest of daily frequency
+
+    Parameters
+    ----------
+    start_time : Union[str, pd.Timestamp]
+        closed start time for backtest
+        **NOTE**: This will be applied to the outmost executor's calendar.
+    end_time : Union[str, pd.Timestamp]
+        closed end time for backtest
+        **NOTE**: This will be applied to the outmost executor's calendar.
+        E.g. Executor[day](Executor[1min]),   setting `end_time == 20XX0301` will include all the minutes on 20XX0301
+    strategy : Union[str, dict, BaseStrategy]
+        for initializing outermost portfolio strategy. Please refer to the docs of init_instance_by_config for more information.
+
+        E.g.
+
+        .. code-block:: python
+
+            # dict
+            strategy = {
+                "class": "TopkDropoutStrategy",
+                "module_path": "qlib.contrib.strategy.signal_strategy",
+                "kwargs": {
+                    "signal": (model, dataset),
+                    "topk": 50,
+                    "n_drop": 5,
+                },
+            }
+            # BaseStrategy
+            pred_score = pd.read_pickle("score.pkl")["score"]
+            STRATEGY_CONFIG = {
+                "topk": 50,
+                "n_drop": 5,
+                "signal": pred_score,
+            }
+            strategy = TopkDropoutStrategy(**STRATEGY_CONFIG)
+            # str example.
+            # 1) specify a pickle object
+            #     - path like 'file:///<path to pickle file>/obj.pkl'
+            # 2) specify a class name
+            #     - "ClassName":  getattr(module, "ClassName")() will be used.
+            # 3) specify module path with class name
+            #     - "a.b.c.ClassName" getattr(<a.b.c.module>, "ClassName")() will be used.
+
+    executor : Union[str, dict, BaseExecutor]
+        for initializing the outermost executor.
+    benchmark: str
+        the benchmark for reporting.
+    account : Union[float, int, Position]
+        information for describing how to creating the account
+
+        For `float` or `int`:
+
+            Using Account with only initial cash
+
+        For `Position`:
+
+            Using Account with a Position
+    exchange_kwargs : dict
+        the kwargs for initializing Exchange
+        E.g.
+
+        .. code-block:: python
+
+            exchange_kwargs = {
+                "freq": freq,
+                "limit_threshold": None, # limit_threshold is None, using C.limit_threshold
+                "deal_price": None, # deal_price is None, using C.deal_price
+                "open_cost": 0.0005,
+                "close_cost": 0.0015,
+                "min_cost": 5,
+            }
+
+    pos_type : str
+        the type of Position.
+
+    Returns
+    -------
+    report_normal: pd.DataFrame
+        backtest report
+    positions_normal: pd.DataFrame
+        backtest positions
+
+    """
+    freq = "day"
+    if executor is None:
+        executor_config = {
+            "time_per_step": freq,
+            "generate_portfolio_metrics": True,
+        }
+        executor = _executor.SimulatorExecutor(**executor_config)
+    _exchange_kwargs = {
+        "freq": freq,
+        "limit_threshold": None,
+        "deal_price": None,
+        "open_cost": 0.0005,
+        "close_cost": 0.0015,
+        "min_cost": 5,
+    }
+    if exchange_kwargs is not None:
+        _exchange_kwargs.update(exchange_kwargs)
+
+    portfolio_metric_dict, indicator_dict = backtest_func(
+        start_time=start_time,
+        end_time=end_time,
+        strategy=strategy,
+        executor=executor,
+        account=account,
+        benchmark=benchmark,
+        exchange_kwargs=_exchange_kwargs,
+        pos_type=pos_type,
+    )
+    analysis_freq = "{0}{1}".format(*Freq.parse(freq))
+
+    report_normal, positions_normal = portfolio_metric_dict.get(analysis_freq)
+
+    return report_normal, positions_normal
+
+
+def long_short_backtest(
+    pred,
+    topk=50,
+    deal_price=None,
+    shift=1,
+    open_cost=0,
+    close_cost=0,
+    trade_unit=None,
+    limit_threshold=None,
+    min_cost=5,
+    subscribe_fields=[],
+    extract_codes=False,
+):
+    """
+    A backtest for long-short strategy
+
+    :param pred:        The trading signal produced on day `T`.
+    :param topk:       The short topk securities and long topk securities.
+    :param deal_price:  The price to deal the trading.
+    :param shift:       Whether to shift prediction by one day.  The trading day will be T+1 if shift==1.
+    :param open_cost:   open transaction cost.
+    :param close_cost:  close transaction cost.
+    :param trade_unit:  100 for China A.
+    :param limit_threshold: limit move 0.1 (10%) for example, long and short with same limit.
+    :param min_cost:    min transaction cost.
+    :param subscribe_fields: subscribe fields.
+    :param extract_codes:  bool.
+                       will we pass the codes extracted from the pred to the exchange.
+                       NOTE: This will be faster with offline qlib.
+    :return:            The result of backtest, it is represented by a dict.
+                        { "long": long_returns(excess),
+                        "short": short_returns(excess),
+                        "long_short": long_short_returns}
+    """
+    if get_level_index(pred, level="datetime") == 1:
+        pred = pred.swaplevel().sort_index()
+
+    if trade_unit is None:
+        trade_unit = C.trade_unit
+    if limit_threshold is None:
+        limit_threshold = C.limit_threshold
+    if deal_price is None:
+        deal_price = C.deal_price
+    if deal_price[0] != "$":
+        deal_price = "$" + deal_price
+
+    subscribe_fields = subscribe_fields.copy()
+    profit_str = f"Ref({deal_price}, -1)/{deal_price} - 1"
+    subscribe_fields.append(profit_str)
+
+    trade_exchange = get_exchange(
+        pred=pred,
+        deal_price=deal_price,
+        subscribe_fields=subscribe_fields,
+        limit_threshold=limit_threshold,
+        open_cost=open_cost,
+        close_cost=close_cost,
+        min_cost=min_cost,
+        trade_unit=trade_unit,
+        extract_codes=extract_codes,
+        shift=shift,
+    )
+
+    _pred_dates = pred.index.get_level_values(level="datetime")
+    predict_dates = D.calendar(start_time=_pred_dates.min(), end_time=_pred_dates.max())
+    trade_dates = np.append(predict_dates[shift:], get_date_range(predict_dates[-1], left_shift=1, right_shift=shift))
+
+    long_returns = {}
+    short_returns = {}
+    ls_returns = {}
+
+    for pdate, date in zip(predict_dates, trade_dates):
+        score = pred.loc(axis=0)[pdate, :]
+        score = score.reset_index().sort_values(by="score", ascending=False)
+
+        long_stocks = list(score.iloc[:topk]["instrument"])
+        short_stocks = list(score.iloc[-topk:]["instrument"])
+
+        score = score.set_index(["datetime", "instrument"]).sort_index()
+
+        long_profit = []
+        short_profit = []
+        all_profit = []
+
+        for stock in long_stocks:
+            if not trade_exchange.is_stock_tradable(stock_id=stock, trade_date=date):
+                continue
+            profit = trade_exchange.get_quote_info(stock_id=stock, start_time=date, end_time=date, field=profit_str)
+            if np.isnan(profit):
+                long_profit.append(0)
+            else:
+                long_profit.append(profit)
+
+        for stock in short_stocks:
+            if not trade_exchange.is_stock_tradable(stock_id=stock, trade_date=date):
+                continue
+            profit = trade_exchange.get_quote_info(stock_id=stock, start_time=date, end_time=date, field=profit_str)
+            if np.isnan(profit):
+                short_profit.append(0)
+            else:
+                short_profit.append(profit * -1)
+
+        for stock in list(score.loc(axis=0)[pdate, :].index.get_level_values(level=0)):
+            # exclude the suspend stock
+            if trade_exchange.check_stock_suspended(stock_id=stock, trade_date=date):
+                continue
+            profit = trade_exchange.get_quote_info(stock_id=stock, start_time=date, end_time=date, field=profit_str)
+            if np.isnan(profit):
+                all_profit.append(0)
+            else:
+                all_profit.append(profit)
+
+        long_returns[date] = np.mean(long_profit) - np.mean(all_profit)
+        short_returns[date] = np.mean(short_profit) + np.mean(all_profit)
+        ls_returns[date] = np.mean(short_profit) + np.mean(long_profit)
+
+    return dict(
+        zip(
+            ["long", "short", "long_short"],
+            map(pd.Series, [long_returns, short_returns, ls_returns]),
+        )
+    )
+
+
+def t_run():
+    pred_FN = "./check_pred.csv"
+    pred: pd.DataFrame = pd.read_csv(pred_FN)
+    pred["datetime"] = pd.to_datetime(pred["datetime"])
+    pred = pred.set_index([pred.columns[0], pred.columns[1]])
+    pred = pred.iloc[:9000]
+    strategy_config = {
+        "topk": 50,
+        "n_drop": 5,
+        "signal": pred,
+    }
+    report_df, positions = backtest_daily(start_time="2017-01-01", end_time="2020-08-01", strategy=strategy_config)
+    print(report_df.head())
+    print(positions.keys())
+    print(positions[list(positions.keys())[0]])
+    return 0
+
+
+if __name__ == "__main__":
+    t_run()
```

## qlib/contrib/evaluate_portfolio.py

 * *Ordering differences only*

```diff
@@ -1,244 +1,244 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-
-from __future__ import division
-from __future__ import print_function
-
-import numpy as np
-import pandas as pd
-from scipy.stats import spearmanr, pearsonr
-
-from ..data import D
-
-from collections import OrderedDict
-
-
-def _get_position_value_from_df(evaluate_date, position, close_data_df):
-    """Get position value by existed close data df
-    close_data_df:
-        pd.DataFrame
-        multi-index
-        close_data_df['$close'][stock_id][evaluate_date]: close price for (stock_id, evaluate_date)
-    position:
-        same in get_position_value()
-    """
-    value = 0
-    for stock_id, report in position.items():
-        if stock_id != "cash":
-            value += report["amount"] * close_data_df["$close"][stock_id][evaluate_date]
-            # value += report['amount'] * report['price']
-    if "cash" in position:
-        value += position["cash"]
-    return value
-
-
-def get_position_value(evaluate_date, position):
-    """sum of close*amount
-
-    get value of position
-
-    use close price
-
-        positions:
-        {
-            Timestamp('2016-01-05 00:00:00'):
-            {
-                'SH600022':
-                {
-                    'amount':100.00,
-                    'price':12.00
-                },
-
-                'cash':100000.0
-            }
-        }
-
-    It means Hold 100.0 'SH600022' and 100000.0 RMB in '2016-01-05'
-    """
-    # load close price for position
-    # position should also consider cash
-    instruments = list(position.keys())
-    instruments = list(set(instruments) - {"cash"})  # filter 'cash'
-    fields = ["$close"]
-    close_data_df = D.features(
-        instruments,
-        fields,
-        start_time=evaluate_date,
-        end_time=evaluate_date,
-        freq="day",
-        disk_cache=0,
-    )
-    value = _get_position_value_from_df(evaluate_date, position, close_data_df)
-    return value
-
-
-def get_position_list_value(positions):
-    # generate instrument list and date for whole poitions
-    instruments = set()
-    for day, position in positions.items():
-        instruments.update(position.keys())
-    instruments = list(set(instruments) - {"cash"})  # filter 'cash'
-    instruments.sort()
-    day_list = list(positions.keys())
-    day_list.sort()
-    start_date, end_date = day_list[0], day_list[-1]
-    # load data
-    fields = ["$close"]
-    close_data_df = D.features(
-        instruments,
-        fields,
-        start_time=start_date,
-        end_time=end_date,
-        freq="day",
-        disk_cache=0,
-    )
-    # generate value
-    # return dict for time:position_value
-    value_dict = OrderedDict()
-    for day, position in positions.items():
-        value = _get_position_value_from_df(evaluate_date=day, position=position, close_data_df=close_data_df)
-        value_dict[day] = value
-    return value_dict
-
-
-def get_daily_return_series_from_positions(positions, init_asset_value):
-    """Parameters
-    generate daily return series from  position view
-    positions: positions generated by strategy
-    init_asset_value : init asset value
-    return: pd.Series of daily return , return_series[date] = daily return rate
-    """
-    value_dict = get_position_list_value(positions)
-    value_series = pd.Series(value_dict)
-    value_series = value_series.sort_index()  # check date
-    return_series = value_series.pct_change()
-    return_series[value_series.index[0]] = (
-        value_series[value_series.index[0]] / init_asset_value - 1
-    )  # update daily return for the first date
-    return return_series
-
-
-def get_annual_return_from_positions(positions, init_asset_value):
-    """Annualized Returns
-
-    p_r = (p_end / p_start)^{(250/n)} - 1
-
-    p_r     annual return
-    p_end   final value
-    p_start init value
-    n       days of backtest
-
-    """
-    date_range_list = sorted(list(positions.keys()))
-    end_time = date_range_list[-1]
-    p_end = get_position_value(end_time, positions[end_time])
-    p_start = init_asset_value
-    n_period = len(date_range_list)
-    annual = pow((p_end / p_start), (250 / n_period)) - 1
-
-    return annual
-
-
-def get_annaul_return_from_return_series(r, method="ci"):
-    """Risk Analysis from daily return series
-
-    Parameters
-    ----------
-    r : pandas.Series
-        daily return series
-    method : str
-        interest calculation method, ci(compound interest)/si(simple interest)
-    """
-    mean = r.mean()
-    annual = (1 + mean) ** 250 - 1 if method == "ci" else mean * 250
-
-    return annual
-
-
-def get_sharpe_ratio_from_return_series(r, risk_free_rate=0.00, method="ci"):
-    """Risk Analysis
-
-    Parameters
-    ----------
-    r : pandas.Series
-        daily return series
-    method : str
-        interest calculation method, ci(compound interest)/si(simple interest)
-    risk_free_rate : float
-        risk_free_rate, default as 0.00, can set as 0.03 etc
-    """
-    std = r.std(ddof=1)
-    annual = get_annaul_return_from_return_series(r, method=method)
-    sharpe = (annual - risk_free_rate) / std / np.sqrt(250)
-
-    return sharpe
-
-
-def get_max_drawdown_from_series(r):
-    """Risk Analysis from asset value
-
-    cumprod way
-
-    Parameters
-    ----------
-    r : pandas.Series
-        daily return series
-    """
-    # mdd = ((r.cumsum() - r.cumsum().cummax()) / (1 + r.cumsum().cummax())).min()
-
-    mdd = (((1 + r).cumprod() - (1 + r).cumprod().cummax()) / ((1 + r).cumprod().cummax())).min()
-
-    return mdd
-
-
-def get_turnover_rate():
-    # in backtest
-    pass
-
-
-def get_beta(r, b):
-    """Risk Analysis  beta
-
-    Parameters
-    ----------
-    r : pandas.Series
-        daily return series of strategy
-    b : pandas.Series
-        daily return series of baseline
-    """
-    cov_r_b = np.cov(r, b)
-    var_b = np.var(b)
-    return cov_r_b / var_b
-
-
-def get_alpha(r, b, risk_free_rate=0.03):
-    beta = get_beta(r, b)
-    annaul_r = get_annaul_return_from_return_series(r)
-    annaul_b = get_annaul_return_from_return_series(b)
-
-    alpha = annaul_r - risk_free_rate - beta * (annaul_b - risk_free_rate)
-
-    return alpha
-
-
-def get_volatility_from_series(r):
-    return r.std(ddof=1)
-
-
-def get_rank_ic(a, b):
-    """Rank IC
-
-    Parameters
-    ----------
-    r : pandas.Series
-        daily score series of feature
-    b : pandas.Series
-        daily return series
-
-    """
-    return spearmanr(a, b).correlation
-
-
-def get_normal_ic(a, b):
-    return pearsonr(a, b)[0]
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+
+from __future__ import division
+from __future__ import print_function
+
+import numpy as np
+import pandas as pd
+from scipy.stats import spearmanr, pearsonr
+
+from ..data import D
+
+from collections import OrderedDict
+
+
+def _get_position_value_from_df(evaluate_date, position, close_data_df):
+    """Get position value by existed close data df
+    close_data_df:
+        pd.DataFrame
+        multi-index
+        close_data_df['$close'][stock_id][evaluate_date]: close price for (stock_id, evaluate_date)
+    position:
+        same in get_position_value()
+    """
+    value = 0
+    for stock_id, report in position.items():
+        if stock_id != "cash":
+            value += report["amount"] * close_data_df["$close"][stock_id][evaluate_date]
+            # value += report['amount'] * report['price']
+    if "cash" in position:
+        value += position["cash"]
+    return value
+
+
+def get_position_value(evaluate_date, position):
+    """sum of close*amount
+
+    get value of position
+
+    use close price
+
+        positions:
+        {
+            Timestamp('2016-01-05 00:00:00'):
+            {
+                'SH600022':
+                {
+                    'amount':100.00,
+                    'price':12.00
+                },
+
+                'cash':100000.0
+            }
+        }
+
+    It means Hold 100.0 'SH600022' and 100000.0 RMB in '2016-01-05'
+    """
+    # load close price for position
+    # position should also consider cash
+    instruments = list(position.keys())
+    instruments = list(set(instruments) - {"cash"})  # filter 'cash'
+    fields = ["$close"]
+    close_data_df = D.features(
+        instruments,
+        fields,
+        start_time=evaluate_date,
+        end_time=evaluate_date,
+        freq="day",
+        disk_cache=0,
+    )
+    value = _get_position_value_from_df(evaluate_date, position, close_data_df)
+    return value
+
+
+def get_position_list_value(positions):
+    # generate instrument list and date for whole poitions
+    instruments = set()
+    for day, position in positions.items():
+        instruments.update(position.keys())
+    instruments = list(set(instruments) - {"cash"})  # filter 'cash'
+    instruments.sort()
+    day_list = list(positions.keys())
+    day_list.sort()
+    start_date, end_date = day_list[0], day_list[-1]
+    # load data
+    fields = ["$close"]
+    close_data_df = D.features(
+        instruments,
+        fields,
+        start_time=start_date,
+        end_time=end_date,
+        freq="day",
+        disk_cache=0,
+    )
+    # generate value
+    # return dict for time:position_value
+    value_dict = OrderedDict()
+    for day, position in positions.items():
+        value = _get_position_value_from_df(evaluate_date=day, position=position, close_data_df=close_data_df)
+        value_dict[day] = value
+    return value_dict
+
+
+def get_daily_return_series_from_positions(positions, init_asset_value):
+    """Parameters
+    generate daily return series from  position view
+    positions: positions generated by strategy
+    init_asset_value : init asset value
+    return: pd.Series of daily return , return_series[date] = daily return rate
+    """
+    value_dict = get_position_list_value(positions)
+    value_series = pd.Series(value_dict)
+    value_series = value_series.sort_index()  # check date
+    return_series = value_series.pct_change()
+    return_series[value_series.index[0]] = (
+        value_series[value_series.index[0]] / init_asset_value - 1
+    )  # update daily return for the first date
+    return return_series
+
+
+def get_annual_return_from_positions(positions, init_asset_value):
+    """Annualized Returns
+
+    p_r = (p_end / p_start)^{(250/n)} - 1
+
+    p_r     annual return
+    p_end   final value
+    p_start init value
+    n       days of backtest
+
+    """
+    date_range_list = sorted(list(positions.keys()))
+    end_time = date_range_list[-1]
+    p_end = get_position_value(end_time, positions[end_time])
+    p_start = init_asset_value
+    n_period = len(date_range_list)
+    annual = pow((p_end / p_start), (250 / n_period)) - 1
+
+    return annual
+
+
+def get_annaul_return_from_return_series(r, method="ci"):
+    """Risk Analysis from daily return series
+
+    Parameters
+    ----------
+    r : pandas.Series
+        daily return series
+    method : str
+        interest calculation method, ci(compound interest)/si(simple interest)
+    """
+    mean = r.mean()
+    annual = (1 + mean) ** 250 - 1 if method == "ci" else mean * 250
+
+    return annual
+
+
+def get_sharpe_ratio_from_return_series(r, risk_free_rate=0.00, method="ci"):
+    """Risk Analysis
+
+    Parameters
+    ----------
+    r : pandas.Series
+        daily return series
+    method : str
+        interest calculation method, ci(compound interest)/si(simple interest)
+    risk_free_rate : float
+        risk_free_rate, default as 0.00, can set as 0.03 etc
+    """
+    std = r.std(ddof=1)
+    annual = get_annaul_return_from_return_series(r, method=method)
+    sharpe = (annual - risk_free_rate) / std / np.sqrt(250)
+
+    return sharpe
+
+
+def get_max_drawdown_from_series(r):
+    """Risk Analysis from asset value
+
+    cumprod way
+
+    Parameters
+    ----------
+    r : pandas.Series
+        daily return series
+    """
+    # mdd = ((r.cumsum() - r.cumsum().cummax()) / (1 + r.cumsum().cummax())).min()
+
+    mdd = (((1 + r).cumprod() - (1 + r).cumprod().cummax()) / ((1 + r).cumprod().cummax())).min()
+
+    return mdd
+
+
+def get_turnover_rate():
+    # in backtest
+    pass
+
+
+def get_beta(r, b):
+    """Risk Analysis  beta
+
+    Parameters
+    ----------
+    r : pandas.Series
+        daily return series of strategy
+    b : pandas.Series
+        daily return series of baseline
+    """
+    cov_r_b = np.cov(r, b)
+    var_b = np.var(b)
+    return cov_r_b / var_b
+
+
+def get_alpha(r, b, risk_free_rate=0.03):
+    beta = get_beta(r, b)
+    annaul_r = get_annaul_return_from_return_series(r)
+    annaul_b = get_annaul_return_from_return_series(b)
+
+    alpha = annaul_r - risk_free_rate - beta * (annaul_b - risk_free_rate)
+
+    return alpha
+
+
+def get_volatility_from_series(r):
+    return r.std(ddof=1)
+
+
+def get_rank_ic(a, b):
+    """Rank IC
+
+    Parameters
+    ----------
+    r : pandas.Series
+        daily score series of feature
+    b : pandas.Series
+        daily return series
+
+    """
+    return spearmanr(a, b).correlation
+
+
+def get_normal_ic(a, b):
+    return pearsonr(a, b)[0]
```

## qlib/contrib/torch.py

 * *Ordering differences only*

```diff
@@ -1,31 +1,31 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-"""
-    This module is not a necessary part of Qlib.
-    They are just some tools for convenience
-    It is should not imported into the core part of qlib
-"""
-import torch
-import numpy as np
-import pandas as pd
-
-
-def data_to_tensor(data, device="cpu", raise_error=False):
-    if isinstance(data, torch.Tensor):
-        if device == "cpu":
-            return data.cpu()
-        else:
-            return data.to(device)
-    if isinstance(data, (pd.DataFrame, pd.Series)):
-        return data_to_tensor(torch.from_numpy(data.values).float(), device)
-    elif isinstance(data, np.ndarray):
-        return data_to_tensor(torch.from_numpy(data).float(), device)
-    elif isinstance(data, (tuple, list)):
-        return [data_to_tensor(i, device) for i in data]
-    elif isinstance(data, dict):
-        return {k: data_to_tensor(v, device) for k, v in data.items()}
-    else:
-        if raise_error:
-            raise ValueError(f"Unsupported data type: {type(data)}.")
-        else:
-            return data
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+"""
+    This module is not a necessary part of Qlib.
+    They are just some tools for convenience
+    It is should not imported into the core part of qlib
+"""
+import torch
+import numpy as np
+import pandas as pd
+
+
+def data_to_tensor(data, device="cpu", raise_error=False):
+    if isinstance(data, torch.Tensor):
+        if device == "cpu":
+            return data.cpu()
+        else:
+            return data.to(device)
+    if isinstance(data, (pd.DataFrame, pd.Series)):
+        return data_to_tensor(torch.from_numpy(data.values).float(), device)
+    elif isinstance(data, np.ndarray):
+        return data_to_tensor(torch.from_numpy(data).float(), device)
+    elif isinstance(data, (tuple, list)):
+        return [data_to_tensor(i, device) for i in data]
+    elif isinstance(data, dict):
+        return {k: data_to_tensor(v, device) for k, v in data.items()}
+    else:
+        if raise_error:
+            raise ValueError(f"Unsupported data type: {type(data)}.")
+        else:
+            return data
```

## qlib/contrib/data/data.py

 * *Ordering differences only*

```diff
@@ -1,55 +1,55 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-# We remove arctic from core framework of Qlib to contrib due to
-# - Arctic has very strict limitation on pandas and numpy version
-#    - https://github.com/man-group/arctic/pull/908
-# - pip fail to computing the right version number!!!!
-#    - Maybe we can solve this problem by poetry
-
-# FIXME: So if you want to use arctic-based provider, please install arctic manually
-# `pip install arctic` may not be enough.
-from arctic import Arctic
-import pandas as pd
-import pymongo
-
-from qlib.data.data import FeatureProvider
-
-
-class ArcticFeatureProvider(FeatureProvider):
-    def __init__(
-        self, uri="127.0.0.1", retry_time=0, market_transaction_time_list=[("09:15", "11:30"), ("13:00", "15:00")]
-    ):
-        super().__init__()
-        self.uri = uri
-        # TODO:
-        # retry connecting if error occurs
-        # does it real matters?
-        self.retry_time = retry_time
-        # NOTE: this is especially important for TResample operator
-        self.market_transaction_time_list = market_transaction_time_list
-
-    def feature(self, instrument, field, start_index, end_index, freq):
-        field = str(field)[1:]
-        with pymongo.MongoClient(self.uri) as client:
-            # TODO: this will result in frequently connecting the server and performance issue
-            arctic = Arctic(client)
-
-            if freq not in arctic.list_libraries():
-                raise ValueError("lib {} not in arctic".format(freq))
-
-            if instrument not in arctic[freq].list_symbols():
-                # instruments does not exist
-                return pd.Series()
-            else:
-                df = arctic[freq].read(instrument, columns=[field], chunk_range=(start_index, end_index))
-                s = df[field]
-
-                if not s.empty:
-                    s = pd.concat(
-                        [
-                            s.between_time(time_tuple[0], time_tuple[1])
-                            for time_tuple in self.market_transaction_time_list
-                        ]
-                    )
-                return s
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+# We remove arctic from core framework of Qlib to contrib due to
+# - Arctic has very strict limitation on pandas and numpy version
+#    - https://github.com/man-group/arctic/pull/908
+# - pip fail to computing the right version number!!!!
+#    - Maybe we can solve this problem by poetry
+
+# FIXME: So if you want to use arctic-based provider, please install arctic manually
+# `pip install arctic` may not be enough.
+from arctic import Arctic
+import pandas as pd
+import pymongo
+
+from qlib.data.data import FeatureProvider
+
+
+class ArcticFeatureProvider(FeatureProvider):
+    def __init__(
+        self, uri="127.0.0.1", retry_time=0, market_transaction_time_list=[("09:15", "11:30"), ("13:00", "15:00")]
+    ):
+        super().__init__()
+        self.uri = uri
+        # TODO:
+        # retry connecting if error occurs
+        # does it real matters?
+        self.retry_time = retry_time
+        # NOTE: this is especially important for TResample operator
+        self.market_transaction_time_list = market_transaction_time_list
+
+    def feature(self, instrument, field, start_index, end_index, freq):
+        field = str(field)[1:]
+        with pymongo.MongoClient(self.uri) as client:
+            # TODO: this will result in frequently connecting the server and performance issue
+            arctic = Arctic(client)
+
+            if freq not in arctic.list_libraries():
+                raise ValueError("lib {} not in arctic".format(freq))
+
+            if instrument not in arctic[freq].list_symbols():
+                # instruments does not exist
+                return pd.Series()
+            else:
+                df = arctic[freq].read(instrument, columns=[field], chunk_range=(start_index, end_index))
+                s = df[field]
+
+                if not s.empty:
+                    s = pd.concat(
+                        [
+                            s.between_time(time_tuple[0], time_tuple[1])
+                            for time_tuple in self.market_transaction_time_list
+                        ]
+                    )
+                return s
```

## qlib/contrib/data/dataset.py

```diff
@@ -1,358 +1,353 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-import copy
-import torch
-import warnings
-import numpy as np
-import pandas as pd
-
-from qlib.data.dataset import DatasetH
-
-
-device = "cuda" if torch.cuda.is_available() else "cpu"
-
-
-def _to_tensor(x):
-    if not isinstance(x, torch.Tensor):
-        return torch.tensor(x, dtype=torch.float, device=device)  # pylint: disable=E1101
-    return x
-
-
-def _create_ts_slices(index, seq_len):
-    """
-    create time series slices from pandas index
-
-    Args:
-        index (pd.MultiIndex): pandas multiindex with <instrument, datetime> order
-        seq_len (int): sequence length
-    """
-    assert isinstance(index, pd.MultiIndex), "unsupported index type"
-    assert seq_len > 0, "sequence length should be larger than 0"
-    assert index.is_monotonic_increasing, "index should be sorted"
-
-    # number of dates for each instrument
-    sample_count_by_insts = index.to_series().groupby(level=0).size().values
-
-    # start index for each instrument
-    start_index_of_insts = np.roll(np.cumsum(sample_count_by_insts), 1)
-    start_index_of_insts[0] = 0
-
-    # all the [start, stop) indices of features
-    # features between [start, stop) will be used to predict label at `stop - 1`
-    slices = []
-    for cur_loc, cur_cnt in zip(start_index_of_insts, sample_count_by_insts):
-        for stop in range(1, cur_cnt + 1):
-            end = cur_loc + stop
-            start = max(end - seq_len, 0)
-            slices.append(slice(start, end))
-    slices = np.array(slices, dtype="object")
-
-    assert len(slices) == len(index)  # the i-th slice = index[i]
-
-    return slices
-
-
-def _get_date_parse_fn(target):
-    """get date parse function
-
-    This method is used to parse date arguments as target type.
-
-    Example:
-        get_date_parse_fn('20120101')('2017-01-01') => '20170101'
-        get_date_parse_fn(20120101)('2017-01-01') => 20170101
-    """
-    if isinstance(target, int):
-
-        def _fn(x):
-            return int(str(x).replace("-", "")[:8])  # 20200201
-
-    elif isinstance(target, str) and len(target) == 8:
-
-        def _fn(x):
-            return str(x).replace("-", "")[:8]  # '20200201'
-
-    else:
-
-        def _fn(x):
-            return x  # '2021-01-01'
-
-    return _fn
-
-
-def _maybe_padding(x, seq_len, zeros=None):
-    """padding 2d <time * feature> data with zeros
-
-    Args:
-        x (np.ndarray): 2d data with shape <time * feature>
-        seq_len (int): target sequence length
-        zeros (np.ndarray): zeros with shape <seq_len * feature>
-    """
-    assert seq_len > 0, "sequence length should be larger than 0"
-    if zeros is None:
-        zeros = np.zeros((seq_len, x.shape[1]), dtype=np.float32)
-    else:
-        assert len(zeros) >= seq_len, "zeros matrix is not large enough for padding"
-    if len(x) != seq_len:  # padding zeros
-        x = np.concatenate([zeros[: seq_len - len(x), : x.shape[1]], x], axis=0)
-    return x
-
-
-class MTSDatasetH(DatasetH):
-    """Memory Augmented Time Series Dataset
-
-    Args:
-        handler (DataHandler): data handler
-        segments (dict): data split segments
-        seq_len (int): time series sequence length
-        horizon (int): label horizon
-        num_states (int): how many memory states to be added
-        memory_mode (str): memory mode (daily or sample)
-        batch_size (int): batch size (<0 will use daily sampling)
-        n_samples (int): number of samples in the same day
-        shuffle (bool): whether shuffle data
-        drop_last (bool): whether drop last batch < batch_size
-        input_size (int): reshape flatten rows as this input_size (backward compatibility)
-    """
-
-    def __init__(
-        self,
-        handler,
-        segments,
-        seq_len=60,
-        horizon=0,
-        num_states=0,
-        memory_mode="sample",
-        batch_size=-1,
-        n_samples=None,
-        shuffle=True,
-        drop_last=False,
-        input_size=None,
-        **kwargs,
-    ):
-
-        assert num_states == 0 or horizon > 0, "please specify `horizon` to avoid data leakage"
-        assert memory_mode in ["sample", "daily"], "unsupported memory mode"
-        assert memory_mode == "sample" or batch_size < 0, "daily memory requires daily sampling (`batch_size < 0`)"
-        assert batch_size != 0, "invalid batch size"
-
-        if batch_size > 0 and n_samples is not None:
-            warnings.warn("`n_samples` can only be used for daily sampling (`batch_size < 0`)")
-
-        self.seq_len = seq_len
-        self.horizon = horizon
-        self.num_states = num_states
-        self.memory_mode = memory_mode
-        self.batch_size = batch_size
-        self.n_samples = n_samples
-        self.shuffle = shuffle
-        self.drop_last = drop_last
-        self.input_size = input_size
-        self.params = (batch_size, n_samples, drop_last, shuffle)  # for train/eval switch
-
-        super().__init__(handler, segments, **kwargs)
-
-    def setup_data(self, handler_kwargs: dict = None, **kwargs):
-
-        super().setup_data(**kwargs)
-
-        if handler_kwargs is not None:
-            self.handler.setup_data(**handler_kwargs)
-
-        # pre-fetch data and change index to <code, date>
-        # NOTE: we will use inplace sort to reduce memory use
-        try:
-            df = self.handler._learn.copy()  # use copy otherwise recorder will fail
-            # FIXME: currently we cannot support switching from `_learn` to `_infer` for inference
-        except Exception:
-            warnings.warn("cannot access `_learn`, will load raw data")
-            df = self.handler._data.copy()
-        df.index = df.index.swaplevel()
-        df.sort_index(inplace=True)
-
-        # convert to numpy
-        self._data = df["feature"].values.astype("float32")
-        np.nan_to_num(self._data, copy=False)  # NOTE: fillna in case users forget using the fillna processor
-        self._label = df["label"].squeeze().values.astype("float32")
-        self._index = df.index
-
-        if self.input_size is not None and self.input_size != self._data.shape[1]:
-            warnings.warn("the data has different shape from input_size and the data will be reshaped")
-            assert self._data.shape[1] % self.input_size == 0, "data mismatch, please check `input_size`"
-
-        # create batch slices
-        self._batch_slices = _create_ts_slices(self._index, self.seq_len)
-
-        # create daily slices
-        daily_slices = {date: [] for date in sorted(self._index.unique(level=1))}  # sorted by date
-        for i, (code, date) in enumerate(self._index):
-            daily_slices[date].append(self._batch_slices[i])
-        self._daily_slices = np.array(list(daily_slices.values()), dtype="object")
-        self._daily_index = pd.Series(list(daily_slices.keys()))  # index is the original date index
-
-        # add memory (sample wise and daily)
-        if self.memory_mode == "sample":
-            self._memory = np.zeros((len(self._data), self.num_states), dtype=np.float32)
-        elif self.memory_mode == "daily":
-            self._memory = np.zeros((len(self._daily_index), self.num_states), dtype=np.float32)
-        else:
-            raise ValueError(f"invalid memory_mode `{self.memory_mode}`")
-
-        # padding tensor
-        self._zeros = np.zeros((self.seq_len, max(self.num_states, self._data.shape[1])), dtype=np.float32)
-
-    def _prepare_seg(self, slc, **kwargs):
-        fn = _get_date_parse_fn(self._index[0][1])
-        if isinstance(slc, slice):
-            start, stop = slc.start, slc.stop
-        elif isinstance(slc, (list, tuple)):
-            start, stop = slc
-        else:
-            raise NotImplementedError(f"This type of input is not supported")
-        start_date = pd.Timestamp(fn(start))
-        end_date = pd.Timestamp(fn(stop))
-        obj = copy.copy(self)  # shallow copy
-        # NOTE: Seriable will disable copy `self._data` so we manually assign them here
-        obj._data = self._data  # reference (no copy)
-        obj._label = self._label
-        obj._index = self._index
-        obj._memory = self._memory
-        obj._zeros = self._zeros
-        # update index for this batch
-        date_index = self._index.get_level_values(1)
-        obj._batch_slices = self._batch_slices[(date_index >= start_date) & (date_index <= end_date)]
-        mask = (self._daily_index.values >= start_date) & (self._daily_index.values <= end_date)
-        obj._daily_slices = self._daily_slices[mask]
-        obj._daily_index = self._daily_index[mask]
-        return obj
-
-    def restore_index(self, index):
-        return self._index[index]
-
-    def restore_daily_index(self, daily_index):
-        return pd.Index(self._daily_index.loc[daily_index])
-
-    def assign_data(self, index, vals):
-        if self.num_states == 0:
-            raise ValueError("cannot assign data as `num_states==0`")
-        if isinstance(vals, torch.Tensor):
-            vals = vals.detach().cpu().numpy()
-        self._memory[index] = vals
-
-    def clear_memory(self):
-        if self.num_states == 0:
-            raise ValueError("cannot clear memory as `num_states==0`")
-        self._memory[:] = 0
-
-    def train(self):
-        """enable traning mode"""
-        self.batch_size, self.n_samples, self.drop_last, self.shuffle = self.params
-
-    def eval(self):
-        """enable evaluation mode"""
-        self.batch_size = -1
-        self.n_samples = None
-        self.drop_last = False
-        self.shuffle = False
-
-    def _get_slices(self):
-        if self.batch_size < 0:  # daily sampling
-            slices = self._daily_slices.copy()
-            batch_size = -1 * self.batch_size
-        else:  # normal sampling
-            slices = self._batch_slices.copy()
-            batch_size = self.batch_size
-        return slices, batch_size
-
-    def __len__(self):
-        slices, batch_size = self._get_slices()
-        if self.drop_last:
-            return len(slices) // batch_size
-        return (len(slices) + batch_size - 1) // batch_size
-
-    def __iter__(self):
-        slices, batch_size = self._get_slices()
-        indices = np.arange(len(slices))
-        if self.shuffle:
-            np.random.shuffle(indices)
-
-        for i in range(len(indices))[::batch_size]:
-            if self.drop_last and i + batch_size > len(indices):
-                break
-
-            data = []  # store features
-            label = []  # store labels
-            index = []  # store index
-            state = []  # store memory states
-            daily_index = []  # store daily index
-            daily_count = []  # store number of samples for each day
-
-            for j in indices[i : i + batch_size]:
-
-                # normal sampling: self.batch_size > 0 => slices is a list => slices_subset is a slice
-                # daily sampling: self.batch_size < 0 => slices is a nested list => slices_subset is a list
-                slices_subset = slices[j]
-
-                # daily sampling
-                # each slices_subset contains a list of slices for multiple stocks
-                # NOTE: daily sampling is used in 1) eval mode, 2) train mode with self.batch_size < 0
-                if self.batch_size < 0:
-
-                    # store daily index
-                    idx = self._daily_index.index[j]  # daily_index.index is the index of the original data
-                    daily_index.append(idx)
-
-                    # store daily memory if specified
-                    # NOTE: daily memory always requires daily sampling (self.batch_size < 0)
-                    if self.memory_mode == "daily":
-                        slc = slice(max(idx - self.seq_len - self.horizon, 0), max(idx - self.horizon, 0))
-                        state.append(_maybe_padding(self._memory[slc], self.seq_len, self._zeros))
-
-                    # down-sample stocks and store count
-                    if self.n_samples and 0 < self.n_samples < len(slices_subset):  # intraday subsample
-                        slices_subset = np.random.choice(slices_subset, self.n_samples, replace=False)
-                    daily_count.append(len(slices_subset))
-
-                # normal sampling
-                # each slices_subset is a single slice
-                # NOTE: normal sampling is used in train mode with self.batch_size > 0
-                else:
-                    slices_subset = [slices_subset]
-
-                for slc in slices_subset:
-
-                    # legacy support for Alpha360 data by `input_size`
-                    if self.input_size:
-                        data.append(self._data[slc.stop - 1].reshape(self.input_size, -1).T)
-                    else:
-                        data.append(_maybe_padding(self._data[slc], self.seq_len, self._zeros))
-
-                    if self.memory_mode == "sample":
-                        state.append(_maybe_padding(self._memory[slc], self.seq_len, self._zeros)[: -self.horizon])
-
-                    label.append(self._label[slc.stop - 1])
-                    index.append(slc.stop - 1)
-
-                    # end slices loop
-
-                # end indices batch loop
-
-            # concate
-            data = _to_tensor(np.stack(data))
-            state = _to_tensor(np.stack(state))
-            label = _to_tensor(np.stack(label))
-            index = np.array(index)
-            daily_index = np.array(daily_index)
-            daily_count = np.array(daily_count)
-
-            # yield -> generator
-            yield {
-                "data": data,
-                "label": label,
-                "state": state,
-                "index": index,
-                "daily_index": daily_index,
-                "daily_count": daily_count,
-            }
-
-        # end indice loop
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+import copy
+import torch
+import warnings
+import numpy as np
+import pandas as pd
+
+from qlib.data.dataset import DatasetH
+
+
+device = "cuda" if torch.cuda.is_available() else "cpu"
+
+
+def _to_tensor(x):
+    if not isinstance(x, torch.Tensor):
+        return torch.tensor(x, dtype=torch.float, device=device)  # pylint: disable=E1101
+    return x
+
+
+def _create_ts_slices(index, seq_len):
+    """
+    create time series slices from pandas index
+
+    Args:
+        index (pd.MultiIndex): pandas multiindex with <instrument, datetime> order
+        seq_len (int): sequence length
+    """
+    assert isinstance(index, pd.MultiIndex), "unsupported index type"
+    assert seq_len > 0, "sequence length should be larger than 0"
+    assert index.is_monotonic_increasing, "index should be sorted"
+
+    # number of dates for each instrument
+    sample_count_by_insts = index.to_series().groupby(level=0).size().values
+
+    # start index for each instrument
+    start_index_of_insts = np.roll(np.cumsum(sample_count_by_insts), 1)
+    start_index_of_insts[0] = 0
+
+    # all the [start, stop) indices of features
+    # features between [start, stop) will be used to predict label at `stop - 1`
+    slices = []
+    for cur_loc, cur_cnt in zip(start_index_of_insts, sample_count_by_insts):
+        for stop in range(1, cur_cnt + 1):
+            end = cur_loc + stop
+            start = max(end - seq_len, 0)
+            slices.append(slice(start, end))
+    slices = np.array(slices, dtype="object")
+
+    assert len(slices) == len(index)  # the i-th slice = index[i]
+
+    return slices
+
+
+def _get_date_parse_fn(target):
+    """get date parse function
+
+    This method is used to parse date arguments as target type.
+
+    Example:
+        get_date_parse_fn('20120101')('2017-01-01') => '20170101'
+        get_date_parse_fn(20120101)('2017-01-01') => 20170101
+    """
+    if isinstance(target, int):
+
+        def _fn(x):
+            return int(str(x).replace("-", "")[:8])  # 20200201
+
+    elif isinstance(target, str) and len(target) == 8:
+
+        def _fn(x):
+            return str(x).replace("-", "")[:8]  # '20200201'
+
+    else:
+
+        def _fn(x):
+            return x  # '2021-01-01'
+
+    return _fn
+
+
+def _maybe_padding(x, seq_len, zeros=None):
+    """padding 2d <time * feature> data with zeros
+
+    Args:
+        x (np.ndarray): 2d data with shape <time * feature>
+        seq_len (int): target sequence length
+        zeros (np.ndarray): zeros with shape <seq_len * feature>
+    """
+    assert seq_len > 0, "sequence length should be larger than 0"
+    if zeros is None:
+        zeros = np.zeros((seq_len, x.shape[1]), dtype=np.float32)
+    else:
+        assert len(zeros) >= seq_len, "zeros matrix is not large enough for padding"
+    if len(x) != seq_len:  # padding zeros
+        x = np.concatenate([zeros[: seq_len - len(x), : x.shape[1]], x], axis=0)
+    return x
+
+
+class MTSDatasetH(DatasetH):
+    """Memory Augmented Time Series Dataset
+
+    Args:
+        handler (DataHandler): data handler
+        segments (dict): data split segments
+        seq_len (int): time series sequence length
+        horizon (int): label horizon
+        num_states (int): how many memory states to be added
+        memory_mode (str): memory mode (daily or sample)
+        batch_size (int): batch size (<0 will use daily sampling)
+        n_samples (int): number of samples in the same day
+        shuffle (bool): whether shuffle data
+        drop_last (bool): whether drop last batch < batch_size
+        input_size (int): reshape flatten rows as this input_size (backward compatibility)
+    """
+
+    def __init__(
+        self,
+        handler,
+        segments,
+        seq_len=60,
+        horizon=0,
+        num_states=0,
+        memory_mode="sample",
+        batch_size=-1,
+        n_samples=None,
+        shuffle=True,
+        drop_last=False,
+        input_size=None,
+        **kwargs,
+    ):
+        assert num_states == 0 or horizon > 0, "please specify `horizon` to avoid data leakage"
+        assert memory_mode in ["sample", "daily"], "unsupported memory mode"
+        assert memory_mode == "sample" or batch_size < 0, "daily memory requires daily sampling (`batch_size < 0`)"
+        assert batch_size != 0, "invalid batch size"
+
+        if batch_size > 0 and n_samples is not None:
+            warnings.warn("`n_samples` can only be used for daily sampling (`batch_size < 0`)")
+
+        self.seq_len = seq_len
+        self.horizon = horizon
+        self.num_states = num_states
+        self.memory_mode = memory_mode
+        self.batch_size = batch_size
+        self.n_samples = n_samples
+        self.shuffle = shuffle
+        self.drop_last = drop_last
+        self.input_size = input_size
+        self.params = (batch_size, n_samples, drop_last, shuffle)  # for train/eval switch
+
+        super().__init__(handler, segments, **kwargs)
+
+    def setup_data(self, handler_kwargs: dict = None, **kwargs):
+        super().setup_data(**kwargs)
+
+        if handler_kwargs is not None:
+            self.handler.setup_data(**handler_kwargs)
+
+        # pre-fetch data and change index to <code, date>
+        # NOTE: we will use inplace sort to reduce memory use
+        try:
+            df = self.handler._learn.copy()  # use copy otherwise recorder will fail
+            # FIXME: currently we cannot support switching from `_learn` to `_infer` for inference
+        except Exception:
+            warnings.warn("cannot access `_learn`, will load raw data")
+            df = self.handler._data.copy()
+        df.index = df.index.swaplevel()
+        df.sort_index(inplace=True)
+
+        # convert to numpy
+        self._data = df["feature"].values.astype("float32")
+        np.nan_to_num(self._data, copy=False)  # NOTE: fillna in case users forget using the fillna processor
+        self._label = df["label"].squeeze().values.astype("float32")
+        self._index = df.index
+
+        if self.input_size is not None and self.input_size != self._data.shape[1]:
+            warnings.warn("the data has different shape from input_size and the data will be reshaped")
+            assert self._data.shape[1] % self.input_size == 0, "data mismatch, please check `input_size`"
+
+        # create batch slices
+        self._batch_slices = _create_ts_slices(self._index, self.seq_len)
+
+        # create daily slices
+        daily_slices = {date: [] for date in sorted(self._index.unique(level=1))}  # sorted by date
+        for i, (code, date) in enumerate(self._index):
+            daily_slices[date].append(self._batch_slices[i])
+        self._daily_slices = np.array(list(daily_slices.values()), dtype="object")
+        self._daily_index = pd.Series(list(daily_slices.keys()))  # index is the original date index
+
+        # add memory (sample wise and daily)
+        if self.memory_mode == "sample":
+            self._memory = np.zeros((len(self._data), self.num_states), dtype=np.float32)
+        elif self.memory_mode == "daily":
+            self._memory = np.zeros((len(self._daily_index), self.num_states), dtype=np.float32)
+        else:
+            raise ValueError(f"invalid memory_mode `{self.memory_mode}`")
+
+        # padding tensor
+        self._zeros = np.zeros((self.seq_len, max(self.num_states, self._data.shape[1])), dtype=np.float32)
+
+    def _prepare_seg(self, slc, **kwargs):
+        fn = _get_date_parse_fn(self._index[0][1])
+        if isinstance(slc, slice):
+            start, stop = slc.start, slc.stop
+        elif isinstance(slc, (list, tuple)):
+            start, stop = slc
+        else:
+            raise NotImplementedError(f"This type of input is not supported")
+        start_date = pd.Timestamp(fn(start))
+        end_date = pd.Timestamp(fn(stop))
+        obj = copy.copy(self)  # shallow copy
+        # NOTE: Seriable will disable copy `self._data` so we manually assign them here
+        obj._data = self._data  # reference (no copy)
+        obj._label = self._label
+        obj._index = self._index
+        obj._memory = self._memory
+        obj._zeros = self._zeros
+        # update index for this batch
+        date_index = self._index.get_level_values(1)
+        obj._batch_slices = self._batch_slices[(date_index >= start_date) & (date_index <= end_date)]
+        mask = (self._daily_index.values >= start_date) & (self._daily_index.values <= end_date)
+        obj._daily_slices = self._daily_slices[mask]
+        obj._daily_index = self._daily_index[mask]
+        return obj
+
+    def restore_index(self, index):
+        return self._index[index]
+
+    def restore_daily_index(self, daily_index):
+        return pd.Index(self._daily_index.loc[daily_index])
+
+    def assign_data(self, index, vals):
+        if self.num_states == 0:
+            raise ValueError("cannot assign data as `num_states==0`")
+        if isinstance(vals, torch.Tensor):
+            vals = vals.detach().cpu().numpy()
+        self._memory[index] = vals
+
+    def clear_memory(self):
+        if self.num_states == 0:
+            raise ValueError("cannot clear memory as `num_states==0`")
+        self._memory[:] = 0
+
+    def train(self):
+        """enable traning mode"""
+        self.batch_size, self.n_samples, self.drop_last, self.shuffle = self.params
+
+    def eval(self):
+        """enable evaluation mode"""
+        self.batch_size = -1
+        self.n_samples = None
+        self.drop_last = False
+        self.shuffle = False
+
+    def _get_slices(self):
+        if self.batch_size < 0:  # daily sampling
+            slices = self._daily_slices.copy()
+            batch_size = -1 * self.batch_size
+        else:  # normal sampling
+            slices = self._batch_slices.copy()
+            batch_size = self.batch_size
+        return slices, batch_size
+
+    def __len__(self):
+        slices, batch_size = self._get_slices()
+        if self.drop_last:
+            return len(slices) // batch_size
+        return (len(slices) + batch_size - 1) // batch_size
+
+    def __iter__(self):
+        slices, batch_size = self._get_slices()
+        indices = np.arange(len(slices))
+        if self.shuffle:
+            np.random.shuffle(indices)
+
+        for i in range(len(indices))[::batch_size]:
+            if self.drop_last and i + batch_size > len(indices):
+                break
+
+            data = []  # store features
+            label = []  # store labels
+            index = []  # store index
+            state = []  # store memory states
+            daily_index = []  # store daily index
+            daily_count = []  # store number of samples for each day
+
+            for j in indices[i : i + batch_size]:
+                # normal sampling: self.batch_size > 0 => slices is a list => slices_subset is a slice
+                # daily sampling: self.batch_size < 0 => slices is a nested list => slices_subset is a list
+                slices_subset = slices[j]
+
+                # daily sampling
+                # each slices_subset contains a list of slices for multiple stocks
+                # NOTE: daily sampling is used in 1) eval mode, 2) train mode with self.batch_size < 0
+                if self.batch_size < 0:
+                    # store daily index
+                    idx = self._daily_index.index[j]  # daily_index.index is the index of the original data
+                    daily_index.append(idx)
+
+                    # store daily memory if specified
+                    # NOTE: daily memory always requires daily sampling (self.batch_size < 0)
+                    if self.memory_mode == "daily":
+                        slc = slice(max(idx - self.seq_len - self.horizon, 0), max(idx - self.horizon, 0))
+                        state.append(_maybe_padding(self._memory[slc], self.seq_len, self._zeros))
+
+                    # down-sample stocks and store count
+                    if self.n_samples and 0 < self.n_samples < len(slices_subset):  # intraday subsample
+                        slices_subset = np.random.choice(slices_subset, self.n_samples, replace=False)
+                    daily_count.append(len(slices_subset))
+
+                # normal sampling
+                # each slices_subset is a single slice
+                # NOTE: normal sampling is used in train mode with self.batch_size > 0
+                else:
+                    slices_subset = [slices_subset]
+
+                for slc in slices_subset:
+                    # legacy support for Alpha360 data by `input_size`
+                    if self.input_size:
+                        data.append(self._data[slc.stop - 1].reshape(self.input_size, -1).T)
+                    else:
+                        data.append(_maybe_padding(self._data[slc], self.seq_len, self._zeros))
+
+                    if self.memory_mode == "sample":
+                        state.append(_maybe_padding(self._memory[slc], self.seq_len, self._zeros)[: -self.horizon])
+
+                    label.append(self._label[slc.stop - 1])
+                    index.append(slc.stop - 1)
+
+                    # end slices loop
+
+                # end indices batch loop
+
+            # concate
+            data = _to_tensor(np.stack(data))
+            state = _to_tensor(np.stack(state))
+            label = _to_tensor(np.stack(label))
+            index = np.array(index)
+            daily_index = np.array(daily_index)
+            daily_count = np.array(daily_count)
+
+            # yield -> generator
+            yield {
+                "data": data,
+                "label": label,
+                "state": state,
+                "index": index,
+                "daily_index": daily_index,
+                "daily_count": daily_count,
+            }
+
+        # end indice loop
```

## qlib/contrib/data/handler.py

 * *Ordering differences only*

```diff
@@ -1,432 +1,432 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-from ...data.dataset.handler import DataHandlerLP
-from ...data.dataset.processor import Processor
-from ...utils import get_callable_kwargs
-from ...data.dataset import processor as processor_module
-from inspect import getfullargspec
-
-
-def check_transform_proc(proc_l, fit_start_time, fit_end_time):
-    new_l = []
-    for p in proc_l:
-        if not isinstance(p, Processor):
-            klass, pkwargs = get_callable_kwargs(p, processor_module)
-            args = getfullargspec(klass).args
-            if "fit_start_time" in args and "fit_end_time" in args:
-                assert (
-                    fit_start_time is not None and fit_end_time is not None
-                ), "Make sure `fit_start_time` and `fit_end_time` are not None."
-                pkwargs.update(
-                    {
-                        "fit_start_time": fit_start_time,
-                        "fit_end_time": fit_end_time,
-                    }
-                )
-            proc_config = {"class": klass.__name__, "kwargs": pkwargs}
-            if isinstance(p, dict) and "module_path" in p:
-                proc_config["module_path"] = p["module_path"]
-            new_l.append(proc_config)
-        else:
-            new_l.append(p)
-    return new_l
-
-
-_DEFAULT_LEARN_PROCESSORS = [
-    {"class": "DropnaLabel"},
-    {"class": "CSZScoreNorm", "kwargs": {"fields_group": "label"}},
-]
-_DEFAULT_INFER_PROCESSORS = [
-    {"class": "ProcessInf", "kwargs": {}},
-    {"class": "ZScoreNorm", "kwargs": {}},
-    {"class": "Fillna", "kwargs": {}},
-]
-
-
-class Alpha360(DataHandlerLP):
-    def __init__(
-        self,
-        instruments="csi500",
-        start_time=None,
-        end_time=None,
-        freq="day",
-        infer_processors=_DEFAULT_INFER_PROCESSORS,
-        learn_processors=_DEFAULT_LEARN_PROCESSORS,
-        fit_start_time=None,
-        fit_end_time=None,
-        filter_pipe=None,
-        inst_processors=None,
-        **kwargs
-    ):
-        infer_processors = check_transform_proc(infer_processors, fit_start_time, fit_end_time)
-        learn_processors = check_transform_proc(learn_processors, fit_start_time, fit_end_time)
-
-        data_loader = {
-            "class": "QlibDataLoader",
-            "kwargs": {
-                "config": {
-                    "feature": self.get_feature_config(),
-                    "label": kwargs.pop("label", self.get_label_config()),
-                },
-                "filter_pipe": filter_pipe,
-                "freq": freq,
-                "inst_processors": inst_processors,
-            },
-        }
-
-        super().__init__(
-            instruments=instruments,
-            start_time=start_time,
-            end_time=end_time,
-            data_loader=data_loader,
-            learn_processors=learn_processors,
-            infer_processors=infer_processors,
-            **kwargs
-        )
-
-    def get_label_config(self):
-        return ["Ref($close, -2)/Ref($close, -1) - 1"], ["LABEL0"]
-
-    @staticmethod
-    def get_feature_config():
-        # NOTE:
-        # Alpha360 tries to provide a dataset with original price data
-        # the original price data includes the prices and volume in the last 60 days.
-        # To make it easier to learn models from this dataset, all the prices and volume
-        # are normalized by the latest price and volume data ( dividing by $close, $volume)
-        # So the latest normalized $close will be 1 (with name CLOSE0), the latest normalized $volume will be 1 (with name VOLUME0)
-        # If further normalization are executed (e.g. centralization),  CLOSE0 and VOLUME0 will be 0.
-        fields = []
-        names = []
-
-        for i in range(59, 0, -1):
-            fields += ["Ref($close, %d)/$close" % i]
-            names += ["CLOSE%d" % i]
-        fields += ["$close/$close"]
-        names += ["CLOSE0"]
-        for i in range(59, 0, -1):
-            fields += ["Ref($open, %d)/$close" % i]
-            names += ["OPEN%d" % i]
-        fields += ["$open/$close"]
-        names += ["OPEN0"]
-        for i in range(59, 0, -1):
-            fields += ["Ref($high, %d)/$close" % i]
-            names += ["HIGH%d" % i]
-        fields += ["$high/$close"]
-        names += ["HIGH0"]
-        for i in range(59, 0, -1):
-            fields += ["Ref($low, %d)/$close" % i]
-            names += ["LOW%d" % i]
-        fields += ["$low/$close"]
-        names += ["LOW0"]
-        for i in range(59, 0, -1):
-            fields += ["Ref($vwap, %d)/$close" % i]
-            names += ["VWAP%d" % i]
-        fields += ["$vwap/$close"]
-        names += ["VWAP0"]
-        for i in range(59, 0, -1):
-            fields += ["Ref($volume, %d)/($volume+1e-12)" % i]
-            names += ["VOLUME%d" % i]
-        fields += ["$volume/($volume+1e-12)"]
-        names += ["VOLUME0"]
-
-        return fields, names
-
-
-class Alpha360vwap(Alpha360):
-    def get_label_config(self):
-        return ["Ref($vwap, -2)/Ref($vwap, -1) - 1"], ["LABEL0"]
-
-
-class Alpha158(DataHandlerLP):
-    def __init__(
-        self,
-        instruments="csi500",
-        start_time=None,
-        end_time=None,
-        freq="day",
-        infer_processors=[],
-        learn_processors=_DEFAULT_LEARN_PROCESSORS,
-        fit_start_time=None,
-        fit_end_time=None,
-        process_type=DataHandlerLP.PTYPE_A,
-        filter_pipe=None,
-        inst_processors=None,
-        **kwargs
-    ):
-        infer_processors = check_transform_proc(infer_processors, fit_start_time, fit_end_time)
-        learn_processors = check_transform_proc(learn_processors, fit_start_time, fit_end_time)
-
-        data_loader = {
-            "class": "QlibDataLoader",
-            "kwargs": {
-                "config": {
-                    "feature": self.get_feature_config(),
-                    "label": kwargs.pop("label", self.get_label_config()),
-                },
-                "filter_pipe": filter_pipe,
-                "freq": freq,
-                "inst_processors": inst_processors,
-            },
-        }
-        super().__init__(
-            instruments=instruments,
-            start_time=start_time,
-            end_time=end_time,
-            data_loader=data_loader,
-            infer_processors=infer_processors,
-            learn_processors=learn_processors,
-            process_type=process_type,
-            **kwargs
-        )
-
-    def get_feature_config(self):
-        conf = {
-            "kbar": {},
-            "price": {
-                "windows": [0],
-                "feature": ["OPEN", "HIGH", "LOW", "VWAP"],
-            },
-            "rolling": {},
-        }
-        return self.parse_config_to_fields(conf)
-
-    def get_label_config(self):
-        return ["Ref($close, -2)/Ref($close, -1) - 1"], ["LABEL0"]
-
-    @staticmethod
-    def parse_config_to_fields(config):
-        """create factors from config
-
-        config = {
-            'kbar': {}, # whether to use some hard-code kbar features
-            'price': { # whether to use raw price features
-                'windows': [0, 1, 2, 3, 4], # use price at n days ago
-                'feature': ['OPEN', 'HIGH', 'LOW'] # which price field to use
-            },
-            'volume': { # whether to use raw volume features
-                'windows': [0, 1, 2, 3, 4], # use volume at n days ago
-            },
-            'rolling': { # whether to use rolling operator based features
-                'windows': [5, 10, 20, 30, 60], # rolling windows size
-                'include': ['ROC', 'MA', 'STD'], # rolling operator to use
-                #if include is None we will use default operators
-                'exclude': ['RANK'], # rolling operator not to use
-            }
-        }
-        """
-        fields = []
-        names = []
-        if "kbar" in config:
-            fields += [
-                "($close-$open)/$open",
-                "($high-$low)/$open",
-                "($close-$open)/($high-$low+1e-12)",
-                "($high-Greater($open, $close))/$open",
-                "($high-Greater($open, $close))/($high-$low+1e-12)",
-                "(Less($open, $close)-$low)/$open",
-                "(Less($open, $close)-$low)/($high-$low+1e-12)",
-                "(2*$close-$high-$low)/$open",
-                "(2*$close-$high-$low)/($high-$low+1e-12)",
-            ]
-            names += [
-                "KMID",
-                "KLEN",
-                "KMID2",
-                "KUP",
-                "KUP2",
-                "KLOW",
-                "KLOW2",
-                "KSFT",
-                "KSFT2",
-            ]
-        if "price" in config:
-            windows = config["price"].get("windows", range(5))
-            feature = config["price"].get("feature", ["OPEN", "HIGH", "LOW", "CLOSE", "VWAP"])
-            for field in feature:
-                field = field.lower()
-                fields += ["Ref($%s, %d)/$close" % (field, d) if d != 0 else "$%s/$close" % field for d in windows]
-                names += [field.upper() + str(d) for d in windows]
-        if "volume" in config:
-            windows = config["volume"].get("windows", range(5))
-            fields += ["Ref($volume, %d)/($volume+1e-12)" % d if d != 0 else "$volume/($volume+1e-12)" for d in windows]
-            names += ["VOLUME" + str(d) for d in windows]
-        if "rolling" in config:
-            windows = config["rolling"].get("windows", [5, 10, 20, 30, 60])
-            include = config["rolling"].get("include", None)
-            exclude = config["rolling"].get("exclude", [])
-            # `exclude` in dataset config unnecessary filed
-            # `include` in dataset config necessary field
-
-            def use(x):
-                return x not in exclude and (include is None or x in include)
-
-            # Some factor ref: https://guorn.com/static/upload/file/3/134065454575605.pdf
-            if use("ROC"):
-                # https://www.investopedia.com/terms/r/rateofchange.asp
-                # Rate of change, the price change in the past d days, divided by latest close price to remove unit
-                fields += ["Ref($close, %d)/$close" % d for d in windows]
-                names += ["ROC%d" % d for d in windows]
-            if use("MA"):
-                # https://www.investopedia.com/ask/answers/071414/whats-difference-between-moving-average-and-weighted-moving-average.asp
-                # Simple Moving Average, the simple moving average in the past d days, divided by latest close price to remove unit
-                fields += ["Mean($close, %d)/$close" % d for d in windows]
-                names += ["MA%d" % d for d in windows]
-            if use("STD"):
-                # The standard diviation of close price for the past d days, divided by latest close price to remove unit
-                fields += ["Std($close, %d)/$close" % d for d in windows]
-                names += ["STD%d" % d for d in windows]
-            if use("BETA"):
-                # The rate of close price change in the past d days, divided by latest close price to remove unit
-                # For example, price increase 10 dollar per day in the past d days, then Slope will be 10.
-                fields += ["Slope($close, %d)/$close" % d for d in windows]
-                names += ["BETA%d" % d for d in windows]
-            if use("RSQR"):
-                # The R-sqaure value of linear regression for the past d days, represent the trend linear
-                fields += ["Rsquare($close, %d)" % d for d in windows]
-                names += ["RSQR%d" % d for d in windows]
-            if use("RESI"):
-                # The redisdual for linear regression for the past d days, represent the trend linearity for past d days.
-                fields += ["Resi($close, %d)/$close" % d for d in windows]
-                names += ["RESI%d" % d for d in windows]
-            if use("MAX"):
-                # The max price for past d days, divided by latest close price to remove unit
-                fields += ["Max($high, %d)/$close" % d for d in windows]
-                names += ["MAX%d" % d for d in windows]
-            if use("LOW"):
-                # The low price for past d days, divided by latest close price to remove unit
-                fields += ["Min($low, %d)/$close" % d for d in windows]
-                names += ["MIN%d" % d for d in windows]
-            if use("QTLU"):
-                # The 80% quantile of past d day's close price, divided by latest close price to remove unit
-                # Used with MIN and MAX
-                fields += ["Quantile($close, %d, 0.8)/$close" % d for d in windows]
-                names += ["QTLU%d" % d for d in windows]
-            if use("QTLD"):
-                # The 20% quantile of past d day's close price, divided by latest close price to remove unit
-                fields += ["Quantile($close, %d, 0.2)/$close" % d for d in windows]
-                names += ["QTLD%d" % d for d in windows]
-            if use("RANK"):
-                # Get the percentile of current close price in past d day's close price.
-                # Represent the current price level comparing to past N days, add additional information to moving average.
-                fields += ["Rank($close, %d)" % d for d in windows]
-                names += ["RANK%d" % d for d in windows]
-            if use("RSV"):
-                # Represent the price position between upper and lower resistent price for past d days.
-                fields += ["($close-Min($low, %d))/(Max($high, %d)-Min($low, %d)+1e-12)" % (d, d, d) for d in windows]
-                names += ["RSV%d" % d for d in windows]
-            if use("IMAX"):
-                # The number of days between current date and previous highest price date.
-                # Part of Aroon Indicator https://www.investopedia.com/terms/a/aroon.asp
-                # The indicator measures the time between highs and the time between lows over a time period.
-                # The idea is that strong uptrends will regularly see new highs, and strong downtrends will regularly see new lows.
-                fields += ["IdxMax($high, %d)/%d" % (d, d) for d in windows]
-                names += ["IMAX%d" % d for d in windows]
-            if use("IMIN"):
-                # The number of days between current date and previous lowest price date.
-                # Part of Aroon Indicator https://www.investopedia.com/terms/a/aroon.asp
-                # The indicator measures the time between highs and the time between lows over a time period.
-                # The idea is that strong uptrends will regularly see new highs, and strong downtrends will regularly see new lows.
-                fields += ["IdxMin($low, %d)/%d" % (d, d) for d in windows]
-                names += ["IMIN%d" % d for d in windows]
-            if use("IMXD"):
-                # The time period between previous lowest-price date occur after highest price date.
-                # Large value suggest downward momemtum.
-                fields += ["(IdxMax($high, %d)-IdxMin($low, %d))/%d" % (d, d, d) for d in windows]
-                names += ["IMXD%d" % d for d in windows]
-            if use("CORR"):
-                # The correlation between absolute close price and log scaled trading volume
-                fields += ["Corr($close, Log($volume+1), %d)" % d for d in windows]
-                names += ["CORR%d" % d for d in windows]
-            if use("CORD"):
-                # The correlation between price change ratio and volume change ratio
-                fields += ["Corr($close/Ref($close,1), Log($volume/Ref($volume, 1)+1), %d)" % d for d in windows]
-                names += ["CORD%d" % d for d in windows]
-            if use("CNTP"):
-                # The percentage of days in past d days that price go up.
-                fields += ["Mean($close>Ref($close, 1), %d)" % d for d in windows]
-                names += ["CNTP%d" % d for d in windows]
-            if use("CNTN"):
-                # The percentage of days in past d days that price go down.
-                fields += ["Mean($close<Ref($close, 1), %d)" % d for d in windows]
-                names += ["CNTN%d" % d for d in windows]
-            if use("CNTD"):
-                # The diff between past up day and past down day
-                fields += ["Mean($close>Ref($close, 1), %d)-Mean($close<Ref($close, 1), %d)" % (d, d) for d in windows]
-                names += ["CNTD%d" % d for d in windows]
-            if use("SUMP"):
-                # The total gain / the absolute total price changed
-                # Similar to RSI indicator. https://www.investopedia.com/terms/r/rsi.asp
-                fields += [
-                    "Sum(Greater($close-Ref($close, 1), 0), %d)/(Sum(Abs($close-Ref($close, 1)), %d)+1e-12)" % (d, d)
-                    for d in windows
-                ]
-                names += ["SUMP%d" % d for d in windows]
-            if use("SUMN"):
-                # The total lose / the absolute total price changed
-                # Can be derived from SUMP by SUMN = 1 - SUMP
-                # Similar to RSI indicator. https://www.investopedia.com/terms/r/rsi.asp
-                fields += [
-                    "Sum(Greater(Ref($close, 1)-$close, 0), %d)/(Sum(Abs($close-Ref($close, 1)), %d)+1e-12)" % (d, d)
-                    for d in windows
-                ]
-                names += ["SUMN%d" % d for d in windows]
-            if use("SUMD"):
-                # The diff ratio between total gain and total lose
-                # Similar to RSI indicator. https://www.investopedia.com/terms/r/rsi.asp
-                fields += [
-                    "(Sum(Greater($close-Ref($close, 1), 0), %d)-Sum(Greater(Ref($close, 1)-$close, 0), %d))"
-                    "/(Sum(Abs($close-Ref($close, 1)), %d)+1e-12)" % (d, d, d)
-                    for d in windows
-                ]
-                names += ["SUMD%d" % d for d in windows]
-            if use("VMA"):
-                # Simple Volume Moving average: https://www.barchart.com/education/technical-indicators/volume_moving_average
-                fields += ["Mean($volume, %d)/($volume+1e-12)" % d for d in windows]
-                names += ["VMA%d" % d for d in windows]
-            if use("VSTD"):
-                # The standard deviation for volume in past d days.
-                fields += ["Std($volume, %d)/($volume+1e-12)" % d for d in windows]
-                names += ["VSTD%d" % d for d in windows]
-            if use("WVMA"):
-                # The volume weighted price change volatility
-                fields += [
-                    "Std(Abs($close/Ref($close, 1)-1)*$volume, %d)/(Mean(Abs($close/Ref($close, 1)-1)*$volume, %d)+1e-12)"
-                    % (d, d)
-                    for d in windows
-                ]
-                names += ["WVMA%d" % d for d in windows]
-            if use("VSUMP"):
-                # The total volume increase / the absolute total volume changed
-                fields += [
-                    "Sum(Greater($volume-Ref($volume, 1), 0), %d)/(Sum(Abs($volume-Ref($volume, 1)), %d)+1e-12)"
-                    % (d, d)
-                    for d in windows
-                ]
-                names += ["VSUMP%d" % d for d in windows]
-            if use("VSUMN"):
-                # The total volume increase / the absolute total volume changed
-                # Can be derived from VSUMP by VSUMN = 1 - VSUMP
-                fields += [
-                    "Sum(Greater(Ref($volume, 1)-$volume, 0), %d)/(Sum(Abs($volume-Ref($volume, 1)), %d)+1e-12)"
-                    % (d, d)
-                    for d in windows
-                ]
-                names += ["VSUMN%d" % d for d in windows]
-            if use("VSUMD"):
-                # The diff ratio between total volume increase and total volume decrease
-                # RSI indicator for volume
-                fields += [
-                    "(Sum(Greater($volume-Ref($volume, 1), 0), %d)-Sum(Greater(Ref($volume, 1)-$volume, 0), %d))"
-                    "/(Sum(Abs($volume-Ref($volume, 1)), %d)+1e-12)" % (d, d, d)
-                    for d in windows
-                ]
-                names += ["VSUMD%d" % d for d in windows]
-
-        return fields, names
-
-
-class Alpha158vwap(Alpha158):
-    def get_label_config(self):
-        return ["Ref($vwap, -2)/Ref($vwap, -1) - 1"], ["LABEL0"]
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+from ...data.dataset.handler import DataHandlerLP
+from ...data.dataset.processor import Processor
+from ...utils import get_callable_kwargs
+from ...data.dataset import processor as processor_module
+from inspect import getfullargspec
+
+
+def check_transform_proc(proc_l, fit_start_time, fit_end_time):
+    new_l = []
+    for p in proc_l:
+        if not isinstance(p, Processor):
+            klass, pkwargs = get_callable_kwargs(p, processor_module)
+            args = getfullargspec(klass).args
+            if "fit_start_time" in args and "fit_end_time" in args:
+                assert (
+                    fit_start_time is not None and fit_end_time is not None
+                ), "Make sure `fit_start_time` and `fit_end_time` are not None."
+                pkwargs.update(
+                    {
+                        "fit_start_time": fit_start_time,
+                        "fit_end_time": fit_end_time,
+                    }
+                )
+            proc_config = {"class": klass.__name__, "kwargs": pkwargs}
+            if isinstance(p, dict) and "module_path" in p:
+                proc_config["module_path"] = p["module_path"]
+            new_l.append(proc_config)
+        else:
+            new_l.append(p)
+    return new_l
+
+
+_DEFAULT_LEARN_PROCESSORS = [
+    {"class": "DropnaLabel"},
+    {"class": "CSZScoreNorm", "kwargs": {"fields_group": "label"}},
+]
+_DEFAULT_INFER_PROCESSORS = [
+    {"class": "ProcessInf", "kwargs": {}},
+    {"class": "ZScoreNorm", "kwargs": {}},
+    {"class": "Fillna", "kwargs": {}},
+]
+
+
+class Alpha360(DataHandlerLP):
+    def __init__(
+        self,
+        instruments="csi500",
+        start_time=None,
+        end_time=None,
+        freq="day",
+        infer_processors=_DEFAULT_INFER_PROCESSORS,
+        learn_processors=_DEFAULT_LEARN_PROCESSORS,
+        fit_start_time=None,
+        fit_end_time=None,
+        filter_pipe=None,
+        inst_processors=None,
+        **kwargs
+    ):
+        infer_processors = check_transform_proc(infer_processors, fit_start_time, fit_end_time)
+        learn_processors = check_transform_proc(learn_processors, fit_start_time, fit_end_time)
+
+        data_loader = {
+            "class": "QlibDataLoader",
+            "kwargs": {
+                "config": {
+                    "feature": self.get_feature_config(),
+                    "label": kwargs.pop("label", self.get_label_config()),
+                },
+                "filter_pipe": filter_pipe,
+                "freq": freq,
+                "inst_processors": inst_processors,
+            },
+        }
+
+        super().__init__(
+            instruments=instruments,
+            start_time=start_time,
+            end_time=end_time,
+            data_loader=data_loader,
+            learn_processors=learn_processors,
+            infer_processors=infer_processors,
+            **kwargs
+        )
+
+    def get_label_config(self):
+        return ["Ref($close, -2)/Ref($close, -1) - 1"], ["LABEL0"]
+
+    @staticmethod
+    def get_feature_config():
+        # NOTE:
+        # Alpha360 tries to provide a dataset with original price data
+        # the original price data includes the prices and volume in the last 60 days.
+        # To make it easier to learn models from this dataset, all the prices and volume
+        # are normalized by the latest price and volume data ( dividing by $close, $volume)
+        # So the latest normalized $close will be 1 (with name CLOSE0), the latest normalized $volume will be 1 (with name VOLUME0)
+        # If further normalization are executed (e.g. centralization),  CLOSE0 and VOLUME0 will be 0.
+        fields = []
+        names = []
+
+        for i in range(59, 0, -1):
+            fields += ["Ref($close, %d)/$close" % i]
+            names += ["CLOSE%d" % i]
+        fields += ["$close/$close"]
+        names += ["CLOSE0"]
+        for i in range(59, 0, -1):
+            fields += ["Ref($open, %d)/$close" % i]
+            names += ["OPEN%d" % i]
+        fields += ["$open/$close"]
+        names += ["OPEN0"]
+        for i in range(59, 0, -1):
+            fields += ["Ref($high, %d)/$close" % i]
+            names += ["HIGH%d" % i]
+        fields += ["$high/$close"]
+        names += ["HIGH0"]
+        for i in range(59, 0, -1):
+            fields += ["Ref($low, %d)/$close" % i]
+            names += ["LOW%d" % i]
+        fields += ["$low/$close"]
+        names += ["LOW0"]
+        for i in range(59, 0, -1):
+            fields += ["Ref($vwap, %d)/$close" % i]
+            names += ["VWAP%d" % i]
+        fields += ["$vwap/$close"]
+        names += ["VWAP0"]
+        for i in range(59, 0, -1):
+            fields += ["Ref($volume, %d)/($volume+1e-12)" % i]
+            names += ["VOLUME%d" % i]
+        fields += ["$volume/($volume+1e-12)"]
+        names += ["VOLUME0"]
+
+        return fields, names
+
+
+class Alpha360vwap(Alpha360):
+    def get_label_config(self):
+        return ["Ref($vwap, -2)/Ref($vwap, -1) - 1"], ["LABEL0"]
+
+
+class Alpha158(DataHandlerLP):
+    def __init__(
+        self,
+        instruments="csi500",
+        start_time=None,
+        end_time=None,
+        freq="day",
+        infer_processors=[],
+        learn_processors=_DEFAULT_LEARN_PROCESSORS,
+        fit_start_time=None,
+        fit_end_time=None,
+        process_type=DataHandlerLP.PTYPE_A,
+        filter_pipe=None,
+        inst_processors=None,
+        **kwargs
+    ):
+        infer_processors = check_transform_proc(infer_processors, fit_start_time, fit_end_time)
+        learn_processors = check_transform_proc(learn_processors, fit_start_time, fit_end_time)
+
+        data_loader = {
+            "class": "QlibDataLoader",
+            "kwargs": {
+                "config": {
+                    "feature": self.get_feature_config(),
+                    "label": kwargs.pop("label", self.get_label_config()),
+                },
+                "filter_pipe": filter_pipe,
+                "freq": freq,
+                "inst_processors": inst_processors,
+            },
+        }
+        super().__init__(
+            instruments=instruments,
+            start_time=start_time,
+            end_time=end_time,
+            data_loader=data_loader,
+            infer_processors=infer_processors,
+            learn_processors=learn_processors,
+            process_type=process_type,
+            **kwargs
+        )
+
+    def get_feature_config(self):
+        conf = {
+            "kbar": {},
+            "price": {
+                "windows": [0],
+                "feature": ["OPEN", "HIGH", "LOW", "VWAP"],
+            },
+            "rolling": {},
+        }
+        return self.parse_config_to_fields(conf)
+
+    def get_label_config(self):
+        return ["Ref($close, -2)/Ref($close, -1) - 1"], ["LABEL0"]
+
+    @staticmethod
+    def parse_config_to_fields(config):
+        """create factors from config
+
+        config = {
+            'kbar': {}, # whether to use some hard-code kbar features
+            'price': { # whether to use raw price features
+                'windows': [0, 1, 2, 3, 4], # use price at n days ago
+                'feature': ['OPEN', 'HIGH', 'LOW'] # which price field to use
+            },
+            'volume': { # whether to use raw volume features
+                'windows': [0, 1, 2, 3, 4], # use volume at n days ago
+            },
+            'rolling': { # whether to use rolling operator based features
+                'windows': [5, 10, 20, 30, 60], # rolling windows size
+                'include': ['ROC', 'MA', 'STD'], # rolling operator to use
+                #if include is None we will use default operators
+                'exclude': ['RANK'], # rolling operator not to use
+            }
+        }
+        """
+        fields = []
+        names = []
+        if "kbar" in config:
+            fields += [
+                "($close-$open)/$open",
+                "($high-$low)/$open",
+                "($close-$open)/($high-$low+1e-12)",
+                "($high-Greater($open, $close))/$open",
+                "($high-Greater($open, $close))/($high-$low+1e-12)",
+                "(Less($open, $close)-$low)/$open",
+                "(Less($open, $close)-$low)/($high-$low+1e-12)",
+                "(2*$close-$high-$low)/$open",
+                "(2*$close-$high-$low)/($high-$low+1e-12)",
+            ]
+            names += [
+                "KMID",
+                "KLEN",
+                "KMID2",
+                "KUP",
+                "KUP2",
+                "KLOW",
+                "KLOW2",
+                "KSFT",
+                "KSFT2",
+            ]
+        if "price" in config:
+            windows = config["price"].get("windows", range(5))
+            feature = config["price"].get("feature", ["OPEN", "HIGH", "LOW", "CLOSE", "VWAP"])
+            for field in feature:
+                field = field.lower()
+                fields += ["Ref($%s, %d)/$close" % (field, d) if d != 0 else "$%s/$close" % field for d in windows]
+                names += [field.upper() + str(d) for d in windows]
+        if "volume" in config:
+            windows = config["volume"].get("windows", range(5))
+            fields += ["Ref($volume, %d)/($volume+1e-12)" % d if d != 0 else "$volume/($volume+1e-12)" for d in windows]
+            names += ["VOLUME" + str(d) for d in windows]
+        if "rolling" in config:
+            windows = config["rolling"].get("windows", [5, 10, 20, 30, 60])
+            include = config["rolling"].get("include", None)
+            exclude = config["rolling"].get("exclude", [])
+            # `exclude` in dataset config unnecessary filed
+            # `include` in dataset config necessary field
+
+            def use(x):
+                return x not in exclude and (include is None or x in include)
+
+            # Some factor ref: https://guorn.com/static/upload/file/3/134065454575605.pdf
+            if use("ROC"):
+                # https://www.investopedia.com/terms/r/rateofchange.asp
+                # Rate of change, the price change in the past d days, divided by latest close price to remove unit
+                fields += ["Ref($close, %d)/$close" % d for d in windows]
+                names += ["ROC%d" % d for d in windows]
+            if use("MA"):
+                # https://www.investopedia.com/ask/answers/071414/whats-difference-between-moving-average-and-weighted-moving-average.asp
+                # Simple Moving Average, the simple moving average in the past d days, divided by latest close price to remove unit
+                fields += ["Mean($close, %d)/$close" % d for d in windows]
+                names += ["MA%d" % d for d in windows]
+            if use("STD"):
+                # The standard diviation of close price for the past d days, divided by latest close price to remove unit
+                fields += ["Std($close, %d)/$close" % d for d in windows]
+                names += ["STD%d" % d for d in windows]
+            if use("BETA"):
+                # The rate of close price change in the past d days, divided by latest close price to remove unit
+                # For example, price increase 10 dollar per day in the past d days, then Slope will be 10.
+                fields += ["Slope($close, %d)/$close" % d for d in windows]
+                names += ["BETA%d" % d for d in windows]
+            if use("RSQR"):
+                # The R-sqaure value of linear regression for the past d days, represent the trend linear
+                fields += ["Rsquare($close, %d)" % d for d in windows]
+                names += ["RSQR%d" % d for d in windows]
+            if use("RESI"):
+                # The redisdual for linear regression for the past d days, represent the trend linearity for past d days.
+                fields += ["Resi($close, %d)/$close" % d for d in windows]
+                names += ["RESI%d" % d for d in windows]
+            if use("MAX"):
+                # The max price for past d days, divided by latest close price to remove unit
+                fields += ["Max($high, %d)/$close" % d for d in windows]
+                names += ["MAX%d" % d for d in windows]
+            if use("LOW"):
+                # The low price for past d days, divided by latest close price to remove unit
+                fields += ["Min($low, %d)/$close" % d for d in windows]
+                names += ["MIN%d" % d for d in windows]
+            if use("QTLU"):
+                # The 80% quantile of past d day's close price, divided by latest close price to remove unit
+                # Used with MIN and MAX
+                fields += ["Quantile($close, %d, 0.8)/$close" % d for d in windows]
+                names += ["QTLU%d" % d for d in windows]
+            if use("QTLD"):
+                # The 20% quantile of past d day's close price, divided by latest close price to remove unit
+                fields += ["Quantile($close, %d, 0.2)/$close" % d for d in windows]
+                names += ["QTLD%d" % d for d in windows]
+            if use("RANK"):
+                # Get the percentile of current close price in past d day's close price.
+                # Represent the current price level comparing to past N days, add additional information to moving average.
+                fields += ["Rank($close, %d)" % d for d in windows]
+                names += ["RANK%d" % d for d in windows]
+            if use("RSV"):
+                # Represent the price position between upper and lower resistent price for past d days.
+                fields += ["($close-Min($low, %d))/(Max($high, %d)-Min($low, %d)+1e-12)" % (d, d, d) for d in windows]
+                names += ["RSV%d" % d for d in windows]
+            if use("IMAX"):
+                # The number of days between current date and previous highest price date.
+                # Part of Aroon Indicator https://www.investopedia.com/terms/a/aroon.asp
+                # The indicator measures the time between highs and the time between lows over a time period.
+                # The idea is that strong uptrends will regularly see new highs, and strong downtrends will regularly see new lows.
+                fields += ["IdxMax($high, %d)/%d" % (d, d) for d in windows]
+                names += ["IMAX%d" % d for d in windows]
+            if use("IMIN"):
+                # The number of days between current date and previous lowest price date.
+                # Part of Aroon Indicator https://www.investopedia.com/terms/a/aroon.asp
+                # The indicator measures the time between highs and the time between lows over a time period.
+                # The idea is that strong uptrends will regularly see new highs, and strong downtrends will regularly see new lows.
+                fields += ["IdxMin($low, %d)/%d" % (d, d) for d in windows]
+                names += ["IMIN%d" % d for d in windows]
+            if use("IMXD"):
+                # The time period between previous lowest-price date occur after highest price date.
+                # Large value suggest downward momemtum.
+                fields += ["(IdxMax($high, %d)-IdxMin($low, %d))/%d" % (d, d, d) for d in windows]
+                names += ["IMXD%d" % d for d in windows]
+            if use("CORR"):
+                # The correlation between absolute close price and log scaled trading volume
+                fields += ["Corr($close, Log($volume+1), %d)" % d for d in windows]
+                names += ["CORR%d" % d for d in windows]
+            if use("CORD"):
+                # The correlation between price change ratio and volume change ratio
+                fields += ["Corr($close/Ref($close,1), Log($volume/Ref($volume, 1)+1), %d)" % d for d in windows]
+                names += ["CORD%d" % d for d in windows]
+            if use("CNTP"):
+                # The percentage of days in past d days that price go up.
+                fields += ["Mean($close>Ref($close, 1), %d)" % d for d in windows]
+                names += ["CNTP%d" % d for d in windows]
+            if use("CNTN"):
+                # The percentage of days in past d days that price go down.
+                fields += ["Mean($close<Ref($close, 1), %d)" % d for d in windows]
+                names += ["CNTN%d" % d for d in windows]
+            if use("CNTD"):
+                # The diff between past up day and past down day
+                fields += ["Mean($close>Ref($close, 1), %d)-Mean($close<Ref($close, 1), %d)" % (d, d) for d in windows]
+                names += ["CNTD%d" % d for d in windows]
+            if use("SUMP"):
+                # The total gain / the absolute total price changed
+                # Similar to RSI indicator. https://www.investopedia.com/terms/r/rsi.asp
+                fields += [
+                    "Sum(Greater($close-Ref($close, 1), 0), %d)/(Sum(Abs($close-Ref($close, 1)), %d)+1e-12)" % (d, d)
+                    for d in windows
+                ]
+                names += ["SUMP%d" % d for d in windows]
+            if use("SUMN"):
+                # The total lose / the absolute total price changed
+                # Can be derived from SUMP by SUMN = 1 - SUMP
+                # Similar to RSI indicator. https://www.investopedia.com/terms/r/rsi.asp
+                fields += [
+                    "Sum(Greater(Ref($close, 1)-$close, 0), %d)/(Sum(Abs($close-Ref($close, 1)), %d)+1e-12)" % (d, d)
+                    for d in windows
+                ]
+                names += ["SUMN%d" % d for d in windows]
+            if use("SUMD"):
+                # The diff ratio between total gain and total lose
+                # Similar to RSI indicator. https://www.investopedia.com/terms/r/rsi.asp
+                fields += [
+                    "(Sum(Greater($close-Ref($close, 1), 0), %d)-Sum(Greater(Ref($close, 1)-$close, 0), %d))"
+                    "/(Sum(Abs($close-Ref($close, 1)), %d)+1e-12)" % (d, d, d)
+                    for d in windows
+                ]
+                names += ["SUMD%d" % d for d in windows]
+            if use("VMA"):
+                # Simple Volume Moving average: https://www.barchart.com/education/technical-indicators/volume_moving_average
+                fields += ["Mean($volume, %d)/($volume+1e-12)" % d for d in windows]
+                names += ["VMA%d" % d for d in windows]
+            if use("VSTD"):
+                # The standard deviation for volume in past d days.
+                fields += ["Std($volume, %d)/($volume+1e-12)" % d for d in windows]
+                names += ["VSTD%d" % d for d in windows]
+            if use("WVMA"):
+                # The volume weighted price change volatility
+                fields += [
+                    "Std(Abs($close/Ref($close, 1)-1)*$volume, %d)/(Mean(Abs($close/Ref($close, 1)-1)*$volume, %d)+1e-12)"
+                    % (d, d)
+                    for d in windows
+                ]
+                names += ["WVMA%d" % d for d in windows]
+            if use("VSUMP"):
+                # The total volume increase / the absolute total volume changed
+                fields += [
+                    "Sum(Greater($volume-Ref($volume, 1), 0), %d)/(Sum(Abs($volume-Ref($volume, 1)), %d)+1e-12)"
+                    % (d, d)
+                    for d in windows
+                ]
+                names += ["VSUMP%d" % d for d in windows]
+            if use("VSUMN"):
+                # The total volume increase / the absolute total volume changed
+                # Can be derived from VSUMP by VSUMN = 1 - VSUMP
+                fields += [
+                    "Sum(Greater(Ref($volume, 1)-$volume, 0), %d)/(Sum(Abs($volume-Ref($volume, 1)), %d)+1e-12)"
+                    % (d, d)
+                    for d in windows
+                ]
+                names += ["VSUMN%d" % d for d in windows]
+            if use("VSUMD"):
+                # The diff ratio between total volume increase and total volume decrease
+                # RSI indicator for volume
+                fields += [
+                    "(Sum(Greater($volume-Ref($volume, 1), 0), %d)-Sum(Greater(Ref($volume, 1)-$volume, 0), %d))"
+                    "/(Sum(Abs($volume-Ref($volume, 1)), %d)+1e-12)" % (d, d, d)
+                    for d in windows
+                ]
+                names += ["VSUMD%d" % d for d in windows]
+
+        return fields, names
+
+
+class Alpha158vwap(Alpha158):
+    def get_label_config(self):
+        return ["Ref($vwap, -2)/Ref($vwap, -1) - 1"], ["LABEL0"]
```

## qlib/contrib/data/highfreq_handler.py

```diff
@@ -1,541 +1,539 @@
-from qlib.data.dataset.handler import DataHandler, DataHandlerLP
-
-from .handler import check_transform_proc
-
-EPSILON = 1e-4
-
-
-class HighFreqHandler(DataHandlerLP):
-    def __init__(
-        self,
-        instruments="csi300",
-        start_time=None,
-        end_time=None,
-        infer_processors=[],
-        learn_processors=[],
-        fit_start_time=None,
-        fit_end_time=None,
-        drop_raw=True,
-    ):
-
-        infer_processors = check_transform_proc(infer_processors, fit_start_time, fit_end_time)
-        learn_processors = check_transform_proc(learn_processors, fit_start_time, fit_end_time)
-
-        data_loader = {
-            "class": "QlibDataLoader",
-            "kwargs": {
-                "config": self.get_feature_config(),
-                "swap_level": False,
-                "freq": "1min",
-            },
-        }
-        super().__init__(
-            instruments=instruments,
-            start_time=start_time,
-            end_time=end_time,
-            data_loader=data_loader,
-            infer_processors=infer_processors,
-            learn_processors=learn_processors,
-            drop_raw=drop_raw,
-        )
-
-    def get_feature_config(self):
-        fields = []
-        names = []
-
-        template_if = "If(IsNull({1}), {0}, {1})"
-        template_paused = "Select(Gt($paused_num, 1.001), {0})"
-
-        def get_normalized_price_feature(price_field, shift=0):
-            # norm with the close price of 237th minute of yesterday.
-            if shift == 0:
-                template_norm = "{0}/DayLast(Ref({1}, 243))"
-            else:
-                template_norm = "Ref({0}, " + str(shift) + ")/DayLast(Ref({1}, 243))"
-
-            template_fillnan = "FFillNan({0})"
-            # calculate -> ffill -> remove paused
-            feature_ops = template_paused.format(
-                template_fillnan.format(
-                    template_norm.format(template_if.format("$close", price_field), template_fillnan.format("$close"))
-                )
-            )
-            return feature_ops
-
-        fields += [get_normalized_price_feature("$open", 0)]
-        fields += [get_normalized_price_feature("$high", 0)]
-        fields += [get_normalized_price_feature("$low", 0)]
-        fields += [get_normalized_price_feature("$close", 0)]
-        fields += [get_normalized_price_feature("$vwap", 0)]
-        names += ["$open", "$high", "$low", "$close", "$vwap"]
-
-        fields += [get_normalized_price_feature("$open", 240)]
-        fields += [get_normalized_price_feature("$high", 240)]
-        fields += [get_normalized_price_feature("$low", 240)]
-        fields += [get_normalized_price_feature("$close", 240)]
-        fields += [get_normalized_price_feature("$vwap", 240)]
-        names += ["$open_1", "$high_1", "$low_1", "$close_1", "$vwap_1"]
-
-        # calculate and fill nan with 0
-        template_gzero = "If(Ge({0}, 0), {0}, 0)"
-        fields += [
-            template_gzero.format(
-                template_paused.format(
-                    "If(IsNull({0}), 0, {0})".format("{0}/Ref(DayLast(Mean({0}, 7200)), 240)".format("$volume"))
-                )
-            )
-        ]
-        names += ["$volume"]
-
-        fields += [
-            template_gzero.format(
-                template_paused.format(
-                    "If(IsNull({0}), 0, {0})".format(
-                        "Ref({0}, 240)/Ref(DayLast(Mean({0}, 7200)), 240)".format("$volume")
-                    )
-                )
-            )
-        ]
-        names += ["$volume_1"]
-
-        return fields, names
-
-
-class HighFreqGeneralHandler(DataHandlerLP):
-    def __init__(
-        self,
-        instruments="csi300",
-        start_time=None,
-        end_time=None,
-        infer_processors=[],
-        learn_processors=[],
-        fit_start_time=None,
-        fit_end_time=None,
-        drop_raw=True,
-        day_length=240,
-        freq="1min",
-        columns=["$open", "$high", "$low", "$close", "$vwap"],
-        inst_processors=None,
-    ):
-        self.day_length = day_length
-        self.columns = columns
-
-        infer_processors = check_transform_proc(infer_processors, fit_start_time, fit_end_time)
-        learn_processors = check_transform_proc(learn_processors, fit_start_time, fit_end_time)
-
-        data_loader = {
-            "class": "QlibDataLoader",
-            "kwargs": {
-                "config": self.get_feature_config(),
-                "swap_level": False,
-                "freq": freq,
-                "inst_processors": inst_processors,
-            },
-        }
-        super().__init__(
-            instruments=instruments,
-            start_time=start_time,
-            end_time=end_time,
-            data_loader=data_loader,
-            infer_processors=infer_processors,
-            learn_processors=learn_processors,
-            drop_raw=drop_raw,
-        )
-
-    def get_feature_config(self):
-        fields = []
-        names = []
-
-        template_if = "If(IsNull({1}), {0}, {1})"
-        template_paused = f"Cut({{0}}, {self.day_length * 2}, None)"
-
-        def get_normalized_price_feature(price_field, shift=0):
-            # norm with the close price of 237th minute of yesterday.
-            if shift == 0:
-                template_norm = f"{{0}}/DayLast(Ref({{1}}, {self.day_length * 2}))"
-            else:
-                template_norm = f"Ref({{0}}, " + str(shift) + f")/DayLast(Ref({{1}}, {self.day_length}))"
-
-            template_fillnan = "FFillNan({0})"
-            # calculate -> ffill -> remove paused
-            feature_ops = template_paused.format(
-                template_fillnan.format(
-                    template_norm.format(template_if.format("$close", price_field), template_fillnan.format("$close"))
-                )
-            )
-            return feature_ops
-
-        for column_name in self.columns:
-            fields.append(get_normalized_price_feature(column_name, 0))
-            names.append(column_name)
-
-        for column_name in self.columns:
-            fields.append(get_normalized_price_feature(column_name, self.day_length))
-            names.append(column_name + "_1")
-
-        # calculate and fill nan with 0
-        fields += [
-            template_paused.format(
-                "If(IsNull({0}), 0, {0})".format(
-                    f"{{0}}/Ref(DayLast(Mean({{0}}, {self.day_length * 30})), {self.day_length})".format("$volume")
-                )
-            )
-        ]
-        names += ["$volume"]
-
-        fields += [
-            template_paused.format(
-                "If(IsNull({0}), 0, {0})".format(
-                    f"Ref({{0}}, {self.day_length})/Ref(DayLast(Mean({{0}}, {self.day_length * 30})), {self.day_length})".format(
-                        "$volume"
-                    )
-                )
-            )
-        ]
-        names += ["$volume_1"]
-
-        return fields, names
-
-
-class HighFreqBacktestHandler(DataHandler):
-    def __init__(
-        self,
-        instruments="csi300",
-        start_time=None,
-        end_time=None,
-    ):
-        data_loader = {
-            "class": "QlibDataLoader",
-            "kwargs": {
-                "config": self.get_feature_config(),
-                "swap_level": False,
-                "freq": "1min",
-            },
-        }
-        super().__init__(
-            instruments=instruments,
-            start_time=start_time,
-            end_time=end_time,
-            data_loader=data_loader,
-        )
-
-    def get_feature_config(self):
-        fields = []
-        names = []
-
-        template_if = "If(IsNull({1}), {0}, {1})"
-        template_paused = "Select(Gt($paused_num, 1.001), {0})"
-        template_fillnan = "FFillNan({0})"
-        fields += [
-            template_fillnan.format(template_paused.format("$close")),
-        ]
-        names += ["$close0"]
-
-        fields += [
-            template_paused.format(
-                template_if.format(
-                    template_fillnan.format("$close"),
-                    "$vwap",
-                )
-            )
-        ]
-        names += ["$vwap0"]
-
-        fields += [template_paused.format("If(IsNull({0}), 0, {0})".format("$volume"))]
-        names += ["$volume0"]
-
-        fields += [template_paused.format("If(IsNull({0}), 0, {0})".format("$factor"))]
-        names += ["$factor0"]
-
-        return fields, names
-
-
-class HighFreqGeneralBacktestHandler(DataHandler):
-    def __init__(
-        self,
-        instruments="csi300",
-        start_time=None,
-        end_time=None,
-        day_length=240,
-        freq="1min",
-        columns=["$close", "$vwap", "$volume"],
-        inst_processors=None,
-    ):
-        self.day_length = day_length
-        self.columns = set(columns)
-        data_loader = {
-            "class": "QlibDataLoader",
-            "kwargs": {
-                "config": self.get_feature_config(),
-                "swap_level": False,
-                "freq": freq,
-                "inst_processors": inst_processors,
-            },
-        }
-        super().__init__(
-            instruments=instruments,
-            start_time=start_time,
-            end_time=end_time,
-            data_loader=data_loader,
-        )
-
-    def get_feature_config(self):
-        fields = []
-        names = []
-
-        if "$close" in self.columns:
-            template_paused = f"Cut({{0}}, {self.day_length * 2}, None)"
-            template_fillnan = "FFillNan({0})"
-            template_if = "If(IsNull({1}), {0}, {1})"
-            fields += [
-                template_paused.format(template_fillnan.format("$close")),
-            ]
-            names += ["$close0"]
-
-        if "$vwap" in self.columns:
-            fields += [
-                template_paused.format(template_if.format(template_fillnan.format("$close"), "$vwap")),
-            ]
-            names += ["$vwap0"]
-
-        if "$volume" in self.columns:
-            fields += [template_paused.format("If(IsNull({0}), 0, {0})".format("$volume"))]
-            names += ["$volume0"]
-
-        return fields, names
-
-
-class HighFreqOrderHandler(DataHandlerLP):
-    def __init__(
-        self,
-        instruments="csi300",
-        start_time=None,
-        end_time=None,
-        infer_processors=[],
-        learn_processors=[],
-        fit_start_time=None,
-        fit_end_time=None,
-        inst_processors=None,
-        drop_raw=True,
-    ):
-
-        infer_processors = check_transform_proc(infer_processors, fit_start_time, fit_end_time)
-        learn_processors = check_transform_proc(learn_processors, fit_start_time, fit_end_time)
-
-        data_loader = {
-            "class": "QlibDataLoader",
-            "kwargs": {
-                "config": self.get_feature_config(),
-                "swap_level": False,
-                "freq": "1min",
-                "inst_processors": inst_processors,
-            },
-        }
-        super().__init__(
-            instruments=instruments,
-            start_time=start_time,
-            end_time=end_time,
-            data_loader=data_loader,
-            infer_processors=infer_processors,
-            learn_processors=learn_processors,
-            drop_raw=drop_raw,
-        )
-
-    def get_feature_config(self):
-        fields = []
-        names = []
-
-        template_if = "If(IsNull({1}), {0}, {1})"
-        template_ifinf = "If(IsInf({1}), {0}, {1})"
-        template_paused = "Select(Gt($paused_num, 1.001), {0})"
-
-        def get_normalized_price_feature(price_field, shift=0):
-            # norm with the close price of 237th minute of yesterday.
-            if shift == 0:
-                template_norm = "{0}/DayLast(Ref({1}, 243))"
-            else:
-                template_norm = "Ref({0}, " + str(shift) + ")/DayLast(Ref({1}, 243))"
-
-            template_fillnan = "FFillNan({0})"
-            # calculate -> ffill -> remove paused
-            feature_ops = template_paused.format(
-                template_fillnan.format(
-                    template_norm.format(template_if.format("$close", price_field), template_fillnan.format("$close"))
-                )
-            )
-            return feature_ops
-
-        def get_normalized_vwap_price_feature(price_field, shift=0):
-            # norm with the close price of 237th minute of yesterday.
-            if shift == 0:
-                template_norm = "{0}/DayLast(Ref({1}, 243))"
-            else:
-                template_norm = "Ref({0}, " + str(shift) + ")/DayLast(Ref({1}, 243))"
-
-            template_fillnan = "FFillNan({0})"
-            # calculate -> ffill -> remove paused
-            feature_ops = template_paused.format(
-                template_fillnan.format(
-                    template_norm.format(
-                        template_if.format("$close", template_ifinf.format("$close", price_field)),
-                        template_fillnan.format("$close"),
-                    )
-                )
-            )
-            return feature_ops
-
-        fields += [get_normalized_price_feature("$open", 0)]
-        fields += [get_normalized_price_feature("$high", 0)]
-        fields += [get_normalized_price_feature("$low", 0)]
-        fields += [get_normalized_price_feature("$close", 0)]
-        fields += [get_normalized_vwap_price_feature("$vwap", 0)]
-        names += ["$open", "$high", "$low", "$close", "$vwap"]
-
-        fields += [get_normalized_price_feature("$open", 240)]
-        fields += [get_normalized_price_feature("$high", 240)]
-        fields += [get_normalized_price_feature("$low", 240)]
-        fields += [get_normalized_price_feature("$close", 240)]
-        fields += [get_normalized_vwap_price_feature("$vwap", 240)]
-        names += ["$open_1", "$high_1", "$low_1", "$close_1", "$vwap_1"]
-
-        fields += [get_normalized_price_feature("$bid", 0)]
-        fields += [get_normalized_price_feature("$ask", 0)]
-        names += ["$bid", "$ask"]
-
-        fields += [get_normalized_price_feature("$bid", 240)]
-        fields += [get_normalized_price_feature("$ask", 240)]
-        names += ["$bid_1", "$ask_1"]
-
-        # calculate and fill nan with 0
-
-        def get_volume_feature(volume_field, shift=0):
-            template_gzero = "If(Ge({0}, 0), {0}, 0)"
-            if shift == 0:
-                feature_ops = template_gzero.format(
-                    template_paused.format(
-                        "If(IsInf({0}), 0, {0})".format(
-                            "If(IsNull({0}), 0, {0})".format(
-                                "{0}/Ref(DayLast(Mean({0}, 7200)), 240)".format(volume_field)
-                            )
-                        )
-                    )
-                )
-            else:
-                feature_ops = template_gzero.format(
-                    template_paused.format(
-                        "If(IsInf({0}), 0, {0})".format(
-                            "If(IsNull({0}), 0, {0})".format(
-                                f"Ref({{0}}, {shift})/Ref(DayLast(Mean({{0}}, 7200)), 240)".format(volume_field)
-                            )
-                        )
-                    )
-                )
-            return feature_ops
-
-        fields += [get_volume_feature("$volume", 0)]
-        names += ["$volume"]
-
-        fields += [get_volume_feature("$volume", 240)]
-        names += ["$volume_1"]
-
-        fields += [get_volume_feature("$bidV", 0)]
-        fields += [get_volume_feature("$bidV1", 0)]
-        fields += [get_volume_feature("$bidV3", 0)]
-        fields += [get_volume_feature("$bidV5", 0)]
-        fields += [get_volume_feature("$askV", 0)]
-        fields += [get_volume_feature("$askV1", 0)]
-        fields += [get_volume_feature("$askV3", 0)]
-        fields += [get_volume_feature("$askV5", 0)]
-        names += ["$bidV", "$bidV1", "$bidV3", "$bidV5", "$askV", "$askV1", "$askV3", "$askV5"]
-
-        fields += [get_volume_feature("$bidV", 240)]
-        fields += [get_volume_feature("$bidV1", 240)]
-        fields += [get_volume_feature("$bidV3", 240)]
-        fields += [get_volume_feature("$bidV5", 240)]
-        fields += [get_volume_feature("$askV", 240)]
-        fields += [get_volume_feature("$askV1", 240)]
-        fields += [get_volume_feature("$askV3", 240)]
-        fields += [get_volume_feature("$askV5", 240)]
-        names += ["$bidV_1", "$bidV1_1", "$bidV3_1", "$bidV5_1", "$askV_1", "$askV1_1", "$askV3_1", "$askV5_1"]
-
-        return fields, names
-
-
-class HighFreqBacktestOrderHandler(DataHandler):
-    def __init__(
-        self,
-        instruments="csi300",
-        start_time=None,
-        end_time=None,
-    ):
-        data_loader = {
-            "class": "QlibDataLoader",
-            "kwargs": {
-                "config": self.get_feature_config(),
-                "swap_level": False,
-                "freq": "1min",
-            },
-        }
-        super().__init__(
-            instruments=instruments,
-            start_time=start_time,
-            end_time=end_time,
-            data_loader=data_loader,
-        )
-
-    def get_feature_config(self):
-        fields = []
-        names = []
-
-        template_if = "If(IsNull({1}), {0}, {1})"
-        template_paused = "Select(Gt($paused_num, 1.001), {0})"
-        template_fillnan = "FFillNan({0})"
-        fields += [
-            template_fillnan.format(template_paused.format("$close")),
-        ]
-        names += ["$close0"]
-
-        fields += [
-            template_paused.format(
-                template_if.format(
-                    template_fillnan.format("$close"),
-                    "$vwap",
-                )
-            )
-        ]
-        names += ["$vwap0"]
-
-        fields += [template_paused.format("If(IsNull({0}), 0, {0})".format("$volume"))]
-        names += ["$volume0"]
-
-        fields += [template_paused.format("If(IsNull({0}), 0, {0})".format("$bid"))]
-        names += ["$bid0"]
-
-        fields += [template_paused.format("If(IsNull({0}), 0, {0})".format("$bidV"))]
-        names += ["$bidV0"]
-
-        fields += [template_paused.format("If(IsNull({0}), 0, {0})".format("$ask"))]
-        names += ["$ask0"]
-
-        fields += [template_paused.format("If(IsNull({0}), 0, {0})".format("$askV"))]
-        names += ["$askV0"]
-
-        fields += [template_paused.format("If(IsNull({0}), 0, {0})".format("($bid + $ask) / 2"))]
-        names += ["$median0"]
-
-        fields += [template_paused.format("If(IsNull({0}), 0, {0})".format("$factor"))]
-        names += ["$factor0"]
-
-        fields += [template_paused.format("If(IsNull({0}), 0, {0})".format("$downlimitmarket"))]
-        names += ["$downlimitmarket0"]
-
-        fields += [template_paused.format("If(IsNull({0}), 0, {0})".format("$uplimitmarket"))]
-        names += ["$uplimitmarket0"]
-
-        fields += [template_paused.format("If(IsNull({0}), 0, {0})".format("$highmarket"))]
-        names += ["$highmarket0"]
-
-        fields += [template_paused.format("If(IsNull({0}), 0, {0})".format("$lowmarket"))]
-        names += ["$lowmarket0"]
-
-        return fields, names
+from qlib.data.dataset.handler import DataHandler, DataHandlerLP
+
+from .handler import check_transform_proc
+
+EPSILON = 1e-4
+
+
+class HighFreqHandler(DataHandlerLP):
+    def __init__(
+        self,
+        instruments="csi300",
+        start_time=None,
+        end_time=None,
+        infer_processors=[],
+        learn_processors=[],
+        fit_start_time=None,
+        fit_end_time=None,
+        drop_raw=True,
+    ):
+        infer_processors = check_transform_proc(infer_processors, fit_start_time, fit_end_time)
+        learn_processors = check_transform_proc(learn_processors, fit_start_time, fit_end_time)
+
+        data_loader = {
+            "class": "QlibDataLoader",
+            "kwargs": {
+                "config": self.get_feature_config(),
+                "swap_level": False,
+                "freq": "1min",
+            },
+        }
+        super().__init__(
+            instruments=instruments,
+            start_time=start_time,
+            end_time=end_time,
+            data_loader=data_loader,
+            infer_processors=infer_processors,
+            learn_processors=learn_processors,
+            drop_raw=drop_raw,
+        )
+
+    def get_feature_config(self):
+        fields = []
+        names = []
+
+        template_if = "If(IsNull({1}), {0}, {1})"
+        template_paused = "Select(Gt($paused_num, 1.001), {0})"
+
+        def get_normalized_price_feature(price_field, shift=0):
+            # norm with the close price of 237th minute of yesterday.
+            if shift == 0:
+                template_norm = "{0}/DayLast(Ref({1}, 243))"
+            else:
+                template_norm = "Ref({0}, " + str(shift) + ")/DayLast(Ref({1}, 243))"
+
+            template_fillnan = "FFillNan({0})"
+            # calculate -> ffill -> remove paused
+            feature_ops = template_paused.format(
+                template_fillnan.format(
+                    template_norm.format(template_if.format("$close", price_field), template_fillnan.format("$close"))
+                )
+            )
+            return feature_ops
+
+        fields += [get_normalized_price_feature("$open", 0)]
+        fields += [get_normalized_price_feature("$high", 0)]
+        fields += [get_normalized_price_feature("$low", 0)]
+        fields += [get_normalized_price_feature("$close", 0)]
+        fields += [get_normalized_price_feature("$vwap", 0)]
+        names += ["$open", "$high", "$low", "$close", "$vwap"]
+
+        fields += [get_normalized_price_feature("$open", 240)]
+        fields += [get_normalized_price_feature("$high", 240)]
+        fields += [get_normalized_price_feature("$low", 240)]
+        fields += [get_normalized_price_feature("$close", 240)]
+        fields += [get_normalized_price_feature("$vwap", 240)]
+        names += ["$open_1", "$high_1", "$low_1", "$close_1", "$vwap_1"]
+
+        # calculate and fill nan with 0
+        template_gzero = "If(Ge({0}, 0), {0}, 0)"
+        fields += [
+            template_gzero.format(
+                template_paused.format(
+                    "If(IsNull({0}), 0, {0})".format("{0}/Ref(DayLast(Mean({0}, 7200)), 240)".format("$volume"))
+                )
+            )
+        ]
+        names += ["$volume"]
+
+        fields += [
+            template_gzero.format(
+                template_paused.format(
+                    "If(IsNull({0}), 0, {0})".format(
+                        "Ref({0}, 240)/Ref(DayLast(Mean({0}, 7200)), 240)".format("$volume")
+                    )
+                )
+            )
+        ]
+        names += ["$volume_1"]
+
+        return fields, names
+
+
+class HighFreqGeneralHandler(DataHandlerLP):
+    def __init__(
+        self,
+        instruments="csi300",
+        start_time=None,
+        end_time=None,
+        infer_processors=[],
+        learn_processors=[],
+        fit_start_time=None,
+        fit_end_time=None,
+        drop_raw=True,
+        day_length=240,
+        freq="1min",
+        columns=["$open", "$high", "$low", "$close", "$vwap"],
+        inst_processors=None,
+    ):
+        self.day_length = day_length
+        self.columns = columns
+
+        infer_processors = check_transform_proc(infer_processors, fit_start_time, fit_end_time)
+        learn_processors = check_transform_proc(learn_processors, fit_start_time, fit_end_time)
+
+        data_loader = {
+            "class": "QlibDataLoader",
+            "kwargs": {
+                "config": self.get_feature_config(),
+                "swap_level": False,
+                "freq": freq,
+                "inst_processors": inst_processors,
+            },
+        }
+        super().__init__(
+            instruments=instruments,
+            start_time=start_time,
+            end_time=end_time,
+            data_loader=data_loader,
+            infer_processors=infer_processors,
+            learn_processors=learn_processors,
+            drop_raw=drop_raw,
+        )
+
+    def get_feature_config(self):
+        fields = []
+        names = []
+
+        template_if = "If(IsNull({1}), {0}, {1})"
+        template_paused = f"Cut({{0}}, {self.day_length * 2}, None)"
+
+        def get_normalized_price_feature(price_field, shift=0):
+            # norm with the close price of 237th minute of yesterday.
+            if shift == 0:
+                template_norm = f"{{0}}/DayLast(Ref({{1}}, {self.day_length * 2}))"
+            else:
+                template_norm = f"Ref({{0}}, " + str(shift) + f")/DayLast(Ref({{1}}, {self.day_length}))"
+
+            template_fillnan = "FFillNan({0})"
+            # calculate -> ffill -> remove paused
+            feature_ops = template_paused.format(
+                template_fillnan.format(
+                    template_norm.format(template_if.format("$close", price_field), template_fillnan.format("$close"))
+                )
+            )
+            return feature_ops
+
+        for column_name in self.columns:
+            fields.append(get_normalized_price_feature(column_name, 0))
+            names.append(column_name)
+
+        for column_name in self.columns:
+            fields.append(get_normalized_price_feature(column_name, self.day_length))
+            names.append(column_name + "_1")
+
+        # calculate and fill nan with 0
+        fields += [
+            template_paused.format(
+                "If(IsNull({0}), 0, {0})".format(
+                    f"{{0}}/Ref(DayLast(Mean({{0}}, {self.day_length * 30})), {self.day_length})".format("$volume")
+                )
+            )
+        ]
+        names += ["$volume"]
+
+        fields += [
+            template_paused.format(
+                "If(IsNull({0}), 0, {0})".format(
+                    f"Ref({{0}}, {self.day_length})/Ref(DayLast(Mean({{0}}, {self.day_length * 30})), {self.day_length})".format(
+                        "$volume"
+                    )
+                )
+            )
+        ]
+        names += ["$volume_1"]
+
+        return fields, names
+
+
+class HighFreqBacktestHandler(DataHandler):
+    def __init__(
+        self,
+        instruments="csi300",
+        start_time=None,
+        end_time=None,
+    ):
+        data_loader = {
+            "class": "QlibDataLoader",
+            "kwargs": {
+                "config": self.get_feature_config(),
+                "swap_level": False,
+                "freq": "1min",
+            },
+        }
+        super().__init__(
+            instruments=instruments,
+            start_time=start_time,
+            end_time=end_time,
+            data_loader=data_loader,
+        )
+
+    def get_feature_config(self):
+        fields = []
+        names = []
+
+        template_if = "If(IsNull({1}), {0}, {1})"
+        template_paused = "Select(Gt($paused_num, 1.001), {0})"
+        template_fillnan = "FFillNan({0})"
+        fields += [
+            template_fillnan.format(template_paused.format("$close")),
+        ]
+        names += ["$close0"]
+
+        fields += [
+            template_paused.format(
+                template_if.format(
+                    template_fillnan.format("$close"),
+                    "$vwap",
+                )
+            )
+        ]
+        names += ["$vwap0"]
+
+        fields += [template_paused.format("If(IsNull({0}), 0, {0})".format("$volume"))]
+        names += ["$volume0"]
+
+        fields += [template_paused.format("If(IsNull({0}), 0, {0})".format("$factor"))]
+        names += ["$factor0"]
+
+        return fields, names
+
+
+class HighFreqGeneralBacktestHandler(DataHandler):
+    def __init__(
+        self,
+        instruments="csi300",
+        start_time=None,
+        end_time=None,
+        day_length=240,
+        freq="1min",
+        columns=["$close", "$vwap", "$volume"],
+        inst_processors=None,
+    ):
+        self.day_length = day_length
+        self.columns = set(columns)
+        data_loader = {
+            "class": "QlibDataLoader",
+            "kwargs": {
+                "config": self.get_feature_config(),
+                "swap_level": False,
+                "freq": freq,
+                "inst_processors": inst_processors,
+            },
+        }
+        super().__init__(
+            instruments=instruments,
+            start_time=start_time,
+            end_time=end_time,
+            data_loader=data_loader,
+        )
+
+    def get_feature_config(self):
+        fields = []
+        names = []
+
+        if "$close" in self.columns:
+            template_paused = f"Cut({{0}}, {self.day_length * 2}, None)"
+            template_fillnan = "FFillNan({0})"
+            template_if = "If(IsNull({1}), {0}, {1})"
+            fields += [
+                template_paused.format(template_fillnan.format("$close")),
+            ]
+            names += ["$close0"]
+
+        if "$vwap" in self.columns:
+            fields += [
+                template_paused.format(template_if.format(template_fillnan.format("$close"), "$vwap")),
+            ]
+            names += ["$vwap0"]
+
+        if "$volume" in self.columns:
+            fields += [template_paused.format("If(IsNull({0}), 0, {0})".format("$volume"))]
+            names += ["$volume0"]
+
+        return fields, names
+
+
+class HighFreqOrderHandler(DataHandlerLP):
+    def __init__(
+        self,
+        instruments="csi300",
+        start_time=None,
+        end_time=None,
+        infer_processors=[],
+        learn_processors=[],
+        fit_start_time=None,
+        fit_end_time=None,
+        inst_processors=None,
+        drop_raw=True,
+    ):
+        infer_processors = check_transform_proc(infer_processors, fit_start_time, fit_end_time)
+        learn_processors = check_transform_proc(learn_processors, fit_start_time, fit_end_time)
+
+        data_loader = {
+            "class": "QlibDataLoader",
+            "kwargs": {
+                "config": self.get_feature_config(),
+                "swap_level": False,
+                "freq": "1min",
+                "inst_processors": inst_processors,
+            },
+        }
+        super().__init__(
+            instruments=instruments,
+            start_time=start_time,
+            end_time=end_time,
+            data_loader=data_loader,
+            infer_processors=infer_processors,
+            learn_processors=learn_processors,
+            drop_raw=drop_raw,
+        )
+
+    def get_feature_config(self):
+        fields = []
+        names = []
+
+        template_if = "If(IsNull({1}), {0}, {1})"
+        template_ifinf = "If(IsInf({1}), {0}, {1})"
+        template_paused = "Select(Gt($paused_num, 1.001), {0})"
+
+        def get_normalized_price_feature(price_field, shift=0):
+            # norm with the close price of 237th minute of yesterday.
+            if shift == 0:
+                template_norm = "{0}/DayLast(Ref({1}, 243))"
+            else:
+                template_norm = "Ref({0}, " + str(shift) + ")/DayLast(Ref({1}, 243))"
+
+            template_fillnan = "FFillNan({0})"
+            # calculate -> ffill -> remove paused
+            feature_ops = template_paused.format(
+                template_fillnan.format(
+                    template_norm.format(template_if.format("$close", price_field), template_fillnan.format("$close"))
+                )
+            )
+            return feature_ops
+
+        def get_normalized_vwap_price_feature(price_field, shift=0):
+            # norm with the close price of 237th minute of yesterday.
+            if shift == 0:
+                template_norm = "{0}/DayLast(Ref({1}, 243))"
+            else:
+                template_norm = "Ref({0}, " + str(shift) + ")/DayLast(Ref({1}, 243))"
+
+            template_fillnan = "FFillNan({0})"
+            # calculate -> ffill -> remove paused
+            feature_ops = template_paused.format(
+                template_fillnan.format(
+                    template_norm.format(
+                        template_if.format("$close", template_ifinf.format("$close", price_field)),
+                        template_fillnan.format("$close"),
+                    )
+                )
+            )
+            return feature_ops
+
+        fields += [get_normalized_price_feature("$open", 0)]
+        fields += [get_normalized_price_feature("$high", 0)]
+        fields += [get_normalized_price_feature("$low", 0)]
+        fields += [get_normalized_price_feature("$close", 0)]
+        fields += [get_normalized_vwap_price_feature("$vwap", 0)]
+        names += ["$open", "$high", "$low", "$close", "$vwap"]
+
+        fields += [get_normalized_price_feature("$open", 240)]
+        fields += [get_normalized_price_feature("$high", 240)]
+        fields += [get_normalized_price_feature("$low", 240)]
+        fields += [get_normalized_price_feature("$close", 240)]
+        fields += [get_normalized_vwap_price_feature("$vwap", 240)]
+        names += ["$open_1", "$high_1", "$low_1", "$close_1", "$vwap_1"]
+
+        fields += [get_normalized_price_feature("$bid", 0)]
+        fields += [get_normalized_price_feature("$ask", 0)]
+        names += ["$bid", "$ask"]
+
+        fields += [get_normalized_price_feature("$bid", 240)]
+        fields += [get_normalized_price_feature("$ask", 240)]
+        names += ["$bid_1", "$ask_1"]
+
+        # calculate and fill nan with 0
+
+        def get_volume_feature(volume_field, shift=0):
+            template_gzero = "If(Ge({0}, 0), {0}, 0)"
+            if shift == 0:
+                feature_ops = template_gzero.format(
+                    template_paused.format(
+                        "If(IsInf({0}), 0, {0})".format(
+                            "If(IsNull({0}), 0, {0})".format(
+                                "{0}/Ref(DayLast(Mean({0}, 7200)), 240)".format(volume_field)
+                            )
+                        )
+                    )
+                )
+            else:
+                feature_ops = template_gzero.format(
+                    template_paused.format(
+                        "If(IsInf({0}), 0, {0})".format(
+                            "If(IsNull({0}), 0, {0})".format(
+                                f"Ref({{0}}, {shift})/Ref(DayLast(Mean({{0}}, 7200)), 240)".format(volume_field)
+                            )
+                        )
+                    )
+                )
+            return feature_ops
+
+        fields += [get_volume_feature("$volume", 0)]
+        names += ["$volume"]
+
+        fields += [get_volume_feature("$volume", 240)]
+        names += ["$volume_1"]
+
+        fields += [get_volume_feature("$bidV", 0)]
+        fields += [get_volume_feature("$bidV1", 0)]
+        fields += [get_volume_feature("$bidV3", 0)]
+        fields += [get_volume_feature("$bidV5", 0)]
+        fields += [get_volume_feature("$askV", 0)]
+        fields += [get_volume_feature("$askV1", 0)]
+        fields += [get_volume_feature("$askV3", 0)]
+        fields += [get_volume_feature("$askV5", 0)]
+        names += ["$bidV", "$bidV1", "$bidV3", "$bidV5", "$askV", "$askV1", "$askV3", "$askV5"]
+
+        fields += [get_volume_feature("$bidV", 240)]
+        fields += [get_volume_feature("$bidV1", 240)]
+        fields += [get_volume_feature("$bidV3", 240)]
+        fields += [get_volume_feature("$bidV5", 240)]
+        fields += [get_volume_feature("$askV", 240)]
+        fields += [get_volume_feature("$askV1", 240)]
+        fields += [get_volume_feature("$askV3", 240)]
+        fields += [get_volume_feature("$askV5", 240)]
+        names += ["$bidV_1", "$bidV1_1", "$bidV3_1", "$bidV5_1", "$askV_1", "$askV1_1", "$askV3_1", "$askV5_1"]
+
+        return fields, names
+
+
+class HighFreqBacktestOrderHandler(DataHandler):
+    def __init__(
+        self,
+        instruments="csi300",
+        start_time=None,
+        end_time=None,
+    ):
+        data_loader = {
+            "class": "QlibDataLoader",
+            "kwargs": {
+                "config": self.get_feature_config(),
+                "swap_level": False,
+                "freq": "1min",
+            },
+        }
+        super().__init__(
+            instruments=instruments,
+            start_time=start_time,
+            end_time=end_time,
+            data_loader=data_loader,
+        )
+
+    def get_feature_config(self):
+        fields = []
+        names = []
+
+        template_if = "If(IsNull({1}), {0}, {1})"
+        template_paused = "Select(Gt($paused_num, 1.001), {0})"
+        template_fillnan = "FFillNan({0})"
+        fields += [
+            template_fillnan.format(template_paused.format("$close")),
+        ]
+        names += ["$close0"]
+
+        fields += [
+            template_paused.format(
+                template_if.format(
+                    template_fillnan.format("$close"),
+                    "$vwap",
+                )
+            )
+        ]
+        names += ["$vwap0"]
+
+        fields += [template_paused.format("If(IsNull({0}), 0, {0})".format("$volume"))]
+        names += ["$volume0"]
+
+        fields += [template_paused.format("If(IsNull({0}), 0, {0})".format("$bid"))]
+        names += ["$bid0"]
+
+        fields += [template_paused.format("If(IsNull({0}), 0, {0})".format("$bidV"))]
+        names += ["$bidV0"]
+
+        fields += [template_paused.format("If(IsNull({0}), 0, {0})".format("$ask"))]
+        names += ["$ask0"]
+
+        fields += [template_paused.format("If(IsNull({0}), 0, {0})".format("$askV"))]
+        names += ["$askV0"]
+
+        fields += [template_paused.format("If(IsNull({0}), 0, {0})".format("($bid + $ask) / 2"))]
+        names += ["$median0"]
+
+        fields += [template_paused.format("If(IsNull({0}), 0, {0})".format("$factor"))]
+        names += ["$factor0"]
+
+        fields += [template_paused.format("If(IsNull({0}), 0, {0})".format("$downlimitmarket"))]
+        names += ["$downlimitmarket0"]
+
+        fields += [template_paused.format("If(IsNull({0}), 0, {0})".format("$uplimitmarket"))]
+        names += ["$uplimitmarket0"]
+
+        fields += [template_paused.format("If(IsNull({0}), 0, {0})".format("$highmarket"))]
+        names += ["$highmarket0"]
+
+        fields += [template_paused.format("If(IsNull({0}), 0, {0})".format("$lowmarket"))]
+        names += ["$lowmarket0"]
+
+        return fields, names
```

## qlib/contrib/data/highfreq_processor.py

```diff
@@ -1,81 +1,80 @@
-import os
-
-import numpy as np
-import pandas as pd
-from qlib.data.dataset.processor import Processor
-from qlib.data.dataset.utils import fetch_df_by_index
-from typing import Dict
-
-
-class HighFreqTrans(Processor):
-    def __init__(self, dtype: str = "bool"):
-        self.dtype = dtype
-
-    def fit(self, df_features):
-        pass
-
-    def __call__(self, df_features):
-        if self.dtype == "bool":
-            return df_features.astype(np.int8)
-        else:
-            return df_features.astype(np.float32)
-
-
-class HighFreqNorm(Processor):
-    def __init__(
-        self,
-        fit_start_time: pd.Timestamp,
-        fit_end_time: pd.Timestamp,
-        feature_save_dir: str,
-        norm_groups: Dict[str, int],
-    ):
-
-        self.fit_start_time = fit_start_time
-        self.fit_end_time = fit_end_time
-        self.feature_save_dir = feature_save_dir
-        self.norm_groups = norm_groups
-
-    def fit(self, df_features) -> None:
-        if os.path.exists(self.feature_save_dir) and len(os.listdir(self.feature_save_dir)) != 0:
-            return
-        os.makedirs(self.feature_save_dir)
-        fetch_df = fetch_df_by_index(df_features, slice(self.fit_start_time, self.fit_end_time), level="datetime")
-        del df_features
-        index = 0
-        names = {}
-        for name, dim in self.norm_groups.items():
-            names[name] = slice(index, index + dim)
-            index += dim
-        for name, name_val in names.items():
-            df_values = fetch_df.iloc(axis=1)[name_val].values
-            if name.endswith("volume"):
-                df_values = np.log1p(df_values)
-            self.feature_mean = np.nanmean(df_values)
-            np.save(self.feature_save_dir + name + "_mean.npy", self.feature_mean)
-            df_values = df_values - self.feature_mean
-            self.feature_std = np.nanstd(np.absolute(df_values))
-            np.save(self.feature_save_dir + name + "_std.npy", self.feature_std)
-            df_values = df_values / self.feature_std
-            np.save(self.feature_save_dir + name + "_vmax.npy", np.nanmax(df_values))
-            np.save(self.feature_save_dir + name + "_vmin.npy", np.nanmin(df_values))
-        return
-
-    def __call__(self, df_features):
-        if "date" in df_features:
-            df_features.droplevel("date", inplace=True)
-        df_values = df_features.values
-        index = 0
-        names = {}
-        for name, dim in self.norm_groups.items():
-            names[name] = slice(index, index + dim)
-            index += dim
-        for name, name_val in names.items():
-            feature_mean = np.load(self.feature_save_dir + name + "_mean.npy")
-            feature_std = np.load(self.feature_save_dir + name + "_std.npy")
-
-            if name.endswith("volume"):
-                df_values[:, name_val] = np.log1p(df_values[:, name_val])
-            df_values[:, name_val] -= feature_mean
-            df_values[:, name_val] /= feature_std
-        df_features = pd.DataFrame(data=df_values, index=df_features.index, columns=df_features.columns)
-        return df_features.fillna(0)
+import os
+
+import numpy as np
+import pandas as pd
+from qlib.data.dataset.processor import Processor
+from qlib.data.dataset.utils import fetch_df_by_index
+from typing import Dict
+
+
+class HighFreqTrans(Processor):
+    def __init__(self, dtype: str = "bool"):
+        self.dtype = dtype
+
+    def fit(self, df_features):
+        pass
+
+    def __call__(self, df_features):
+        if self.dtype == "bool":
+            return df_features.astype(np.int8)
+        else:
+            return df_features.astype(np.float32)
+
+
+class HighFreqNorm(Processor):
+    def __init__(
+        self,
+        fit_start_time: pd.Timestamp,
+        fit_end_time: pd.Timestamp,
+        feature_save_dir: str,
+        norm_groups: Dict[str, int],
+    ):
+        self.fit_start_time = fit_start_time
+        self.fit_end_time = fit_end_time
+        self.feature_save_dir = feature_save_dir
+        self.norm_groups = norm_groups
+
+    def fit(self, df_features) -> None:
+        if os.path.exists(self.feature_save_dir) and len(os.listdir(self.feature_save_dir)) != 0:
+            return
+        os.makedirs(self.feature_save_dir)
+        fetch_df = fetch_df_by_index(df_features, slice(self.fit_start_time, self.fit_end_time), level="datetime")
+        del df_features
+        index = 0
+        names = {}
+        for name, dim in self.norm_groups.items():
+            names[name] = slice(index, index + dim)
+            index += dim
+        for name, name_val in names.items():
+            df_values = fetch_df.iloc(axis=1)[name_val].values
+            if name.endswith("volume"):
+                df_values = np.log1p(df_values)
+            self.feature_mean = np.nanmean(df_values)
+            np.save(self.feature_save_dir + name + "_mean.npy", self.feature_mean)
+            df_values = df_values - self.feature_mean
+            self.feature_std = np.nanstd(np.absolute(df_values))
+            np.save(self.feature_save_dir + name + "_std.npy", self.feature_std)
+            df_values = df_values / self.feature_std
+            np.save(self.feature_save_dir + name + "_vmax.npy", np.nanmax(df_values))
+            np.save(self.feature_save_dir + name + "_vmin.npy", np.nanmin(df_values))
+        return
+
+    def __call__(self, df_features):
+        if "date" in df_features:
+            df_features.droplevel("date", inplace=True)
+        df_values = df_features.values
+        index = 0
+        names = {}
+        for name, dim in self.norm_groups.items():
+            names[name] = slice(index, index + dim)
+            index += dim
+        for name, name_val in names.items():
+            feature_mean = np.load(self.feature_save_dir + name + "_mean.npy")
+            feature_std = np.load(self.feature_save_dir + name + "_std.npy")
+
+            if name.endswith("volume"):
+                df_values[:, name_val] = np.log1p(df_values[:, name_val])
+            df_values[:, name_val] -= feature_mean
+            df_values[:, name_val] /= feature_std
+        df_features = pd.DataFrame(data=df_values, index=df_features.index, columns=df_features.columns)
+        return df_features.fillna(0)
```

## qlib/contrib/data/highfreq_provider.py

 * *Ordering differences only*

```diff
@@ -1,304 +1,304 @@
-import os
-import time
-import datetime
-from typing import Optional
-
-import qlib
-from qlib import get_module_logger
-from qlib.data import D
-from qlib.config import REG_CN
-from qlib.utils import init_instance_by_config
-from qlib.data.dataset.handler import DataHandlerLP
-from qlib.data.data import Cal
-from qlib.contrib.ops.high_freq import get_calendar_day, DayLast, FFillNan, BFillNan, Date, Select, IsNull, IsInf, Cut
-import pickle as pkl
-from joblib import Parallel, delayed
-
-
-class HighFreqProvider:
-    def __init__(
-        self,
-        start_time: str,
-        end_time: str,
-        train_end_time: str,
-        valid_start_time: str,
-        valid_end_time: str,
-        test_start_time: str,
-        qlib_conf: dict,
-        feature_conf: dict,
-        label_conf: Optional[dict] = None,
-        backtest_conf: dict = None,
-        freq: str = "1min",
-        **kwargs,
-    ) -> None:
-        self.start_time = start_time
-        self.end_time = end_time
-        self.test_start_time = test_start_time
-        self.train_end_time = train_end_time
-        self.valid_start_time = valid_start_time
-        self.valid_end_time = valid_end_time
-        self._init_qlib(qlib_conf)
-        self.feature_conf = feature_conf
-        self.label_conf = label_conf
-        self.backtest_conf = backtest_conf
-        self.qlib_conf = qlib_conf
-        self.logger = get_module_logger("HighFreqProvider")
-        self.freq = freq
-
-    def get_pre_datasets(self):
-        """Generate the training, validation and test datasets for prediction
-
-        Returns:
-            Tuple[BaseDataset, BaseDataset, BaseDataset]: The training and test datasets
-        """
-
-        dict_feature_path = self.feature_conf["path"]
-        train_feature_path = dict_feature_path[:-4] + "_train.pkl"
-        valid_feature_path = dict_feature_path[:-4] + "_valid.pkl"
-        test_feature_path = dict_feature_path[:-4] + "_test.pkl"
-
-        dict_label_path = self.label_conf["path"]
-        train_label_path = dict_label_path[:-4] + "_train.pkl"
-        valid_label_path = dict_label_path[:-4] + "_valid.pkl"
-        test_label_path = dict_label_path[:-4] + "_test.pkl"
-
-        if (
-            not os.path.isfile(train_feature_path)
-            or not os.path.isfile(valid_feature_path)
-            or not os.path.isfile(test_feature_path)
-        ):
-            xtrain, xvalid, xtest = self._gen_data(self.feature_conf)
-            xtrain.to_pickle(train_feature_path)
-            xvalid.to_pickle(valid_feature_path)
-            xtest.to_pickle(test_feature_path)
-            del xtrain, xvalid, xtest
-
-        if (
-            not os.path.isfile(train_label_path)
-            or not os.path.isfile(valid_label_path)
-            or not os.path.isfile(test_label_path)
-        ):
-            ytrain, yvalid, ytest = self._gen_data(self.label_conf)
-            ytrain.to_pickle(train_label_path)
-            yvalid.to_pickle(valid_label_path)
-            ytest.to_pickle(test_label_path)
-            del ytrain, yvalid, ytest
-
-        feature = {
-            "train": train_feature_path,
-            "valid": valid_feature_path,
-            "test": test_feature_path,
-        }
-
-        label = {
-            "train": train_label_path,
-            "valid": valid_label_path,
-            "test": test_label_path,
-        }
-
-        return feature, label
-
-    def get_backtest(self, **kwargs) -> None:
-        self._gen_data(self.backtest_conf)
-
-    def _init_qlib(self, qlib_conf):
-        """initialize qlib"""
-
-        qlib.init(
-            region=REG_CN,
-            auto_mount=False,
-            custom_ops=[DayLast, FFillNan, BFillNan, Date, Select, IsNull, IsInf, Cut],
-            expression_cache=None,
-            **qlib_conf,
-        )
-
-    def _prepare_calender_cache(self):
-        """preload the calendar for cache"""
-
-        # This code used the copy-on-write feature of Linux
-        # to avoid calculating the calendar multiple times in the subprocess.
-        # This code may accelerate, but may be not useful on Windows and Mac Os
-        Cal.calendar(freq=self.freq)
-        get_calendar_day(freq=self.freq)
-
-    def _gen_dataframe(self, config, datasets=["train", "valid", "test"]):
-        try:
-            path = config.pop("path")
-        except KeyError as e:
-            raise ValueError("Must specify the path to save the dataset.") from e
-        if os.path.isfile(path):
-            start = time.time()
-            self.logger.info(f"[{__name__}]Dataset exists, load from disk.")
-
-            # res = dataset.prepare(['train', 'valid', 'test'])
-            with open(path, "rb") as f:
-                data = pkl.load(f)
-            if isinstance(data, dict):
-                res = [data[i] for i in datasets]
-            else:
-                res = data.prepare(datasets)
-            self.logger.info(f"[{__name__}]Data loaded, time cost: {time.time() - start:.2f}")
-        else:
-            if not os.path.exists(os.path.dirname(path)):
-                os.makedirs(os.path.dirname(path))
-            self.logger.info(f"[{__name__}]Generating dataset")
-            start_time = time.time()
-            self._prepare_calender_cache()
-            dataset = init_instance_by_config(config)
-            trainset, validset, testset = dataset.prepare(["train", "valid", "test"])
-            data = {
-                "train": trainset,
-                "valid": validset,
-                "test": testset,
-            }
-            with open(path, "wb") as f:
-                pkl.dump(data, f)
-            with open(path[:-4] + "train.pkl", "wb") as f:
-                pkl.dump(trainset, f)
-            with open(path[:-4] + "valid.pkl", "wb") as f:
-                pkl.dump(validset, f)
-            with open(path[:-4] + "test.pkl", "wb") as f:
-                pkl.dump(testset, f)
-            res = [data[i] for i in datasets]
-            self.logger.info(f"[{__name__}]Data generated, time cost: {(time.time() - start_time):.2f}")
-        return res
-
-    def _gen_data(self, config, datasets=["train", "valid", "test"]):
-        try:
-            path = config.pop("path")
-        except KeyError as e:
-            raise ValueError("Must specify the path to save the dataset.") from e
-        if os.path.isfile(path):
-            start = time.time()
-            self.logger.info(f"[{__name__}]Dataset exists, load from disk.")
-
-            # res = dataset.prepare(['train', 'valid', 'test'])
-            with open(path, "rb") as f:
-                data = pkl.load(f)
-            if isinstance(data, dict):
-                res = [data[i] for i in datasets]
-            else:
-                res = data.prepare(datasets)
-            self.logger.info(f"[{__name__}]Data loaded, time cost: {time.time() - start:.2f}")
-        else:
-            if not os.path.exists(os.path.dirname(path)):
-                os.makedirs(os.path.dirname(path))
-            self.logger.info(f"[{__name__}]Generating dataset")
-            start_time = time.time()
-            self._prepare_calender_cache()
-            dataset = init_instance_by_config(config)
-            dataset.config(dump_all=True, recursive=True)
-            dataset.to_pickle(path)
-            res = dataset.prepare(datasets)
-            self.logger.info(f"[{__name__}]Data generated, time cost: {(time.time() - start_time):.2f}")
-        return res
-
-    def _gen_dataset(self, config):
-        try:
-            path = config.pop("path")
-        except KeyError as e:
-            raise ValueError("Must specify the path to save the dataset.") from e
-        if os.path.isfile(path):
-            start = time.time()
-            self.logger.info(f"[{__name__}]Dataset exists, load from disk.")
-
-            with open(path, "rb") as f:
-                dataset = pkl.load(f)
-            self.logger.info(f"[{__name__}]Data loaded, time cost: {time.time() - start:.2f}")
-        else:
-            start = time.time()
-            if not os.path.exists(os.path.dirname(path)):
-                os.makedirs(os.path.dirname(path))
-            self.logger.info(f"[{__name__}]Generating dataset")
-            self._prepare_calender_cache()
-            dataset = init_instance_by_config(config)
-            self.logger.info(f"[{__name__}]Dataset init, time cost: {time.time() - start:.2f}")
-            dataset.prepare(["train", "valid", "test"])
-            self.logger.info(f"[{__name__}]Dataset prepared, time cost: {time.time() - start:.2f}")
-            dataset.config(dump_all=True, recursive=True)
-            dataset.to_pickle(path)
-        return dataset
-
-    def _gen_day_dataset(self, config, conf_type):
-        try:
-            path = config.pop("path")
-        except KeyError as e:
-            raise ValueError("Must specify the path to save the dataset.") from e
-
-        if os.path.isfile(path + "tmp_dataset.pkl"):
-            start = time.time()
-            self.logger.info(f"[{__name__}]Dataset exists, load from disk.")
-        else:
-            start = time.time()
-            if not os.path.exists(os.path.dirname(path)):
-                os.makedirs(os.path.dirname(path))
-            self.logger.info(f"[{__name__}]Generating dataset")
-            self._prepare_calender_cache()
-            dataset = init_instance_by_config(config)
-            self.logger.info(f"[{__name__}]Dataset init, time cost: {time.time() - start:.2f}")
-            dataset.config(dump_all=False, recursive=True)
-            dataset.to_pickle(path + "tmp_dataset.pkl")
-
-        with open(path + "tmp_dataset.pkl", "rb") as f:
-            new_dataset = pkl.load(f)
-
-        time_list = D.calendar(start_time=self.start_time, end_time=self.end_time, freq=self.freq)[::240]
-
-        def generate_dataset(times):
-            if os.path.isfile(path + times.strftime("%Y-%m-%d") + ".pkl"):
-                print("exist " + times.strftime("%Y-%m-%d"))
-                return
-            self._init_qlib(self.qlib_conf)
-            end_times = times + datetime.timedelta(days=1)
-            new_dataset.handler.config(**{"start_time": times, "end_time": end_times})
-            if conf_type == "backtest":
-                new_dataset.handler.setup_data()
-            else:
-                new_dataset.handler.setup_data(init_type=DataHandlerLP.IT_LS)
-            new_dataset.config(dump_all=True, recursive=True)
-            new_dataset.to_pickle(path + times.strftime("%Y-%m-%d") + ".pkl")
-
-        Parallel(n_jobs=8)(delayed(generate_dataset)(times) for times in time_list)
-
-    def _gen_stock_dataset(self, config, conf_type):
-        try:
-            path = config.pop("path")
-        except KeyError as e:
-            raise ValueError("Must specify the path to save the dataset.") from e
-
-        if os.path.isfile(path + "tmp_dataset.pkl"):
-            start = time.time()
-            self.logger.info(f"[{__name__}]Dataset exists, load from disk.")
-        else:
-            start = time.time()
-            if not os.path.exists(os.path.dirname(path)):
-                os.makedirs(os.path.dirname(path))
-            self.logger.info(f"[{__name__}]Generating dataset")
-            self._prepare_calender_cache()
-            dataset = init_instance_by_config(config)
-            self.logger.info(f"[{__name__}]Dataset init, time cost: {time.time() - start:.2f}")
-            dataset.config(dump_all=False, recursive=True)
-            dataset.to_pickle(path + "tmp_dataset.pkl")
-
-        with open(path + "tmp_dataset.pkl", "rb") as f:
-            new_dataset = pkl.load(f)
-
-        instruments = D.instruments(market="all")
-        stock_list = D.list_instruments(
-            instruments=instruments, start_time=self.start_time, end_time=self.end_time, freq=self.freq, as_list=True
-        )
-
-        def generate_dataset(stock):
-            if os.path.isfile(path + stock + ".pkl"):
-                print("exist " + stock)
-                return
-            self._init_qlib(self.qlib_conf)
-            new_dataset.handler.config(**{"instruments": [stock]})
-            if conf_type == "backtest":
-                new_dataset.handler.setup_data()
-            else:
-                new_dataset.handler.setup_data(init_type=DataHandlerLP.IT_LS)
-            new_dataset.config(dump_all=True, recursive=True)
-            new_dataset.to_pickle(path + stock + ".pkl")
-
-        Parallel(n_jobs=32)(delayed(generate_dataset)(stock) for stock in stock_list)
+import os
+import time
+import datetime
+from typing import Optional
+
+import qlib
+from qlib import get_module_logger
+from qlib.data import D
+from qlib.config import REG_CN
+from qlib.utils import init_instance_by_config
+from qlib.data.dataset.handler import DataHandlerLP
+from qlib.data.data import Cal
+from qlib.contrib.ops.high_freq import get_calendar_day, DayLast, FFillNan, BFillNan, Date, Select, IsNull, IsInf, Cut
+import pickle as pkl
+from joblib import Parallel, delayed
+
+
+class HighFreqProvider:
+    def __init__(
+        self,
+        start_time: str,
+        end_time: str,
+        train_end_time: str,
+        valid_start_time: str,
+        valid_end_time: str,
+        test_start_time: str,
+        qlib_conf: dict,
+        feature_conf: dict,
+        label_conf: Optional[dict] = None,
+        backtest_conf: dict = None,
+        freq: str = "1min",
+        **kwargs,
+    ) -> None:
+        self.start_time = start_time
+        self.end_time = end_time
+        self.test_start_time = test_start_time
+        self.train_end_time = train_end_time
+        self.valid_start_time = valid_start_time
+        self.valid_end_time = valid_end_time
+        self._init_qlib(qlib_conf)
+        self.feature_conf = feature_conf
+        self.label_conf = label_conf
+        self.backtest_conf = backtest_conf
+        self.qlib_conf = qlib_conf
+        self.logger = get_module_logger("HighFreqProvider")
+        self.freq = freq
+
+    def get_pre_datasets(self):
+        """Generate the training, validation and test datasets for prediction
+
+        Returns:
+            Tuple[BaseDataset, BaseDataset, BaseDataset]: The training and test datasets
+        """
+
+        dict_feature_path = self.feature_conf["path"]
+        train_feature_path = dict_feature_path[:-4] + "_train.pkl"
+        valid_feature_path = dict_feature_path[:-4] + "_valid.pkl"
+        test_feature_path = dict_feature_path[:-4] + "_test.pkl"
+
+        dict_label_path = self.label_conf["path"]
+        train_label_path = dict_label_path[:-4] + "_train.pkl"
+        valid_label_path = dict_label_path[:-4] + "_valid.pkl"
+        test_label_path = dict_label_path[:-4] + "_test.pkl"
+
+        if (
+            not os.path.isfile(train_feature_path)
+            or not os.path.isfile(valid_feature_path)
+            or not os.path.isfile(test_feature_path)
+        ):
+            xtrain, xvalid, xtest = self._gen_data(self.feature_conf)
+            xtrain.to_pickle(train_feature_path)
+            xvalid.to_pickle(valid_feature_path)
+            xtest.to_pickle(test_feature_path)
+            del xtrain, xvalid, xtest
+
+        if (
+            not os.path.isfile(train_label_path)
+            or not os.path.isfile(valid_label_path)
+            or not os.path.isfile(test_label_path)
+        ):
+            ytrain, yvalid, ytest = self._gen_data(self.label_conf)
+            ytrain.to_pickle(train_label_path)
+            yvalid.to_pickle(valid_label_path)
+            ytest.to_pickle(test_label_path)
+            del ytrain, yvalid, ytest
+
+        feature = {
+            "train": train_feature_path,
+            "valid": valid_feature_path,
+            "test": test_feature_path,
+        }
+
+        label = {
+            "train": train_label_path,
+            "valid": valid_label_path,
+            "test": test_label_path,
+        }
+
+        return feature, label
+
+    def get_backtest(self, **kwargs) -> None:
+        self._gen_data(self.backtest_conf)
+
+    def _init_qlib(self, qlib_conf):
+        """initialize qlib"""
+
+        qlib.init(
+            region=REG_CN,
+            auto_mount=False,
+            custom_ops=[DayLast, FFillNan, BFillNan, Date, Select, IsNull, IsInf, Cut],
+            expression_cache=None,
+            **qlib_conf,
+        )
+
+    def _prepare_calender_cache(self):
+        """preload the calendar for cache"""
+
+        # This code used the copy-on-write feature of Linux
+        # to avoid calculating the calendar multiple times in the subprocess.
+        # This code may accelerate, but may be not useful on Windows and Mac Os
+        Cal.calendar(freq=self.freq)
+        get_calendar_day(freq=self.freq)
+
+    def _gen_dataframe(self, config, datasets=["train", "valid", "test"]):
+        try:
+            path = config.pop("path")
+        except KeyError as e:
+            raise ValueError("Must specify the path to save the dataset.") from e
+        if os.path.isfile(path):
+            start = time.time()
+            self.logger.info(f"[{__name__}]Dataset exists, load from disk.")
+
+            # res = dataset.prepare(['train', 'valid', 'test'])
+            with open(path, "rb") as f:
+                data = pkl.load(f)
+            if isinstance(data, dict):
+                res = [data[i] for i in datasets]
+            else:
+                res = data.prepare(datasets)
+            self.logger.info(f"[{__name__}]Data loaded, time cost: {time.time() - start:.2f}")
+        else:
+            if not os.path.exists(os.path.dirname(path)):
+                os.makedirs(os.path.dirname(path))
+            self.logger.info(f"[{__name__}]Generating dataset")
+            start_time = time.time()
+            self._prepare_calender_cache()
+            dataset = init_instance_by_config(config)
+            trainset, validset, testset = dataset.prepare(["train", "valid", "test"])
+            data = {
+                "train": trainset,
+                "valid": validset,
+                "test": testset,
+            }
+            with open(path, "wb") as f:
+                pkl.dump(data, f)
+            with open(path[:-4] + "train.pkl", "wb") as f:
+                pkl.dump(trainset, f)
+            with open(path[:-4] + "valid.pkl", "wb") as f:
+                pkl.dump(validset, f)
+            with open(path[:-4] + "test.pkl", "wb") as f:
+                pkl.dump(testset, f)
+            res = [data[i] for i in datasets]
+            self.logger.info(f"[{__name__}]Data generated, time cost: {(time.time() - start_time):.2f}")
+        return res
+
+    def _gen_data(self, config, datasets=["train", "valid", "test"]):
+        try:
+            path = config.pop("path")
+        except KeyError as e:
+            raise ValueError("Must specify the path to save the dataset.") from e
+        if os.path.isfile(path):
+            start = time.time()
+            self.logger.info(f"[{__name__}]Dataset exists, load from disk.")
+
+            # res = dataset.prepare(['train', 'valid', 'test'])
+            with open(path, "rb") as f:
+                data = pkl.load(f)
+            if isinstance(data, dict):
+                res = [data[i] for i in datasets]
+            else:
+                res = data.prepare(datasets)
+            self.logger.info(f"[{__name__}]Data loaded, time cost: {time.time() - start:.2f}")
+        else:
+            if not os.path.exists(os.path.dirname(path)):
+                os.makedirs(os.path.dirname(path))
+            self.logger.info(f"[{__name__}]Generating dataset")
+            start_time = time.time()
+            self._prepare_calender_cache()
+            dataset = init_instance_by_config(config)
+            dataset.config(dump_all=True, recursive=True)
+            dataset.to_pickle(path)
+            res = dataset.prepare(datasets)
+            self.logger.info(f"[{__name__}]Data generated, time cost: {(time.time() - start_time):.2f}")
+        return res
+
+    def _gen_dataset(self, config):
+        try:
+            path = config.pop("path")
+        except KeyError as e:
+            raise ValueError("Must specify the path to save the dataset.") from e
+        if os.path.isfile(path):
+            start = time.time()
+            self.logger.info(f"[{__name__}]Dataset exists, load from disk.")
+
+            with open(path, "rb") as f:
+                dataset = pkl.load(f)
+            self.logger.info(f"[{__name__}]Data loaded, time cost: {time.time() - start:.2f}")
+        else:
+            start = time.time()
+            if not os.path.exists(os.path.dirname(path)):
+                os.makedirs(os.path.dirname(path))
+            self.logger.info(f"[{__name__}]Generating dataset")
+            self._prepare_calender_cache()
+            dataset = init_instance_by_config(config)
+            self.logger.info(f"[{__name__}]Dataset init, time cost: {time.time() - start:.2f}")
+            dataset.prepare(["train", "valid", "test"])
+            self.logger.info(f"[{__name__}]Dataset prepared, time cost: {time.time() - start:.2f}")
+            dataset.config(dump_all=True, recursive=True)
+            dataset.to_pickle(path)
+        return dataset
+
+    def _gen_day_dataset(self, config, conf_type):
+        try:
+            path = config.pop("path")
+        except KeyError as e:
+            raise ValueError("Must specify the path to save the dataset.") from e
+
+        if os.path.isfile(path + "tmp_dataset.pkl"):
+            start = time.time()
+            self.logger.info(f"[{__name__}]Dataset exists, load from disk.")
+        else:
+            start = time.time()
+            if not os.path.exists(os.path.dirname(path)):
+                os.makedirs(os.path.dirname(path))
+            self.logger.info(f"[{__name__}]Generating dataset")
+            self._prepare_calender_cache()
+            dataset = init_instance_by_config(config)
+            self.logger.info(f"[{__name__}]Dataset init, time cost: {time.time() - start:.2f}")
+            dataset.config(dump_all=False, recursive=True)
+            dataset.to_pickle(path + "tmp_dataset.pkl")
+
+        with open(path + "tmp_dataset.pkl", "rb") as f:
+            new_dataset = pkl.load(f)
+
+        time_list = D.calendar(start_time=self.start_time, end_time=self.end_time, freq=self.freq)[::240]
+
+        def generate_dataset(times):
+            if os.path.isfile(path + times.strftime("%Y-%m-%d") + ".pkl"):
+                print("exist " + times.strftime("%Y-%m-%d"))
+                return
+            self._init_qlib(self.qlib_conf)
+            end_times = times + datetime.timedelta(days=1)
+            new_dataset.handler.config(**{"start_time": times, "end_time": end_times})
+            if conf_type == "backtest":
+                new_dataset.handler.setup_data()
+            else:
+                new_dataset.handler.setup_data(init_type=DataHandlerLP.IT_LS)
+            new_dataset.config(dump_all=True, recursive=True)
+            new_dataset.to_pickle(path + times.strftime("%Y-%m-%d") + ".pkl")
+
+        Parallel(n_jobs=8)(delayed(generate_dataset)(times) for times in time_list)
+
+    def _gen_stock_dataset(self, config, conf_type):
+        try:
+            path = config.pop("path")
+        except KeyError as e:
+            raise ValueError("Must specify the path to save the dataset.") from e
+
+        if os.path.isfile(path + "tmp_dataset.pkl"):
+            start = time.time()
+            self.logger.info(f"[{__name__}]Dataset exists, load from disk.")
+        else:
+            start = time.time()
+            if not os.path.exists(os.path.dirname(path)):
+                os.makedirs(os.path.dirname(path))
+            self.logger.info(f"[{__name__}]Generating dataset")
+            self._prepare_calender_cache()
+            dataset = init_instance_by_config(config)
+            self.logger.info(f"[{__name__}]Dataset init, time cost: {time.time() - start:.2f}")
+            dataset.config(dump_all=False, recursive=True)
+            dataset.to_pickle(path + "tmp_dataset.pkl")
+
+        with open(path + "tmp_dataset.pkl", "rb") as f:
+            new_dataset = pkl.load(f)
+
+        instruments = D.instruments(market="all")
+        stock_list = D.list_instruments(
+            instruments=instruments, start_time=self.start_time, end_time=self.end_time, freq=self.freq, as_list=True
+        )
+
+        def generate_dataset(stock):
+            if os.path.isfile(path + stock + ".pkl"):
+                print("exist " + stock)
+                return
+            self._init_qlib(self.qlib_conf)
+            new_dataset.handler.config(**{"instruments": [stock]})
+            if conf_type == "backtest":
+                new_dataset.handler.setup_data()
+            else:
+                new_dataset.handler.setup_data(init_type=DataHandlerLP.IT_LS)
+            new_dataset.config(dump_all=True, recursive=True)
+            new_dataset.to_pickle(path + stock + ".pkl")
+
+        Parallel(n_jobs=32)(delayed(generate_dataset)(stock) for stock in stock_list)
```

## qlib/contrib/data/processor.py

 * *Ordering differences only*

```diff
@@ -1,115 +1,115 @@
-import numpy as np
-
-from ...log import TimeInspector
-from ...data.dataset.processor import Processor, get_group_columns
-
-
-class ConfigSectionProcessor(Processor):
-    """
-    This processor is designed for Alpha158. And will be replaced by simple processors in the future
-    """
-
-    def __init__(self, fields_group=None, **kwargs):
-        super().__init__()
-        # Options
-        self.fillna_feature = kwargs.get("fillna_feature", True)
-        self.fillna_label = kwargs.get("fillna_label", True)
-        self.clip_feature_outlier = kwargs.get("clip_feature_outlier", False)
-        self.shrink_feature_outlier = kwargs.get("shrink_feature_outlier", True)
-        self.clip_label_outlier = kwargs.get("clip_label_outlier", False)
-
-        self.fields_group = None
-
-    def __call__(self, df):
-        return self._transform(df)
-
-    def _transform(self, df):
-        def _label_norm(x):
-            x = x - x.mean()  # copy
-            x /= x.std()
-            if self.clip_label_outlier:
-                x.clip(-3, 3, inplace=True)
-            if self.fillna_label:
-                x.fillna(0, inplace=True)
-            return x
-
-        def _feature_norm(x):
-            x = x - x.median()  # copy
-            x /= x.abs().median() * 1.4826
-            if self.clip_feature_outlier:
-                x.clip(-3, 3, inplace=True)
-            if self.shrink_feature_outlier:
-                x.where(x <= 3, 3 + (x - 3).div(x.max() - 3) * 0.5, inplace=True)
-                x.where(x >= -3, -3 - (x + 3).div(x.min() + 3) * 0.5, inplace=True)
-            if self.fillna_feature:
-                x.fillna(0, inplace=True)
-            return x
-
-        TimeInspector.set_time_mark()
-
-        # Copy the focus part and change it to single level
-        selected_cols = get_group_columns(df, self.fields_group)
-        df_focus = df[selected_cols].copy()
-        if len(df_focus.columns.levels) > 1:
-            df_focus = df_focus.droplevel(level=0)
-
-        # Label
-        cols = df_focus.columns[df_focus.columns.str.contains("^LABEL")]
-        df_focus[cols] = df_focus[cols].groupby(level="datetime").apply(_label_norm)
-
-        # Features
-        cols = df_focus.columns[df_focus.columns.str.contains("^KLEN|^KLOW|^KUP")]
-        df_focus[cols] = df_focus[cols].apply(lambda x: x**0.25).groupby(level="datetime").apply(_feature_norm)
-
-        cols = df_focus.columns[df_focus.columns.str.contains("^KLOW2|^KUP2")]
-        df_focus[cols] = df_focus[cols].apply(lambda x: x**0.5).groupby(level="datetime").apply(_feature_norm)
-
-        _cols = [
-            "KMID",
-            "KSFT",
-            "OPEN",
-            "HIGH",
-            "LOW",
-            "CLOSE",
-            "VWAP",
-            "ROC",
-            "MA",
-            "BETA",
-            "RESI",
-            "QTLU",
-            "QTLD",
-            "RSV",
-            "SUMP",
-            "SUMN",
-            "SUMD",
-            "VSUMP",
-            "VSUMN",
-            "VSUMD",
-        ]
-        pat = "|".join(["^" + x for x in _cols])
-        cols = df_focus.columns[df_focus.columns.str.contains(pat) & (~df_focus.columns.isin(["HIGH0", "LOW0"]))]
-        df_focus[cols] = df_focus[cols].groupby(level="datetime").apply(_feature_norm)
-
-        cols = df_focus.columns[df_focus.columns.str.contains("^STD|^VOLUME|^VMA|^VSTD")]
-        df_focus[cols] = df_focus[cols].apply(np.log).groupby(level="datetime").apply(_feature_norm)
-
-        cols = df_focus.columns[df_focus.columns.str.contains("^RSQR")]
-        df_focus[cols] = df_focus[cols].fillna(0).groupby(level="datetime").apply(_feature_norm)
-
-        cols = df_focus.columns[df_focus.columns.str.contains("^MAX|^HIGH0")]
-        df_focus[cols] = df_focus[cols].apply(lambda x: (x - 1) ** 0.5).groupby(level="datetime").apply(_feature_norm)
-
-        cols = df_focus.columns[df_focus.columns.str.contains("^MIN|^LOW0")]
-        df_focus[cols] = df_focus[cols].apply(lambda x: (1 - x) ** 0.5).groupby(level="datetime").apply(_feature_norm)
-
-        cols = df_focus.columns[df_focus.columns.str.contains("^CORR|^CORD")]
-        df_focus[cols] = df_focus[cols].apply(np.exp).groupby(level="datetime").apply(_feature_norm)
-
-        cols = df_focus.columns[df_focus.columns.str.contains("^WVMA")]
-        df_focus[cols] = df_focus[cols].apply(np.log1p).groupby(level="datetime").apply(_feature_norm)
-
-        df[selected_cols] = df_focus.values
-
-        TimeInspector.log_cost_time("Finished preprocessing data.")
-
-        return df
+import numpy as np
+
+from ...log import TimeInspector
+from ...data.dataset.processor import Processor, get_group_columns
+
+
+class ConfigSectionProcessor(Processor):
+    """
+    This processor is designed for Alpha158. And will be replaced by simple processors in the future
+    """
+
+    def __init__(self, fields_group=None, **kwargs):
+        super().__init__()
+        # Options
+        self.fillna_feature = kwargs.get("fillna_feature", True)
+        self.fillna_label = kwargs.get("fillna_label", True)
+        self.clip_feature_outlier = kwargs.get("clip_feature_outlier", False)
+        self.shrink_feature_outlier = kwargs.get("shrink_feature_outlier", True)
+        self.clip_label_outlier = kwargs.get("clip_label_outlier", False)
+
+        self.fields_group = None
+
+    def __call__(self, df):
+        return self._transform(df)
+
+    def _transform(self, df):
+        def _label_norm(x):
+            x = x - x.mean()  # copy
+            x /= x.std()
+            if self.clip_label_outlier:
+                x.clip(-3, 3, inplace=True)
+            if self.fillna_label:
+                x.fillna(0, inplace=True)
+            return x
+
+        def _feature_norm(x):
+            x = x - x.median()  # copy
+            x /= x.abs().median() * 1.4826
+            if self.clip_feature_outlier:
+                x.clip(-3, 3, inplace=True)
+            if self.shrink_feature_outlier:
+                x.where(x <= 3, 3 + (x - 3).div(x.max() - 3) * 0.5, inplace=True)
+                x.where(x >= -3, -3 - (x + 3).div(x.min() + 3) * 0.5, inplace=True)
+            if self.fillna_feature:
+                x.fillna(0, inplace=True)
+            return x
+
+        TimeInspector.set_time_mark()
+
+        # Copy the focus part and change it to single level
+        selected_cols = get_group_columns(df, self.fields_group)
+        df_focus = df[selected_cols].copy()
+        if len(df_focus.columns.levels) > 1:
+            df_focus = df_focus.droplevel(level=0)
+
+        # Label
+        cols = df_focus.columns[df_focus.columns.str.contains("^LABEL")]
+        df_focus[cols] = df_focus[cols].groupby(level="datetime").apply(_label_norm)
+
+        # Features
+        cols = df_focus.columns[df_focus.columns.str.contains("^KLEN|^KLOW|^KUP")]
+        df_focus[cols] = df_focus[cols].apply(lambda x: x**0.25).groupby(level="datetime").apply(_feature_norm)
+
+        cols = df_focus.columns[df_focus.columns.str.contains("^KLOW2|^KUP2")]
+        df_focus[cols] = df_focus[cols].apply(lambda x: x**0.5).groupby(level="datetime").apply(_feature_norm)
+
+        _cols = [
+            "KMID",
+            "KSFT",
+            "OPEN",
+            "HIGH",
+            "LOW",
+            "CLOSE",
+            "VWAP",
+            "ROC",
+            "MA",
+            "BETA",
+            "RESI",
+            "QTLU",
+            "QTLD",
+            "RSV",
+            "SUMP",
+            "SUMN",
+            "SUMD",
+            "VSUMP",
+            "VSUMN",
+            "VSUMD",
+        ]
+        pat = "|".join(["^" + x for x in _cols])
+        cols = df_focus.columns[df_focus.columns.str.contains(pat) & (~df_focus.columns.isin(["HIGH0", "LOW0"]))]
+        df_focus[cols] = df_focus[cols].groupby(level="datetime").apply(_feature_norm)
+
+        cols = df_focus.columns[df_focus.columns.str.contains("^STD|^VOLUME|^VMA|^VSTD")]
+        df_focus[cols] = df_focus[cols].apply(np.log).groupby(level="datetime").apply(_feature_norm)
+
+        cols = df_focus.columns[df_focus.columns.str.contains("^RSQR")]
+        df_focus[cols] = df_focus[cols].fillna(0).groupby(level="datetime").apply(_feature_norm)
+
+        cols = df_focus.columns[df_focus.columns.str.contains("^MAX|^HIGH0")]
+        df_focus[cols] = df_focus[cols].apply(lambda x: (x - 1) ** 0.5).groupby(level="datetime").apply(_feature_norm)
+
+        cols = df_focus.columns[df_focus.columns.str.contains("^MIN|^LOW0")]
+        df_focus[cols] = df_focus[cols].apply(lambda x: (1 - x) ** 0.5).groupby(level="datetime").apply(_feature_norm)
+
+        cols = df_focus.columns[df_focus.columns.str.contains("^CORR|^CORD")]
+        df_focus[cols] = df_focus[cols].apply(np.exp).groupby(level="datetime").apply(_feature_norm)
+
+        cols = df_focus.columns[df_focus.columns.str.contains("^WVMA")]
+        df_focus[cols] = df_focus[cols].apply(np.log1p).groupby(level="datetime").apply(_feature_norm)
+
+        df[selected_cols] = df_focus.values
+
+        TimeInspector.log_cost_time("Finished preprocessing data.")
+
+        return df
```

## qlib/contrib/data/utils/sepdf.py

 * *Ordering differences only*

```diff
@@ -1,210 +1,210 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-import pandas as pd
-from typing import Dict, Iterable, Union
-
-
-def align_index(df_dict, join):
-    res = {}
-    for k, df in df_dict.items():
-        if join is not None and k != join:
-            df = df.reindex(df_dict[join].index)
-        res[k] = df
-    return res
-
-
-# Mocking the pd.DataFrame class
-class SepDataFrame:
-    """
-    (Sep)erate DataFrame
-    We usually concat multiple dataframe to be processed together(Such as feature, label, weight, filter).
-    However, they are usually be used separately at last.
-    This will result in extra cost for concatenating and splitting data(reshaping and copying data in the memory is very expensive)
-
-    SepDataFrame tries to act like a DataFrame whose column with multiindex
-    """
-
-    # TODO:
-    # SepDataFrame try to behave like pandas dataframe,  but it is still not them same
-    # Contributions are welcome to make it more complete.
-
-    def __init__(self, df_dict: Dict[str, pd.DataFrame], join: str, skip_align=False):
-        """
-        initialize the data based on the dataframe dictionary
-
-        Parameters
-        ----------
-        df_dict : Dict[str, pd.DataFrame]
-            dataframe dictionary
-        join : str
-            how to join the data
-            It will reindex the dataframe based on the join key.
-            If join is None, the reindex step will be skipped
-
-        skip_align :
-            for some cases, we can improve performance by skipping aligning index
-        """
-        self.join = join
-
-        if skip_align:
-            self._df_dict = df_dict
-        else:
-            self._df_dict = align_index(df_dict, join)
-
-    @property
-    def loc(self):
-        return SDFLoc(self, join=self.join)
-
-    @property
-    def index(self):
-        return self._df_dict[self.join].index
-
-    def apply_each(self, method: str, skip_align=True, *args, **kwargs):
-        """
-        Assumptions:
-        - inplace methods will return None
-        """
-        inplace = False
-        df_dict = {}
-        for k, df in self._df_dict.items():
-            df_dict[k] = getattr(df, method)(*args, **kwargs)
-            if df_dict[k] is None:
-                inplace = True
-        if not inplace:
-            return SepDataFrame(df_dict=df_dict, join=self.join, skip_align=skip_align)
-
-    def sort_index(self, *args, **kwargs):
-        return self.apply_each("sort_index", True, *args, **kwargs)
-
-    def copy(self, *args, **kwargs):
-        return self.apply_each("copy", True, *args, **kwargs)
-
-    def _update_join(self):
-        if self.join not in self:
-            if len(self._df_dict) > 0:
-                self.join = next(iter(self._df_dict.keys()))
-            else:
-                # NOTE: this will change the behavior of previous reindex when all the keys are empty
-                self.join = None
-
-    def __getitem__(self, item):
-        # TODO: behave more like pandas when multiindex
-        return self._df_dict[item]
-
-    def __setitem__(self, item: str, df: Union[pd.DataFrame, pd.Series]):
-        # TODO: consider the join behavior
-        if not isinstance(item, tuple):
-            self._df_dict[item] = df
-        else:
-            # NOTE: corner case of MultiIndex
-            _df_dict_key, *col_name = item
-            col_name = tuple(col_name)
-            if _df_dict_key in self._df_dict:
-                if len(col_name) == 1:
-                    col_name = col_name[0]
-                self._df_dict[_df_dict_key][col_name] = df
-            else:
-                if isinstance(df, pd.Series):
-                    if len(col_name) == 1:
-                        col_name = col_name[0]
-                    self._df_dict[_df_dict_key] = df.to_frame(col_name)
-                else:
-                    df_copy = df.copy()  # avoid changing df
-                    df_copy.columns = pd.MultiIndex.from_tuples([(*col_name, *idx) for idx in df.columns.to_list()])
-                    self._df_dict[_df_dict_key] = df_copy
-
-    def __delitem__(self, item: str):
-        del self._df_dict[item]
-        self._update_join()
-
-    def __contains__(self, item):
-        return item in self._df_dict
-
-    def __len__(self):
-        return len(self._df_dict[self.join])
-
-    def droplevel(self, *args, **kwargs):
-        raise NotImplementedError(f"Please implement the `droplevel` method")
-
-    @property
-    def columns(self):
-        dfs = []
-        for k, df in self._df_dict.items():
-            df = df.head(0)
-            df.columns = pd.MultiIndex.from_product([[k], df.columns])
-            dfs.append(df)
-        return pd.concat(dfs, axis=1).columns
-
-    # Useless methods
-    @staticmethod
-    def merge(df_dict: Dict[str, pd.DataFrame], join: str):
-        all_df = df_dict[join]
-        for k, df in df_dict.items():
-            if k != join:
-                all_df = all_df.join(df)
-        return all_df
-
-
-class SDFLoc:
-    """Mock Class"""
-
-    def __init__(self, sdf: SepDataFrame, join):
-        self._sdf = sdf
-        self.axis = None
-        self.join = join
-
-    def __call__(self, axis):
-        self.axis = axis
-        return self
-
-    def __getitem__(self, args):
-        if self.axis == 1:
-            if isinstance(args, str):
-                return self._sdf[args]
-            elif isinstance(args, (tuple, list)):
-                new_df_dict = {k: self._sdf[k] for k in args}
-                return SepDataFrame(new_df_dict, join=self.join if self.join in args else args[0], skip_align=True)
-            else:
-                raise NotImplementedError(f"This type of input is not supported")
-        elif self.axis == 0:
-            return SepDataFrame(
-                {k: df.loc(axis=0)[args] for k, df in self._sdf._df_dict.items()}, join=self.join, skip_align=True
-            )
-        else:
-            df = self._sdf
-            if isinstance(args, tuple):
-                ax0, *ax1 = args
-                if len(ax1) == 0:
-                    ax1 = None
-                if ax1 is not None:
-                    df = df.loc(axis=1)[ax1]
-                if ax0 is not None:
-                    df = df.loc(axis=0)[ax0]
-                return df
-            else:
-                return df.loc(axis=0)[args]
-
-
-# Patch pandas DataFrame
-# Tricking isinstance to accept SepDataFrame as its subclass
-import builtins
-
-
-def _isinstance(instance, cls):
-    if isinstance_orig(instance, SepDataFrame):  # pylint: disable=E0602  # noqa: F821
-        if isinstance(cls, Iterable):
-            for c in cls:
-                if c is pd.DataFrame:
-                    return True
-        elif cls is pd.DataFrame:
-            return True
-    return isinstance_orig(instance, cls)  # pylint: disable=E0602  # noqa: F821
-
-
-builtins.isinstance_orig = builtins.isinstance
-builtins.isinstance = _isinstance
-
-if __name__ == "__main__":
-    sdf = SepDataFrame({}, join=None)
-    print(isinstance(sdf, (pd.DataFrame,)))
-    print(isinstance(sdf, pd.DataFrame))
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+import pandas as pd
+from typing import Dict, Iterable, Union
+
+
+def align_index(df_dict, join):
+    res = {}
+    for k, df in df_dict.items():
+        if join is not None and k != join:
+            df = df.reindex(df_dict[join].index)
+        res[k] = df
+    return res
+
+
+# Mocking the pd.DataFrame class
+class SepDataFrame:
+    """
+    (Sep)erate DataFrame
+    We usually concat multiple dataframe to be processed together(Such as feature, label, weight, filter).
+    However, they are usually be used separately at last.
+    This will result in extra cost for concatenating and splitting data(reshaping and copying data in the memory is very expensive)
+
+    SepDataFrame tries to act like a DataFrame whose column with multiindex
+    """
+
+    # TODO:
+    # SepDataFrame try to behave like pandas dataframe,  but it is still not them same
+    # Contributions are welcome to make it more complete.
+
+    def __init__(self, df_dict: Dict[str, pd.DataFrame], join: str, skip_align=False):
+        """
+        initialize the data based on the dataframe dictionary
+
+        Parameters
+        ----------
+        df_dict : Dict[str, pd.DataFrame]
+            dataframe dictionary
+        join : str
+            how to join the data
+            It will reindex the dataframe based on the join key.
+            If join is None, the reindex step will be skipped
+
+        skip_align :
+            for some cases, we can improve performance by skipping aligning index
+        """
+        self.join = join
+
+        if skip_align:
+            self._df_dict = df_dict
+        else:
+            self._df_dict = align_index(df_dict, join)
+
+    @property
+    def loc(self):
+        return SDFLoc(self, join=self.join)
+
+    @property
+    def index(self):
+        return self._df_dict[self.join].index
+
+    def apply_each(self, method: str, skip_align=True, *args, **kwargs):
+        """
+        Assumptions:
+        - inplace methods will return None
+        """
+        inplace = False
+        df_dict = {}
+        for k, df in self._df_dict.items():
+            df_dict[k] = getattr(df, method)(*args, **kwargs)
+            if df_dict[k] is None:
+                inplace = True
+        if not inplace:
+            return SepDataFrame(df_dict=df_dict, join=self.join, skip_align=skip_align)
+
+    def sort_index(self, *args, **kwargs):
+        return self.apply_each("sort_index", True, *args, **kwargs)
+
+    def copy(self, *args, **kwargs):
+        return self.apply_each("copy", True, *args, **kwargs)
+
+    def _update_join(self):
+        if self.join not in self:
+            if len(self._df_dict) > 0:
+                self.join = next(iter(self._df_dict.keys()))
+            else:
+                # NOTE: this will change the behavior of previous reindex when all the keys are empty
+                self.join = None
+
+    def __getitem__(self, item):
+        # TODO: behave more like pandas when multiindex
+        return self._df_dict[item]
+
+    def __setitem__(self, item: str, df: Union[pd.DataFrame, pd.Series]):
+        # TODO: consider the join behavior
+        if not isinstance(item, tuple):
+            self._df_dict[item] = df
+        else:
+            # NOTE: corner case of MultiIndex
+            _df_dict_key, *col_name = item
+            col_name = tuple(col_name)
+            if _df_dict_key in self._df_dict:
+                if len(col_name) == 1:
+                    col_name = col_name[0]
+                self._df_dict[_df_dict_key][col_name] = df
+            else:
+                if isinstance(df, pd.Series):
+                    if len(col_name) == 1:
+                        col_name = col_name[0]
+                    self._df_dict[_df_dict_key] = df.to_frame(col_name)
+                else:
+                    df_copy = df.copy()  # avoid changing df
+                    df_copy.columns = pd.MultiIndex.from_tuples([(*col_name, *idx) for idx in df.columns.to_list()])
+                    self._df_dict[_df_dict_key] = df_copy
+
+    def __delitem__(self, item: str):
+        del self._df_dict[item]
+        self._update_join()
+
+    def __contains__(self, item):
+        return item in self._df_dict
+
+    def __len__(self):
+        return len(self._df_dict[self.join])
+
+    def droplevel(self, *args, **kwargs):
+        raise NotImplementedError(f"Please implement the `droplevel` method")
+
+    @property
+    def columns(self):
+        dfs = []
+        for k, df in self._df_dict.items():
+            df = df.head(0)
+            df.columns = pd.MultiIndex.from_product([[k], df.columns])
+            dfs.append(df)
+        return pd.concat(dfs, axis=1).columns
+
+    # Useless methods
+    @staticmethod
+    def merge(df_dict: Dict[str, pd.DataFrame], join: str):
+        all_df = df_dict[join]
+        for k, df in df_dict.items():
+            if k != join:
+                all_df = all_df.join(df)
+        return all_df
+
+
+class SDFLoc:
+    """Mock Class"""
+
+    def __init__(self, sdf: SepDataFrame, join):
+        self._sdf = sdf
+        self.axis = None
+        self.join = join
+
+    def __call__(self, axis):
+        self.axis = axis
+        return self
+
+    def __getitem__(self, args):
+        if self.axis == 1:
+            if isinstance(args, str):
+                return self._sdf[args]
+            elif isinstance(args, (tuple, list)):
+                new_df_dict = {k: self._sdf[k] for k in args}
+                return SepDataFrame(new_df_dict, join=self.join if self.join in args else args[0], skip_align=True)
+            else:
+                raise NotImplementedError(f"This type of input is not supported")
+        elif self.axis == 0:
+            return SepDataFrame(
+                {k: df.loc(axis=0)[args] for k, df in self._sdf._df_dict.items()}, join=self.join, skip_align=True
+            )
+        else:
+            df = self._sdf
+            if isinstance(args, tuple):
+                ax0, *ax1 = args
+                if len(ax1) == 0:
+                    ax1 = None
+                if ax1 is not None:
+                    df = df.loc(axis=1)[ax1]
+                if ax0 is not None:
+                    df = df.loc(axis=0)[ax0]
+                return df
+            else:
+                return df.loc(axis=0)[args]
+
+
+# Patch pandas DataFrame
+# Tricking isinstance to accept SepDataFrame as its subclass
+import builtins
+
+
+def _isinstance(instance, cls):
+    if isinstance_orig(instance, SepDataFrame):  # pylint: disable=E0602  # noqa: F821
+        if isinstance(cls, Iterable):
+            for c in cls:
+                if c is pd.DataFrame:
+                    return True
+        elif cls is pd.DataFrame:
+            return True
+    return isinstance_orig(instance, cls)  # pylint: disable=E0602  # noqa: F821
+
+
+builtins.isinstance_orig = builtins.isinstance
+builtins.isinstance = _isinstance
+
+if __name__ == "__main__":
+    sdf = SepDataFrame({}, join=None)
+    print(isinstance(sdf, (pd.DataFrame,)))
+    print(isinstance(sdf, pd.DataFrame))
```

## qlib/contrib/eva/alpha.py

 * *Ordering differences only*

```diff
@@ -1,212 +1,212 @@
-"""
-Here is a batch of evaluation functions.
-
-The interface should be redesigned carefully in the future.
-"""
-import pandas as pd
-from typing import Tuple
-from qlib import get_module_logger
-from qlib.utils.paral import complex_parallel, DelayedDict
-from joblib import Parallel, delayed
-
-
-def calc_long_short_prec(
-    pred: pd.Series, label: pd.Series, date_col="datetime", quantile: float = 0.2, dropna=False, is_alpha=False
-) -> Tuple[pd.Series, pd.Series]:
-    """
-    calculate the precision for long and short operation
-
-
-    :param pred/label: index is **pd.MultiIndex**, index name is **[datetime, instruments]**; columns names is **[score]**.
-
-            .. code-block:: python
-                                                  score
-                datetime            instrument
-                2020-12-01 09:30:00 SH600068    0.553634
-                                    SH600195    0.550017
-                                    SH600276    0.540321
-                                    SH600584    0.517297
-                                    SH600715    0.544674
-    label :
-        label
-    date_col :
-        date_col
-
-    Returns
-    -------
-    (pd.Series, pd.Series)
-        long precision and short precision in time level
-    """
-    if is_alpha:
-        label = label - label.mean(level=date_col)
-    if int(1 / quantile) >= len(label.index.get_level_values(1).unique()):
-        raise ValueError("Need more instruments to calculate precision")
-
-    df = pd.DataFrame({"pred": pred, "label": label})
-    if dropna:
-        df.dropna(inplace=True)
-
-    group = df.groupby(level=date_col)
-
-    def N(x):
-        return int(len(x) * quantile)
-
-    # find the top/low quantile of prediction and treat them as long and short target
-    long = group.apply(lambda x: x.nlargest(N(x), columns="pred").label).reset_index(level=0, drop=True)
-    short = group.apply(lambda x: x.nsmallest(N(x), columns="pred").label).reset_index(level=0, drop=True)
-
-    groupll = long.groupby(date_col)
-    l_dom = groupll.apply(lambda x: x > 0)
-    l_c = groupll.count()
-
-    groups = short.groupby(date_col)
-    s_dom = groups.apply(lambda x: x < 0)
-    s_c = groups.count()
-    return (l_dom.groupby(date_col).sum() / l_c), (s_dom.groupby(date_col).sum() / s_c)
-
-
-def calc_long_short_return(
-    pred: pd.Series,
-    label: pd.Series,
-    date_col: str = "datetime",
-    quantile: float = 0.2,
-    dropna: bool = False,
-) -> Tuple[pd.Series, pd.Series]:
-    """
-    calculate long-short return
-
-    Note:
-        `label` must be raw stock returns.
-
-    Parameters
-    ----------
-    pred : pd.Series
-        stock predictions
-    label : pd.Series
-        stock returns
-    date_col : str
-        datetime index name
-    quantile : float
-        long-short quantile
-
-    Returns
-    ----------
-    long_short_r : pd.Series
-        daily long-short returns
-    long_avg_r : pd.Series
-        daily long-average returns
-    """
-    df = pd.DataFrame({"pred": pred, "label": label})
-    if dropna:
-        df.dropna(inplace=True)
-    group = df.groupby(level=date_col)
-
-    def N(x):
-        return int(len(x) * quantile)
-
-    r_long = group.apply(lambda x: x.nlargest(N(x), columns="pred").label.mean())
-    r_short = group.apply(lambda x: x.nsmallest(N(x), columns="pred").label.mean())
-    r_avg = group.label.mean()
-    return (r_long - r_short) / 2, r_avg
-
-
-def pred_autocorr(pred: pd.Series, lag=1, inst_col="instrument", date_col="datetime"):
-    """pred_autocorr.
-
-    Limitation:
-    - If the datetime is not sequential densely, the correlation will be calulated based on adjacent dates. (some users may expected NaN)
-
-    :param pred: pd.Series with following format
-                instrument  datetime
-                SH600000    2016-01-04   -0.000403
-                            2016-01-05   -0.000753
-                            2016-01-06   -0.021801
-                            2016-01-07   -0.065230
-                            2016-01-08   -0.062465
-    :type pred: pd.Series
-    :param lag:
-    """
-    if isinstance(pred, pd.DataFrame):
-        pred = pred.iloc[:, 0]
-        get_module_logger("pred_autocorr").warning(f"Only the first column in {pred.columns} of `pred` is kept")
-    pred_ustk = pred.sort_index().unstack(inst_col)
-    corr_s = {}
-    for (idx, cur), (_, prev) in zip(pred_ustk.iterrows(), pred_ustk.shift(lag).iterrows()):
-        corr_s[idx] = cur.corr(prev)
-    corr_s = pd.Series(corr_s).sort_index()
-    return corr_s
-
-
-def pred_autocorr_all(pred_dict, n_jobs=-1, **kwargs):
-    """
-    calculate auto correlation for pred_dict
-
-    Parameters
-    ----------
-    pred_dict : dict
-        A dict like {<method_name>:  <prediction>}
-    kwargs :
-        all these arguments will be passed into pred_autocorr
-    """
-    ac_dict = {}
-    for k, pred in pred_dict.items():
-        ac_dict[k] = delayed(pred_autocorr)(pred, **kwargs)
-    return complex_parallel(Parallel(n_jobs=n_jobs, verbose=10), ac_dict)
-
-
-def calc_ic(pred: pd.Series, label: pd.Series, date_col="datetime", dropna=False) -> (pd.Series, pd.Series):
-    """calc_ic.
-
-    Parameters
-    ----------
-    pred :
-        pred
-    label :
-        label
-    date_col :
-        date_col
-
-    Returns
-    -------
-    (pd.Series, pd.Series)
-        ic and rank ic
-    """
-    df = pd.DataFrame({"pred": pred, "label": label})
-    ic = df.groupby(date_col).apply(lambda df: df["pred"].corr(df["label"]))
-    ric = df.groupby(date_col).apply(lambda df: df["pred"].corr(df["label"], method="spearman"))
-    if dropna:
-        return ic.dropna(), ric.dropna()
-    else:
-        return ic, ric
-
-
-def calc_all_ic(pred_dict_all, label, date_col="datetime", dropna=False, n_jobs=-1):
-    """calc_all_ic.
-
-    Parameters
-    ----------
-    pred_dict_all :
-        A dict like {<method_name>:  <prediction>}
-    label:
-        A pd.Series of label values
-
-    Returns
-    -------
-    {'Q2+IND_z': {'ic': <ic series like>
-                          2016-01-04   -0.057407
-                          ...
-                          2020-05-28    0.183470
-                          2020-05-29    0.171393
-                  'ric': <rank ic series like>
-                          2016-01-04   -0.040888
-                          ...
-                          2020-05-28    0.236665
-                          2020-05-29    0.183886
-                  }
-    ...}
-    """
-    pred_all_ics = {}
-    for k, pred in pred_dict_all.items():
-        pred_all_ics[k] = DelayedDict(["ic", "ric"], delayed(calc_ic)(pred, label, date_col=date_col, dropna=dropna))
-    pred_all_ics = complex_parallel(Parallel(n_jobs=n_jobs, verbose=10), pred_all_ics)
-    return pred_all_ics
+"""
+Here is a batch of evaluation functions.
+
+The interface should be redesigned carefully in the future.
+"""
+import pandas as pd
+from typing import Tuple
+from qlib import get_module_logger
+from qlib.utils.paral import complex_parallel, DelayedDict
+from joblib import Parallel, delayed
+
+
+def calc_long_short_prec(
+    pred: pd.Series, label: pd.Series, date_col="datetime", quantile: float = 0.2, dropna=False, is_alpha=False
+) -> Tuple[pd.Series, pd.Series]:
+    """
+    calculate the precision for long and short operation
+
+
+    :param pred/label: index is **pd.MultiIndex**, index name is **[datetime, instruments]**; columns names is **[score]**.
+
+            .. code-block:: python
+                                                  score
+                datetime            instrument
+                2020-12-01 09:30:00 SH600068    0.553634
+                                    SH600195    0.550017
+                                    SH600276    0.540321
+                                    SH600584    0.517297
+                                    SH600715    0.544674
+    label :
+        label
+    date_col :
+        date_col
+
+    Returns
+    -------
+    (pd.Series, pd.Series)
+        long precision and short precision in time level
+    """
+    if is_alpha:
+        label = label - label.mean(level=date_col)
+    if int(1 / quantile) >= len(label.index.get_level_values(1).unique()):
+        raise ValueError("Need more instruments to calculate precision")
+
+    df = pd.DataFrame({"pred": pred, "label": label})
+    if dropna:
+        df.dropna(inplace=True)
+
+    group = df.groupby(level=date_col)
+
+    def N(x):
+        return int(len(x) * quantile)
+
+    # find the top/low quantile of prediction and treat them as long and short target
+    long = group.apply(lambda x: x.nlargest(N(x), columns="pred").label).reset_index(level=0, drop=True)
+    short = group.apply(lambda x: x.nsmallest(N(x), columns="pred").label).reset_index(level=0, drop=True)
+
+    groupll = long.groupby(date_col)
+    l_dom = groupll.apply(lambda x: x > 0)
+    l_c = groupll.count()
+
+    groups = short.groupby(date_col)
+    s_dom = groups.apply(lambda x: x < 0)
+    s_c = groups.count()
+    return (l_dom.groupby(date_col).sum() / l_c), (s_dom.groupby(date_col).sum() / s_c)
+
+
+def calc_long_short_return(
+    pred: pd.Series,
+    label: pd.Series,
+    date_col: str = "datetime",
+    quantile: float = 0.2,
+    dropna: bool = False,
+) -> Tuple[pd.Series, pd.Series]:
+    """
+    calculate long-short return
+
+    Note:
+        `label` must be raw stock returns.
+
+    Parameters
+    ----------
+    pred : pd.Series
+        stock predictions
+    label : pd.Series
+        stock returns
+    date_col : str
+        datetime index name
+    quantile : float
+        long-short quantile
+
+    Returns
+    ----------
+    long_short_r : pd.Series
+        daily long-short returns
+    long_avg_r : pd.Series
+        daily long-average returns
+    """
+    df = pd.DataFrame({"pred": pred, "label": label})
+    if dropna:
+        df.dropna(inplace=True)
+    group = df.groupby(level=date_col)
+
+    def N(x):
+        return int(len(x) * quantile)
+
+    r_long = group.apply(lambda x: x.nlargest(N(x), columns="pred").label.mean())
+    r_short = group.apply(lambda x: x.nsmallest(N(x), columns="pred").label.mean())
+    r_avg = group.label.mean()
+    return (r_long - r_short) / 2, r_avg
+
+
+def pred_autocorr(pred: pd.Series, lag=1, inst_col="instrument", date_col="datetime"):
+    """pred_autocorr.
+
+    Limitation:
+    - If the datetime is not sequential densely, the correlation will be calulated based on adjacent dates. (some users may expected NaN)
+
+    :param pred: pd.Series with following format
+                instrument  datetime
+                SH600000    2016-01-04   -0.000403
+                            2016-01-05   -0.000753
+                            2016-01-06   -0.021801
+                            2016-01-07   -0.065230
+                            2016-01-08   -0.062465
+    :type pred: pd.Series
+    :param lag:
+    """
+    if isinstance(pred, pd.DataFrame):
+        pred = pred.iloc[:, 0]
+        get_module_logger("pred_autocorr").warning(f"Only the first column in {pred.columns} of `pred` is kept")
+    pred_ustk = pred.sort_index().unstack(inst_col)
+    corr_s = {}
+    for (idx, cur), (_, prev) in zip(pred_ustk.iterrows(), pred_ustk.shift(lag).iterrows()):
+        corr_s[idx] = cur.corr(prev)
+    corr_s = pd.Series(corr_s).sort_index()
+    return corr_s
+
+
+def pred_autocorr_all(pred_dict, n_jobs=-1, **kwargs):
+    """
+    calculate auto correlation for pred_dict
+
+    Parameters
+    ----------
+    pred_dict : dict
+        A dict like {<method_name>:  <prediction>}
+    kwargs :
+        all these arguments will be passed into pred_autocorr
+    """
+    ac_dict = {}
+    for k, pred in pred_dict.items():
+        ac_dict[k] = delayed(pred_autocorr)(pred, **kwargs)
+    return complex_parallel(Parallel(n_jobs=n_jobs, verbose=10), ac_dict)
+
+
+def calc_ic(pred: pd.Series, label: pd.Series, date_col="datetime", dropna=False) -> (pd.Series, pd.Series):
+    """calc_ic.
+
+    Parameters
+    ----------
+    pred :
+        pred
+    label :
+        label
+    date_col :
+        date_col
+
+    Returns
+    -------
+    (pd.Series, pd.Series)
+        ic and rank ic
+    """
+    df = pd.DataFrame({"pred": pred, "label": label})
+    ic = df.groupby(date_col).apply(lambda df: df["pred"].corr(df["label"]))
+    ric = df.groupby(date_col).apply(lambda df: df["pred"].corr(df["label"], method="spearman"))
+    if dropna:
+        return ic.dropna(), ric.dropna()
+    else:
+        return ic, ric
+
+
+def calc_all_ic(pred_dict_all, label, date_col="datetime", dropna=False, n_jobs=-1):
+    """calc_all_ic.
+
+    Parameters
+    ----------
+    pred_dict_all :
+        A dict like {<method_name>:  <prediction>}
+    label:
+        A pd.Series of label values
+
+    Returns
+    -------
+    {'Q2+IND_z': {'ic': <ic series like>
+                          2016-01-04   -0.057407
+                          ...
+                          2020-05-28    0.183470
+                          2020-05-29    0.171393
+                  'ric': <rank ic series like>
+                          2016-01-04   -0.040888
+                          ...
+                          2020-05-28    0.236665
+                          2020-05-29    0.183886
+                  }
+    ...}
+    """
+    pred_all_ics = {}
+    for k, pred in pred_dict_all.items():
+        pred_all_ics[k] = DelayedDict(["ic", "ric"], delayed(calc_ic)(pred, label, date_col=date_col, dropna=dropna))
+    pred_all_ics = complex_parallel(Parallel(n_jobs=n_jobs, verbose=10), pred_all_ics)
+    return pred_all_ics
```

## qlib/contrib/meta/__init__.py

 * *Ordering differences only*

```diff
@@ -1,7 +1,7 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-from .data_selection import MetaTaskDS, MetaDatasetDS, MetaModelDS
-
-
-__all__ = ["MetaTaskDS", "MetaDatasetDS", "MetaModelDS"]
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+from .data_selection import MetaTaskDS, MetaDatasetDS, MetaModelDS
+
+
+__all__ = ["MetaTaskDS", "MetaDatasetDS", "MetaModelDS"]
```

## qlib/contrib/meta/data_selection/__init__.py

 * *Ordering differences only*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-from .dataset import MetaDatasetDS, MetaTaskDS
-from .model import MetaModelDS
-
-
-__all__ = ["MetaDatasetDS", "MetaTaskDS", "MetaModelDS"]
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+from .dataset import MetaDatasetDS, MetaTaskDS
+from .model import MetaModelDS
+
+
+__all__ = ["MetaDatasetDS", "MetaTaskDS", "MetaModelDS"]
```

## qlib/contrib/meta/data_selection/dataset.py

```diff
@@ -1,390 +1,392 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-import pandas as pd
-import numpy as np
-from copy import deepcopy
-from joblib import Parallel, delayed  # pylint: disable=E0401
-from typing import Dict, List, Union, Text, Tuple
-from qlib.data.dataset.utils import init_task_handler
-from qlib.data.dataset import DatasetH
-from qlib.contrib.torch import data_to_tensor
-from qlib.model.meta.task import MetaTask
-from qlib.model.meta.dataset import MetaTaskDataset
-from qlib.model.trainer import TrainerR
-from qlib.log import get_module_logger
-from qlib.utils import auto_filter_kwargs, get_date_by_shift, init_instance_by_config
-from qlib.utils.data import deepcopy_basic_type
-from qlib.workflow import R
-from qlib.workflow.task.gen import RollingGen, task_generator
-from qlib.workflow.task.utils import TimeAdjuster
-from tqdm.auto import tqdm
-
-
-class InternalData:
-    def __init__(self, task_tpl: dict, step: int, exp_name: str):
-        self.task_tpl = task_tpl
-        self.step = step
-        self.exp_name = exp_name
-
-    def setup(self, trainer=TrainerR, trainer_kwargs={}):
-        """
-        after running this function `self.data_ic_df` will become set.
-        Each col represents a data.
-        Each row represents the Timestamp of performance of that data.
-        For example,
-
-        .. code-block:: python
-
-                       2021-06-21 2021-06-04 2021-05-21 2021-05-07 2021-04-20 2021-04-06 2021-03-22 2021-03-08  ...
-                       2021-07-02 2021-06-18 2021-06-03 2021-05-20 2021-05-06 2021-04-19 2021-04-02 2021-03-19  ...
-            datetime                                                                                            ...
-            2018-01-02   0.079782   0.115975   0.070866   0.028849  -0.081170   0.140380   0.063864   0.110987  ...
-            2018-01-03   0.123386   0.107789   0.071037   0.045278  -0.060782   0.167446   0.089779   0.124476  ...
-            2018-01-04   0.140775   0.097206   0.063702   0.042415  -0.078164   0.173218   0.098914   0.114389  ...
-            2018-01-05   0.030320  -0.037209  -0.044536  -0.047267  -0.081888   0.045648   0.059947   0.047652  ...
-            2018-01-08   0.107201   0.009219  -0.015995  -0.036594  -0.086633   0.108965   0.122164   0.108508  ...
-            ...               ...        ...        ...        ...        ...        ...        ...        ...  ...
-
-        """
-
-        # 1) prepare the prediction of proxy models
-        perf_task_tpl = deepcopy(self.task_tpl)  # this task is supposed to contains no complicated objects
-
-        trainer = auto_filter_kwargs(trainer)(experiment_name=self.exp_name, **trainer_kwargs)
-        # NOTE:
-        # The handler is initialized for only once.
-        if not trainer.has_worker():
-            self.dh = init_task_handler(perf_task_tpl)
-            self.dh.config(dump_all=False)  # in some cases, the data handler are saved to disk with `dump_all=True`
-        else:
-            self.dh = init_instance_by_config(perf_task_tpl["dataset"]["kwargs"]["handler"])
-        assert self.dh.dump_all is False  # otherwise, it will save all the detailed data
-
-        seg = perf_task_tpl["dataset"]["kwargs"]["segments"]
-
-        # We want to split the training time period into small segments.
-        perf_task_tpl["dataset"]["kwargs"]["segments"] = {
-            "train": (DatasetH.get_min_time(seg), DatasetH.get_max_time(seg)),
-            "test": (None, None),
-        }
-
-        # NOTE:
-        # we play a trick here
-        # treat the training segments as test to create the rolling tasks
-        rg = RollingGen(step=self.step, test_key="train", train_key=None, task_copy_func=deepcopy_basic_type)
-        gen_task = task_generator(perf_task_tpl, [rg])
-
-        recorders = R.list_recorders(experiment_name=self.exp_name)
-        if len(gen_task) == len(recorders):
-            get_module_logger("Internal Data").info("the data has been initialized")
-        else:
-            # train new models
-            assert 0 == len(recorders), "An empty experiment is required for setup `InternalData`"
-            trainer.train(gen_task)
-
-        # 2) extract the similarity matrix
-        label_df = self.dh.fetch(col_set="label")
-        # for
-        recorders = R.list_recorders(experiment_name=self.exp_name)
-
-        key_l = []
-        ic_l = []
-        for _, rec in tqdm(recorders.items(), desc="calc"):
-            pred = rec.load_object("pred.pkl")
-            task = rec.load_object("task")
-            data_key = task["dataset"]["kwargs"]["segments"]["train"]
-            key_l.append(data_key)
-            ic_l.append(delayed(self._calc_perf)(pred.iloc[:, 0], label_df.iloc[:, 0]))
-
-        ic_l = Parallel(n_jobs=-1)(ic_l)
-        self.data_ic_df = pd.DataFrame(dict(zip(key_l, ic_l)))
-        self.data_ic_df = self.data_ic_df.sort_index().sort_index(axis=1)
-
-        del self.dh  # handler is not useful now
-
-    def _calc_perf(self, pred, label):
-        df = pd.DataFrame({"pred": pred, "label": label})
-        df = df.groupby("datetime").corr(method="spearman")
-        corr = df.loc(axis=0)[:, "pred"]["label"].droplevel(axis=0, level=-1)
-        return corr
-
-    def update(self):
-        """update the data for online trading"""
-        # TODO:
-        # when new data are totally(including label) available
-        # - update the prediction
-        # - update the data similarity map(if applied)
-
-
-class MetaTaskDS(MetaTask):
-    """Meta Task for Data Selection"""
-
-    def __init__(self, task: dict, meta_info: pd.DataFrame, mode: str = MetaTask.PROC_MODE_FULL, fill_method="max"):
-        """
-
-        The description of the processed data
-
-            time_perf: A array with shape  <hist_step_n * step, data pieces>  ->  data piece performance
-
-            time_belong:  A array with shape <sample, data pieces>  -> belong or not (1. or 0.)
-            array([[1., 0., 0., ..., 0., 0., 0.],
-                   [1., 0., 0., ..., 0., 0., 0.],
-                   [1., 0., 0., ..., 0., 0., 0.],
-                   ...,
-                   [0., 0., 0., ..., 0., 0., 1.],
-                   [0., 0., 0., ..., 0., 0., 1.],
-                   [0., 0., 0., ..., 0., 0., 1.]])
-
-        Parameters
-        ----------
-        meta_info: pd.DataFrame
-            please refer to the docs of _prepare_meta_ipt for detailed explanation.
-        """
-        super().__init__(task, meta_info)
-        self.fill_method = fill_method
-
-        time_perf = self._get_processed_meta_info()
-        self.processed_meta_input = {"time_perf": time_perf}
-        # FIXME: memory issue in this step
-        if mode == MetaTask.PROC_MODE_FULL:
-            # process metainfo_
-            ds = self.get_dataset()
-
-            # these three lines occupied 70% of the time of initializing MetaTaskDS
-            d_train, d_test = ds.prepare(["train", "test"], col_set=["feature", "label"])
-            prev_size = d_test.shape[0]
-            d_train = d_train.dropna(axis=0)
-            d_test = d_test.dropna(axis=0)
-            if prev_size == 0 or d_test.shape[0] / prev_size <= 0.1:
-                raise ValueError(f"Most of samples are dropped. Please check this task: {task}")
-
-            assert (
-                d_test.groupby("datetime").size().shape[0] >= 5
-            ), "In this segment, this trading dates is less than 5, you'd better check the data."
-
-            sample_time_belong = np.zeros((d_train.shape[0], time_perf.shape[1]))
-            for i, col in enumerate(time_perf.columns):
-                # these two lines of code occupied 20% of the time of initializing MetaTaskDS
-                slc = slice(*d_train.index.slice_locs(start=col[0], end=col[1]))
-                sample_time_belong[slc, i] = 1.0
-
-            # If you want that last month also belongs to the last time_perf
-            # Assumptions: the latest data has similar performance like the last month
-            sample_time_belong[sample_time_belong.sum(axis=1) != 1, -1] = 1.0
-
-            self.processed_meta_input.update(
-                dict(
-                    X=d_train["feature"],
-                    y=d_train["label"].iloc[:, 0],
-                    X_test=d_test["feature"],
-                    y_test=d_test["label"].iloc[:, 0],
-                    time_belong=sample_time_belong,
-                    test_idx=d_test["label"].index,
-                )
-            )
-
-        # TODO: set device: I think this is not necessary to converting data format.
-        self.processed_meta_input = data_to_tensor(self.processed_meta_input)
-
-    def _get_processed_meta_info(self):
-        meta_info_norm = self.meta_info.sub(self.meta_info.mean(axis=1), axis=0)
-        if self.fill_method.startswith("max"):
-            suffix = self.fill_method.lstrip("max")
-            if suffix == "seg":
-                fill_value = {}
-                for col in meta_info_norm.columns:
-                    fill_value[col] = meta_info_norm.loc[meta_info_norm[col].isna(), :].dropna(axis=1).mean().max()
-                fill_value = pd.Series(fill_value).sort_index()
-                # The NaN Values are filled segment-wise. Below is an exampleof fill_value
-                # 2009-01-05  2009-02-06    0.145809
-                # 2009-02-09  2009-03-06    0.148005
-                # 2009-03-09  2009-04-03    0.090385
-                # 2009-04-07  2009-05-05    0.114318
-                # 2009-05-06  2009-06-04    0.119328
-                # ...
-                meta_info_norm = meta_info_norm.fillna(fill_value)
-            else:
-                if len(suffix) > 0:
-                    get_module_logger("MetaTaskDS").warning(
-                        f"fill_method={self.fill_method}; the info after can't be correctly parsed. Please check your parameters."
-                    )
-                fill_value = meta_info_norm.max(axis=1)
-                # fill it with row max to align with previous implementation
-                # This will magnify the data similarity when data is in daily freq
-
-                # the fill value corresponds to data like this
-                # It get a performance value for each day.
-                # The performance value are get from other models on this day
-                # 2009-01-16    0.276320
-                # 2009-01-19    0.280603
-                #                 ...
-                # 2011-06-27    0.203773
-                meta_info_norm = meta_info_norm.T.fillna(fill_value).T
-        elif self.fill_method == "zero":
-            # It will fillna(0.0) at the end.
-            pass
-        else:
-            raise NotImplementedError(f"This type of input is not supported")
-        meta_info_norm = meta_info_norm.fillna(0.0)  # always fill zero in case of NaN
-        return meta_info_norm
-
-    def get_meta_input(self):
-        return self.processed_meta_input
-
-
-class MetaDatasetDS(MetaTaskDataset):
-    def __init__(
-        self,
-        *,
-        task_tpl: Union[dict, list],
-        step: int,
-        trunc_days: int = None,
-        rolling_ext_days: int = 0,
-        exp_name: Union[str, InternalData],
-        segments: Union[Dict[Text, Tuple], float],
-        hist_step_n: int = 10,
-        task_mode: str = MetaTask.PROC_MODE_FULL,
-        fill_method: str = "max",
-    ):
-        """
-        A dataset for meta model.
-
-        Parameters
-        ----------
-        task_tpl : Union[dict, list]
-            Decide what tasks are used.
-            - dict : the task template, the prepared task is generated with `step`, `trunc_days` and `RollingGen`
-            - list : when list, use the list of tasks directly
-                     the list is supposed to be sorted according timeline
-        step : int
-            the rolling step
-        trunc_days: int
-            days to be truncated based on the test start
-        rolling_ext_days: int
-            sometimes users want to train meta models for a longer test period but with smaller rolling steps for more task samples.
-            the total length of test periods will be `step + rolling_ext_days`
-
-        exp_name : Union[str, InternalData]
-            Decide what meta_info are used for prediction.
-            - str: the name of the experiment to store the performance of data
-            - InternalData: a prepared internal data
-        segments: Union[Dict[Text, Tuple], float]
-            the segments to divide data
-            both left and right
-            if segments is a float:
-                the float represents the percentage of data for training
-        hist_step_n: int
-            length of historical steps for the meta infomation
-        task_mode : str
-            Please refer to the docs of MetaTask
-        """
-        super().__init__(segments=segments)
-        if isinstance(exp_name, InternalData):
-            self.internal_data = exp_name
-        else:
-            self.internal_data = InternalData(task_tpl, step=step, exp_name=exp_name)
-            self.internal_data.setup()
-        self.task_tpl = deepcopy(task_tpl)  # FIXME: if the handler is shared, how to avoid the explosion of the memroy.
-        self.trunc_days = trunc_days
-        self.hist_step_n = hist_step_n
-        self.step = step
-
-        if isinstance(task_tpl, dict):
-            rg = RollingGen(
-                step=step, trunc_days=trunc_days, task_copy_func=deepcopy_basic_type
-            )  # NOTE: trunc_days is very important !!!!
-            task_iter = rg(task_tpl)
-            if rolling_ext_days > 0:
-                self.ta = TimeAdjuster(future=True)
-                for t in task_iter:
-                    t["dataset"]["kwargs"]["segments"]["test"] = self.ta.shift(
-                        t["dataset"]["kwargs"]["segments"]["test"], step=rolling_ext_days, rtype=RollingGen.ROLL_EX
-                    )
-            if task_mode == MetaTask.PROC_MODE_FULL:
-                # Only pre initializing the task when full task is req
-                # initializing handler and share it.
-                init_task_handler(task_tpl)
-        else:
-            assert isinstance(task_tpl, list)
-            task_iter = task_tpl
-
-        self.task_list = []
-        self.meta_task_l = []
-        logger = get_module_logger("MetaDatasetDS")
-        logger.info(f"Example task for training meta model: {task_iter[0]}")
-        for t in tqdm(task_iter, desc="creating meta tasks"):
-            try:
-                self.meta_task_l.append(
-                    MetaTaskDS(t, meta_info=self._prepare_meta_ipt(t), mode=task_mode, fill_method=fill_method)
-                )
-                self.task_list.append(t)
-            except ValueError as e:
-                logger.warning(f"ValueError: {e}")
-        assert len(self.meta_task_l) > 0, "No meta tasks found. Please check the data and setting"
-
-    def _prepare_meta_ipt(self, task) -> pd.DataFrame:
-        """
-        Please refer to `self.internal_data.setup` for detailed information about `self.internal_data.data_ic_df`
-
-        Indices with format below can be successfully sliced by  `ic_df.loc[:end, pd.IndexSlice[:, :end]]`
-
-               2021-06-21 2021-06-04 .. 2021-03-22 2021-03-08
-               2021-07-02 2021-06-18 .. 2021-04-02 None
-
-        Returns
-        -------
-            a pd.DataFrame with similar content below.
-            - each column corresponds to a trained model named by the training data range
-            - each row corresponds to a day of data tested by the models of the columns
-            - The rows cells that overlaps with the data used by columns are masked
-
-
-                       2009-01-05 2009-02-09 ... 2011-04-27 2011-05-26
-                       2009-02-06 2009-03-06 ... 2011-05-25 2011-06-23
-            datetime                         ...
-            2009-01-13        NaN   0.310639 ...  -0.169057   0.137792
-            2009-01-14        NaN   0.261086 ...  -0.143567   0.082581
-            ...               ...        ... ...        ...        ...
-            2011-06-30  -0.054907  -0.020219 ...  -0.023226        NaN
-            2011-07-01  -0.075762  -0.026626 ...  -0.003167        NaN
-
-        """
-        ic_df = self.internal_data.data_ic_df
-
-        segs = task["dataset"]["kwargs"]["segments"]
-        end = max(segs[k][1] for k in ("train", "valid") if k in segs)
-        ic_df_avail = ic_df.loc[:end, pd.IndexSlice[:, :end]]
-
-        # meta data set focus on the **information** instead of preprocess
-        # 1) filter the overlap info
-        def mask_overlap(s):
-            """
-            mask overlap information
-            data after self.name[end] with self.trunc_days that contains future info are also considered as overlap info
-
-            Approximately the diagnal + horizon length of data are masked.
-            """
-            start, end = s.name
-            end = get_date_by_shift(trading_date=end, shift=self.trunc_days - 1, future=True)
-            return s.mask((s.index >= start) & (s.index <= end))
-
-        ic_df_avail = ic_df_avail.apply(mask_overlap)  # apply to each col
-
-        # 2) filter the info with too long periods
-        total_len = self.step * self.hist_step_n
-        if ic_df_avail.shape[0] >= total_len:
-            return ic_df_avail.iloc[-total_len:]
-        else:
-            raise ValueError("the history of distribution data is not long enough.")
-
-    def _prepare_seg(self, segment: Text) -> List[MetaTask]:
-        if isinstance(self.segments, float):
-            train_task_n = int(len(self.meta_task_l) * self.segments)
-            if segment == "train":
-                return self.meta_task_l[:train_task_n]
-            elif segment == "test":
-                return self.meta_task_l[train_task_n:]
-            else:
-                raise NotImplementedError(f"This type of input is not supported")
-        else:
-            raise NotImplementedError(f"This type of input is not supported")
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+import pandas as pd
+import numpy as np
+from copy import deepcopy
+from joblib import Parallel, delayed  # pylint: disable=E0401
+from typing import Dict, List, Union, Text, Tuple
+from qlib.data.dataset.utils import init_task_handler
+from qlib.data.dataset import DatasetH
+from qlib.contrib.torch import data_to_tensor
+from qlib.model.meta.task import MetaTask
+from qlib.model.meta.dataset import MetaTaskDataset
+from qlib.model.trainer import TrainerR
+from qlib.log import get_module_logger
+from qlib.utils import auto_filter_kwargs, get_date_by_shift, init_instance_by_config
+from qlib.utils.data import deepcopy_basic_type
+from qlib.workflow import R
+from qlib.workflow.task.gen import RollingGen, task_generator
+from qlib.workflow.task.utils import TimeAdjuster
+from tqdm.auto import tqdm
+
+
+class InternalData:
+    def __init__(self, task_tpl: dict, step: int, exp_name: str):
+        self.task_tpl = task_tpl
+        self.step = step
+        self.exp_name = exp_name
+
+    def setup(self, trainer=TrainerR, trainer_kwargs={}):
+        """
+        after running this function `self.data_ic_df` will become set.
+        Each col represents a data.
+        Each row represents the Timestamp of performance of that data.
+        For example,
+
+        .. code-block:: python
+
+                       2021-06-21 2021-06-04 2021-05-21 2021-05-07 2021-04-20 2021-04-06 2021-03-22 2021-03-08  ...
+                       2021-07-02 2021-06-18 2021-06-03 2021-05-20 2021-05-06 2021-04-19 2021-04-02 2021-03-19  ...
+            datetime                                                                                            ...
+            2018-01-02   0.079782   0.115975   0.070866   0.028849  -0.081170   0.140380   0.063864   0.110987  ...
+            2018-01-03   0.123386   0.107789   0.071037   0.045278  -0.060782   0.167446   0.089779   0.124476  ...
+            2018-01-04   0.140775   0.097206   0.063702   0.042415  -0.078164   0.173218   0.098914   0.114389  ...
+            2018-01-05   0.030320  -0.037209  -0.044536  -0.047267  -0.081888   0.045648   0.059947   0.047652  ...
+            2018-01-08   0.107201   0.009219  -0.015995  -0.036594  -0.086633   0.108965   0.122164   0.108508  ...
+            ...               ...        ...        ...        ...        ...        ...        ...        ...  ...
+
+        """
+
+        # 1) prepare the prediction of proxy models
+        perf_task_tpl = deepcopy(self.task_tpl)  # this task is supposed to contains no complicated objects
+        # The only thing we want to save is the prediction
+        perf_task_tpl["record"] = ["qlib.workflow.record_temp.SignalRecord"]
+
+        trainer = auto_filter_kwargs(trainer)(experiment_name=self.exp_name, **trainer_kwargs)
+        # NOTE:
+        # The handler is initialized for only once.
+        if not trainer.has_worker():
+            self.dh = init_task_handler(perf_task_tpl)
+            self.dh.config(dump_all=False)  # in some cases, the data handler are saved to disk with `dump_all=True`
+        else:
+            self.dh = init_instance_by_config(perf_task_tpl["dataset"]["kwargs"]["handler"])
+        assert self.dh.dump_all is False  # otherwise, it will save all the detailed data
+
+        seg = perf_task_tpl["dataset"]["kwargs"]["segments"]
+
+        # We want to split the training time period into small segments.
+        perf_task_tpl["dataset"]["kwargs"]["segments"] = {
+            "train": (DatasetH.get_min_time(seg), DatasetH.get_max_time(seg)),
+            "test": (None, None),
+        }
+
+        # NOTE:
+        # we play a trick here
+        # treat the training segments as test to create the rolling tasks
+        rg = RollingGen(step=self.step, test_key="train", train_key=None, task_copy_func=deepcopy_basic_type)
+        gen_task = task_generator(perf_task_tpl, [rg])
+
+        recorders = R.list_recorders(experiment_name=self.exp_name)
+        if len(gen_task) == len(recorders):
+            get_module_logger("Internal Data").info("the data has been initialized")
+        else:
+            # train new models
+            assert 0 == len(recorders), "An empty experiment is required for setup `InternalData`"
+            trainer.train(gen_task)
+
+        # 2) extract the similarity matrix
+        label_df = self.dh.fetch(col_set="label")
+        # for
+        recorders = R.list_recorders(experiment_name=self.exp_name)
+
+        key_l = []
+        ic_l = []
+        for _, rec in tqdm(recorders.items(), desc="calc"):
+            pred = rec.load_object("pred.pkl")
+            task = rec.load_object("task")
+            data_key = task["dataset"]["kwargs"]["segments"]["train"]
+            key_l.append(data_key)
+            ic_l.append(delayed(self._calc_perf)(pred.iloc[:, 0], label_df.iloc[:, 0]))
+
+        ic_l = Parallel(n_jobs=-1)(ic_l)
+        self.data_ic_df = pd.DataFrame(dict(zip(key_l, ic_l)))
+        self.data_ic_df = self.data_ic_df.sort_index().sort_index(axis=1)
+
+        del self.dh  # handler is not useful now
+
+    def _calc_perf(self, pred, label):
+        df = pd.DataFrame({"pred": pred, "label": label})
+        df = df.groupby("datetime").corr(method="spearman")
+        corr = df.loc(axis=0)[:, "pred"]["label"].droplevel(axis=0, level=-1)
+        return corr
+
+    def update(self):
+        """update the data for online trading"""
+        # TODO:
+        # when new data are totally(including label) available
+        # - update the prediction
+        # - update the data similarity map(if applied)
+
+
+class MetaTaskDS(MetaTask):
+    """Meta Task for Data Selection"""
+
+    def __init__(self, task: dict, meta_info: pd.DataFrame, mode: str = MetaTask.PROC_MODE_FULL, fill_method="max"):
+        """
+
+        The description of the processed data
+
+            time_perf: A array with shape  <hist_step_n * step, data pieces>  ->  data piece performance
+
+            time_belong:  A array with shape <sample, data pieces>  -> belong or not (1. or 0.)
+            array([[1., 0., 0., ..., 0., 0., 0.],
+                   [1., 0., 0., ..., 0., 0., 0.],
+                   [1., 0., 0., ..., 0., 0., 0.],
+                   ...,
+                   [0., 0., 0., ..., 0., 0., 1.],
+                   [0., 0., 0., ..., 0., 0., 1.],
+                   [0., 0., 0., ..., 0., 0., 1.]])
+
+        Parameters
+        ----------
+        meta_info: pd.DataFrame
+            please refer to the docs of _prepare_meta_ipt for detailed explanation.
+        """
+        super().__init__(task, meta_info)
+        self.fill_method = fill_method
+
+        time_perf = self._get_processed_meta_info()
+        self.processed_meta_input = {"time_perf": time_perf}
+        # FIXME: memory issue in this step
+        if mode == MetaTask.PROC_MODE_FULL:
+            # process metainfo_
+            ds = self.get_dataset()
+
+            # these three lines occupied 70% of the time of initializing MetaTaskDS
+            d_train, d_test = ds.prepare(["train", "test"], col_set=["feature", "label"])
+            prev_size = d_test.shape[0]
+            d_train = d_train.dropna(axis=0)
+            d_test = d_test.dropna(axis=0)
+            if prev_size == 0 or d_test.shape[0] / prev_size <= 0.1:
+                raise ValueError(f"Most of samples are dropped. Please check this task: {task}")
+
+            assert (
+                d_test.groupby("datetime").size().shape[0] >= 5
+            ), "In this segment, this trading dates is less than 5, you'd better check the data."
+
+            sample_time_belong = np.zeros((d_train.shape[0], time_perf.shape[1]))
+            for i, col in enumerate(time_perf.columns):
+                # these two lines of code occupied 20% of the time of initializing MetaTaskDS
+                slc = slice(*d_train.index.slice_locs(start=col[0], end=col[1]))
+                sample_time_belong[slc, i] = 1.0
+
+            # If you want that last month also belongs to the last time_perf
+            # Assumptions: the latest data has similar performance like the last month
+            sample_time_belong[sample_time_belong.sum(axis=1) != 1, -1] = 1.0
+
+            self.processed_meta_input.update(
+                dict(
+                    X=d_train["feature"],
+                    y=d_train["label"].iloc[:, 0],
+                    X_test=d_test["feature"],
+                    y_test=d_test["label"].iloc[:, 0],
+                    time_belong=sample_time_belong,
+                    test_idx=d_test["label"].index,
+                )
+            )
+
+        # TODO: set device: I think this is not necessary to converting data format.
+        self.processed_meta_input = data_to_tensor(self.processed_meta_input)
+
+    def _get_processed_meta_info(self):
+        meta_info_norm = self.meta_info.sub(self.meta_info.mean(axis=1), axis=0)
+        if self.fill_method.startswith("max"):
+            suffix = self.fill_method.lstrip("max")
+            if suffix == "seg":
+                fill_value = {}
+                for col in meta_info_norm.columns:
+                    fill_value[col] = meta_info_norm.loc[meta_info_norm[col].isna(), :].dropna(axis=1).mean().max()
+                fill_value = pd.Series(fill_value).sort_index()
+                # The NaN Values are filled segment-wise. Below is an exampleof fill_value
+                # 2009-01-05  2009-02-06    0.145809
+                # 2009-02-09  2009-03-06    0.148005
+                # 2009-03-09  2009-04-03    0.090385
+                # 2009-04-07  2009-05-05    0.114318
+                # 2009-05-06  2009-06-04    0.119328
+                # ...
+                meta_info_norm = meta_info_norm.fillna(fill_value)
+            else:
+                if len(suffix) > 0:
+                    get_module_logger("MetaTaskDS").warning(
+                        f"fill_method={self.fill_method}; the info after can't be correctly parsed. Please check your parameters."
+                    )
+                fill_value = meta_info_norm.max(axis=1)
+                # fill it with row max to align with previous implementation
+                # This will magnify the data similarity when data is in daily freq
+
+                # the fill value corresponds to data like this
+                # It get a performance value for each day.
+                # The performance value are get from other models on this day
+                # 2009-01-16    0.276320
+                # 2009-01-19    0.280603
+                #                 ...
+                # 2011-06-27    0.203773
+                meta_info_norm = meta_info_norm.T.fillna(fill_value).T
+        elif self.fill_method == "zero":
+            # It will fillna(0.0) at the end.
+            pass
+        else:
+            raise NotImplementedError(f"This type of input is not supported")
+        meta_info_norm = meta_info_norm.fillna(0.0)  # always fill zero in case of NaN
+        return meta_info_norm
+
+    def get_meta_input(self):
+        return self.processed_meta_input
+
+
+class MetaDatasetDS(MetaTaskDataset):
+    def __init__(
+        self,
+        *,
+        task_tpl: Union[dict, list],
+        step: int,
+        trunc_days: int = None,
+        rolling_ext_days: int = 0,
+        exp_name: Union[str, InternalData],
+        segments: Union[Dict[Text, Tuple], float],
+        hist_step_n: int = 10,
+        task_mode: str = MetaTask.PROC_MODE_FULL,
+        fill_method: str = "max",
+    ):
+        """
+        A dataset for meta model.
+
+        Parameters
+        ----------
+        task_tpl : Union[dict, list]
+            Decide what tasks are used.
+            - dict : the task template, the prepared task is generated with `step`, `trunc_days` and `RollingGen`
+            - list : when list, use the list of tasks directly
+                     the list is supposed to be sorted according timeline
+        step : int
+            the rolling step
+        trunc_days: int
+            days to be truncated based on the test start
+        rolling_ext_days: int
+            sometimes users want to train meta models for a longer test period but with smaller rolling steps for more task samples.
+            the total length of test periods will be `step + rolling_ext_days`
+
+        exp_name : Union[str, InternalData]
+            Decide what meta_info are used for prediction.
+            - str: the name of the experiment to store the performance of data
+            - InternalData: a prepared internal data
+        segments: Union[Dict[Text, Tuple], float]
+            the segments to divide data
+            both left and right
+            if segments is a float:
+                the float represents the percentage of data for training
+        hist_step_n: int
+            length of historical steps for the meta infomation
+        task_mode : str
+            Please refer to the docs of MetaTask
+        """
+        super().__init__(segments=segments)
+        if isinstance(exp_name, InternalData):
+            self.internal_data = exp_name
+        else:
+            self.internal_data = InternalData(task_tpl, step=step, exp_name=exp_name)
+            self.internal_data.setup()
+        self.task_tpl = deepcopy(task_tpl)  # FIXME: if the handler is shared, how to avoid the explosion of the memroy.
+        self.trunc_days = trunc_days
+        self.hist_step_n = hist_step_n
+        self.step = step
+
+        if isinstance(task_tpl, dict):
+            rg = RollingGen(
+                step=step, trunc_days=trunc_days, task_copy_func=deepcopy_basic_type
+            )  # NOTE: trunc_days is very important !!!!
+            task_iter = rg(task_tpl)
+            if rolling_ext_days > 0:
+                self.ta = TimeAdjuster(future=True)
+                for t in task_iter:
+                    t["dataset"]["kwargs"]["segments"]["test"] = self.ta.shift(
+                        t["dataset"]["kwargs"]["segments"]["test"], step=rolling_ext_days, rtype=RollingGen.ROLL_EX
+                    )
+            if task_mode == MetaTask.PROC_MODE_FULL:
+                # Only pre initializing the task when full task is req
+                # initializing handler and share it.
+                init_task_handler(task_tpl)
+        else:
+            assert isinstance(task_tpl, list)
+            task_iter = task_tpl
+
+        self.task_list = []
+        self.meta_task_l = []
+        logger = get_module_logger("MetaDatasetDS")
+        logger.info(f"Example task for training meta model: {task_iter[0]}")
+        for t in tqdm(task_iter, desc="creating meta tasks"):
+            try:
+                self.meta_task_l.append(
+                    MetaTaskDS(t, meta_info=self._prepare_meta_ipt(t), mode=task_mode, fill_method=fill_method)
+                )
+                self.task_list.append(t)
+            except ValueError as e:
+                logger.warning(f"ValueError: {e}")
+        assert len(self.meta_task_l) > 0, "No meta tasks found. Please check the data and setting"
+
+    def _prepare_meta_ipt(self, task) -> pd.DataFrame:
+        """
+        Please refer to `self.internal_data.setup` for detailed information about `self.internal_data.data_ic_df`
+
+        Indices with format below can be successfully sliced by  `ic_df.loc[:end, pd.IndexSlice[:, :end]]`
+
+               2021-06-21 2021-06-04 .. 2021-03-22 2021-03-08
+               2021-07-02 2021-06-18 .. 2021-04-02 None
+
+        Returns
+        -------
+            a pd.DataFrame with similar content below.
+            - each column corresponds to a trained model named by the training data range
+            - each row corresponds to a day of data tested by the models of the columns
+            - The rows cells that overlaps with the data used by columns are masked
+
+
+                       2009-01-05 2009-02-09 ... 2011-04-27 2011-05-26
+                       2009-02-06 2009-03-06 ... 2011-05-25 2011-06-23
+            datetime                         ...
+            2009-01-13        NaN   0.310639 ...  -0.169057   0.137792
+            2009-01-14        NaN   0.261086 ...  -0.143567   0.082581
+            ...               ...        ... ...        ...        ...
+            2011-06-30  -0.054907  -0.020219 ...  -0.023226        NaN
+            2011-07-01  -0.075762  -0.026626 ...  -0.003167        NaN
+
+        """
+        ic_df = self.internal_data.data_ic_df
+
+        segs = task["dataset"]["kwargs"]["segments"]
+        end = max(segs[k][1] for k in ("train", "valid") if k in segs)
+        ic_df_avail = ic_df.loc[:end, pd.IndexSlice[:, :end]]
+
+        # meta data set focus on the **information** instead of preprocess
+        # 1) filter the overlap info
+        def mask_overlap(s):
+            """
+            mask overlap information
+            data after self.name[end] with self.trunc_days that contains future info are also considered as overlap info
+
+            Approximately the diagnal + horizon length of data are masked.
+            """
+            start, end = s.name
+            end = get_date_by_shift(trading_date=end, shift=self.trunc_days - 1, future=True)
+            return s.mask((s.index >= start) & (s.index <= end))
+
+        ic_df_avail = ic_df_avail.apply(mask_overlap)  # apply to each col
+
+        # 2) filter the info with too long periods
+        total_len = self.step * self.hist_step_n
+        if ic_df_avail.shape[0] >= total_len:
+            return ic_df_avail.iloc[-total_len:]
+        else:
+            raise ValueError("the history of distribution data is not long enough.")
+
+    def _prepare_seg(self, segment: Text) -> List[MetaTask]:
+        if isinstance(self.segments, float):
+            train_task_n = int(len(self.meta_task_l) * self.segments)
+            if segment == "train":
+                return self.meta_task_l[:train_task_n]
+            elif segment == "test":
+                return self.meta_task_l[train_task_n:]
+            else:
+                raise NotImplementedError(f"This type of input is not supported")
+        else:
+            raise NotImplementedError(f"This type of input is not supported")
```

## qlib/contrib/meta/data_selection/model.py

 * *Ordering differences only*

```diff
@@ -1,184 +1,184 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-import pandas as pd
-import numpy as np
-import torch
-from torch import nn
-from torch import optim
-from tqdm.auto import tqdm
-import copy
-from typing import Union, List
-
-from ....model.meta.dataset import MetaTaskDataset
-from ....model.meta.model import MetaTaskModel
-from ....workflow import R
-from .utils import ICLoss
-from .dataset import MetaDatasetDS
-
-from qlib.log import get_module_logger
-from qlib.model.meta.task import MetaTask
-from qlib.data.dataset.weight import Reweighter
-from qlib.contrib.meta.data_selection.net import PredNet
-
-logger = get_module_logger("data selection")
-
-
-class TimeReweighter(Reweighter):
-    def __init__(self, time_weight: pd.Series):
-        self.time_weight = time_weight
-
-    def reweight(self, data: Union[pd.DataFrame, pd.Series]):
-        # TODO: handling TSDataSampler
-        w_s = pd.Series(1.0, index=data.index)
-        for k, w in self.time_weight.items():
-            w_s.loc[slice(*k)] = w
-        logger.info(f"Reweighting result: {w_s}")
-        return w_s
-
-
-class MetaModelDS(MetaTaskModel):
-    """
-    The meta-model for meta-learning-based data selection.
-    """
-
-    def __init__(
-        self,
-        step,
-        hist_step_n,
-        clip_method="tanh",
-        clip_weight=2.0,
-        criterion="ic_loss",
-        lr=0.0001,
-        max_epoch=100,
-        seed=43,
-        alpha=0.0,
-    ):
-        self.step = step
-        self.hist_step_n = hist_step_n
-        self.clip_method = clip_method
-        self.clip_weight = clip_weight
-        self.criterion = criterion
-        self.lr = lr
-        self.max_epoch = max_epoch
-        self.fitted = False
-        self.alpha = alpha
-        torch.manual_seed(seed)
-
-    def run_epoch(self, phase, task_list, epoch, opt, loss_l, ignore_weight=False):
-        if phase == "train":
-            self.tn.train()
-            torch.set_grad_enabled(True)
-        else:
-            self.tn.eval()
-            torch.set_grad_enabled(False)
-        running_loss = 0.0
-        pred_y_all = []
-        for task in tqdm(task_list, desc=f"{phase} Task", leave=False):
-            meta_input = task.get_meta_input()
-            pred, weights = self.tn(
-                meta_input["X"],
-                meta_input["y"],
-                meta_input["time_perf"],
-                meta_input["time_belong"],
-                meta_input["X_test"],
-                ignore_weight=ignore_weight,
-            )
-            if self.criterion == "mse":
-                criterion = nn.MSELoss()
-                loss = criterion(pred, meta_input["y_test"])
-            elif self.criterion == "ic_loss":
-                criterion = ICLoss()
-                try:
-                    loss = criterion(pred, meta_input["y_test"], meta_input["test_idx"], skip_size=50)
-                except ValueError as e:
-                    get_module_logger("MetaModelDS").warning(f"Exception `{e}` when calculating IC loss")
-                    continue
-
-            assert not np.isnan(loss.detach().item()), "NaN loss!"
-
-            if phase == "train":
-                opt.zero_grad()
-                loss.backward()
-                opt.step()
-            elif phase == "test":
-                pass
-
-            pred_y_all.append(
-                pd.DataFrame(
-                    {
-                        "pred": pd.Series(pred.detach().cpu().numpy(), index=meta_input["test_idx"]),
-                        "label": pd.Series(meta_input["y_test"].detach().cpu().numpy(), index=meta_input["test_idx"]),
-                    }
-                )
-            )
-            running_loss += loss.detach().item()
-        running_loss = running_loss / len(task_list)
-        loss_l.setdefault(phase, []).append(running_loss)
-
-        pred_y_all = pd.concat(pred_y_all)
-        ic = pred_y_all.groupby("datetime").apply(lambda df: df["pred"].corr(df["label"], method="spearman")).mean()
-
-        R.log_metrics(**{f"loss/{phase}": running_loss, "step": epoch})
-        R.log_metrics(**{f"ic/{phase}": ic, "step": epoch})
-
-    def fit(self, meta_dataset: MetaDatasetDS):
-        """
-        The meta-learning-based data selection interacts directly with meta-dataset due to the close-form proxy measurement.
-
-        Parameters
-        ----------
-        meta_dataset : MetaDatasetDS
-            The meta-model takes the meta-dataset for its training process.
-        """
-
-        if not self.fitted:
-            for k in set(["lr", "step", "hist_step_n", "clip_method", "clip_weight", "criterion", "max_epoch"]):
-                R.log_params(**{k: getattr(self, k)})
-
-        # FIXME: get test tasks for just checking the performance
-        phases = ["train", "test"]
-        meta_tasks_l = meta_dataset.prepare_tasks(phases)
-
-        if len(meta_tasks_l[1]):
-            R.log_params(
-                **dict(proxy_test_begin=meta_tasks_l[1][0].task["dataset"]["kwargs"]["segments"]["test"])
-            )  # debug: record when the test phase starts
-
-        self.tn = PredNet(
-            step=self.step,
-            hist_step_n=self.hist_step_n,
-            clip_weight=self.clip_weight,
-            clip_method=self.clip_method,
-            alpha=self.alpha,
-        )
-
-        opt = optim.Adam(self.tn.parameters(), lr=self.lr)
-
-        # run weight with no weight
-        for phase, task_list in zip(phases, meta_tasks_l):
-            self.run_epoch(f"{phase}_noweight", task_list, 0, opt, {}, ignore_weight=True)
-            self.run_epoch(f"{phase}_init", task_list, 0, opt, {})
-
-        # run training
-        loss_l = {}
-        for epoch in tqdm(range(self.max_epoch), desc="epoch"):
-            for phase, task_list in zip(phases, meta_tasks_l):
-                self.run_epoch(phase, task_list, epoch, opt, loss_l)
-            R.save_objects(**{"model.pkl": self.tn})
-        self.fitted = True
-
-    def _prepare_task(self, task: MetaTask) -> dict:
-        meta_ipt = task.get_meta_input()
-        weights = self.tn.twm(meta_ipt["time_perf"])
-
-        weight_s = pd.Series(weights.detach().cpu().numpy(), index=task.meta_info.columns)
-        task = copy.copy(task.task)  # NOTE: this is a shallow copy.
-        task["reweighter"] = TimeReweighter(weight_s)
-        return task
-
-    def inference(self, meta_dataset: MetaTaskDataset) -> List[dict]:
-        res = []
-        for mt in meta_dataset.prepare_tasks("test"):
-            res.append(self._prepare_task(mt))
-        return res
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+import pandas as pd
+import numpy as np
+import torch
+from torch import nn
+from torch import optim
+from tqdm.auto import tqdm
+import copy
+from typing import Union, List
+
+from ....model.meta.dataset import MetaTaskDataset
+from ....model.meta.model import MetaTaskModel
+from ....workflow import R
+from .utils import ICLoss
+from .dataset import MetaDatasetDS
+
+from qlib.log import get_module_logger
+from qlib.model.meta.task import MetaTask
+from qlib.data.dataset.weight import Reweighter
+from qlib.contrib.meta.data_selection.net import PredNet
+
+logger = get_module_logger("data selection")
+
+
+class TimeReweighter(Reweighter):
+    def __init__(self, time_weight: pd.Series):
+        self.time_weight = time_weight
+
+    def reweight(self, data: Union[pd.DataFrame, pd.Series]):
+        # TODO: handling TSDataSampler
+        w_s = pd.Series(1.0, index=data.index)
+        for k, w in self.time_weight.items():
+            w_s.loc[slice(*k)] = w
+        logger.info(f"Reweighting result: {w_s}")
+        return w_s
+
+
+class MetaModelDS(MetaTaskModel):
+    """
+    The meta-model for meta-learning-based data selection.
+    """
+
+    def __init__(
+        self,
+        step,
+        hist_step_n,
+        clip_method="tanh",
+        clip_weight=2.0,
+        criterion="ic_loss",
+        lr=0.0001,
+        max_epoch=100,
+        seed=43,
+        alpha=0.0,
+    ):
+        self.step = step
+        self.hist_step_n = hist_step_n
+        self.clip_method = clip_method
+        self.clip_weight = clip_weight
+        self.criterion = criterion
+        self.lr = lr
+        self.max_epoch = max_epoch
+        self.fitted = False
+        self.alpha = alpha
+        torch.manual_seed(seed)
+
+    def run_epoch(self, phase, task_list, epoch, opt, loss_l, ignore_weight=False):
+        if phase == "train":
+            self.tn.train()
+            torch.set_grad_enabled(True)
+        else:
+            self.tn.eval()
+            torch.set_grad_enabled(False)
+        running_loss = 0.0
+        pred_y_all = []
+        for task in tqdm(task_list, desc=f"{phase} Task", leave=False):
+            meta_input = task.get_meta_input()
+            pred, weights = self.tn(
+                meta_input["X"],
+                meta_input["y"],
+                meta_input["time_perf"],
+                meta_input["time_belong"],
+                meta_input["X_test"],
+                ignore_weight=ignore_weight,
+            )
+            if self.criterion == "mse":
+                criterion = nn.MSELoss()
+                loss = criterion(pred, meta_input["y_test"])
+            elif self.criterion == "ic_loss":
+                criterion = ICLoss()
+                try:
+                    loss = criterion(pred, meta_input["y_test"], meta_input["test_idx"], skip_size=50)
+                except ValueError as e:
+                    get_module_logger("MetaModelDS").warning(f"Exception `{e}` when calculating IC loss")
+                    continue
+
+            assert not np.isnan(loss.detach().item()), "NaN loss!"
+
+            if phase == "train":
+                opt.zero_grad()
+                loss.backward()
+                opt.step()
+            elif phase == "test":
+                pass
+
+            pred_y_all.append(
+                pd.DataFrame(
+                    {
+                        "pred": pd.Series(pred.detach().cpu().numpy(), index=meta_input["test_idx"]),
+                        "label": pd.Series(meta_input["y_test"].detach().cpu().numpy(), index=meta_input["test_idx"]),
+                    }
+                )
+            )
+            running_loss += loss.detach().item()
+        running_loss = running_loss / len(task_list)
+        loss_l.setdefault(phase, []).append(running_loss)
+
+        pred_y_all = pd.concat(pred_y_all)
+        ic = pred_y_all.groupby("datetime").apply(lambda df: df["pred"].corr(df["label"], method="spearman")).mean()
+
+        R.log_metrics(**{f"loss/{phase}": running_loss, "step": epoch})
+        R.log_metrics(**{f"ic/{phase}": ic, "step": epoch})
+
+    def fit(self, meta_dataset: MetaDatasetDS):
+        """
+        The meta-learning-based data selection interacts directly with meta-dataset due to the close-form proxy measurement.
+
+        Parameters
+        ----------
+        meta_dataset : MetaDatasetDS
+            The meta-model takes the meta-dataset for its training process.
+        """
+
+        if not self.fitted:
+            for k in set(["lr", "step", "hist_step_n", "clip_method", "clip_weight", "criterion", "max_epoch"]):
+                R.log_params(**{k: getattr(self, k)})
+
+        # FIXME: get test tasks for just checking the performance
+        phases = ["train", "test"]
+        meta_tasks_l = meta_dataset.prepare_tasks(phases)
+
+        if len(meta_tasks_l[1]):
+            R.log_params(
+                **dict(proxy_test_begin=meta_tasks_l[1][0].task["dataset"]["kwargs"]["segments"]["test"])
+            )  # debug: record when the test phase starts
+
+        self.tn = PredNet(
+            step=self.step,
+            hist_step_n=self.hist_step_n,
+            clip_weight=self.clip_weight,
+            clip_method=self.clip_method,
+            alpha=self.alpha,
+        )
+
+        opt = optim.Adam(self.tn.parameters(), lr=self.lr)
+
+        # run weight with no weight
+        for phase, task_list in zip(phases, meta_tasks_l):
+            self.run_epoch(f"{phase}_noweight", task_list, 0, opt, {}, ignore_weight=True)
+            self.run_epoch(f"{phase}_init", task_list, 0, opt, {})
+
+        # run training
+        loss_l = {}
+        for epoch in tqdm(range(self.max_epoch), desc="epoch"):
+            for phase, task_list in zip(phases, meta_tasks_l):
+                self.run_epoch(phase, task_list, epoch, opt, loss_l)
+            R.save_objects(**{"model.pkl": self.tn})
+        self.fitted = True
+
+    def _prepare_task(self, task: MetaTask) -> dict:
+        meta_ipt = task.get_meta_input()
+        weights = self.tn.twm(meta_ipt["time_perf"])
+
+        weight_s = pd.Series(weights.detach().cpu().numpy(), index=task.meta_info.columns)
+        task = copy.copy(task.task)  # NOTE: this is a shallow copy.
+        task["reweighter"] = TimeReweighter(weight_s)
+        return task
+
+    def inference(self, meta_dataset: MetaTaskDataset) -> List[dict]:
+        res = []
+        for mt in meta_dataset.prepare_tasks("test"):
+            res.append(self._prepare_task(mt))
+        return res
```

## qlib/contrib/meta/data_selection/net.py

 * *Ordering differences only*

```diff
@@ -1,74 +1,74 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-import numpy as np
-import torch
-from torch import nn
-
-from .utils import preds_to_weight_with_clamp, SingleMetaBase
-
-
-class TimeWeightMeta(SingleMetaBase):
-    def __init__(self, hist_step_n, clip_weight=None, clip_method="clamp"):
-        # clip_method includes "tanh" or "clamp"
-        super().__init__(hist_step_n, clip_weight, clip_method)
-        self.linear = nn.Linear(hist_step_n, 1)
-        self.k = nn.Parameter(torch.Tensor([8.0]))
-
-    def forward(self, time_perf, time_belong=None, return_preds=False):
-        hist_step_n = self.linear.in_features
-        # NOTE: the reshape order is very important
-        time_perf = time_perf.reshape(hist_step_n, time_perf.shape[0] // hist_step_n, *time_perf.shape[1:])
-        time_perf = torch.mean(time_perf, dim=1, keepdim=False)
-
-        preds = []
-        for i in range(time_perf.shape[1]):
-            preds.append(self.linear(time_perf[:, i]))
-        preds = torch.cat(preds)
-        preds = preds - torch.mean(preds)  # avoid using future information
-        preds = preds * self.k
-        if return_preds:
-            if time_belong is None:
-                return preds
-            else:
-                return time_belong @ preds
-        else:
-            weights = preds_to_weight_with_clamp(preds, self.clip_weight, self.clip_method)
-            if time_belong is None:
-                return weights
-            else:
-                return time_belong @ weights
-
-
-class PredNet(nn.Module):
-    def __init__(self, step, hist_step_n, clip_weight=None, clip_method="tanh", alpha: float = 0.0):
-        """
-        Parameters
-        ----------
-        alpha : float
-            the regularization for sub model (useful when align meta model with linear submodel)
-        """
-        super().__init__()
-        self.step = step
-        self.twm = TimeWeightMeta(hist_step_n=hist_step_n, clip_weight=clip_weight, clip_method=clip_method)
-        self.init_paramters(hist_step_n)
-        self.alpha = alpha
-
-    def get_sample_weights(self, X, time_perf, time_belong, ignore_weight=False):
-        weights = torch.from_numpy(np.ones(X.shape[0])).float().to(X.device)
-        if not ignore_weight:
-            if time_perf is not None:
-                weights_t = self.twm(time_perf, time_belong)
-                weights = weights * weights_t
-        return weights
-
-    def forward(self, X, y, time_perf, time_belong, X_test, ignore_weight=False):
-        """Please refer to the docs of MetaTaskDS for the description of the variables"""
-        weights = self.get_sample_weights(X, time_perf, time_belong, ignore_weight=ignore_weight)
-        X_w = X.T * weights.view(1, -1)
-        theta = torch.inverse(X_w @ X + self.alpha * torch.eye(X_w.shape[0])) @ X_w @ y
-        return X_test @ theta, weights
-
-    def init_paramters(self, hist_step_n):
-        self.twm.linear.weight.data = 1.0 / hist_step_n + self.twm.linear.weight.data * 0.01
-        self.twm.linear.bias.data.fill_(0.0)
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+import numpy as np
+import torch
+from torch import nn
+
+from .utils import preds_to_weight_with_clamp, SingleMetaBase
+
+
+class TimeWeightMeta(SingleMetaBase):
+    def __init__(self, hist_step_n, clip_weight=None, clip_method="clamp"):
+        # clip_method includes "tanh" or "clamp"
+        super().__init__(hist_step_n, clip_weight, clip_method)
+        self.linear = nn.Linear(hist_step_n, 1)
+        self.k = nn.Parameter(torch.Tensor([8.0]))
+
+    def forward(self, time_perf, time_belong=None, return_preds=False):
+        hist_step_n = self.linear.in_features
+        # NOTE: the reshape order is very important
+        time_perf = time_perf.reshape(hist_step_n, time_perf.shape[0] // hist_step_n, *time_perf.shape[1:])
+        time_perf = torch.mean(time_perf, dim=1, keepdim=False)
+
+        preds = []
+        for i in range(time_perf.shape[1]):
+            preds.append(self.linear(time_perf[:, i]))
+        preds = torch.cat(preds)
+        preds = preds - torch.mean(preds)  # avoid using future information
+        preds = preds * self.k
+        if return_preds:
+            if time_belong is None:
+                return preds
+            else:
+                return time_belong @ preds
+        else:
+            weights = preds_to_weight_with_clamp(preds, self.clip_weight, self.clip_method)
+            if time_belong is None:
+                return weights
+            else:
+                return time_belong @ weights
+
+
+class PredNet(nn.Module):
+    def __init__(self, step, hist_step_n, clip_weight=None, clip_method="tanh", alpha: float = 0.0):
+        """
+        Parameters
+        ----------
+        alpha : float
+            the regularization for sub model (useful when align meta model with linear submodel)
+        """
+        super().__init__()
+        self.step = step
+        self.twm = TimeWeightMeta(hist_step_n=hist_step_n, clip_weight=clip_weight, clip_method=clip_method)
+        self.init_paramters(hist_step_n)
+        self.alpha = alpha
+
+    def get_sample_weights(self, X, time_perf, time_belong, ignore_weight=False):
+        weights = torch.from_numpy(np.ones(X.shape[0])).float().to(X.device)
+        if not ignore_weight:
+            if time_perf is not None:
+                weights_t = self.twm(time_perf, time_belong)
+                weights = weights * weights_t
+        return weights
+
+    def forward(self, X, y, time_perf, time_belong, X_test, ignore_weight=False):
+        """Please refer to the docs of MetaTaskDS for the description of the variables"""
+        weights = self.get_sample_weights(X, time_perf, time_belong, ignore_weight=ignore_weight)
+        X_w = X.T * weights.view(1, -1)
+        theta = torch.inverse(X_w @ X + self.alpha * torch.eye(X_w.shape[0])) @ X_w @ y
+        return X_test @ theta, weights
+
+    def init_paramters(self, hist_step_n):
+        self.twm.linear.weight.data = 1.0 / hist_step_n + self.twm.linear.weight.data * 0.01
+        self.twm.linear.bias.data.fill_(0.0)
```

## qlib/contrib/meta/data_selection/utils.py

 * *Ordering differences only*

```diff
@@ -1,113 +1,113 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-import numpy as np
-import torch
-from torch import nn
-
-from qlib.constant import EPS
-from qlib.log import get_module_logger
-
-
-class ICLoss(nn.Module):
-    def forward(self, pred, y, idx, skip_size=50):
-        """forward.
-        FIXME:
-        - Some times it will be a slightly different from the result from `pandas.corr()`
-        - It may be caused by the precision problem of model;
-
-        :param pred:
-        :param y:
-        :param idx: Assume the level of the idx is (date, inst), and it is sorted
-        """
-        prev = None
-        diff_point = []
-        for i, (date, inst) in enumerate(idx):
-            if date != prev:
-                diff_point.append(i)
-            prev = date
-        diff_point.append(None)
-        # The lengths of diff_point will be one more larger then diff_point
-
-        ic_all = 0.0
-        skip_n = 0
-        for start_i, end_i in zip(diff_point, diff_point[1:]):
-            pred_focus = pred[start_i:end_i]  # TODO: just for fake
-            if pred_focus.shape[0] < skip_size:
-                # skip some days which have very small amount of stock.
-                skip_n += 1
-                continue
-            y_focus = y[start_i:end_i]
-            if pred_focus.std() < EPS or y_focus.std() < EPS:
-                # These cases often happend at the end of test data.
-                # Usually caused by fillna(0.)
-                skip_n += 1
-                continue
-
-            ic_day = torch.dot(
-                (pred_focus - pred_focus.mean()) / np.sqrt(pred_focus.shape[0]) / pred_focus.std(),
-                (y_focus - y_focus.mean()) / np.sqrt(y_focus.shape[0]) / y_focus.std(),
-            )
-            ic_all += ic_day
-        if len(diff_point) - 1 - skip_n <= 0:
-            raise ValueError("No enough data for calculating IC")
-        if skip_n > 0:
-            get_module_logger("ICLoss").info(
-                f"{skip_n} days are skipped due to zero std or small scale of valid samples."
-            )
-        ic_mean = ic_all / (len(diff_point) - 1 - skip_n)
-        return -ic_mean  # ic loss
-
-
-def preds_to_weight_with_clamp(preds, clip_weight=None, clip_method="tanh"):
-    """
-    Clip the weights.
-
-    Parameters
-    ----------
-    clip_weight: float
-        The clip threshold.
-    clip_method: str
-        The clip method. Current available: "clamp", "tanh", and "sigmoid".
-    """
-    if clip_weight is not None:
-        if clip_method == "clamp":
-            weights = torch.exp(preds)
-            weights = weights.clamp(1.0 / clip_weight, clip_weight)
-        elif clip_method == "tanh":
-            weights = torch.exp(torch.tanh(preds) * np.log(clip_weight))
-        elif clip_method == "sigmoid":
-            # intuitively assume its sum is 1
-            if clip_weight == 0.0:
-                weights = torch.ones_like(preds)
-            else:
-                sm = nn.Sigmoid()
-                weights = sm(preds) * clip_weight  # TODO: The clip_weight is useless here.
-                weights = weights / torch.sum(weights) * weights.numel()
-        else:
-            raise ValueError("Unknown clip_method")
-    else:
-        weights = torch.exp(preds)
-    return weights
-
-
-class SingleMetaBase(nn.Module):
-    def __init__(self, hist_n, clip_weight=None, clip_method="clamp"):
-        # method can be tanh or clamp
-        super().__init__()
-        self.clip_weight = clip_weight
-        if clip_method in ["tanh", "clamp"]:
-            if self.clip_weight is not None and self.clip_weight < 1.0:
-                self.clip_weight = 1 / self.clip_weight
-        self.clip_method = clip_method
-
-    def is_enabled(self):
-        if self.clip_weight is None:
-            return True
-        if self.clip_method == "sigmoid":
-            if self.clip_weight > 0.0:
-                return True
-        else:
-            if self.clip_weight > 1.0:
-                return True
-        return False
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+import numpy as np
+import torch
+from torch import nn
+
+from qlib.constant import EPS
+from qlib.log import get_module_logger
+
+
+class ICLoss(nn.Module):
+    def forward(self, pred, y, idx, skip_size=50):
+        """forward.
+        FIXME:
+        - Some times it will be a slightly different from the result from `pandas.corr()`
+        - It may be caused by the precision problem of model;
+
+        :param pred:
+        :param y:
+        :param idx: Assume the level of the idx is (date, inst), and it is sorted
+        """
+        prev = None
+        diff_point = []
+        for i, (date, inst) in enumerate(idx):
+            if date != prev:
+                diff_point.append(i)
+            prev = date
+        diff_point.append(None)
+        # The lengths of diff_point will be one more larger then diff_point
+
+        ic_all = 0.0
+        skip_n = 0
+        for start_i, end_i in zip(diff_point, diff_point[1:]):
+            pred_focus = pred[start_i:end_i]  # TODO: just for fake
+            if pred_focus.shape[0] < skip_size:
+                # skip some days which have very small amount of stock.
+                skip_n += 1
+                continue
+            y_focus = y[start_i:end_i]
+            if pred_focus.std() < EPS or y_focus.std() < EPS:
+                # These cases often happend at the end of test data.
+                # Usually caused by fillna(0.)
+                skip_n += 1
+                continue
+
+            ic_day = torch.dot(
+                (pred_focus - pred_focus.mean()) / np.sqrt(pred_focus.shape[0]) / pred_focus.std(),
+                (y_focus - y_focus.mean()) / np.sqrt(y_focus.shape[0]) / y_focus.std(),
+            )
+            ic_all += ic_day
+        if len(diff_point) - 1 - skip_n <= 0:
+            raise ValueError("No enough data for calculating IC")
+        if skip_n > 0:
+            get_module_logger("ICLoss").info(
+                f"{skip_n} days are skipped due to zero std or small scale of valid samples."
+            )
+        ic_mean = ic_all / (len(diff_point) - 1 - skip_n)
+        return -ic_mean  # ic loss
+
+
+def preds_to_weight_with_clamp(preds, clip_weight=None, clip_method="tanh"):
+    """
+    Clip the weights.
+
+    Parameters
+    ----------
+    clip_weight: float
+        The clip threshold.
+    clip_method: str
+        The clip method. Current available: "clamp", "tanh", and "sigmoid".
+    """
+    if clip_weight is not None:
+        if clip_method == "clamp":
+            weights = torch.exp(preds)
+            weights = weights.clamp(1.0 / clip_weight, clip_weight)
+        elif clip_method == "tanh":
+            weights = torch.exp(torch.tanh(preds) * np.log(clip_weight))
+        elif clip_method == "sigmoid":
+            # intuitively assume its sum is 1
+            if clip_weight == 0.0:
+                weights = torch.ones_like(preds)
+            else:
+                sm = nn.Sigmoid()
+                weights = sm(preds) * clip_weight  # TODO: The clip_weight is useless here.
+                weights = weights / torch.sum(weights) * weights.numel()
+        else:
+            raise ValueError("Unknown clip_method")
+    else:
+        weights = torch.exp(preds)
+    return weights
+
+
+class SingleMetaBase(nn.Module):
+    def __init__(self, hist_n, clip_weight=None, clip_method="clamp"):
+        # method can be tanh or clamp
+        super().__init__()
+        self.clip_weight = clip_weight
+        if clip_method in ["tanh", "clamp"]:
+            if self.clip_weight is not None and self.clip_weight < 1.0:
+                self.clip_weight = 1 / self.clip_weight
+        self.clip_method = clip_method
+
+    def is_enabled(self):
+        if self.clip_weight is None:
+            return True
+        if self.clip_method == "sigmoid":
+            if self.clip_weight > 0.0:
+                return True
+        else:
+            if self.clip_weight > 1.0:
+                return True
+        return False
```

## qlib/contrib/model/__init__.py

 * *Ordering differences only*

```diff
@@ -1,43 +1,43 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-try:
-    from .catboost_model import CatBoostModel
-except ModuleNotFoundError:
-    CatBoostModel = None
-    print("ModuleNotFoundError. CatBoostModel are skipped. (optional: maybe installing CatBoostModel can fix it.)")
-try:
-    from .double_ensemble import DEnsembleModel
-    from .gbdt import LGBModel
-except ModuleNotFoundError:
-    DEnsembleModel, LGBModel = None, None
-    print(
-        "ModuleNotFoundError. DEnsembleModel and LGBModel are skipped. (optional: maybe installing lightgbm can fix it.)"
-    )
-try:
-    from .xgboost import XGBModel
-except ModuleNotFoundError:
-    XGBModel = None
-    print("ModuleNotFoundError. XGBModel is skipped(optional: maybe installing xgboost can fix it).")
-try:
-    from .linear import LinearModel
-except ModuleNotFoundError:
-    LinearModel = None
-    print("ModuleNotFoundError. LinearModel is skipped(optional: maybe installing scipy and sklearn can fix it).")
-# import pytorch models
-try:
-    from .pytorch_alstm import ALSTM
-    from .pytorch_gats import GATs
-    from .pytorch_gru import GRU
-    from .pytorch_lstm import LSTM
-    from .pytorch_nn import DNNModelPytorch
-    from .pytorch_tabnet import TabnetModel
-    from .pytorch_sfm import SFM_Model
-    from .pytorch_tcn import TCN
-    from .pytorch_add import ADD
-
-    pytorch_classes = (ALSTM, GATs, GRU, LSTM, DNNModelPytorch, TabnetModel, SFM_Model, TCN, ADD)
-except ModuleNotFoundError:
-    pytorch_classes = ()
-    print("ModuleNotFoundError.  PyTorch models are skipped (optional: maybe installing pytorch can fix it).")
-
-all_model_classes = (CatBoostModel, DEnsembleModel, LGBModel, XGBModel, LinearModel) + pytorch_classes
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+try:
+    from .catboost_model import CatBoostModel
+except ModuleNotFoundError:
+    CatBoostModel = None
+    print("ModuleNotFoundError. CatBoostModel are skipped. (optional: maybe installing CatBoostModel can fix it.)")
+try:
+    from .double_ensemble import DEnsembleModel
+    from .gbdt import LGBModel
+except ModuleNotFoundError:
+    DEnsembleModel, LGBModel = None, None
+    print(
+        "ModuleNotFoundError. DEnsembleModel and LGBModel are skipped. (optional: maybe installing lightgbm can fix it.)"
+    )
+try:
+    from .xgboost import XGBModel
+except ModuleNotFoundError:
+    XGBModel = None
+    print("ModuleNotFoundError. XGBModel is skipped(optional: maybe installing xgboost can fix it).")
+try:
+    from .linear import LinearModel
+except ModuleNotFoundError:
+    LinearModel = None
+    print("ModuleNotFoundError. LinearModel is skipped(optional: maybe installing scipy and sklearn can fix it).")
+# import pytorch models
+try:
+    from .pytorch_alstm import ALSTM
+    from .pytorch_gats import GATs
+    from .pytorch_gru import GRU
+    from .pytorch_lstm import LSTM
+    from .pytorch_nn import DNNModelPytorch
+    from .pytorch_tabnet import TabnetModel
+    from .pytorch_sfm import SFM_Model
+    from .pytorch_tcn import TCN
+    from .pytorch_add import ADD
+
+    pytorch_classes = (ALSTM, GATs, GRU, LSTM, DNNModelPytorch, TabnetModel, SFM_Model, TCN, ADD)
+except ModuleNotFoundError:
+    pytorch_classes = ()
+    print("ModuleNotFoundError.  PyTorch models are skipped (optional: maybe installing pytorch can fix it).")
+
+all_model_classes = (CatBoostModel, DEnsembleModel, LGBModel, XGBModel, LinearModel) + pytorch_classes
```

## qlib/contrib/model/catboost_model.py

 * *Ordering differences only*

```diff
@@ -1,100 +1,100 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-import numpy as np
-import pandas as pd
-from typing import Text, Union
-from catboost import Pool, CatBoost
-from catboost.utils import get_gpu_device_count
-
-from ...model.base import Model
-from ...data.dataset import DatasetH
-from ...data.dataset.handler import DataHandlerLP
-from ...model.interpret.base import FeatureInt
-from ...data.dataset.weight import Reweighter
-
-
-class CatBoostModel(Model, FeatureInt):
-    """CatBoost Model"""
-
-    def __init__(self, loss="RMSE", **kwargs):
-        # There are more options
-        if loss not in {"RMSE", "Logloss"}:
-            raise NotImplementedError
-        self._params = {"loss_function": loss}
-        self._params.update(kwargs)
-        self.model = None
-
-    def fit(
-        self,
-        dataset: DatasetH,
-        num_boost_round=1000,
-        early_stopping_rounds=50,
-        verbose_eval=20,
-        evals_result=dict(),
-        reweighter=None,
-        **kwargs
-    ):
-        df_train, df_valid = dataset.prepare(
-            ["train", "valid"],
-            col_set=["feature", "label"],
-            data_key=DataHandlerLP.DK_L,
-        )
-        if df_train.empty or df_valid.empty:
-            raise ValueError("Empty data from dataset, please check your dataset config.")
-        x_train, y_train = df_train["feature"], df_train["label"]
-        x_valid, y_valid = df_valid["feature"], df_valid["label"]
-
-        # CatBoost needs 1D array as its label
-        if y_train.values.ndim == 2 and y_train.values.shape[1] == 1:
-            y_train_1d, y_valid_1d = np.squeeze(y_train.values), np.squeeze(y_valid.values)
-        else:
-            raise ValueError("CatBoost doesn't support multi-label training")
-
-        if reweighter is None:
-            w_train = None
-            w_valid = None
-        elif isinstance(reweighter, Reweighter):
-            w_train = reweighter.reweight(df_train).values
-            w_valid = reweighter.reweight(df_valid).values
-        else:
-            raise ValueError("Unsupported reweighter type.")
-
-        train_pool = Pool(data=x_train, label=y_train_1d, weight=w_train)
-        valid_pool = Pool(data=x_valid, label=y_valid_1d, weight=w_valid)
-
-        # Initialize the catboost model
-        self._params["iterations"] = num_boost_round
-        self._params["early_stopping_rounds"] = early_stopping_rounds
-        self._params["verbose_eval"] = verbose_eval
-        self._params["task_type"] = "GPU" if get_gpu_device_count() > 0 else "CPU"
-        self.model = CatBoost(self._params, **kwargs)
-
-        # train the model
-        self.model.fit(train_pool, eval_set=valid_pool, use_best_model=True, **kwargs)
-
-        evals_result = self.model.get_evals_result()
-        evals_result["train"] = list(evals_result["learn"].values())[0]
-        evals_result["valid"] = list(evals_result["validation"].values())[0]
-
-    def predict(self, dataset: DatasetH, segment: Union[Text, slice] = "test"):
-        if self.model is None:
-            raise ValueError("model is not fitted yet!")
-        x_test = dataset.prepare(segment, col_set="feature", data_key=DataHandlerLP.DK_I)
-        return pd.Series(self.model.predict(x_test.values), index=x_test.index)
-
-    def get_feature_importance(self, *args, **kwargs) -> pd.Series:
-        """get feature importance
-
-        Notes
-        -----
-            parameters references:
-            https://catboost.ai/docs/concepts/python-reference_catboost_get_feature_importance.html#python-reference_catboost_get_feature_importance
-        """
-        return pd.Series(
-            data=self.model.get_feature_importance(*args, **kwargs), index=self.model.feature_names_
-        ).sort_values(ascending=False)
-
-
-if __name__ == "__main__":
-    cat = CatBoostModel()
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+import numpy as np
+import pandas as pd
+from typing import Text, Union
+from catboost import Pool, CatBoost
+from catboost.utils import get_gpu_device_count
+
+from ...model.base import Model
+from ...data.dataset import DatasetH
+from ...data.dataset.handler import DataHandlerLP
+from ...model.interpret.base import FeatureInt
+from ...data.dataset.weight import Reweighter
+
+
+class CatBoostModel(Model, FeatureInt):
+    """CatBoost Model"""
+
+    def __init__(self, loss="RMSE", **kwargs):
+        # There are more options
+        if loss not in {"RMSE", "Logloss"}:
+            raise NotImplementedError
+        self._params = {"loss_function": loss}
+        self._params.update(kwargs)
+        self.model = None
+
+    def fit(
+        self,
+        dataset: DatasetH,
+        num_boost_round=1000,
+        early_stopping_rounds=50,
+        verbose_eval=20,
+        evals_result=dict(),
+        reweighter=None,
+        **kwargs
+    ):
+        df_train, df_valid = dataset.prepare(
+            ["train", "valid"],
+            col_set=["feature", "label"],
+            data_key=DataHandlerLP.DK_L,
+        )
+        if df_train.empty or df_valid.empty:
+            raise ValueError("Empty data from dataset, please check your dataset config.")
+        x_train, y_train = df_train["feature"], df_train["label"]
+        x_valid, y_valid = df_valid["feature"], df_valid["label"]
+
+        # CatBoost needs 1D array as its label
+        if y_train.values.ndim == 2 and y_train.values.shape[1] == 1:
+            y_train_1d, y_valid_1d = np.squeeze(y_train.values), np.squeeze(y_valid.values)
+        else:
+            raise ValueError("CatBoost doesn't support multi-label training")
+
+        if reweighter is None:
+            w_train = None
+            w_valid = None
+        elif isinstance(reweighter, Reweighter):
+            w_train = reweighter.reweight(df_train).values
+            w_valid = reweighter.reweight(df_valid).values
+        else:
+            raise ValueError("Unsupported reweighter type.")
+
+        train_pool = Pool(data=x_train, label=y_train_1d, weight=w_train)
+        valid_pool = Pool(data=x_valid, label=y_valid_1d, weight=w_valid)
+
+        # Initialize the catboost model
+        self._params["iterations"] = num_boost_round
+        self._params["early_stopping_rounds"] = early_stopping_rounds
+        self._params["verbose_eval"] = verbose_eval
+        self._params["task_type"] = "GPU" if get_gpu_device_count() > 0 else "CPU"
+        self.model = CatBoost(self._params, **kwargs)
+
+        # train the model
+        self.model.fit(train_pool, eval_set=valid_pool, use_best_model=True, **kwargs)
+
+        evals_result = self.model.get_evals_result()
+        evals_result["train"] = list(evals_result["learn"].values())[0]
+        evals_result["valid"] = list(evals_result["validation"].values())[0]
+
+    def predict(self, dataset: DatasetH, segment: Union[Text, slice] = "test"):
+        if self.model is None:
+            raise ValueError("model is not fitted yet!")
+        x_test = dataset.prepare(segment, col_set="feature", data_key=DataHandlerLP.DK_I)
+        return pd.Series(self.model.predict(x_test.values), index=x_test.index)
+
+    def get_feature_importance(self, *args, **kwargs) -> pd.Series:
+        """get feature importance
+
+        Notes
+        -----
+            parameters references:
+            https://catboost.ai/docs/concepts/python-reference_catboost_get_feature_importance.html#python-reference_catboost_get_feature_importance
+        """
+        return pd.Series(
+            data=self.model.get_feature_importance(*args, **kwargs), index=self.model.feature_names_
+        ).sort_values(ascending=False)
+
+
+if __name__ == "__main__":
+    cat = CatBoostModel()
```

## qlib/contrib/model/double_ensemble.py

 * *Ordering differences only*

```diff
@@ -1,277 +1,277 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-import lightgbm as lgb
-import numpy as np
-import pandas as pd
-from typing import Text, Union
-from ...model.base import Model
-from ...data.dataset import DatasetH
-from ...data.dataset.handler import DataHandlerLP
-from ...model.interpret.base import FeatureInt
-from ...log import get_module_logger
-
-
-class DEnsembleModel(Model, FeatureInt):
-    """Double Ensemble Model"""
-
-    def __init__(
-        self,
-        base_model="gbm",
-        loss="mse",
-        num_models=6,
-        enable_sr=True,
-        enable_fs=True,
-        alpha1=1.0,
-        alpha2=1.0,
-        bins_sr=10,
-        bins_fs=5,
-        decay=None,
-        sample_ratios=None,
-        sub_weights=None,
-        epochs=100,
-        early_stopping_rounds=None,
-        **kwargs
-    ):
-        self.base_model = base_model  # "gbm" or "mlp", specifically, we use lgbm for "gbm"
-        self.num_models = num_models  # the number of sub-models
-        self.enable_sr = enable_sr
-        self.enable_fs = enable_fs
-        self.alpha1 = alpha1
-        self.alpha2 = alpha2
-        self.bins_sr = bins_sr
-        self.bins_fs = bins_fs
-        self.decay = decay
-        if sample_ratios is None:  # the default values for sample_ratios
-            sample_ratios = [0.8, 0.7, 0.6, 0.5, 0.4]
-        if sub_weights is None:  # the default values for sub_weights
-            sub_weights = [1] * self.num_models
-        if not len(sample_ratios) == bins_fs:
-            raise ValueError("The length of sample_ratios should be equal to bins_fs.")
-        self.sample_ratios = sample_ratios
-        if not len(sub_weights) == num_models:
-            raise ValueError("The length of sub_weights should be equal to num_models.")
-        self.sub_weights = sub_weights
-        self.epochs = epochs
-        self.logger = get_module_logger("DEnsembleModel")
-        self.logger.info("Double Ensemble Model...")
-        self.ensemble = []  # the current ensemble model, a list contains all the sub-models
-        self.sub_features = []  # the features for each sub model in the form of pandas.Index
-        self.params = {"objective": loss}
-        self.params.update(kwargs)
-        self.loss = loss
-        self.early_stopping_rounds = early_stopping_rounds
-
-    def fit(self, dataset: DatasetH):
-        df_train, df_valid = dataset.prepare(
-            ["train", "valid"], col_set=["feature", "label"], data_key=DataHandlerLP.DK_L
-        )
-        if df_train.empty or df_valid.empty:
-            raise ValueError("Empty data from dataset, please check your dataset config.")
-        x_train, y_train = df_train["feature"], df_train["label"]
-        # initialize the sample weights
-        N, F = x_train.shape
-        weights = pd.Series(np.ones(N, dtype=float))
-        # initialize the features
-        features = x_train.columns
-        pred_sub = pd.DataFrame(np.zeros((N, self.num_models), dtype=float), index=x_train.index)
-        # train sub-models
-        for k in range(self.num_models):
-            self.sub_features.append(features)
-            self.logger.info("Training sub-model: ({}/{})".format(k + 1, self.num_models))
-            model_k = self.train_submodel(df_train, df_valid, weights, features)
-            self.ensemble.append(model_k)
-            # no further sample re-weight and feature selection needed for the last sub-model
-            if k + 1 == self.num_models:
-                break
-
-            self.logger.info("Retrieving loss curve and loss values...")
-            loss_curve = self.retrieve_loss_curve(model_k, df_train, features)
-            pred_k = self.predict_sub(model_k, df_train, features)
-            pred_sub.iloc[:, k] = pred_k
-            pred_ensemble = (pred_sub.iloc[:, : k + 1] * self.sub_weights[0 : k + 1]).sum(axis=1) / np.sum(
-                self.sub_weights[0 : k + 1]
-            )
-            loss_values = pd.Series(self.get_loss(y_train.values.squeeze(), pred_ensemble.values))
-
-            if self.enable_sr:
-                self.logger.info("Sample re-weighting...")
-                weights = self.sample_reweight(loss_curve, loss_values, k + 1)
-
-            if self.enable_fs:
-                self.logger.info("Feature selection...")
-                features = self.feature_selection(df_train, loss_values)
-
-    def train_submodel(self, df_train, df_valid, weights, features):
-        dtrain, dvalid = self._prepare_data_gbm(df_train, df_valid, weights, features)
-        evals_result = dict()
-
-        callbacks = [lgb.log_evaluation(20), lgb.record_evaluation(evals_result)]
-        if self.early_stopping_rounds:
-            callbacks.append(lgb.early_stopping(self.early_stopping_rounds))
-            self.logger.info("Training with early_stopping...")
-
-        model = lgb.train(
-            self.params,
-            dtrain,
-            num_boost_round=self.epochs,
-            valid_sets=[dtrain, dvalid],
-            valid_names=["train", "valid"],
-            callbacks=callbacks,
-        )
-        evals_result["train"] = list(evals_result["train"].values())[0]
-        evals_result["valid"] = list(evals_result["valid"].values())[0]
-        return model
-
-    def _prepare_data_gbm(self, df_train, df_valid, weights, features):
-        x_train, y_train = df_train["feature"].loc[:, features], df_train["label"]
-        x_valid, y_valid = df_valid["feature"].loc[:, features], df_valid["label"]
-
-        # Lightgbm need 1D array as its label
-        if y_train.values.ndim == 2 and y_train.values.shape[1] == 1:
-            y_train, y_valid = np.squeeze(y_train.values), np.squeeze(y_valid.values)
-        else:
-            raise ValueError("LightGBM doesn't support multi-label training")
-
-        dtrain = lgb.Dataset(x_train, label=y_train, weight=weights)
-        dvalid = lgb.Dataset(x_valid, label=y_valid)
-        return dtrain, dvalid
-
-    def sample_reweight(self, loss_curve, loss_values, k_th):
-        """
-        the SR module of Double Ensemble
-        :param loss_curve: the shape is NxT
-        the loss curve for the previous sub-model, where the element (i, t) if the error on the i-th sample
-        after the t-th iteration in the training of the previous sub-model.
-        :param loss_values: the shape is N
-        the loss of the current ensemble on the i-th sample.
-        :param k_th: the index of the current sub-model, starting from 1
-        :return: weights
-        the weights for all the samples.
-        """
-        # normalize loss_curve and loss_values with ranking
-        loss_curve_norm = loss_curve.rank(axis=0, pct=True)
-        loss_values_norm = (-loss_values).rank(pct=True)
-
-        # calculate l_start and l_end from loss_curve
-        N, T = loss_curve.shape
-        part = np.maximum(int(T * 0.1), 1)
-        l_start = loss_curve_norm.iloc[:, :part].mean(axis=1)
-        l_end = loss_curve_norm.iloc[:, -part:].mean(axis=1)
-
-        # calculate h-value for each sample
-        h1 = loss_values_norm
-        h2 = (l_end / l_start).rank(pct=True)
-        h = pd.DataFrame({"h_value": self.alpha1 * h1 + self.alpha2 * h2})
-
-        # calculate weights
-        h["bins"] = pd.cut(h["h_value"], self.bins_sr)
-        h_avg = h.groupby("bins")["h_value"].mean()
-        weights = pd.Series(np.zeros(N, dtype=float))
-        for b in h_avg.index:
-            weights[h["bins"] == b] = 1.0 / (self.decay**k_th * h_avg[b] + 0.1)
-        return weights
-
-    def feature_selection(self, df_train, loss_values):
-        """
-        the FS module of Double Ensemble
-        :param df_train: the shape is NxF
-        :param loss_values: the shape is N
-        the loss of the current ensemble on the i-th sample.
-        :return: res_feat: in the form of pandas.Index
-
-        """
-        x_train, y_train = df_train["feature"], df_train["label"]
-        features = x_train.columns
-        N, F = x_train.shape
-        g = pd.DataFrame({"g_value": np.zeros(F, dtype=float)})
-        M = len(self.ensemble)
-
-        # shuffle specific columns and calculate g-value for each feature
-        x_train_tmp = x_train.copy()
-        for i_f, feat in enumerate(features):
-            x_train_tmp.loc[:, feat] = np.random.permutation(x_train_tmp.loc[:, feat].values)
-            pred = pd.Series(np.zeros(N), index=x_train_tmp.index)
-            for i_s, submodel in enumerate(self.ensemble):
-                pred += (
-                    pd.Series(
-                        submodel.predict(x_train_tmp.loc[:, self.sub_features[i_s]].values), index=x_train_tmp.index
-                    )
-                    / M
-                )
-            loss_feat = self.get_loss(y_train.values.squeeze(), pred.values)
-            g.loc[i_f, "g_value"] = np.mean(loss_feat - loss_values) / (np.std(loss_feat - loss_values) + 1e-7)
-            x_train_tmp.loc[:, feat] = x_train.loc[:, feat].copy()
-
-        # one column in train features is all-nan # if g['g_value'].isna().any()
-        g["g_value"].replace(np.nan, 0, inplace=True)
-
-        # divide features into bins_fs bins
-        g["bins"] = pd.cut(g["g_value"], self.bins_fs)
-
-        # randomly sample features from bins to construct the new features
-        res_feat = []
-        sorted_bins = sorted(g["bins"].unique(), reverse=True)
-        for i_b, b in enumerate(sorted_bins):
-            b_feat = features[g["bins"] == b]
-            num_feat = int(np.ceil(self.sample_ratios[i_b] * len(b_feat)))
-            res_feat = res_feat + np.random.choice(b_feat, size=num_feat, replace=False).tolist()
-        return pd.Index(set(res_feat))
-
-    def get_loss(self, label, pred):
-        if self.loss == "mse":
-            return (label - pred) ** 2
-        else:
-            raise ValueError("not implemented yet")
-
-    def retrieve_loss_curve(self, model, df_train, features):
-        if self.base_model == "gbm":
-            num_trees = model.num_trees()
-            x_train, y_train = df_train["feature"].loc[:, features], df_train["label"]
-            # Lightgbm need 1D array as its label
-            if y_train.values.ndim == 2 and y_train.values.shape[1] == 1:
-                y_train = np.squeeze(y_train.values)
-            else:
-                raise ValueError("LightGBM doesn't support multi-label training")
-
-            N = x_train.shape[0]
-            loss_curve = pd.DataFrame(np.zeros((N, num_trees)))
-            pred_tree = np.zeros(N, dtype=float)
-            for i_tree in range(num_trees):
-                pred_tree += model.predict(x_train.values, start_iteration=i_tree, num_iteration=1)
-                loss_curve.iloc[:, i_tree] = self.get_loss(y_train, pred_tree)
-        else:
-            raise ValueError("not implemented yet")
-        return loss_curve
-
-    def predict(self, dataset: DatasetH, segment: Union[Text, slice] = "test"):
-        if self.ensemble is None:
-            raise ValueError("model is not fitted yet!")
-        x_test = dataset.prepare(segment, col_set="feature", data_key=DataHandlerLP.DK_I)
-        pred = pd.Series(np.zeros(x_test.shape[0]), index=x_test.index)
-        for i_sub, submodel in enumerate(self.ensemble):
-            feat_sub = self.sub_features[i_sub]
-            pred += (
-                pd.Series(submodel.predict(x_test.loc[:, feat_sub].values), index=x_test.index)
-                * self.sub_weights[i_sub]
-            )
-        pred = pred / np.sum(self.sub_weights)
-        return pred
-
-    def predict_sub(self, submodel, df_data, features):
-        x_data = df_data["feature"].loc[:, features]
-        pred_sub = pd.Series(submodel.predict(x_data.values), index=x_data.index)
-        return pred_sub
-
-    def get_feature_importance(self, *args, **kwargs) -> pd.Series:
-        """get feature importance
-
-        Notes
-        -----
-            parameters reference:
-            https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.Booster.html?highlight=feature_importance#lightgbm.Booster.feature_importance
-        """
-        res = []
-        for _model, _weight in zip(self.ensemble, self.sub_weights):
-            res.append(pd.Series(_model.feature_importance(*args, **kwargs), index=_model.feature_name()) * _weight)
-        return pd.concat(res, axis=1, sort=False).sum(axis=1).sort_values(ascending=False)
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+import lightgbm as lgb
+import numpy as np
+import pandas as pd
+from typing import Text, Union
+from ...model.base import Model
+from ...data.dataset import DatasetH
+from ...data.dataset.handler import DataHandlerLP
+from ...model.interpret.base import FeatureInt
+from ...log import get_module_logger
+
+
+class DEnsembleModel(Model, FeatureInt):
+    """Double Ensemble Model"""
+
+    def __init__(
+        self,
+        base_model="gbm",
+        loss="mse",
+        num_models=6,
+        enable_sr=True,
+        enable_fs=True,
+        alpha1=1.0,
+        alpha2=1.0,
+        bins_sr=10,
+        bins_fs=5,
+        decay=None,
+        sample_ratios=None,
+        sub_weights=None,
+        epochs=100,
+        early_stopping_rounds=None,
+        **kwargs
+    ):
+        self.base_model = base_model  # "gbm" or "mlp", specifically, we use lgbm for "gbm"
+        self.num_models = num_models  # the number of sub-models
+        self.enable_sr = enable_sr
+        self.enable_fs = enable_fs
+        self.alpha1 = alpha1
+        self.alpha2 = alpha2
+        self.bins_sr = bins_sr
+        self.bins_fs = bins_fs
+        self.decay = decay
+        if sample_ratios is None:  # the default values for sample_ratios
+            sample_ratios = [0.8, 0.7, 0.6, 0.5, 0.4]
+        if sub_weights is None:  # the default values for sub_weights
+            sub_weights = [1] * self.num_models
+        if not len(sample_ratios) == bins_fs:
+            raise ValueError("The length of sample_ratios should be equal to bins_fs.")
+        self.sample_ratios = sample_ratios
+        if not len(sub_weights) == num_models:
+            raise ValueError("The length of sub_weights should be equal to num_models.")
+        self.sub_weights = sub_weights
+        self.epochs = epochs
+        self.logger = get_module_logger("DEnsembleModel")
+        self.logger.info("Double Ensemble Model...")
+        self.ensemble = []  # the current ensemble model, a list contains all the sub-models
+        self.sub_features = []  # the features for each sub model in the form of pandas.Index
+        self.params = {"objective": loss}
+        self.params.update(kwargs)
+        self.loss = loss
+        self.early_stopping_rounds = early_stopping_rounds
+
+    def fit(self, dataset: DatasetH):
+        df_train, df_valid = dataset.prepare(
+            ["train", "valid"], col_set=["feature", "label"], data_key=DataHandlerLP.DK_L
+        )
+        if df_train.empty or df_valid.empty:
+            raise ValueError("Empty data from dataset, please check your dataset config.")
+        x_train, y_train = df_train["feature"], df_train["label"]
+        # initialize the sample weights
+        N, F = x_train.shape
+        weights = pd.Series(np.ones(N, dtype=float))
+        # initialize the features
+        features = x_train.columns
+        pred_sub = pd.DataFrame(np.zeros((N, self.num_models), dtype=float), index=x_train.index)
+        # train sub-models
+        for k in range(self.num_models):
+            self.sub_features.append(features)
+            self.logger.info("Training sub-model: ({}/{})".format(k + 1, self.num_models))
+            model_k = self.train_submodel(df_train, df_valid, weights, features)
+            self.ensemble.append(model_k)
+            # no further sample re-weight and feature selection needed for the last sub-model
+            if k + 1 == self.num_models:
+                break
+
+            self.logger.info("Retrieving loss curve and loss values...")
+            loss_curve = self.retrieve_loss_curve(model_k, df_train, features)
+            pred_k = self.predict_sub(model_k, df_train, features)
+            pred_sub.iloc[:, k] = pred_k
+            pred_ensemble = (pred_sub.iloc[:, : k + 1] * self.sub_weights[0 : k + 1]).sum(axis=1) / np.sum(
+                self.sub_weights[0 : k + 1]
+            )
+            loss_values = pd.Series(self.get_loss(y_train.values.squeeze(), pred_ensemble.values))
+
+            if self.enable_sr:
+                self.logger.info("Sample re-weighting...")
+                weights = self.sample_reweight(loss_curve, loss_values, k + 1)
+
+            if self.enable_fs:
+                self.logger.info("Feature selection...")
+                features = self.feature_selection(df_train, loss_values)
+
+    def train_submodel(self, df_train, df_valid, weights, features):
+        dtrain, dvalid = self._prepare_data_gbm(df_train, df_valid, weights, features)
+        evals_result = dict()
+
+        callbacks = [lgb.log_evaluation(20), lgb.record_evaluation(evals_result)]
+        if self.early_stopping_rounds:
+            callbacks.append(lgb.early_stopping(self.early_stopping_rounds))
+            self.logger.info("Training with early_stopping...")
+
+        model = lgb.train(
+            self.params,
+            dtrain,
+            num_boost_round=self.epochs,
+            valid_sets=[dtrain, dvalid],
+            valid_names=["train", "valid"],
+            callbacks=callbacks,
+        )
+        evals_result["train"] = list(evals_result["train"].values())[0]
+        evals_result["valid"] = list(evals_result["valid"].values())[0]
+        return model
+
+    def _prepare_data_gbm(self, df_train, df_valid, weights, features):
+        x_train, y_train = df_train["feature"].loc[:, features], df_train["label"]
+        x_valid, y_valid = df_valid["feature"].loc[:, features], df_valid["label"]
+
+        # Lightgbm need 1D array as its label
+        if y_train.values.ndim == 2 and y_train.values.shape[1] == 1:
+            y_train, y_valid = np.squeeze(y_train.values), np.squeeze(y_valid.values)
+        else:
+            raise ValueError("LightGBM doesn't support multi-label training")
+
+        dtrain = lgb.Dataset(x_train, label=y_train, weight=weights)
+        dvalid = lgb.Dataset(x_valid, label=y_valid)
+        return dtrain, dvalid
+
+    def sample_reweight(self, loss_curve, loss_values, k_th):
+        """
+        the SR module of Double Ensemble
+        :param loss_curve: the shape is NxT
+        the loss curve for the previous sub-model, where the element (i, t) if the error on the i-th sample
+        after the t-th iteration in the training of the previous sub-model.
+        :param loss_values: the shape is N
+        the loss of the current ensemble on the i-th sample.
+        :param k_th: the index of the current sub-model, starting from 1
+        :return: weights
+        the weights for all the samples.
+        """
+        # normalize loss_curve and loss_values with ranking
+        loss_curve_norm = loss_curve.rank(axis=0, pct=True)
+        loss_values_norm = (-loss_values).rank(pct=True)
+
+        # calculate l_start and l_end from loss_curve
+        N, T = loss_curve.shape
+        part = np.maximum(int(T * 0.1), 1)
+        l_start = loss_curve_norm.iloc[:, :part].mean(axis=1)
+        l_end = loss_curve_norm.iloc[:, -part:].mean(axis=1)
+
+        # calculate h-value for each sample
+        h1 = loss_values_norm
+        h2 = (l_end / l_start).rank(pct=True)
+        h = pd.DataFrame({"h_value": self.alpha1 * h1 + self.alpha2 * h2})
+
+        # calculate weights
+        h["bins"] = pd.cut(h["h_value"], self.bins_sr)
+        h_avg = h.groupby("bins")["h_value"].mean()
+        weights = pd.Series(np.zeros(N, dtype=float))
+        for b in h_avg.index:
+            weights[h["bins"] == b] = 1.0 / (self.decay**k_th * h_avg[b] + 0.1)
+        return weights
+
+    def feature_selection(self, df_train, loss_values):
+        """
+        the FS module of Double Ensemble
+        :param df_train: the shape is NxF
+        :param loss_values: the shape is N
+        the loss of the current ensemble on the i-th sample.
+        :return: res_feat: in the form of pandas.Index
+
+        """
+        x_train, y_train = df_train["feature"], df_train["label"]
+        features = x_train.columns
+        N, F = x_train.shape
+        g = pd.DataFrame({"g_value": np.zeros(F, dtype=float)})
+        M = len(self.ensemble)
+
+        # shuffle specific columns and calculate g-value for each feature
+        x_train_tmp = x_train.copy()
+        for i_f, feat in enumerate(features):
+            x_train_tmp.loc[:, feat] = np.random.permutation(x_train_tmp.loc[:, feat].values)
+            pred = pd.Series(np.zeros(N), index=x_train_tmp.index)
+            for i_s, submodel in enumerate(self.ensemble):
+                pred += (
+                    pd.Series(
+                        submodel.predict(x_train_tmp.loc[:, self.sub_features[i_s]].values), index=x_train_tmp.index
+                    )
+                    / M
+                )
+            loss_feat = self.get_loss(y_train.values.squeeze(), pred.values)
+            g.loc[i_f, "g_value"] = np.mean(loss_feat - loss_values) / (np.std(loss_feat - loss_values) + 1e-7)
+            x_train_tmp.loc[:, feat] = x_train.loc[:, feat].copy()
+
+        # one column in train features is all-nan # if g['g_value'].isna().any()
+        g["g_value"].replace(np.nan, 0, inplace=True)
+
+        # divide features into bins_fs bins
+        g["bins"] = pd.cut(g["g_value"], self.bins_fs)
+
+        # randomly sample features from bins to construct the new features
+        res_feat = []
+        sorted_bins = sorted(g["bins"].unique(), reverse=True)
+        for i_b, b in enumerate(sorted_bins):
+            b_feat = features[g["bins"] == b]
+            num_feat = int(np.ceil(self.sample_ratios[i_b] * len(b_feat)))
+            res_feat = res_feat + np.random.choice(b_feat, size=num_feat, replace=False).tolist()
+        return pd.Index(set(res_feat))
+
+    def get_loss(self, label, pred):
+        if self.loss == "mse":
+            return (label - pred) ** 2
+        else:
+            raise ValueError("not implemented yet")
+
+    def retrieve_loss_curve(self, model, df_train, features):
+        if self.base_model == "gbm":
+            num_trees = model.num_trees()
+            x_train, y_train = df_train["feature"].loc[:, features], df_train["label"]
+            # Lightgbm need 1D array as its label
+            if y_train.values.ndim == 2 and y_train.values.shape[1] == 1:
+                y_train = np.squeeze(y_train.values)
+            else:
+                raise ValueError("LightGBM doesn't support multi-label training")
+
+            N = x_train.shape[0]
+            loss_curve = pd.DataFrame(np.zeros((N, num_trees)))
+            pred_tree = np.zeros(N, dtype=float)
+            for i_tree in range(num_trees):
+                pred_tree += model.predict(x_train.values, start_iteration=i_tree, num_iteration=1)
+                loss_curve.iloc[:, i_tree] = self.get_loss(y_train, pred_tree)
+        else:
+            raise ValueError("not implemented yet")
+        return loss_curve
+
+    def predict(self, dataset: DatasetH, segment: Union[Text, slice] = "test"):
+        if self.ensemble is None:
+            raise ValueError("model is not fitted yet!")
+        x_test = dataset.prepare(segment, col_set="feature", data_key=DataHandlerLP.DK_I)
+        pred = pd.Series(np.zeros(x_test.shape[0]), index=x_test.index)
+        for i_sub, submodel in enumerate(self.ensemble):
+            feat_sub = self.sub_features[i_sub]
+            pred += (
+                pd.Series(submodel.predict(x_test.loc[:, feat_sub].values), index=x_test.index)
+                * self.sub_weights[i_sub]
+            )
+        pred = pred / np.sum(self.sub_weights)
+        return pred
+
+    def predict_sub(self, submodel, df_data, features):
+        x_data = df_data["feature"].loc[:, features]
+        pred_sub = pd.Series(submodel.predict(x_data.values), index=x_data.index)
+        return pred_sub
+
+    def get_feature_importance(self, *args, **kwargs) -> pd.Series:
+        """get feature importance
+
+        Notes
+        -----
+            parameters reference:
+            https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.Booster.html?highlight=feature_importance#lightgbm.Booster.feature_importance
+        """
+        res = []
+        for _model, _weight in zip(self.ensemble, self.sub_weights):
+            res.append(pd.Series(_model.feature_importance(*args, **kwargs), index=_model.feature_name()) * _weight)
+        return pd.concat(res, axis=1, sort=False).sum(axis=1).sort_values(ascending=False)
```

## qlib/contrib/model/gbdt.py

 * *Ordering differences only*

```diff
@@ -1,124 +1,124 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-import numpy as np
-import pandas as pd
-import lightgbm as lgb
-from typing import List, Text, Tuple, Union
-from ...model.base import ModelFT
-from ...data.dataset import DatasetH
-from ...data.dataset.handler import DataHandlerLP
-from ...model.interpret.base import LightGBMFInt
-from ...data.dataset.weight import Reweighter
-from qlib.workflow import R
-
-
-class LGBModel(ModelFT, LightGBMFInt):
-    """LightGBM Model"""
-
-    def __init__(self, loss="mse", early_stopping_rounds=50, num_boost_round=1000, **kwargs):
-        if loss not in {"mse", "binary"}:
-            raise NotImplementedError
-        self.params = {"objective": loss, "verbosity": -1}
-        self.params.update(kwargs)
-        self.early_stopping_rounds = early_stopping_rounds
-        self.num_boost_round = num_boost_round
-        self.model = None
-
-    def _prepare_data(self, dataset: DatasetH, reweighter=None) -> List[Tuple[lgb.Dataset, str]]:
-        """
-        The motivation of current version is to make validation optional
-        - train segment is necessary;
-        """
-        ds_l = []
-        assert "train" in dataset.segments
-        for key in ["train", "valid"]:
-            if key in dataset.segments:
-                df = dataset.prepare(key, col_set=["feature", "label"], data_key=DataHandlerLP.DK_L)
-                if df.empty:
-                    raise ValueError("Empty data from dataset, please check your dataset config.")
-                x, y = df["feature"], df["label"]
-
-                # Lightgbm need 1D array as its label
-                if y.values.ndim == 2 and y.values.shape[1] == 1:
-                    y = np.squeeze(y.values)
-                else:
-                    raise ValueError("LightGBM doesn't support multi-label training")
-
-                if reweighter is None:
-                    w = None
-                elif isinstance(reweighter, Reweighter):
-                    w = reweighter.reweight(df)
-                else:
-                    raise ValueError("Unsupported reweighter type.")
-                ds_l.append((lgb.Dataset(x.values, label=y, weight=w), key))
-        return ds_l
-
-    def fit(
-        self,
-        dataset: DatasetH,
-        num_boost_round=None,
-        early_stopping_rounds=None,
-        verbose_eval=20,
-        evals_result=None,
-        reweighter=None,
-        **kwargs,
-    ):
-        if evals_result is None:
-            evals_result = {}  # in case of unsafety of Python default values
-        ds_l = self._prepare_data(dataset, reweighter)
-        ds, names = list(zip(*ds_l))
-        early_stopping_callback = lgb.early_stopping(
-            self.early_stopping_rounds if early_stopping_rounds is None else early_stopping_rounds
-        )
-        # NOTE: if you encounter error here. Please upgrade your lightgbm
-        verbose_eval_callback = lgb.log_evaluation(period=verbose_eval)
-        evals_result_callback = lgb.record_evaluation(evals_result)
-        self.model = lgb.train(
-            self.params,
-            ds[0],  # training dataset
-            num_boost_round=self.num_boost_round if num_boost_round is None else num_boost_round,
-            valid_sets=ds,
-            valid_names=names,
-            callbacks=[early_stopping_callback, verbose_eval_callback, evals_result_callback],
-            **kwargs,
-        )
-        for k in names:
-            for key, val in evals_result[k].items():
-                name = f"{key}.{k}"
-                for epoch, m in enumerate(val):
-                    R.log_metrics(**{name.replace("@", "_"): m}, step=epoch)
-
-    def predict(self, dataset: DatasetH, segment: Union[Text, slice] = "test"):
-        if self.model is None:
-            raise ValueError("model is not fitted yet!")
-        x_test = dataset.prepare(segment, col_set="feature", data_key=DataHandlerLP.DK_I)
-        return pd.Series(self.model.predict(x_test.values), index=x_test.index)
-
-    def finetune(self, dataset: DatasetH, num_boost_round=10, verbose_eval=20, reweighter=None):
-        """
-        finetune model
-
-        Parameters
-        ----------
-        dataset : DatasetH
-            dataset for finetuning
-        num_boost_round : int
-            number of round to finetune model
-        verbose_eval : int
-            verbose level
-        """
-        # Based on existing model and finetune by train more rounds
-        dtrain, _ = self._prepare_data(dataset, reweighter)  # pylint: disable=W0632
-        if dtrain.empty:
-            raise ValueError("Empty data from dataset, please check your dataset config.")
-        verbose_eval_callback = lgb.log_evaluation(period=verbose_eval)
-        self.model = lgb.train(
-            self.params,
-            dtrain,
-            num_boost_round=num_boost_round,
-            init_model=self.model,
-            valid_sets=[dtrain],
-            valid_names=["train"],
-            callbacks=[verbose_eval_callback],
-        )
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+import numpy as np
+import pandas as pd
+import lightgbm as lgb
+from typing import List, Text, Tuple, Union
+from ...model.base import ModelFT
+from ...data.dataset import DatasetH
+from ...data.dataset.handler import DataHandlerLP
+from ...model.interpret.base import LightGBMFInt
+from ...data.dataset.weight import Reweighter
+from qlib.workflow import R
+
+
+class LGBModel(ModelFT, LightGBMFInt):
+    """LightGBM Model"""
+
+    def __init__(self, loss="mse", early_stopping_rounds=50, num_boost_round=1000, **kwargs):
+        if loss not in {"mse", "binary"}:
+            raise NotImplementedError
+        self.params = {"objective": loss, "verbosity": -1}
+        self.params.update(kwargs)
+        self.early_stopping_rounds = early_stopping_rounds
+        self.num_boost_round = num_boost_round
+        self.model = None
+
+    def _prepare_data(self, dataset: DatasetH, reweighter=None) -> List[Tuple[lgb.Dataset, str]]:
+        """
+        The motivation of current version is to make validation optional
+        - train segment is necessary;
+        """
+        ds_l = []
+        assert "train" in dataset.segments
+        for key in ["train", "valid"]:
+            if key in dataset.segments:
+                df = dataset.prepare(key, col_set=["feature", "label"], data_key=DataHandlerLP.DK_L)
+                if df.empty:
+                    raise ValueError("Empty data from dataset, please check your dataset config.")
+                x, y = df["feature"], df["label"]
+
+                # Lightgbm need 1D array as its label
+                if y.values.ndim == 2 and y.values.shape[1] == 1:
+                    y = np.squeeze(y.values)
+                else:
+                    raise ValueError("LightGBM doesn't support multi-label training")
+
+                if reweighter is None:
+                    w = None
+                elif isinstance(reweighter, Reweighter):
+                    w = reweighter.reweight(df)
+                else:
+                    raise ValueError("Unsupported reweighter type.")
+                ds_l.append((lgb.Dataset(x.values, label=y, weight=w), key))
+        return ds_l
+
+    def fit(
+        self,
+        dataset: DatasetH,
+        num_boost_round=None,
+        early_stopping_rounds=None,
+        verbose_eval=20,
+        evals_result=None,
+        reweighter=None,
+        **kwargs,
+    ):
+        if evals_result is None:
+            evals_result = {}  # in case of unsafety of Python default values
+        ds_l = self._prepare_data(dataset, reweighter)
+        ds, names = list(zip(*ds_l))
+        early_stopping_callback = lgb.early_stopping(
+            self.early_stopping_rounds if early_stopping_rounds is None else early_stopping_rounds
+        )
+        # NOTE: if you encounter error here. Please upgrade your lightgbm
+        verbose_eval_callback = lgb.log_evaluation(period=verbose_eval)
+        evals_result_callback = lgb.record_evaluation(evals_result)
+        self.model = lgb.train(
+            self.params,
+            ds[0],  # training dataset
+            num_boost_round=self.num_boost_round if num_boost_round is None else num_boost_round,
+            valid_sets=ds,
+            valid_names=names,
+            callbacks=[early_stopping_callback, verbose_eval_callback, evals_result_callback],
+            **kwargs,
+        )
+        for k in names:
+            for key, val in evals_result[k].items():
+                name = f"{key}.{k}"
+                for epoch, m in enumerate(val):
+                    R.log_metrics(**{name.replace("@", "_"): m}, step=epoch)
+
+    def predict(self, dataset: DatasetH, segment: Union[Text, slice] = "test"):
+        if self.model is None:
+            raise ValueError("model is not fitted yet!")
+        x_test = dataset.prepare(segment, col_set="feature", data_key=DataHandlerLP.DK_I)
+        return pd.Series(self.model.predict(x_test.values), index=x_test.index)
+
+    def finetune(self, dataset: DatasetH, num_boost_round=10, verbose_eval=20, reweighter=None):
+        """
+        finetune model
+
+        Parameters
+        ----------
+        dataset : DatasetH
+            dataset for finetuning
+        num_boost_round : int
+            number of round to finetune model
+        verbose_eval : int
+            verbose level
+        """
+        # Based on existing model and finetune by train more rounds
+        dtrain, _ = self._prepare_data(dataset, reweighter)  # pylint: disable=W0632
+        if dtrain.empty:
+            raise ValueError("Empty data from dataset, please check your dataset config.")
+        verbose_eval_callback = lgb.log_evaluation(period=verbose_eval)
+        self.model = lgb.train(
+            self.params,
+            dtrain,
+            num_boost_round=num_boost_round,
+            init_model=self.model,
+            valid_sets=[dtrain],
+            valid_names=["train"],
+            callbacks=[verbose_eval_callback],
+        )
```

## qlib/contrib/model/highfreq_gdbt_model.py

 * *Ordering differences only*

```diff
@@ -1,165 +1,165 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-import warnings
-import numpy as np
-import pandas as pd
-import lightgbm as lgb
-
-from ...model.base import ModelFT
-from ...data.dataset import DatasetH
-from ...data.dataset.handler import DataHandlerLP
-from ...model.interpret.base import LightGBMFInt
-
-
-class HFLGBModel(ModelFT, LightGBMFInt):
-    """LightGBM Model for high frequency prediction"""
-
-    def __init__(self, loss="mse", **kwargs):
-        if loss not in {"mse", "binary"}:
-            raise NotImplementedError
-        self.params = {"objective": loss, "verbosity": -1}
-        self.params.update(kwargs)
-        self.model = None
-
-    def _cal_signal_metrics(self, y_test, l_cut, r_cut):
-        """
-        Calcaute the signal metrics by daily level
-        """
-        up_pre, down_pre = [], []
-        up_alpha_ll, down_alpha_ll = [], []
-        for date in y_test.index.get_level_values(0).unique():
-            df_res = y_test.loc[date].sort_values("pred")
-            if int(l_cut * len(df_res)) < 10:
-                warnings.warn("Warning: threhold is too low or instruments number is not enough")
-                continue
-            top = df_res.iloc[: int(l_cut * len(df_res))]
-            bottom = df_res.iloc[int(r_cut * len(df_res)) :]
-
-            down_precision = len(top[top[top.columns[0]] < 0]) / (len(top))
-            up_precision = len(bottom[bottom[top.columns[0]] > 0]) / (len(bottom))
-
-            down_alpha = top[top.columns[0]].mean()
-            up_alpha = bottom[bottom.columns[0]].mean()
-
-            up_pre.append(up_precision)
-            down_pre.append(down_precision)
-            up_alpha_ll.append(up_alpha)
-            down_alpha_ll.append(down_alpha)
-
-        return (
-            np.array(up_pre).mean(),
-            np.array(down_pre).mean(),
-            np.array(up_alpha_ll).mean(),
-            np.array(down_alpha_ll).mean(),
-        )
-
-    def hf_signal_test(self, dataset: DatasetH, threhold=0.2):
-        """
-        Test the signal in high frequency test set
-        """
-        if self.model is None:
-            raise ValueError("Model hasn't been trained yet")
-        df_test = dataset.prepare("test", col_set=["feature", "label"], data_key=DataHandlerLP.DK_I)
-        df_test.dropna(inplace=True)
-        x_test, y_test = df_test["feature"], df_test["label"]
-        # Convert label into alpha
-        y_test[y_test.columns[0]] = y_test[y_test.columns[0]] - y_test[y_test.columns[0]].mean(level=0)
-
-        res = pd.Series(self.model.predict(x_test.values), index=x_test.index)
-        y_test["pred"] = res
-
-        up_p, down_p, up_a, down_a = self._cal_signal_metrics(y_test, threhold, 1 - threhold)
-        print("===============================")
-        print("High frequency signal test")
-        print("===============================")
-        print("Test set precision: ")
-        print("Positive precision: {}, Negative precision: {}".format(up_p, down_p))
-        print("Test Alpha Average in test set: ")
-        print("Positive average alpha: {}, Negative average alpha: {}".format(up_a, down_a))
-
-    def _prepare_data(self, dataset: DatasetH):
-        df_train, df_valid = dataset.prepare(
-            ["train", "valid"], col_set=["feature", "label"], data_key=DataHandlerLP.DK_L
-        )
-        if df_train.empty or df_valid.empty:
-            raise ValueError("Empty data from dataset, please check your dataset config.")
-
-        x_train, y_train = df_train["feature"], df_train["label"]
-        x_valid, y_valid = df_valid["feature"], df_valid["label"]
-        if y_train.values.ndim == 2 and y_train.values.shape[1] == 1:
-            l_name = df_train["label"].columns[0]
-            # Convert label into alpha
-            df_train["label"][l_name] = df_train["label"][l_name] - df_train["label"][l_name].mean(level=0)
-            df_valid["label"][l_name] = df_valid["label"][l_name] - df_valid["label"][l_name].mean(level=0)
-
-            def mapping_fn(x):
-                return 0 if x < 0 else 1
-
-            df_train["label_c"] = df_train["label"][l_name].apply(mapping_fn)
-            df_valid["label_c"] = df_valid["label"][l_name].apply(mapping_fn)
-            x_train, y_train = df_train["feature"], df_train["label_c"].values
-            x_valid, y_valid = df_valid["feature"], df_valid["label_c"].values
-        else:
-            raise ValueError("LightGBM doesn't support multi-label training")
-
-        dtrain = lgb.Dataset(x_train, label=y_train)
-        dvalid = lgb.Dataset(x_valid, label=y_valid)
-        return dtrain, dvalid
-
-    def fit(
-        self,
-        dataset: DatasetH,
-        num_boost_round=1000,
-        early_stopping_rounds=50,
-        verbose_eval=20,
-        evals_result=None,
-    ):
-        if evals_result is None:
-            evals_result = dict()
-        dtrain, dvalid = self._prepare_data(dataset)
-        early_stopping_callback = lgb.early_stopping(early_stopping_rounds)
-        verbose_eval_callback = lgb.log_evaluation(period=verbose_eval)
-        evals_result_callback = lgb.record_evaluation(evals_result)
-        self.model = lgb.train(
-            self.params,
-            dtrain,
-            num_boost_round=num_boost_round,
-            valid_sets=[dtrain, dvalid],
-            valid_names=["train", "valid"],
-            callbacks=[early_stopping_callback, verbose_eval_callback, evals_result_callback],
-        )
-        evals_result["train"] = list(evals_result["train"].values())[0]
-        evals_result["valid"] = list(evals_result["valid"].values())[0]
-
-    def predict(self, dataset):
-        if self.model is None:
-            raise ValueError("model is not fitted yet!")
-        x_test = dataset.prepare("test", col_set="feature", data_key=DataHandlerLP.DK_I)
-        return pd.Series(self.model.predict(x_test.values), index=x_test.index)
-
-    def finetune(self, dataset: DatasetH, num_boost_round=10, verbose_eval=20):
-        """
-        finetune model
-
-        Parameters
-        ----------
-        dataset : DatasetH
-            dataset for finetuning
-        num_boost_round : int
-            number of round to finetune model
-        verbose_eval : int
-            verbose level
-        """
-        # Based on existing model and finetune by train more rounds
-        dtrain, _ = self._prepare_data(dataset)
-        verbose_eval_callback = lgb.log_evaluation(period=verbose_eval)
-        self.model = lgb.train(
-            self.params,
-            dtrain,
-            num_boost_round=num_boost_round,
-            init_model=self.model,
-            valid_sets=[dtrain],
-            valid_names=["train"],
-            callbacks=[verbose_eval_callback],
-        )
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+import warnings
+import numpy as np
+import pandas as pd
+import lightgbm as lgb
+
+from ...model.base import ModelFT
+from ...data.dataset import DatasetH
+from ...data.dataset.handler import DataHandlerLP
+from ...model.interpret.base import LightGBMFInt
+
+
+class HFLGBModel(ModelFT, LightGBMFInt):
+    """LightGBM Model for high frequency prediction"""
+
+    def __init__(self, loss="mse", **kwargs):
+        if loss not in {"mse", "binary"}:
+            raise NotImplementedError
+        self.params = {"objective": loss, "verbosity": -1}
+        self.params.update(kwargs)
+        self.model = None
+
+    def _cal_signal_metrics(self, y_test, l_cut, r_cut):
+        """
+        Calcaute the signal metrics by daily level
+        """
+        up_pre, down_pre = [], []
+        up_alpha_ll, down_alpha_ll = [], []
+        for date in y_test.index.get_level_values(0).unique():
+            df_res = y_test.loc[date].sort_values("pred")
+            if int(l_cut * len(df_res)) < 10:
+                warnings.warn("Warning: threhold is too low or instruments number is not enough")
+                continue
+            top = df_res.iloc[: int(l_cut * len(df_res))]
+            bottom = df_res.iloc[int(r_cut * len(df_res)) :]
+
+            down_precision = len(top[top[top.columns[0]] < 0]) / (len(top))
+            up_precision = len(bottom[bottom[top.columns[0]] > 0]) / (len(bottom))
+
+            down_alpha = top[top.columns[0]].mean()
+            up_alpha = bottom[bottom.columns[0]].mean()
+
+            up_pre.append(up_precision)
+            down_pre.append(down_precision)
+            up_alpha_ll.append(up_alpha)
+            down_alpha_ll.append(down_alpha)
+
+        return (
+            np.array(up_pre).mean(),
+            np.array(down_pre).mean(),
+            np.array(up_alpha_ll).mean(),
+            np.array(down_alpha_ll).mean(),
+        )
+
+    def hf_signal_test(self, dataset: DatasetH, threhold=0.2):
+        """
+        Test the signal in high frequency test set
+        """
+        if self.model is None:
+            raise ValueError("Model hasn't been trained yet")
+        df_test = dataset.prepare("test", col_set=["feature", "label"], data_key=DataHandlerLP.DK_I)
+        df_test.dropna(inplace=True)
+        x_test, y_test = df_test["feature"], df_test["label"]
+        # Convert label into alpha
+        y_test[y_test.columns[0]] = y_test[y_test.columns[0]] - y_test[y_test.columns[0]].mean(level=0)
+
+        res = pd.Series(self.model.predict(x_test.values), index=x_test.index)
+        y_test["pred"] = res
+
+        up_p, down_p, up_a, down_a = self._cal_signal_metrics(y_test, threhold, 1 - threhold)
+        print("===============================")
+        print("High frequency signal test")
+        print("===============================")
+        print("Test set precision: ")
+        print("Positive precision: {}, Negative precision: {}".format(up_p, down_p))
+        print("Test Alpha Average in test set: ")
+        print("Positive average alpha: {}, Negative average alpha: {}".format(up_a, down_a))
+
+    def _prepare_data(self, dataset: DatasetH):
+        df_train, df_valid = dataset.prepare(
+            ["train", "valid"], col_set=["feature", "label"], data_key=DataHandlerLP.DK_L
+        )
+        if df_train.empty or df_valid.empty:
+            raise ValueError("Empty data from dataset, please check your dataset config.")
+
+        x_train, y_train = df_train["feature"], df_train["label"]
+        x_valid, y_valid = df_valid["feature"], df_valid["label"]
+        if y_train.values.ndim == 2 and y_train.values.shape[1] == 1:
+            l_name = df_train["label"].columns[0]
+            # Convert label into alpha
+            df_train["label"][l_name] = df_train["label"][l_name] - df_train["label"][l_name].mean(level=0)
+            df_valid["label"][l_name] = df_valid["label"][l_name] - df_valid["label"][l_name].mean(level=0)
+
+            def mapping_fn(x):
+                return 0 if x < 0 else 1
+
+            df_train["label_c"] = df_train["label"][l_name].apply(mapping_fn)
+            df_valid["label_c"] = df_valid["label"][l_name].apply(mapping_fn)
+            x_train, y_train = df_train["feature"], df_train["label_c"].values
+            x_valid, y_valid = df_valid["feature"], df_valid["label_c"].values
+        else:
+            raise ValueError("LightGBM doesn't support multi-label training")
+
+        dtrain = lgb.Dataset(x_train, label=y_train)
+        dvalid = lgb.Dataset(x_valid, label=y_valid)
+        return dtrain, dvalid
+
+    def fit(
+        self,
+        dataset: DatasetH,
+        num_boost_round=1000,
+        early_stopping_rounds=50,
+        verbose_eval=20,
+        evals_result=None,
+    ):
+        if evals_result is None:
+            evals_result = dict()
+        dtrain, dvalid = self._prepare_data(dataset)
+        early_stopping_callback = lgb.early_stopping(early_stopping_rounds)
+        verbose_eval_callback = lgb.log_evaluation(period=verbose_eval)
+        evals_result_callback = lgb.record_evaluation(evals_result)
+        self.model = lgb.train(
+            self.params,
+            dtrain,
+            num_boost_round=num_boost_round,
+            valid_sets=[dtrain, dvalid],
+            valid_names=["train", "valid"],
+            callbacks=[early_stopping_callback, verbose_eval_callback, evals_result_callback],
+        )
+        evals_result["train"] = list(evals_result["train"].values())[0]
+        evals_result["valid"] = list(evals_result["valid"].values())[0]
+
+    def predict(self, dataset):
+        if self.model is None:
+            raise ValueError("model is not fitted yet!")
+        x_test = dataset.prepare("test", col_set="feature", data_key=DataHandlerLP.DK_I)
+        return pd.Series(self.model.predict(x_test.values), index=x_test.index)
+
+    def finetune(self, dataset: DatasetH, num_boost_round=10, verbose_eval=20):
+        """
+        finetune model
+
+        Parameters
+        ----------
+        dataset : DatasetH
+            dataset for finetuning
+        num_boost_round : int
+            number of round to finetune model
+        verbose_eval : int
+            verbose level
+        """
+        # Based on existing model and finetune by train more rounds
+        dtrain, _ = self._prepare_data(dataset)
+        verbose_eval_callback = lgb.log_evaluation(period=verbose_eval)
+        self.model = lgb.train(
+            self.params,
+            dtrain,
+            num_boost_round=num_boost_round,
+            init_model=self.model,
+            valid_sets=[dtrain],
+            valid_names=["train"],
+            callbacks=[verbose_eval_callback],
+        )
```

## qlib/contrib/model/linear.py

 * *Ordering differences only*

```diff
@@ -1,112 +1,112 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-import numpy as np
-import pandas as pd
-from typing import Text, Union
-from qlib.log import get_module_logger
-from qlib.data.dataset.weight import Reweighter
-from scipy.optimize import nnls
-from sklearn.linear_model import LinearRegression, Ridge, Lasso
-
-from ...model.base import Model
-from ...data.dataset import DatasetH
-from ...data.dataset.handler import DataHandlerLP
-
-
-class LinearModel(Model):
-    """Linear Model
-
-    Solve one of the following regression problems:
-        - `ols`: min_w |y - Xw|^2_2
-        - `nnls`: min_w |y - Xw|^2_2, s.t. w >= 0
-        - `ridge`: min_w |y - Xw|^2_2 + \alpha*|w|^2_2
-        - `lasso`: min_w |y - Xw|^2_2 + \alpha*|w|_1
-    where `w` is the regression coefficient.
-    """
-
-    OLS = "ols"
-    NNLS = "nnls"
-    RIDGE = "ridge"
-    LASSO = "lasso"
-
-    def __init__(self, estimator="ols", alpha=0.0, fit_intercept=False, include_valid: bool = False):
-        """
-        Parameters
-        ----------
-        estimator : str
-            which estimator to use for linear regression
-        alpha : float
-            l1 or l2 regularization parameter
-        fit_intercept : bool
-            whether fit intercept
-        include_valid: bool
-            Should the validation data be included for training?
-            The validation data should be included
-        """
-        assert estimator in [self.OLS, self.NNLS, self.RIDGE, self.LASSO], f"unsupported estimator `{estimator}`"
-        self.estimator = estimator
-
-        assert alpha == 0 or estimator in [self.RIDGE, self.LASSO], f"alpha is only supported in `ridge`&`lasso`"
-        self.alpha = alpha
-
-        self.fit_intercept = fit_intercept
-
-        self.coef_ = None
-        self.include_valid = include_valid
-
-    def fit(self, dataset: DatasetH, reweighter: Reweighter = None):
-        df_train = dataset.prepare("train", col_set=["feature", "label"], data_key=DataHandlerLP.DK_L)
-        if self.include_valid:
-            try:
-                df_valid = dataset.prepare("valid", col_set=["feature", "label"], data_key=DataHandlerLP.DK_L)
-                df_train = pd.concat([df_train, df_valid])
-            except KeyError:
-                get_module_logger("LinearModel").info("include_valid=True, but valid does not exist")
-        if df_train.empty:
-            raise ValueError("Empty data from dataset, please check your dataset config.")
-        if reweighter is not None:
-            w: pd.Series = reweighter.reweight(df_train)
-            w = w.values
-        else:
-            w = None
-        X, y = df_train["feature"].values, np.squeeze(df_train["label"].values)
-
-        if self.estimator in [self.OLS, self.RIDGE, self.LASSO]:
-            self._fit(X, y, w)
-        elif self.estimator == self.NNLS:
-            self._fit_nnls(X, y, w)
-        else:
-            raise ValueError(f"unknown estimator `{self.estimator}`")
-
-        return self
-
-    def _fit(self, X, y, w):
-        if self.estimator == self.OLS:
-            model = LinearRegression(fit_intercept=self.fit_intercept, copy_X=False)
-        else:
-            model = {self.RIDGE: Ridge, self.LASSO: Lasso}[self.estimator](
-                alpha=self.alpha, fit_intercept=self.fit_intercept, copy_X=False
-            )
-        model.fit(X, y, sample_weight=w)
-        self.coef_ = model.coef_
-        self.intercept_ = model.intercept_
-
-    def _fit_nnls(self, X, y, w=None):
-        if w is not None:
-            raise NotImplementedError("TODO: support nnls with weight")  # TODO
-        if self.fit_intercept:
-            X = np.c_[X, np.ones(len(X))]  # NOTE: mem copy
-        coef = nnls(X, y)[0]
-        if self.fit_intercept:
-            self.coef_ = coef[:-1]
-            self.intercept_ = coef[-1]
-        else:
-            self.coef_ = coef
-            self.intercept_ = 0.0
-
-    def predict(self, dataset: DatasetH, segment: Union[Text, slice] = "test"):
-        if self.coef_ is None:
-            raise ValueError("model is not fitted yet!")
-        x_test = dataset.prepare(segment, col_set="feature", data_key=DataHandlerLP.DK_I)
-        return pd.Series(x_test.values @ self.coef_ + self.intercept_, index=x_test.index)
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+import numpy as np
+import pandas as pd
+from typing import Text, Union
+from qlib.log import get_module_logger
+from qlib.data.dataset.weight import Reweighter
+from scipy.optimize import nnls
+from sklearn.linear_model import LinearRegression, Ridge, Lasso
+
+from ...model.base import Model
+from ...data.dataset import DatasetH
+from ...data.dataset.handler import DataHandlerLP
+
+
+class LinearModel(Model):
+    """Linear Model
+
+    Solve one of the following regression problems:
+        - `ols`: min_w |y - Xw|^2_2
+        - `nnls`: min_w |y - Xw|^2_2, s.t. w >= 0
+        - `ridge`: min_w |y - Xw|^2_2 + \alpha*|w|^2_2
+        - `lasso`: min_w |y - Xw|^2_2 + \alpha*|w|_1
+    where `w` is the regression coefficient.
+    """
+
+    OLS = "ols"
+    NNLS = "nnls"
+    RIDGE = "ridge"
+    LASSO = "lasso"
+
+    def __init__(self, estimator="ols", alpha=0.0, fit_intercept=False, include_valid: bool = False):
+        """
+        Parameters
+        ----------
+        estimator : str
+            which estimator to use for linear regression
+        alpha : float
+            l1 or l2 regularization parameter
+        fit_intercept : bool
+            whether fit intercept
+        include_valid: bool
+            Should the validation data be included for training?
+            The validation data should be included
+        """
+        assert estimator in [self.OLS, self.NNLS, self.RIDGE, self.LASSO], f"unsupported estimator `{estimator}`"
+        self.estimator = estimator
+
+        assert alpha == 0 or estimator in [self.RIDGE, self.LASSO], f"alpha is only supported in `ridge`&`lasso`"
+        self.alpha = alpha
+
+        self.fit_intercept = fit_intercept
+
+        self.coef_ = None
+        self.include_valid = include_valid
+
+    def fit(self, dataset: DatasetH, reweighter: Reweighter = None):
+        df_train = dataset.prepare("train", col_set=["feature", "label"], data_key=DataHandlerLP.DK_L)
+        if self.include_valid:
+            try:
+                df_valid = dataset.prepare("valid", col_set=["feature", "label"], data_key=DataHandlerLP.DK_L)
+                df_train = pd.concat([df_train, df_valid])
+            except KeyError:
+                get_module_logger("LinearModel").info("include_valid=True, but valid does not exist")
+        if df_train.empty:
+            raise ValueError("Empty data from dataset, please check your dataset config.")
+        if reweighter is not None:
+            w: pd.Series = reweighter.reweight(df_train)
+            w = w.values
+        else:
+            w = None
+        X, y = df_train["feature"].values, np.squeeze(df_train["label"].values)
+
+        if self.estimator in [self.OLS, self.RIDGE, self.LASSO]:
+            self._fit(X, y, w)
+        elif self.estimator == self.NNLS:
+            self._fit_nnls(X, y, w)
+        else:
+            raise ValueError(f"unknown estimator `{self.estimator}`")
+
+        return self
+
+    def _fit(self, X, y, w):
+        if self.estimator == self.OLS:
+            model = LinearRegression(fit_intercept=self.fit_intercept, copy_X=False)
+        else:
+            model = {self.RIDGE: Ridge, self.LASSO: Lasso}[self.estimator](
+                alpha=self.alpha, fit_intercept=self.fit_intercept, copy_X=False
+            )
+        model.fit(X, y, sample_weight=w)
+        self.coef_ = model.coef_
+        self.intercept_ = model.intercept_
+
+    def _fit_nnls(self, X, y, w=None):
+        if w is not None:
+            raise NotImplementedError("TODO: support nnls with weight")  # TODO
+        if self.fit_intercept:
+            X = np.c_[X, np.ones(len(X))]  # NOTE: mem copy
+        coef = nnls(X, y)[0]
+        if self.fit_intercept:
+            self.coef_ = coef[:-1]
+            self.intercept_ = coef[-1]
+        else:
+            self.coef_ = coef
+            self.intercept_ = 0.0
+
+    def predict(self, dataset: DatasetH, segment: Union[Text, slice] = "test"):
+        if self.coef_ is None:
+            raise ValueError("model is not fitted yet!")
+        x_test = dataset.prepare(segment, col_set="feature", data_key=DataHandlerLP.DK_I)
+        return pd.Series(x_test.values @ self.coef_ + self.intercept_, index=x_test.index)
```

## qlib/contrib/model/pytorch_adarnn.py

```diff
@@ -1,792 +1,790 @@
-# Copyright (c) Microsoft Corporation.
-import os
-from torch.utils.data import Dataset, DataLoader
-
-import copy
-from typing import Text, Union
-
-import numpy as np
-import pandas as pd
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-import torch.optim as optim
-from torch.autograd import Function
-from qlib.contrib.model.pytorch_utils import count_parameters
-from qlib.data.dataset import DatasetH
-from qlib.data.dataset.handler import DataHandlerLP
-from qlib.log import get_module_logger
-from qlib.model.base import Model
-from qlib.utils import get_or_create_path
-
-
-class ADARNN(Model):
-    """ADARNN Model
-
-    Parameters
-    ----------
-    d_feat : int
-        input dimension for each time step
-    metric: str
-        the evaluation metric used in early stop
-    optimizer : str
-        optimizer name
-    GPU : str
-        the GPU ID(s) used for training
-    """
-
-    def __init__(
-        self,
-        d_feat=6,
-        hidden_size=64,
-        num_layers=2,
-        dropout=0.0,
-        n_epochs=200,
-        pre_epoch=40,
-        dw=0.5,
-        loss_type="cosine",
-        len_seq=60,
-        len_win=0,
-        lr=0.001,
-        metric="mse",
-        batch_size=2000,
-        early_stop=20,
-        loss="mse",
-        optimizer="adam",
-        n_splits=2,
-        GPU=0,
-        seed=None,
-        **_
-    ):
-        # Set logger.
-        self.logger = get_module_logger("ADARNN")
-        self.logger.info("ADARNN pytorch version...")
-        os.environ["CUDA_VISIBLE_DEVICES"] = str(GPU)
-
-        # set hyper-parameters.
-        self.d_feat = d_feat
-        self.hidden_size = hidden_size
-        self.num_layers = num_layers
-        self.dropout = dropout
-        self.n_epochs = n_epochs
-        self.pre_epoch = pre_epoch
-        self.dw = dw
-        self.loss_type = loss_type
-        self.len_seq = len_seq
-        self.len_win = len_win
-        self.lr = lr
-        self.metric = metric
-        self.batch_size = batch_size
-        self.early_stop = early_stop
-        self.optimizer = optimizer.lower()
-        self.loss = loss
-        self.n_splits = n_splits
-        self.device = torch.device("cuda:%d" % GPU if torch.cuda.is_available() and GPU >= 0 else "cpu")
-        self.seed = seed
-
-        self.logger.info(
-            "ADARNN parameters setting:"
-            "\nd_feat : {}"
-            "\nhidden_size : {}"
-            "\nnum_layers : {}"
-            "\ndropout : {}"
-            "\nn_epochs : {}"
-            "\nlr : {}"
-            "\nmetric : {}"
-            "\nbatch_size : {}"
-            "\nearly_stop : {}"
-            "\noptimizer : {}"
-            "\nloss_type : {}"
-            "\nvisible_GPU : {}"
-            "\nuse_GPU : {}"
-            "\nseed : {}".format(
-                d_feat,
-                hidden_size,
-                num_layers,
-                dropout,
-                n_epochs,
-                lr,
-                metric,
-                batch_size,
-                early_stop,
-                optimizer.lower(),
-                loss,
-                GPU,
-                self.use_gpu,
-                seed,
-            )
-        )
-
-        if self.seed is not None:
-            np.random.seed(self.seed)
-            torch.manual_seed(self.seed)
-
-        n_hiddens = [hidden_size for _ in range(num_layers)]
-        self.model = AdaRNN(
-            use_bottleneck=False,
-            bottleneck_width=64,
-            n_input=d_feat,
-            n_hiddens=n_hiddens,
-            n_output=1,
-            dropout=dropout,
-            model_type="AdaRNN",
-            len_seq=len_seq,
-            trans_loss=loss_type,
-        )
-        self.logger.info("model:\n{:}".format(self.model))
-        self.logger.info("model size: {:.4f} MB".format(count_parameters(self.model)))
-
-        if optimizer.lower() == "adam":
-            self.train_optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
-        elif optimizer.lower() == "gd":
-            self.train_optimizer = optim.SGD(self.model.parameters(), lr=self.lr)
-        else:
-            raise NotImplementedError("optimizer {} is not supported!".format(optimizer))
-
-        self.fitted = False
-        self.model.to(self.device)
-
-    @property
-    def use_gpu(self):
-        return self.device != torch.device("cpu")
-
-    def train_AdaRNN(self, train_loader_list, epoch, dist_old=None, weight_mat=None):
-        self.model.train()
-        criterion = nn.MSELoss()
-        dist_mat = torch.zeros(self.num_layers, self.len_seq).to(self.device)
-        len_loader = np.inf
-        for loader in train_loader_list:
-            if len(loader) < len_loader:
-                len_loader = len(loader)
-        for data_all in zip(*train_loader_list):
-            #  for data_all in zip(*train_loader_list):
-            self.train_optimizer.zero_grad()
-            list_feat = []
-            list_label = []
-            for data in data_all:
-                # feature :[36, 24, 6]
-                feature, label_reg = data[0].to(self.device).float(), data[1].to(self.device).float()
-                list_feat.append(feature)
-                list_label.append(label_reg)
-            flag = False
-            index = get_index(len(data_all) - 1)
-            for temp_index in index:
-                s1 = temp_index[0]
-                s2 = temp_index[1]
-                if list_feat[s1].shape[0] != list_feat[s2].shape[0]:
-                    flag = True
-                    break
-            if flag:
-                continue
-
-            total_loss = torch.zeros(1).to(self.device)
-            for i, n in enumerate(index):
-                feature_s = list_feat[n[0]]
-                feature_t = list_feat[n[1]]
-                label_reg_s = list_label[n[0]]
-                label_reg_t = list_label[n[1]]
-                feature_all = torch.cat((feature_s, feature_t), 0)
-
-                if epoch < self.pre_epoch:
-                    pred_all, loss_transfer, out_weight_list = self.model.forward_pre_train(
-                        feature_all, len_win=self.len_win
-                    )
-                else:
-                    pred_all, loss_transfer, dist, weight_mat = self.model.forward_Boosting(feature_all, weight_mat)
-                    dist_mat = dist_mat + dist
-                pred_s = pred_all[0 : feature_s.size(0)]
-                pred_t = pred_all[feature_s.size(0) :]
-
-                loss_s = criterion(pred_s, label_reg_s)
-                loss_t = criterion(pred_t, label_reg_t)
-
-                total_loss = total_loss + loss_s + loss_t + self.dw * loss_transfer
-            self.train_optimizer.zero_grad()
-            total_loss.backward()
-            torch.nn.utils.clip_grad_value_(self.model.parameters(), 3.0)
-            self.train_optimizer.step()
-        if epoch >= self.pre_epoch:
-            if epoch > self.pre_epoch:
-                weight_mat = self.model.update_weight_Boosting(weight_mat, dist_old, dist_mat)
-            return weight_mat, dist_mat
-        else:
-            weight_mat = self.transform_type(out_weight_list)
-            return weight_mat, None
-
-    @staticmethod
-    def calc_all_metrics(pred):
-        """pred is a pandas dataframe that has two attributes: score (pred) and label (real)"""
-        res = {}
-        ic = pred.groupby(level="datetime").apply(lambda x: x.label.corr(x.score))
-        rank_ic = pred.groupby(level="datetime").apply(lambda x: x.label.corr(x.score, method="spearman"))
-        res["ic"] = ic.mean()
-        res["icir"] = ic.mean() / ic.std()
-        res["ric"] = rank_ic.mean()
-        res["ricir"] = rank_ic.mean() / rank_ic.std()
-        res["mse"] = -(pred["label"] - pred["score"]).mean()
-        res["loss"] = res["mse"]
-        return res
-
-    def test_epoch(self, df):
-        self.model.eval()
-        preds = self.infer(df["feature"])
-        label = df["label"].squeeze()
-        preds = pd.DataFrame({"label": label, "score": preds}, index=df.index)
-        metrics = self.calc_all_metrics(preds)
-        return metrics
-
-    def log_metrics(self, mode, metrics):
-        metrics = ["{}/{}: {:.6f}".format(k, mode, v) for k, v in metrics.items()]
-        metrics = ", ".join(metrics)
-        self.logger.info(metrics)
-
-    def fit(
-        self,
-        dataset: DatasetH,
-        evals_result=dict(),
-        save_path=None,
-    ):
-
-        df_train, df_valid = dataset.prepare(
-            ["train", "valid"],
-            col_set=["feature", "label"],
-            data_key=DataHandlerLP.DK_L,
-        )
-        #  splits = ['2011-06-30']
-        days = df_train.index.get_level_values(level=0).unique()
-        train_splits = np.array_split(days, self.n_splits)
-        train_splits = [df_train[s[0] : s[-1]] for s in train_splits]
-        train_loader_list = [get_stock_loader(df, self.batch_size) for df in train_splits]
-
-        save_path = get_or_create_path(save_path)
-        stop_steps = 0
-        evals_result["train"] = []
-        evals_result["valid"] = []
-
-        # train
-        self.logger.info("training...")
-        self.fitted = True
-        best_score = -np.inf
-        best_epoch = 0
-        weight_mat, dist_mat = None, None
-
-        for step in range(self.n_epochs):
-            self.logger.info("Epoch%d:", step)
-            self.logger.info("training...")
-            weight_mat, dist_mat = self.train_AdaRNN(train_loader_list, step, dist_mat, weight_mat)
-            self.logger.info("evaluating...")
-            train_metrics = self.test_epoch(df_train)
-            valid_metrics = self.test_epoch(df_valid)
-            self.log_metrics("train: ", train_metrics)
-            self.log_metrics("valid: ", valid_metrics)
-
-            valid_score = valid_metrics[self.metric]
-            train_score = train_metrics[self.metric]
-            evals_result["train"].append(train_score)
-            evals_result["valid"].append(valid_score)
-            if valid_score > best_score:
-                best_score = valid_score
-                stop_steps = 0
-                best_epoch = step
-                best_param = copy.deepcopy(self.model.state_dict())
-            else:
-                stop_steps += 1
-                if stop_steps >= self.early_stop:
-                    self.logger.info("early stop")
-                    break
-
-        self.logger.info("best score: %.6lf @ %d" % (best_score, best_epoch))
-        self.model.load_state_dict(best_param)
-        torch.save(best_param, save_path)
-
-        if self.use_gpu:
-            torch.cuda.empty_cache()
-        return best_score
-
-    def predict(self, dataset: DatasetH, segment: Union[Text, slice] = "test"):
-        if not self.fitted:
-            raise ValueError("model is not fitted yet!")
-        x_test = dataset.prepare(segment, col_set="feature", data_key=DataHandlerLP.DK_I)
-        return self.infer(x_test)
-
-    def infer(self, x_test):
-        index = x_test.index
-        self.model.eval()
-        x_values = x_test.values
-        sample_num = x_values.shape[0]
-        x_values = x_values.reshape(sample_num, self.d_feat, -1).transpose(0, 2, 1)
-        preds = []
-
-        for begin in range(sample_num)[:: self.batch_size]:
-
-            if sample_num - begin < self.batch_size:
-                end = sample_num
-            else:
-                end = begin + self.batch_size
-
-            x_batch = torch.from_numpy(x_values[begin:end]).float().to(self.device)
-
-            with torch.no_grad():
-                pred = self.model.predict(x_batch).detach().cpu().numpy()
-
-            preds.append(pred)
-
-        return pd.Series(np.concatenate(preds), index=index)
-
-    def transform_type(self, init_weight):
-        weight = torch.ones(self.num_layers, self.len_seq).to(self.device)
-        for i in range(self.num_layers):
-            for j in range(self.len_seq):
-                weight[i, j] = init_weight[i][j].item()
-        return weight
-
-
-class data_loader(Dataset):
-    def __init__(self, df):
-        self.df_feature = df["feature"]
-        self.df_label_reg = df["label"]
-        self.df_index = df.index
-        self.df_feature = torch.tensor(
-            self.df_feature.values.reshape(-1, 6, 60).transpose(0, 2, 1), dtype=torch.float32
-        )
-        self.df_label_reg = torch.tensor(self.df_label_reg.values.reshape(-1), dtype=torch.float32)
-
-    def __getitem__(self, index):
-        sample, label_reg = self.df_feature[index], self.df_label_reg[index]
-        return sample, label_reg
-
-    def __len__(self):
-        return len(self.df_feature)
-
-
-def get_stock_loader(df, batch_size, shuffle=True):
-    train_loader = DataLoader(data_loader(df), batch_size=batch_size, shuffle=shuffle)
-    return train_loader
-
-
-def get_index(num_domain=2):
-    index = []
-    for i in range(num_domain):
-        for j in range(i + 1, num_domain + 1):
-            index.append((i, j))
-    return index
-
-
-class AdaRNN(nn.Module):
-    """
-    model_type:  'Boosting', 'AdaRNN'
-    """
-
-    def __init__(
-        self,
-        use_bottleneck=False,
-        bottleneck_width=256,
-        n_input=128,
-        n_hiddens=[64, 64],
-        n_output=6,
-        dropout=0.0,
-        len_seq=9,
-        model_type="AdaRNN",
-        trans_loss="mmd",
-        GPU=0,
-    ):
-        super(AdaRNN, self).__init__()
-        self.use_bottleneck = use_bottleneck
-        self.n_input = n_input
-        self.num_layers = len(n_hiddens)
-        self.hiddens = n_hiddens
-        self.n_output = n_output
-        self.model_type = model_type
-        self.trans_loss = trans_loss
-        self.len_seq = len_seq
-        self.device = torch.device("cuda:%d" % GPU if torch.cuda.is_available() and GPU >= 0 else "cpu")
-        in_size = self.n_input
-
-        features = nn.ModuleList()
-        for hidden in n_hiddens:
-            rnn = nn.GRU(input_size=in_size, num_layers=1, hidden_size=hidden, batch_first=True, dropout=dropout)
-            features.append(rnn)
-            in_size = hidden
-        self.features = nn.Sequential(*features)
-
-        if use_bottleneck is True:  # finance
-            self.bottleneck = nn.Sequential(
-                nn.Linear(n_hiddens[-1], bottleneck_width),
-                nn.Linear(bottleneck_width, bottleneck_width),
-                nn.BatchNorm1d(bottleneck_width),
-                nn.ReLU(),
-                nn.Dropout(),
-            )
-            self.bottleneck[0].weight.data.normal_(0, 0.005)
-            self.bottleneck[0].bias.data.fill_(0.1)
-            self.bottleneck[1].weight.data.normal_(0, 0.005)
-            self.bottleneck[1].bias.data.fill_(0.1)
-            self.fc = nn.Linear(bottleneck_width, n_output)
-            torch.nn.init.xavier_normal_(self.fc.weight)
-        else:
-            self.fc_out = nn.Linear(n_hiddens[-1], self.n_output)
-
-        if self.model_type == "AdaRNN":
-            gate = nn.ModuleList()
-            for i in range(len(n_hiddens)):
-                gate_weight = nn.Linear(len_seq * self.hiddens[i] * 2, len_seq)
-                gate.append(gate_weight)
-            self.gate = gate
-
-            bnlst = nn.ModuleList()
-            for i in range(len(n_hiddens)):
-                bnlst.append(nn.BatchNorm1d(len_seq))
-            self.bn_lst = bnlst
-            self.softmax = torch.nn.Softmax(dim=0)
-            self.init_layers()
-
-    def init_layers(self):
-        for i in range(len(self.hiddens)):
-            self.gate[i].weight.data.normal_(0, 0.05)
-            self.gate[i].bias.data.fill_(0.0)
-
-    def forward_pre_train(self, x, len_win=0):
-        out = self.gru_features(x)
-        fea = out[0]  # [2N,L,H]
-        if self.use_bottleneck is True:
-            fea_bottleneck = self.bottleneck(fea[:, -1, :])
-            fc_out = self.fc(fea_bottleneck).squeeze()
-        else:
-            fc_out = self.fc_out(fea[:, -1, :]).squeeze()  # [N,]
-
-        out_list_all, out_weight_list = out[1], out[2]
-        out_list_s, out_list_t = self.get_features(out_list_all)
-        loss_transfer = torch.zeros((1,)).to(self.device)
-        for i, n in enumerate(out_list_s):
-            criterion_transder = TransferLoss(loss_type=self.trans_loss, input_dim=n.shape[2])
-            h_start = 0
-            for j in range(h_start, self.len_seq, 1):
-                i_start = j - len_win if j - len_win >= 0 else 0
-                i_end = j + len_win if j + len_win < self.len_seq else self.len_seq - 1
-                for k in range(i_start, i_end + 1):
-                    weight = (
-                        out_weight_list[i][j]
-                        if self.model_type == "AdaRNN"
-                        else 1 / (self.len_seq - h_start) * (2 * len_win + 1)
-                    )
-                    loss_transfer = loss_transfer + weight * criterion_transder.compute(
-                        n[:, j, :], out_list_t[i][:, k, :]
-                    )
-        return fc_out, loss_transfer, out_weight_list
-
-    def gru_features(self, x, predict=False):
-        x_input = x
-        out = None
-        out_lis = []
-        out_weight_list = [] if (self.model_type == "AdaRNN") else None
-        for i in range(self.num_layers):
-            out, _ = self.features[i](x_input.float())
-            x_input = out
-            out_lis.append(out)
-            if self.model_type == "AdaRNN" and predict is False:
-                out_gate = self.process_gate_weight(x_input, i)
-                out_weight_list.append(out_gate)
-        return out, out_lis, out_weight_list
-
-    def process_gate_weight(self, out, index):
-        x_s = out[0 : int(out.shape[0] // 2)]
-        x_t = out[out.shape[0] // 2 : out.shape[0]]
-        x_all = torch.cat((x_s, x_t), 2)
-        x_all = x_all.view(x_all.shape[0], -1)
-        weight = torch.sigmoid(self.bn_lst[index](self.gate[index](x_all.float())))
-        weight = torch.mean(weight, dim=0)
-        res = self.softmax(weight).squeeze()
-        return res
-
-    @staticmethod
-    def get_features(output_list):
-        fea_list_src, fea_list_tar = [], []
-        for fea in output_list:
-            fea_list_src.append(fea[0 : fea.size(0) // 2])
-            fea_list_tar.append(fea[fea.size(0) // 2 :])
-        return fea_list_src, fea_list_tar
-
-    # For Boosting-based
-    def forward_Boosting(self, x, weight_mat=None):
-        out = self.gru_features(x)
-        fea = out[0]
-        if self.use_bottleneck:
-            fea_bottleneck = self.bottleneck(fea[:, -1, :])
-            fc_out = self.fc(fea_bottleneck).squeeze()
-        else:
-            fc_out = self.fc_out(fea[:, -1, :]).squeeze()
-
-        out_list_all = out[1]
-        out_list_s, out_list_t = self.get_features(out_list_all)
-        loss_transfer = torch.zeros((1,)).to(self.device)
-        if weight_mat is None:
-            weight = (1.0 / self.len_seq * torch.ones(self.num_layers, self.len_seq)).to(self.device)
-        else:
-            weight = weight_mat
-        dist_mat = torch.zeros(self.num_layers, self.len_seq).to(self.device)
-        for i, n in enumerate(out_list_s):
-            criterion_transder = TransferLoss(loss_type=self.trans_loss, input_dim=n.shape[2])
-            for j in range(self.len_seq):
-                loss_trans = criterion_transder.compute(n[:, j, :], out_list_t[i][:, j, :])
-                loss_transfer = loss_transfer + weight[i, j] * loss_trans
-                dist_mat[i, j] = loss_trans
-        return fc_out, loss_transfer, dist_mat, weight
-
-    # For Boosting-based
-    def update_weight_Boosting(self, weight_mat, dist_old, dist_new):
-        epsilon = 1e-5
-        dist_old = dist_old.detach()
-        dist_new = dist_new.detach()
-        ind = dist_new > dist_old + epsilon
-        weight_mat[ind] = weight_mat[ind] * (1 + torch.sigmoid(dist_new[ind] - dist_old[ind]))
-        weight_norm = torch.norm(weight_mat, dim=1, p=1)
-        weight_mat = weight_mat / weight_norm.t().unsqueeze(1).repeat(1, self.len_seq)
-        return weight_mat
-
-    def predict(self, x):
-        out = self.gru_features(x, predict=True)
-        fea = out[0]
-        if self.use_bottleneck is True:
-            fea_bottleneck = self.bottleneck(fea[:, -1, :])
-            fc_out = self.fc(fea_bottleneck).squeeze()
-        else:
-            fc_out = self.fc_out(fea[:, -1, :]).squeeze()
-        return fc_out
-
-
-class TransferLoss:
-    def __init__(self, loss_type="cosine", input_dim=512, GPU=0):
-        """
-        Supported loss_type: mmd(mmd_lin), mmd_rbf, coral, cosine, kl, js, mine, adv
-        """
-        self.loss_type = loss_type
-        self.input_dim = input_dim
-        self.device = torch.device("cuda:%d" % GPU if torch.cuda.is_available() and GPU >= 0 else "cpu")
-
-    def compute(self, X, Y):
-        """Compute adaptation loss
-
-        Arguments:
-            X {tensor} -- source matrix
-            Y {tensor} -- target matrix
-
-        Returns:
-            [tensor] -- transfer loss
-        """
-        if self.loss_type in ("mmd_lin", "mmd"):
-            mmdloss = MMD_loss(kernel_type="linear")
-            loss = mmdloss(X, Y)
-        elif self.loss_type == "coral":
-            loss = CORAL(X, Y, self.device)
-        elif self.loss_type in ("cosine", "cos"):
-            loss = 1 - cosine(X, Y)
-        elif self.loss_type == "kl":
-            loss = kl_div(X, Y)
-        elif self.loss_type == "js":
-            loss = js(X, Y)
-        elif self.loss_type == "mine":
-            mine_model = Mine_estimator(input_dim=self.input_dim, hidden_dim=60).to(self.device)
-            loss = mine_model(X, Y)
-        elif self.loss_type == "adv":
-            loss = adv(X, Y, self.device, input_dim=self.input_dim, hidden_dim=32)
-        elif self.loss_type == "mmd_rbf":
-            mmdloss = MMD_loss(kernel_type="rbf")
-            loss = mmdloss(X, Y)
-        elif self.loss_type == "pairwise":
-            pair_mat = pairwise_dist(X, Y)
-            loss = torch.norm(pair_mat)
-
-        return loss
-
-
-def cosine(source, target):
-    source, target = source.mean(), target.mean()
-    cos = nn.CosineSimilarity(dim=0)
-    loss = cos(source, target)
-    return loss.mean()
-
-
-class ReverseLayerF(Function):
-    @staticmethod
-    def forward(ctx, x, alpha):
-        ctx.alpha = alpha
-        return x.view_as(x)
-
-    @staticmethod
-    def backward(ctx, grad_output):
-        output = grad_output.neg() * ctx.alpha
-        return output, None
-
-
-class Discriminator(nn.Module):
-    def __init__(self, input_dim=256, hidden_dim=256):
-        super(Discriminator, self).__init__()
-        self.input_dim = input_dim
-        self.hidden_dim = hidden_dim
-        self.dis1 = nn.Linear(input_dim, hidden_dim)
-        self.dis2 = nn.Linear(hidden_dim, 1)
-
-    def forward(self, x):
-        x = F.relu(self.dis1(x))
-        x = self.dis2(x)
-        x = torch.sigmoid(x)
-        return x
-
-
-def adv(source, target, device, input_dim=256, hidden_dim=512):
-    domain_loss = nn.BCELoss()
-    # !!! Pay attention to .cuda !!!
-    adv_net = Discriminator(input_dim, hidden_dim).to(device)
-    domain_src = torch.ones(len(source)).to(device)
-    domain_tar = torch.zeros(len(target)).to(device)
-    domain_src, domain_tar = domain_src.view(domain_src.shape[0], 1), domain_tar.view(domain_tar.shape[0], 1)
-    reverse_src = ReverseLayerF.apply(source, 1)
-    reverse_tar = ReverseLayerF.apply(target, 1)
-    pred_src = adv_net(reverse_src)
-    pred_tar = adv_net(reverse_tar)
-    loss_s, loss_t = domain_loss(pred_src, domain_src), domain_loss(pred_tar, domain_tar)
-    loss = loss_s + loss_t
-    return loss
-
-
-def CORAL(source, target, device):
-    d = source.size(1)
-    ns, nt = source.size(0), target.size(0)
-
-    # source covariance
-    tmp_s = torch.ones((1, ns)).to(device) @ source
-    cs = (source.t() @ source - (tmp_s.t() @ tmp_s) / ns) / (ns - 1)
-
-    # target covariance
-    tmp_t = torch.ones((1, nt)).to(device) @ target
-    ct = (target.t() @ target - (tmp_t.t() @ tmp_t) / nt) / (nt - 1)
-
-    # frobenius norm
-    loss = (cs - ct).pow(2).sum()
-    loss = loss / (4 * d * d)
-
-    return loss
-
-
-class MMD_loss(nn.Module):
-    def __init__(self, kernel_type="linear", kernel_mul=2.0, kernel_num=5):
-        super(MMD_loss, self).__init__()
-        self.kernel_num = kernel_num
-        self.kernel_mul = kernel_mul
-        self.fix_sigma = None
-        self.kernel_type = kernel_type
-
-    @staticmethod
-    def guassian_kernel(source, target, kernel_mul=2.0, kernel_num=5, fix_sigma=None):
-        n_samples = int(source.size()[0]) + int(target.size()[0])
-        total = torch.cat([source, target], dim=0)
-        total0 = total.unsqueeze(0).expand(int(total.size(0)), int(total.size(0)), int(total.size(1)))
-        total1 = total.unsqueeze(1).expand(int(total.size(0)), int(total.size(0)), int(total.size(1)))
-        L2_distance = ((total0 - total1) ** 2).sum(2)
-        if fix_sigma:
-            bandwidth = fix_sigma
-        else:
-            bandwidth = torch.sum(L2_distance.data) / (n_samples**2 - n_samples)
-        bandwidth /= kernel_mul ** (kernel_num // 2)
-        bandwidth_list = [bandwidth * (kernel_mul**i) for i in range(kernel_num)]
-        kernel_val = [torch.exp(-L2_distance / bandwidth_temp) for bandwidth_temp in bandwidth_list]
-        return sum(kernel_val)
-
-    @staticmethod
-    def linear_mmd(X, Y):
-        delta = X.mean(axis=0) - Y.mean(axis=0)
-        loss = delta.dot(delta.T)
-        return loss
-
-    def forward(self, source, target):
-        if self.kernel_type == "linear":
-            return self.linear_mmd(source, target)
-        elif self.kernel_type == "rbf":
-            batch_size = int(source.size()[0])
-            kernels = self.guassian_kernel(
-                source, target, kernel_mul=self.kernel_mul, kernel_num=self.kernel_num, fix_sigma=self.fix_sigma
-            )
-            with torch.no_grad():
-                XX = torch.mean(kernels[:batch_size, :batch_size])
-                YY = torch.mean(kernels[batch_size:, batch_size:])
-                XY = torch.mean(kernels[:batch_size, batch_size:])
-                YX = torch.mean(kernels[batch_size:, :batch_size])
-                loss = torch.mean(XX + YY - XY - YX)
-            return loss
-
-
-class Mine_estimator(nn.Module):
-    def __init__(self, input_dim=2048, hidden_dim=512):
-        super(Mine_estimator, self).__init__()
-        self.mine_model = Mine(input_dim, hidden_dim)
-
-    def forward(self, X, Y):
-        Y_shffle = Y[torch.randperm(len(Y))]
-        loss_joint = self.mine_model(X, Y)
-        loss_marginal = self.mine_model(X, Y_shffle)
-        ret = torch.mean(loss_joint) - torch.log(torch.mean(torch.exp(loss_marginal)))
-        loss = -ret
-        return loss
-
-
-class Mine(nn.Module):
-    def __init__(self, input_dim=2048, hidden_dim=512):
-        super(Mine, self).__init__()
-        self.fc1_x = nn.Linear(input_dim, hidden_dim)
-        self.fc1_y = nn.Linear(input_dim, hidden_dim)
-        self.fc2 = nn.Linear(hidden_dim, 1)
-
-    def forward(self, x, y):
-        h1 = F.leaky_relu(self.fc1_x(x) + self.fc1_y(y))
-        h2 = self.fc2(h1)
-        return h2
-
-
-def pairwise_dist(X, Y):
-    n, d = X.shape
-    m, _ = Y.shape
-    assert d == Y.shape[1]
-    a = X.unsqueeze(1).expand(n, m, d)
-    b = Y.unsqueeze(0).expand(n, m, d)
-    return torch.pow(a - b, 2).sum(2)
-
-
-def pairwise_dist_np(X, Y):
-    n, d = X.shape
-    m, _ = Y.shape
-    assert d == Y.shape[1]
-    a = np.expand_dims(X, 1)
-    b = np.expand_dims(Y, 0)
-    a = np.tile(a, (1, m, 1))
-    b = np.tile(b, (n, 1, 1))
-    return np.power(a - b, 2).sum(2)
-
-
-def pa(X, Y):
-    XY = np.dot(X, Y.T)
-    XX = np.sum(np.square(X), axis=1)
-    XX = np.transpose([XX])
-    YY = np.sum(np.square(Y), axis=1)
-    dist = XX + YY - 2 * XY
-
-    return dist
-
-
-def kl_div(source, target):
-    if len(source) < len(target):
-        target = target[: len(source)]
-    elif len(source) > len(target):
-        source = source[: len(target)]
-    criterion = nn.KLDivLoss(reduction="batchmean")
-    loss = criterion(source.log(), target)
-    return loss
-
-
-def js(source, target):
-    if len(source) < len(target):
-        target = target[: len(source)]
-    elif len(source) > len(target):
-        source = source[: len(target)]
-    M = 0.5 * (source + target)
-    loss_1, loss_2 = kl_div(source, M), kl_div(target, M)
-    return 0.5 * (loss_1 + loss_2)
+# Copyright (c) Microsoft Corporation.
+import os
+from torch.utils.data import Dataset, DataLoader
+
+import copy
+from typing import Text, Union
+
+import numpy as np
+import pandas as pd
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
+import torch.optim as optim
+from torch.autograd import Function
+from qlib.contrib.model.pytorch_utils import count_parameters
+from qlib.data.dataset import DatasetH
+from qlib.data.dataset.handler import DataHandlerLP
+from qlib.log import get_module_logger
+from qlib.model.base import Model
+from qlib.utils import get_or_create_path
+
+
+class ADARNN(Model):
+    """ADARNN Model
+
+    Parameters
+    ----------
+    d_feat : int
+        input dimension for each time step
+    metric: str
+        the evaluation metric used in early stop
+    optimizer : str
+        optimizer name
+    GPU : str
+        the GPU ID(s) used for training
+    """
+
+    def __init__(
+        self,
+        d_feat=6,
+        hidden_size=64,
+        num_layers=2,
+        dropout=0.0,
+        n_epochs=200,
+        pre_epoch=40,
+        dw=0.5,
+        loss_type="cosine",
+        len_seq=60,
+        len_win=0,
+        lr=0.001,
+        metric="mse",
+        batch_size=2000,
+        early_stop=20,
+        loss="mse",
+        optimizer="adam",
+        n_splits=2,
+        GPU=0,
+        seed=None,
+        **_
+    ):
+        # Set logger.
+        self.logger = get_module_logger("ADARNN")
+        self.logger.info("ADARNN pytorch version...")
+        os.environ["CUDA_VISIBLE_DEVICES"] = str(GPU)
+
+        # set hyper-parameters.
+        self.d_feat = d_feat
+        self.hidden_size = hidden_size
+        self.num_layers = num_layers
+        self.dropout = dropout
+        self.n_epochs = n_epochs
+        self.pre_epoch = pre_epoch
+        self.dw = dw
+        self.loss_type = loss_type
+        self.len_seq = len_seq
+        self.len_win = len_win
+        self.lr = lr
+        self.metric = metric
+        self.batch_size = batch_size
+        self.early_stop = early_stop
+        self.optimizer = optimizer.lower()
+        self.loss = loss
+        self.n_splits = n_splits
+        self.device = torch.device("cuda:%d" % GPU if torch.cuda.is_available() and GPU >= 0 else "cpu")
+        self.seed = seed
+
+        self.logger.info(
+            "ADARNN parameters setting:"
+            "\nd_feat : {}"
+            "\nhidden_size : {}"
+            "\nnum_layers : {}"
+            "\ndropout : {}"
+            "\nn_epochs : {}"
+            "\nlr : {}"
+            "\nmetric : {}"
+            "\nbatch_size : {}"
+            "\nearly_stop : {}"
+            "\noptimizer : {}"
+            "\nloss_type : {}"
+            "\nvisible_GPU : {}"
+            "\nuse_GPU : {}"
+            "\nseed : {}".format(
+                d_feat,
+                hidden_size,
+                num_layers,
+                dropout,
+                n_epochs,
+                lr,
+                metric,
+                batch_size,
+                early_stop,
+                optimizer.lower(),
+                loss,
+                GPU,
+                self.use_gpu,
+                seed,
+            )
+        )
+
+        if self.seed is not None:
+            np.random.seed(self.seed)
+            torch.manual_seed(self.seed)
+
+        n_hiddens = [hidden_size for _ in range(num_layers)]
+        self.model = AdaRNN(
+            use_bottleneck=False,
+            bottleneck_width=64,
+            n_input=d_feat,
+            n_hiddens=n_hiddens,
+            n_output=1,
+            dropout=dropout,
+            model_type="AdaRNN",
+            len_seq=len_seq,
+            trans_loss=loss_type,
+        )
+        self.logger.info("model:\n{:}".format(self.model))
+        self.logger.info("model size: {:.4f} MB".format(count_parameters(self.model)))
+
+        if optimizer.lower() == "adam":
+            self.train_optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
+        elif optimizer.lower() == "gd":
+            self.train_optimizer = optim.SGD(self.model.parameters(), lr=self.lr)
+        else:
+            raise NotImplementedError("optimizer {} is not supported!".format(optimizer))
+
+        self.fitted = False
+        self.model.to(self.device)
+
+    @property
+    def use_gpu(self):
+        return self.device != torch.device("cpu")
+
+    def train_AdaRNN(self, train_loader_list, epoch, dist_old=None, weight_mat=None):
+        self.model.train()
+        criterion = nn.MSELoss()
+        dist_mat = torch.zeros(self.num_layers, self.len_seq).to(self.device)
+        len_loader = np.inf
+        for loader in train_loader_list:
+            if len(loader) < len_loader:
+                len_loader = len(loader)
+        for data_all in zip(*train_loader_list):
+            #  for data_all in zip(*train_loader_list):
+            self.train_optimizer.zero_grad()
+            list_feat = []
+            list_label = []
+            for data in data_all:
+                # feature :[36, 24, 6]
+                feature, label_reg = data[0].to(self.device).float(), data[1].to(self.device).float()
+                list_feat.append(feature)
+                list_label.append(label_reg)
+            flag = False
+            index = get_index(len(data_all) - 1)
+            for temp_index in index:
+                s1 = temp_index[0]
+                s2 = temp_index[1]
+                if list_feat[s1].shape[0] != list_feat[s2].shape[0]:
+                    flag = True
+                    break
+            if flag:
+                continue
+
+            total_loss = torch.zeros(1).to(self.device)
+            for i, n in enumerate(index):
+                feature_s = list_feat[n[0]]
+                feature_t = list_feat[n[1]]
+                label_reg_s = list_label[n[0]]
+                label_reg_t = list_label[n[1]]
+                feature_all = torch.cat((feature_s, feature_t), 0)
+
+                if epoch < self.pre_epoch:
+                    pred_all, loss_transfer, out_weight_list = self.model.forward_pre_train(
+                        feature_all, len_win=self.len_win
+                    )
+                else:
+                    pred_all, loss_transfer, dist, weight_mat = self.model.forward_Boosting(feature_all, weight_mat)
+                    dist_mat = dist_mat + dist
+                pred_s = pred_all[0 : feature_s.size(0)]
+                pred_t = pred_all[feature_s.size(0) :]
+
+                loss_s = criterion(pred_s, label_reg_s)
+                loss_t = criterion(pred_t, label_reg_t)
+
+                total_loss = total_loss + loss_s + loss_t + self.dw * loss_transfer
+            self.train_optimizer.zero_grad()
+            total_loss.backward()
+            torch.nn.utils.clip_grad_value_(self.model.parameters(), 3.0)
+            self.train_optimizer.step()
+        if epoch >= self.pre_epoch:
+            if epoch > self.pre_epoch:
+                weight_mat = self.model.update_weight_Boosting(weight_mat, dist_old, dist_mat)
+            return weight_mat, dist_mat
+        else:
+            weight_mat = self.transform_type(out_weight_list)
+            return weight_mat, None
+
+    @staticmethod
+    def calc_all_metrics(pred):
+        """pred is a pandas dataframe that has two attributes: score (pred) and label (real)"""
+        res = {}
+        ic = pred.groupby(level="datetime").apply(lambda x: x.label.corr(x.score))
+        rank_ic = pred.groupby(level="datetime").apply(lambda x: x.label.corr(x.score, method="spearman"))
+        res["ic"] = ic.mean()
+        res["icir"] = ic.mean() / ic.std()
+        res["ric"] = rank_ic.mean()
+        res["ricir"] = rank_ic.mean() / rank_ic.std()
+        res["mse"] = -(pred["label"] - pred["score"]).mean()
+        res["loss"] = res["mse"]
+        return res
+
+    def test_epoch(self, df):
+        self.model.eval()
+        preds = self.infer(df["feature"])
+        label = df["label"].squeeze()
+        preds = pd.DataFrame({"label": label, "score": preds}, index=df.index)
+        metrics = self.calc_all_metrics(preds)
+        return metrics
+
+    def log_metrics(self, mode, metrics):
+        metrics = ["{}/{}: {:.6f}".format(k, mode, v) for k, v in metrics.items()]
+        metrics = ", ".join(metrics)
+        self.logger.info(metrics)
+
+    def fit(
+        self,
+        dataset: DatasetH,
+        evals_result=dict(),
+        save_path=None,
+    ):
+        df_train, df_valid = dataset.prepare(
+            ["train", "valid"],
+            col_set=["feature", "label"],
+            data_key=DataHandlerLP.DK_L,
+        )
+        #  splits = ['2011-06-30']
+        days = df_train.index.get_level_values(level=0).unique()
+        train_splits = np.array_split(days, self.n_splits)
+        train_splits = [df_train[s[0] : s[-1]] for s in train_splits]
+        train_loader_list = [get_stock_loader(df, self.batch_size) for df in train_splits]
+
+        save_path = get_or_create_path(save_path)
+        stop_steps = 0
+        evals_result["train"] = []
+        evals_result["valid"] = []
+
+        # train
+        self.logger.info("training...")
+        self.fitted = True
+        best_score = -np.inf
+        best_epoch = 0
+        weight_mat, dist_mat = None, None
+
+        for step in range(self.n_epochs):
+            self.logger.info("Epoch%d:", step)
+            self.logger.info("training...")
+            weight_mat, dist_mat = self.train_AdaRNN(train_loader_list, step, dist_mat, weight_mat)
+            self.logger.info("evaluating...")
+            train_metrics = self.test_epoch(df_train)
+            valid_metrics = self.test_epoch(df_valid)
+            self.log_metrics("train: ", train_metrics)
+            self.log_metrics("valid: ", valid_metrics)
+
+            valid_score = valid_metrics[self.metric]
+            train_score = train_metrics[self.metric]
+            evals_result["train"].append(train_score)
+            evals_result["valid"].append(valid_score)
+            if valid_score > best_score:
+                best_score = valid_score
+                stop_steps = 0
+                best_epoch = step
+                best_param = copy.deepcopy(self.model.state_dict())
+            else:
+                stop_steps += 1
+                if stop_steps >= self.early_stop:
+                    self.logger.info("early stop")
+                    break
+
+        self.logger.info("best score: %.6lf @ %d" % (best_score, best_epoch))
+        self.model.load_state_dict(best_param)
+        torch.save(best_param, save_path)
+
+        if self.use_gpu:
+            torch.cuda.empty_cache()
+        return best_score
+
+    def predict(self, dataset: DatasetH, segment: Union[Text, slice] = "test"):
+        if not self.fitted:
+            raise ValueError("model is not fitted yet!")
+        x_test = dataset.prepare(segment, col_set="feature", data_key=DataHandlerLP.DK_I)
+        return self.infer(x_test)
+
+    def infer(self, x_test):
+        index = x_test.index
+        self.model.eval()
+        x_values = x_test.values
+        sample_num = x_values.shape[0]
+        x_values = x_values.reshape(sample_num, self.d_feat, -1).transpose(0, 2, 1)
+        preds = []
+
+        for begin in range(sample_num)[:: self.batch_size]:
+            if sample_num - begin < self.batch_size:
+                end = sample_num
+            else:
+                end = begin + self.batch_size
+
+            x_batch = torch.from_numpy(x_values[begin:end]).float().to(self.device)
+
+            with torch.no_grad():
+                pred = self.model.predict(x_batch).detach().cpu().numpy()
+
+            preds.append(pred)
+
+        return pd.Series(np.concatenate(preds), index=index)
+
+    def transform_type(self, init_weight):
+        weight = torch.ones(self.num_layers, self.len_seq).to(self.device)
+        for i in range(self.num_layers):
+            for j in range(self.len_seq):
+                weight[i, j] = init_weight[i][j].item()
+        return weight
+
+
+class data_loader(Dataset):
+    def __init__(self, df):
+        self.df_feature = df["feature"]
+        self.df_label_reg = df["label"]
+        self.df_index = df.index
+        self.df_feature = torch.tensor(
+            self.df_feature.values.reshape(-1, 6, 60).transpose(0, 2, 1), dtype=torch.float32
+        )
+        self.df_label_reg = torch.tensor(self.df_label_reg.values.reshape(-1), dtype=torch.float32)
+
+    def __getitem__(self, index):
+        sample, label_reg = self.df_feature[index], self.df_label_reg[index]
+        return sample, label_reg
+
+    def __len__(self):
+        return len(self.df_feature)
+
+
+def get_stock_loader(df, batch_size, shuffle=True):
+    train_loader = DataLoader(data_loader(df), batch_size=batch_size, shuffle=shuffle)
+    return train_loader
+
+
+def get_index(num_domain=2):
+    index = []
+    for i in range(num_domain):
+        for j in range(i + 1, num_domain + 1):
+            index.append((i, j))
+    return index
+
+
+class AdaRNN(nn.Module):
+    """
+    model_type:  'Boosting', 'AdaRNN'
+    """
+
+    def __init__(
+        self,
+        use_bottleneck=False,
+        bottleneck_width=256,
+        n_input=128,
+        n_hiddens=[64, 64],
+        n_output=6,
+        dropout=0.0,
+        len_seq=9,
+        model_type="AdaRNN",
+        trans_loss="mmd",
+        GPU=0,
+    ):
+        super(AdaRNN, self).__init__()
+        self.use_bottleneck = use_bottleneck
+        self.n_input = n_input
+        self.num_layers = len(n_hiddens)
+        self.hiddens = n_hiddens
+        self.n_output = n_output
+        self.model_type = model_type
+        self.trans_loss = trans_loss
+        self.len_seq = len_seq
+        self.device = torch.device("cuda:%d" % GPU if torch.cuda.is_available() and GPU >= 0 else "cpu")
+        in_size = self.n_input
+
+        features = nn.ModuleList()
+        for hidden in n_hiddens:
+            rnn = nn.GRU(input_size=in_size, num_layers=1, hidden_size=hidden, batch_first=True, dropout=dropout)
+            features.append(rnn)
+            in_size = hidden
+        self.features = nn.Sequential(*features)
+
+        if use_bottleneck is True:  # finance
+            self.bottleneck = nn.Sequential(
+                nn.Linear(n_hiddens[-1], bottleneck_width),
+                nn.Linear(bottleneck_width, bottleneck_width),
+                nn.BatchNorm1d(bottleneck_width),
+                nn.ReLU(),
+                nn.Dropout(),
+            )
+            self.bottleneck[0].weight.data.normal_(0, 0.005)
+            self.bottleneck[0].bias.data.fill_(0.1)
+            self.bottleneck[1].weight.data.normal_(0, 0.005)
+            self.bottleneck[1].bias.data.fill_(0.1)
+            self.fc = nn.Linear(bottleneck_width, n_output)
+            torch.nn.init.xavier_normal_(self.fc.weight)
+        else:
+            self.fc_out = nn.Linear(n_hiddens[-1], self.n_output)
+
+        if self.model_type == "AdaRNN":
+            gate = nn.ModuleList()
+            for i in range(len(n_hiddens)):
+                gate_weight = nn.Linear(len_seq * self.hiddens[i] * 2, len_seq)
+                gate.append(gate_weight)
+            self.gate = gate
+
+            bnlst = nn.ModuleList()
+            for i in range(len(n_hiddens)):
+                bnlst.append(nn.BatchNorm1d(len_seq))
+            self.bn_lst = bnlst
+            self.softmax = torch.nn.Softmax(dim=0)
+            self.init_layers()
+
+    def init_layers(self):
+        for i in range(len(self.hiddens)):
+            self.gate[i].weight.data.normal_(0, 0.05)
+            self.gate[i].bias.data.fill_(0.0)
+
+    def forward_pre_train(self, x, len_win=0):
+        out = self.gru_features(x)
+        fea = out[0]  # [2N,L,H]
+        if self.use_bottleneck is True:
+            fea_bottleneck = self.bottleneck(fea[:, -1, :])
+            fc_out = self.fc(fea_bottleneck).squeeze()
+        else:
+            fc_out = self.fc_out(fea[:, -1, :]).squeeze()  # [N,]
+
+        out_list_all, out_weight_list = out[1], out[2]
+        out_list_s, out_list_t = self.get_features(out_list_all)
+        loss_transfer = torch.zeros((1,)).to(self.device)
+        for i, n in enumerate(out_list_s):
+            criterion_transder = TransferLoss(loss_type=self.trans_loss, input_dim=n.shape[2])
+            h_start = 0
+            for j in range(h_start, self.len_seq, 1):
+                i_start = j - len_win if j - len_win >= 0 else 0
+                i_end = j + len_win if j + len_win < self.len_seq else self.len_seq - 1
+                for k in range(i_start, i_end + 1):
+                    weight = (
+                        out_weight_list[i][j]
+                        if self.model_type == "AdaRNN"
+                        else 1 / (self.len_seq - h_start) * (2 * len_win + 1)
+                    )
+                    loss_transfer = loss_transfer + weight * criterion_transder.compute(
+                        n[:, j, :], out_list_t[i][:, k, :]
+                    )
+        return fc_out, loss_transfer, out_weight_list
+
+    def gru_features(self, x, predict=False):
+        x_input = x
+        out = None
+        out_lis = []
+        out_weight_list = [] if (self.model_type == "AdaRNN") else None
+        for i in range(self.num_layers):
+            out, _ = self.features[i](x_input.float())
+            x_input = out
+            out_lis.append(out)
+            if self.model_type == "AdaRNN" and predict is False:
+                out_gate = self.process_gate_weight(x_input, i)
+                out_weight_list.append(out_gate)
+        return out, out_lis, out_weight_list
+
+    def process_gate_weight(self, out, index):
+        x_s = out[0 : int(out.shape[0] // 2)]
+        x_t = out[out.shape[0] // 2 : out.shape[0]]
+        x_all = torch.cat((x_s, x_t), 2)
+        x_all = x_all.view(x_all.shape[0], -1)
+        weight = torch.sigmoid(self.bn_lst[index](self.gate[index](x_all.float())))
+        weight = torch.mean(weight, dim=0)
+        res = self.softmax(weight).squeeze()
+        return res
+
+    @staticmethod
+    def get_features(output_list):
+        fea_list_src, fea_list_tar = [], []
+        for fea in output_list:
+            fea_list_src.append(fea[0 : fea.size(0) // 2])
+            fea_list_tar.append(fea[fea.size(0) // 2 :])
+        return fea_list_src, fea_list_tar
+
+    # For Boosting-based
+    def forward_Boosting(self, x, weight_mat=None):
+        out = self.gru_features(x)
+        fea = out[0]
+        if self.use_bottleneck:
+            fea_bottleneck = self.bottleneck(fea[:, -1, :])
+            fc_out = self.fc(fea_bottleneck).squeeze()
+        else:
+            fc_out = self.fc_out(fea[:, -1, :]).squeeze()
+
+        out_list_all = out[1]
+        out_list_s, out_list_t = self.get_features(out_list_all)
+        loss_transfer = torch.zeros((1,)).to(self.device)
+        if weight_mat is None:
+            weight = (1.0 / self.len_seq * torch.ones(self.num_layers, self.len_seq)).to(self.device)
+        else:
+            weight = weight_mat
+        dist_mat = torch.zeros(self.num_layers, self.len_seq).to(self.device)
+        for i, n in enumerate(out_list_s):
+            criterion_transder = TransferLoss(loss_type=self.trans_loss, input_dim=n.shape[2])
+            for j in range(self.len_seq):
+                loss_trans = criterion_transder.compute(n[:, j, :], out_list_t[i][:, j, :])
+                loss_transfer = loss_transfer + weight[i, j] * loss_trans
+                dist_mat[i, j] = loss_trans
+        return fc_out, loss_transfer, dist_mat, weight
+
+    # For Boosting-based
+    def update_weight_Boosting(self, weight_mat, dist_old, dist_new):
+        epsilon = 1e-5
+        dist_old = dist_old.detach()
+        dist_new = dist_new.detach()
+        ind = dist_new > dist_old + epsilon
+        weight_mat[ind] = weight_mat[ind] * (1 + torch.sigmoid(dist_new[ind] - dist_old[ind]))
+        weight_norm = torch.norm(weight_mat, dim=1, p=1)
+        weight_mat = weight_mat / weight_norm.t().unsqueeze(1).repeat(1, self.len_seq)
+        return weight_mat
+
+    def predict(self, x):
+        out = self.gru_features(x, predict=True)
+        fea = out[0]
+        if self.use_bottleneck is True:
+            fea_bottleneck = self.bottleneck(fea[:, -1, :])
+            fc_out = self.fc(fea_bottleneck).squeeze()
+        else:
+            fc_out = self.fc_out(fea[:, -1, :]).squeeze()
+        return fc_out
+
+
+class TransferLoss:
+    def __init__(self, loss_type="cosine", input_dim=512, GPU=0):
+        """
+        Supported loss_type: mmd(mmd_lin), mmd_rbf, coral, cosine, kl, js, mine, adv
+        """
+        self.loss_type = loss_type
+        self.input_dim = input_dim
+        self.device = torch.device("cuda:%d" % GPU if torch.cuda.is_available() and GPU >= 0 else "cpu")
+
+    def compute(self, X, Y):
+        """Compute adaptation loss
+
+        Arguments:
+            X {tensor} -- source matrix
+            Y {tensor} -- target matrix
+
+        Returns:
+            [tensor] -- transfer loss
+        """
+        if self.loss_type in ("mmd_lin", "mmd"):
+            mmdloss = MMD_loss(kernel_type="linear")
+            loss = mmdloss(X, Y)
+        elif self.loss_type == "coral":
+            loss = CORAL(X, Y, self.device)
+        elif self.loss_type in ("cosine", "cos"):
+            loss = 1 - cosine(X, Y)
+        elif self.loss_type == "kl":
+            loss = kl_div(X, Y)
+        elif self.loss_type == "js":
+            loss = js(X, Y)
+        elif self.loss_type == "mine":
+            mine_model = Mine_estimator(input_dim=self.input_dim, hidden_dim=60).to(self.device)
+            loss = mine_model(X, Y)
+        elif self.loss_type == "adv":
+            loss = adv(X, Y, self.device, input_dim=self.input_dim, hidden_dim=32)
+        elif self.loss_type == "mmd_rbf":
+            mmdloss = MMD_loss(kernel_type="rbf")
+            loss = mmdloss(X, Y)
+        elif self.loss_type == "pairwise":
+            pair_mat = pairwise_dist(X, Y)
+            loss = torch.norm(pair_mat)
+
+        return loss
+
+
+def cosine(source, target):
+    source, target = source.mean(), target.mean()
+    cos = nn.CosineSimilarity(dim=0)
+    loss = cos(source, target)
+    return loss.mean()
+
+
+class ReverseLayerF(Function):
+    @staticmethod
+    def forward(ctx, x, alpha):
+        ctx.alpha = alpha
+        return x.view_as(x)
+
+    @staticmethod
+    def backward(ctx, grad_output):
+        output = grad_output.neg() * ctx.alpha
+        return output, None
+
+
+class Discriminator(nn.Module):
+    def __init__(self, input_dim=256, hidden_dim=256):
+        super(Discriminator, self).__init__()
+        self.input_dim = input_dim
+        self.hidden_dim = hidden_dim
+        self.dis1 = nn.Linear(input_dim, hidden_dim)
+        self.dis2 = nn.Linear(hidden_dim, 1)
+
+    def forward(self, x):
+        x = F.relu(self.dis1(x))
+        x = self.dis2(x)
+        x = torch.sigmoid(x)
+        return x
+
+
+def adv(source, target, device, input_dim=256, hidden_dim=512):
+    domain_loss = nn.BCELoss()
+    # !!! Pay attention to .cuda !!!
+    adv_net = Discriminator(input_dim, hidden_dim).to(device)
+    domain_src = torch.ones(len(source)).to(device)
+    domain_tar = torch.zeros(len(target)).to(device)
+    domain_src, domain_tar = domain_src.view(domain_src.shape[0], 1), domain_tar.view(domain_tar.shape[0], 1)
+    reverse_src = ReverseLayerF.apply(source, 1)
+    reverse_tar = ReverseLayerF.apply(target, 1)
+    pred_src = adv_net(reverse_src)
+    pred_tar = adv_net(reverse_tar)
+    loss_s, loss_t = domain_loss(pred_src, domain_src), domain_loss(pred_tar, domain_tar)
+    loss = loss_s + loss_t
+    return loss
+
+
+def CORAL(source, target, device):
+    d = source.size(1)
+    ns, nt = source.size(0), target.size(0)
+
+    # source covariance
+    tmp_s = torch.ones((1, ns)).to(device) @ source
+    cs = (source.t() @ source - (tmp_s.t() @ tmp_s) / ns) / (ns - 1)
+
+    # target covariance
+    tmp_t = torch.ones((1, nt)).to(device) @ target
+    ct = (target.t() @ target - (tmp_t.t() @ tmp_t) / nt) / (nt - 1)
+
+    # frobenius norm
+    loss = (cs - ct).pow(2).sum()
+    loss = loss / (4 * d * d)
+
+    return loss
+
+
+class MMD_loss(nn.Module):
+    def __init__(self, kernel_type="linear", kernel_mul=2.0, kernel_num=5):
+        super(MMD_loss, self).__init__()
+        self.kernel_num = kernel_num
+        self.kernel_mul = kernel_mul
+        self.fix_sigma = None
+        self.kernel_type = kernel_type
+
+    @staticmethod
+    def guassian_kernel(source, target, kernel_mul=2.0, kernel_num=5, fix_sigma=None):
+        n_samples = int(source.size()[0]) + int(target.size()[0])
+        total = torch.cat([source, target], dim=0)
+        total0 = total.unsqueeze(0).expand(int(total.size(0)), int(total.size(0)), int(total.size(1)))
+        total1 = total.unsqueeze(1).expand(int(total.size(0)), int(total.size(0)), int(total.size(1)))
+        L2_distance = ((total0 - total1) ** 2).sum(2)
+        if fix_sigma:
+            bandwidth = fix_sigma
+        else:
+            bandwidth = torch.sum(L2_distance.data) / (n_samples**2 - n_samples)
+        bandwidth /= kernel_mul ** (kernel_num // 2)
+        bandwidth_list = [bandwidth * (kernel_mul**i) for i in range(kernel_num)]
+        kernel_val = [torch.exp(-L2_distance / bandwidth_temp) for bandwidth_temp in bandwidth_list]
+        return sum(kernel_val)
+
+    @staticmethod
+    def linear_mmd(X, Y):
+        delta = X.mean(axis=0) - Y.mean(axis=0)
+        loss = delta.dot(delta.T)
+        return loss
+
+    def forward(self, source, target):
+        if self.kernel_type == "linear":
+            return self.linear_mmd(source, target)
+        elif self.kernel_type == "rbf":
+            batch_size = int(source.size()[0])
+            kernels = self.guassian_kernel(
+                source, target, kernel_mul=self.kernel_mul, kernel_num=self.kernel_num, fix_sigma=self.fix_sigma
+            )
+            with torch.no_grad():
+                XX = torch.mean(kernels[:batch_size, :batch_size])
+                YY = torch.mean(kernels[batch_size:, batch_size:])
+                XY = torch.mean(kernels[:batch_size, batch_size:])
+                YX = torch.mean(kernels[batch_size:, :batch_size])
+                loss = torch.mean(XX + YY - XY - YX)
+            return loss
+
+
+class Mine_estimator(nn.Module):
+    def __init__(self, input_dim=2048, hidden_dim=512):
+        super(Mine_estimator, self).__init__()
+        self.mine_model = Mine(input_dim, hidden_dim)
+
+    def forward(self, X, Y):
+        Y_shffle = Y[torch.randperm(len(Y))]
+        loss_joint = self.mine_model(X, Y)
+        loss_marginal = self.mine_model(X, Y_shffle)
+        ret = torch.mean(loss_joint) - torch.log(torch.mean(torch.exp(loss_marginal)))
+        loss = -ret
+        return loss
+
+
+class Mine(nn.Module):
+    def __init__(self, input_dim=2048, hidden_dim=512):
+        super(Mine, self).__init__()
+        self.fc1_x = nn.Linear(input_dim, hidden_dim)
+        self.fc1_y = nn.Linear(input_dim, hidden_dim)
+        self.fc2 = nn.Linear(hidden_dim, 1)
+
+    def forward(self, x, y):
+        h1 = F.leaky_relu(self.fc1_x(x) + self.fc1_y(y))
+        h2 = self.fc2(h1)
+        return h2
+
+
+def pairwise_dist(X, Y):
+    n, d = X.shape
+    m, _ = Y.shape
+    assert d == Y.shape[1]
+    a = X.unsqueeze(1).expand(n, m, d)
+    b = Y.unsqueeze(0).expand(n, m, d)
+    return torch.pow(a - b, 2).sum(2)
+
+
+def pairwise_dist_np(X, Y):
+    n, d = X.shape
+    m, _ = Y.shape
+    assert d == Y.shape[1]
+    a = np.expand_dims(X, 1)
+    b = np.expand_dims(Y, 0)
+    a = np.tile(a, (1, m, 1))
+    b = np.tile(b, (n, 1, 1))
+    return np.power(a - b, 2).sum(2)
+
+
+def pa(X, Y):
+    XY = np.dot(X, Y.T)
+    XX = np.sum(np.square(X), axis=1)
+    XX = np.transpose([XX])
+    YY = np.sum(np.square(Y), axis=1)
+    dist = XX + YY - 2 * XY
+
+    return dist
+
+
+def kl_div(source, target):
+    if len(source) < len(target):
+        target = target[: len(source)]
+    elif len(source) > len(target):
+        source = source[: len(target)]
+    criterion = nn.KLDivLoss(reduction="batchmean")
+    loss = criterion(source.log(), target)
+    return loss
+
+
+def js(source, target):
+    if len(source) < len(target):
+        target = target[: len(source)]
+    elif len(source) > len(target):
+        source = source[: len(target)]
+    M = 0.5 * (source + target)
+    loss_1, loss_2 = kl_div(source, M), kl_div(target, M)
+    return 0.5 * (loss_1 + loss_2)
```

## qlib/contrib/model/pytorch_add.py

 * *Ordering differences only*

```diff
@@ -1,597 +1,597 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-from __future__ import division
-from __future__ import print_function
-
-
-import copy
-import math
-from typing import Text, Union
-
-import numpy as np
-import pandas as pd
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-import torch.optim as optim
-from qlib.contrib.model.pytorch_gru import GRUModel
-from qlib.contrib.model.pytorch_lstm import LSTMModel
-from qlib.contrib.model.pytorch_utils import count_parameters
-from qlib.data.dataset import DatasetH
-from qlib.data.dataset.handler import DataHandlerLP
-from qlib.log import get_module_logger
-from qlib.model.base import Model
-from qlib.utils import get_or_create_path
-from torch.autograd import Function
-
-
-class ADD(Model):
-    """ADD Model
-
-    Parameters
-    ----------
-     lr : float
-         learning rate
-     d_feat : int
-         input dimensions for each time step
-     metric : str
-         the evaluation metric used in early stop
-     optimizer : str
-         optimizer name
-     GPU : int
-         the GPU ID used for training
-    """
-
-    def __init__(
-        self,
-        d_feat=6,
-        hidden_size=64,
-        num_layers=2,
-        dropout=0.0,
-        dec_dropout=0.0,
-        n_epochs=200,
-        lr=0.001,
-        metric="mse",
-        batch_size=5000,
-        early_stop=20,
-        base_model="GRU",
-        model_path=None,
-        optimizer="adam",
-        gamma=0.1,
-        gamma_clip=0.4,
-        mu=0.05,
-        GPU=0,
-        seed=None,
-        **kwargs
-    ):
-        # Set logger.
-        self.logger = get_module_logger("ADD")
-        self.logger.info("ADD pytorch version...")
-
-        # set hyper-parameters.
-        self.d_feat = d_feat
-        self.hidden_size = hidden_size
-        self.num_layers = num_layers
-        self.dropout = dropout
-        self.dec_dropout = dec_dropout
-        self.n_epochs = n_epochs
-        self.lr = lr
-        self.metric = metric
-        self.batch_size = batch_size
-        self.early_stop = early_stop
-        self.optimizer = optimizer.lower()
-        self.base_model = base_model
-        self.model_path = model_path
-        self.device = torch.device("cuda:%d" % (GPU) if torch.cuda.is_available() and GPU >= 0 else "cpu")
-        self.seed = seed
-
-        self.gamma = gamma
-        self.gamma_clip = gamma_clip
-        self.mu = mu
-
-        self.logger.info(
-            "ADD parameters setting:"
-            "\nd_feat : {}"
-            "\nhidden_size : {}"
-            "\nnum_layers : {}"
-            "\ndropout : {}"
-            "\ndec_dropout : {}"
-            "\nn_epochs : {}"
-            "\nlr : {}"
-            "\nmetric : {}"
-            "\nbatch_size : {}"
-            "\nearly_stop : {}"
-            "\noptimizer : {}"
-            "\nbase_model : {}"
-            "\nmodel_path : {}"
-            "\ngamma : {}"
-            "\ngamma_clip : {}"
-            "\nmu : {}"
-            "\ndevice : {}"
-            "\nuse_GPU : {}"
-            "\nseed : {}".format(
-                d_feat,
-                hidden_size,
-                num_layers,
-                dropout,
-                dec_dropout,
-                n_epochs,
-                lr,
-                metric,
-                batch_size,
-                early_stop,
-                optimizer.lower(),
-                base_model,
-                model_path,
-                gamma,
-                gamma_clip,
-                mu,
-                self.device,
-                self.use_gpu,
-                seed,
-            )
-        )
-
-        if self.seed is not None:
-            np.random.seed(self.seed)
-            torch.manual_seed(self.seed)
-
-        self.ADD_model = ADDModel(
-            d_feat=self.d_feat,
-            hidden_size=self.hidden_size,
-            num_layers=self.num_layers,
-            dropout=self.dropout,
-            dec_dropout=self.dec_dropout,
-            base_model=self.base_model,
-            gamma=self.gamma,
-            gamma_clip=self.gamma_clip,
-        )
-        self.logger.info("model:\n{:}".format(self.ADD_model))
-        self.logger.info("model size: {:.4f} MB".format(count_parameters(self.ADD_model)))
-
-        if optimizer.lower() == "adam":
-            self.train_optimizer = optim.Adam(self.ADD_model.parameters(), lr=self.lr)
-        elif optimizer.lower() == "gd":
-            self.train_optimizer = optim.SGD(self.ADD_model.parameters(), lr=self.lr)
-        else:
-            raise NotImplementedError("optimizer {} is not supported!".format(optimizer))
-
-        self.fitted = False
-        self.ADD_model.to(self.device)
-
-    @property
-    def use_gpu(self):
-        return self.device != torch.device("cpu")
-
-    def loss_pre_excess(self, pred_excess, label_excess, record=None):
-        mask = ~torch.isnan(label_excess)
-        pre_excess_loss = F.mse_loss(pred_excess[mask], label_excess[mask])
-        if record is not None:
-            record["pre_excess_loss"] = pre_excess_loss.item()
-        return pre_excess_loss
-
-    def loss_pre_market(self, pred_market, label_market, record=None):
-        pre_market_loss = F.cross_entropy(pred_market, label_market)
-        if record is not None:
-            record["pre_market_loss"] = pre_market_loss.item()
-        return pre_market_loss
-
-    def loss_pre(self, pred_excess, label_excess, pred_market, label_market, record=None):
-        pre_loss = self.loss_pre_excess(pred_excess, label_excess, record) + self.loss_pre_market(
-            pred_market, label_market, record
-        )
-        if record is not None:
-            record["pre_loss"] = pre_loss.item()
-        return pre_loss
-
-    def loss_adv_excess(self, adv_excess, label_excess, record=None):
-        mask = ~torch.isnan(label_excess)
-        adv_excess_loss = F.mse_loss(adv_excess.squeeze()[mask], label_excess[mask])
-        if record is not None:
-            record["adv_excess_loss"] = adv_excess_loss.item()
-        return adv_excess_loss
-
-    def loss_adv_market(self, adv_market, label_market, record=None):
-        adv_market_loss = F.cross_entropy(adv_market, label_market)
-        if record is not None:
-            record["adv_market_loss"] = adv_market_loss.item()
-        return adv_market_loss
-
-    def loss_adv(self, adv_excess, label_excess, adv_market, label_market, record=None):
-        adv_loss = self.loss_adv_excess(adv_excess, label_excess, record) + self.loss_adv_market(
-            adv_market, label_market, record
-        )
-        if record is not None:
-            record["adv_loss"] = adv_loss.item()
-        return adv_loss
-
-    def loss_fn(self, x, preds, label_excess, label_market, record=None):
-        loss = (
-            self.loss_pre(preds["excess"], label_excess, preds["market"], label_market, record)
-            + self.loss_adv(preds["adv_excess"], label_excess, preds["adv_market"], label_market, record)
-            + self.mu * self.loss_rec(x, preds["reconstructed_feature"], record)
-        )
-        if record is not None:
-            record["loss"] = loss.item()
-        return loss
-
-    def loss_rec(self, x, rec_x, record=None):
-        x = x.reshape(len(x), self.d_feat, -1)
-        x = x.permute(0, 2, 1)
-        rec_loss = F.mse_loss(x, rec_x)
-        if record is not None:
-            record["rec_loss"] = rec_loss.item()
-        return rec_loss
-
-    def get_daily_inter(self, df, shuffle=False):
-        # organize the train data into daily batches
-        daily_count = df.groupby(level=0).size().values
-        daily_index = np.roll(np.cumsum(daily_count), 1)
-        daily_index[0] = 0
-        if shuffle:
-            # shuffle data
-            daily_shuffle = list(zip(daily_index, daily_count))
-            np.random.shuffle(daily_shuffle)
-            daily_index, daily_count = zip(*daily_shuffle)
-        return daily_index, daily_count
-
-    def cal_ic_metrics(self, pred, label):
-        metrics = {}
-        metrics["mse"] = -F.mse_loss(pred, label).item()
-        metrics["loss"] = metrics["mse"]
-        pred = pd.Series(pred.cpu().detach().numpy())
-        label = pd.Series(label.cpu().detach().numpy())
-        metrics["ic"] = pred.corr(label)
-        metrics["ric"] = pred.corr(label, method="spearman")
-        return metrics
-
-    def test_epoch(self, data_x, data_y, data_m):
-        x_values = data_x.values
-        y_values = np.squeeze(data_y.values)
-        m_values = np.squeeze(data_m.values.astype(int))
-        self.ADD_model.eval()
-
-        metrics_list = []
-
-        daily_index, daily_count = self.get_daily_inter(data_x, shuffle=False)
-
-        for idx, count in zip(daily_index, daily_count):
-            batch = slice(idx, idx + count)
-            feature = torch.from_numpy(x_values[batch]).float().to(self.device)
-            label_excess = torch.from_numpy(y_values[batch]).float().to(self.device)
-            label_market = torch.from_numpy(m_values[batch]).long().to(self.device)
-
-            metrics = {}
-            preds = self.ADD_model(feature)
-            self.loss_fn(feature, preds, label_excess, label_market, metrics)
-            metrics.update(self.cal_ic_metrics(preds["excess"], label_excess))
-            metrics_list.append(metrics)
-        metrics = {}
-        keys = metrics_list[0].keys()
-        for k in keys:
-            vs = [m[k] for m in metrics_list]
-            metrics[k] = sum(vs) / len(vs)
-
-        return metrics
-
-    def train_epoch(self, x_train_values, y_train_values, m_train_values):
-        self.ADD_model.train()
-
-        indices = np.arange(len(x_train_values))
-        np.random.shuffle(indices)
-
-        cur_step = 1
-
-        for i in range(len(indices))[:: self.batch_size]:
-            if len(indices) - i < self.batch_size:
-                break
-            batch = indices[i : i + self.batch_size]
-            feature = torch.from_numpy(x_train_values[batch]).float().to(self.device)
-            label_excess = torch.from_numpy(y_train_values[batch]).float().to(self.device)
-            label_market = torch.from_numpy(m_train_values[batch]).long().to(self.device)
-
-            preds = self.ADD_model(feature)
-
-            loss = self.loss_fn(feature, preds, label_excess, label_market)
-
-            self.train_optimizer.zero_grad()
-            loss.backward()
-            torch.nn.utils.clip_grad_value_(self.ADD_model.parameters(), 3.0)
-            self.train_optimizer.step()
-            cur_step += 1
-
-    def log_metrics(self, mode, metrics):
-        metrics = ["{}/{}: {:.6f}".format(k, mode, v) for k, v in metrics.items()]
-        metrics = ", ".join(metrics)
-        self.logger.info(metrics)
-
-    def bootstrap_fit(self, x_train, y_train, m_train, x_valid, y_valid, m_valid):
-        stop_steps = 0
-        best_score = -np.inf
-        best_epoch = 0
-
-        # train
-        self.logger.info("training...")
-        self.fitted = True
-        x_train_values = x_train.values
-        y_train_values = np.squeeze(y_train.values)
-        m_train_values = np.squeeze(m_train.values.astype(int))
-
-        for step in range(self.n_epochs):
-            self.logger.info("Epoch%d:", step)
-            self.logger.info("training...")
-            self.train_epoch(x_train_values, y_train_values, m_train_values)
-            self.logger.info("evaluating...")
-            train_metrics = self.test_epoch(x_train, y_train, m_train)
-            valid_metrics = self.test_epoch(x_valid, y_valid, m_valid)
-            self.log_metrics("train", train_metrics)
-            self.log_metrics("valid", valid_metrics)
-
-            if self.metric in valid_metrics:
-                val_score = valid_metrics[self.metric]
-            else:
-                raise ValueError("unknown metric name `%s`" % self.metric)
-            if val_score > best_score:
-                best_score = val_score
-                stop_steps = 0
-                best_epoch = step
-                best_param = copy.deepcopy(self.ADD_model.state_dict())
-            else:
-                stop_steps += 1
-                if stop_steps >= self.early_stop:
-                    self.logger.info("early stop")
-                    break
-            self.ADD_model.before_adv_excess.step_alpha()
-            self.ADD_model.before_adv_market.step_alpha()
-        self.logger.info("bootstrap_fit best score: {:.6f} @ {}".format(best_score, best_epoch))
-        self.ADD_model.load_state_dict(best_param)
-        return best_score
-
-    def gen_market_label(self, df, raw_label):
-        market_label = raw_label.groupby("datetime").mean().squeeze()
-        bins = [-np.inf, self.lo, self.hi, np.inf]
-        market_label = pd.cut(market_label, bins, labels=False)
-        market_label.name = ("market_return", "market_return")
-        df = df.join(market_label)
-        return df
-
-    def fit_thresh(self, train_label):
-        market_label = train_label.groupby("datetime").mean().squeeze()
-        self.lo, self.hi = market_label.quantile([1 / 3, 2 / 3])
-
-    def fit(
-        self,
-        dataset: DatasetH,
-        evals_result=dict(),
-        save_path=None,
-    ):
-        label_train, label_valid = dataset.prepare(
-            ["train", "valid"],
-            col_set=["label"],
-            data_key=DataHandlerLP.DK_R,
-        )
-        self.fit_thresh(label_train)
-        df_train, df_valid = dataset.prepare(
-            ["train", "valid"],
-            col_set=["feature", "label"],
-            data_key=DataHandlerLP.DK_L,
-        )
-        df_train = self.gen_market_label(df_train, label_train)
-        df_valid = self.gen_market_label(df_valid, label_valid)
-
-        x_train, y_train, m_train = df_train["feature"], df_train["label"], df_train["market_return"]
-        x_valid, y_valid, m_valid = df_valid["feature"], df_valid["label"], df_valid["market_return"]
-
-        evals_result["train"] = []
-        evals_result["valid"] = []
-        # load pretrained base_model
-
-        if self.base_model == "LSTM":
-            pretrained_model = LSTMModel()
-        elif self.base_model == "GRU":
-            pretrained_model = GRUModel()
-        else:
-            raise ValueError("unknown base model name `%s`" % self.base_model)
-
-        if self.model_path is not None:
-            self.logger.info("Loading pretrained model...")
-            pretrained_model.load_state_dict(torch.load(self.model_path, map_location=self.device))
-
-            model_dict = self.ADD_model.enc_excess.state_dict()
-            pretrained_dict = {k: v for k, v in pretrained_model.rnn.state_dict().items() if k in model_dict}
-            model_dict.update(pretrained_dict)
-            self.ADD_model.enc_excess.load_state_dict(model_dict)
-            model_dict = self.ADD_model.enc_market.state_dict()
-            pretrained_dict = {k: v for k, v in pretrained_model.rnn.state_dict().items() if k in model_dict}
-            model_dict.update(pretrained_dict)
-            self.ADD_model.enc_market.load_state_dict(model_dict)
-            self.logger.info("Loading pretrained model Done...")
-
-        self.bootstrap_fit(x_train, y_train, m_train, x_valid, y_valid, m_valid)
-
-        best_param = copy.deepcopy(self.ADD_model.state_dict())
-        save_path = get_or_create_path(save_path)
-        torch.save(best_param, save_path)
-        if self.use_gpu:
-            torch.cuda.empty_cache()
-
-    def predict(self, dataset: DatasetH, segment: Union[Text, slice] = "test"):
-        x_test = dataset.prepare(segment, col_set="feature", data_key=DataHandlerLP.DK_I)
-        index = x_test.index
-        self.ADD_model.eval()
-        x_values = x_test.values
-        preds = []
-
-        daily_index, daily_count = self.get_daily_inter(x_test, shuffle=False)
-
-        for idx, count in zip(daily_index, daily_count):
-            batch = slice(idx, idx + count)
-            x_batch = torch.from_numpy(x_values[batch]).float().to(self.device)
-
-            with torch.no_grad():
-                pred = self.ADD_model(x_batch)
-                pred = pred["excess"].detach().cpu().numpy()
-
-            preds.append(pred)
-
-        r = pd.Series(np.concatenate(preds), index=index)
-        return r
-
-
-class ADDModel(nn.Module):
-    def __init__(
-        self,
-        d_feat=6,
-        hidden_size=64,
-        num_layers=1,
-        dropout=0.0,
-        dec_dropout=0.5,
-        base_model="GRU",
-        gamma=0.1,
-        gamma_clip=0.4,
-    ):
-        super().__init__()
-        self.d_feat = d_feat
-        self.base_model = base_model
-        if base_model == "GRU":
-            self.enc_excess, self.enc_market = [
-                nn.GRU(
-                    input_size=d_feat,
-                    hidden_size=hidden_size,
-                    num_layers=num_layers,
-                    batch_first=True,
-                    dropout=dropout,
-                )
-                for _ in range(2)
-            ]
-        elif base_model == "LSTM":
-            self.enc_excess, self.enc_market = [
-                nn.LSTM(
-                    input_size=d_feat,
-                    hidden_size=hidden_size,
-                    num_layers=num_layers,
-                    batch_first=True,
-                    dropout=dropout,
-                )
-                for _ in range(2)
-            ]
-        else:
-            raise ValueError("unknown base model name `%s`" % base_model)
-        self.dec = Decoder(d_feat, 2 * hidden_size, num_layers, dec_dropout, base_model)
-
-        ctx_size = hidden_size * num_layers
-        self.pred_excess, self.adv_excess = [
-            nn.Sequential(nn.Linear(ctx_size, ctx_size), nn.BatchNorm1d(ctx_size), nn.Tanh(), nn.Linear(ctx_size, 1))
-            for _ in range(2)
-        ]
-        self.adv_market, self.pred_market = [
-            nn.Sequential(nn.Linear(ctx_size, ctx_size), nn.BatchNorm1d(ctx_size), nn.Tanh(), nn.Linear(ctx_size, 3))
-            for _ in range(2)
-        ]
-        self.before_adv_market, self.before_adv_excess = [RevGrad(gamma, gamma_clip) for _ in range(2)]
-
-    def forward(self, x):
-        x = x.reshape(len(x), self.d_feat, -1)
-        N = x.shape[0]
-        T = x.shape[-1]
-        x = x.permute(0, 2, 1)
-
-        out, hidden_excess = self.enc_excess(x)
-        out, hidden_market = self.enc_market(x)
-        if self.base_model == "LSTM":
-            feature_excess = hidden_excess[0].permute(1, 0, 2).reshape(N, -1)
-            feature_market = hidden_market[0].permute(1, 0, 2).reshape(N, -1)
-        else:
-            feature_excess = hidden_excess.permute(1, 0, 2).reshape(N, -1)
-            feature_market = hidden_market.permute(1, 0, 2).reshape(N, -1)
-        predicts = {}
-        predicts["excess"] = self.pred_excess(feature_excess).squeeze(1)
-        predicts["market"] = self.pred_market(feature_market)
-        predicts["adv_market"] = self.adv_market(self.before_adv_market(feature_excess))
-        predicts["adv_excess"] = self.adv_excess(self.before_adv_excess(feature_market).squeeze(1))
-        if self.base_model == "LSTM":
-            hidden = [torch.cat([hidden_excess[i], hidden_market[i]], -1) for i in range(2)]
-        else:
-            hidden = torch.cat([hidden_excess, hidden_market], -1)
-        x = torch.zeros_like(x[:, 1, :])
-        reconstructed_feature = []
-        for i in range(T):
-            x, hidden = self.dec(x, hidden)
-            reconstructed_feature.append(x)
-        reconstructed_feature = torch.stack(reconstructed_feature, 1)
-        predicts["reconstructed_feature"] = reconstructed_feature
-        return predicts
-
-
-class Decoder(nn.Module):
-    def __init__(self, d_feat=6, hidden_size=128, num_layers=1, dropout=0.5, base_model="GRU"):
-        super().__init__()
-        self.base_model = base_model
-        if base_model == "GRU":
-            self.rnn = nn.GRU(
-                input_size=d_feat,
-                hidden_size=hidden_size,
-                num_layers=num_layers,
-                batch_first=True,
-                dropout=dropout,
-            )
-        elif base_model == "LSTM":
-            self.rnn = nn.LSTM(
-                input_size=d_feat,
-                hidden_size=hidden_size,
-                num_layers=num_layers,
-                batch_first=True,
-                dropout=dropout,
-            )
-        else:
-            raise ValueError("unknown base model name `%s`" % base_model)
-
-        self.fc = nn.Linear(hidden_size, d_feat)
-
-    def forward(self, x, hidden):
-        x = x.unsqueeze(1)
-        output, hidden = self.rnn(x, hidden)
-        output = output.squeeze(1)
-        pred = self.fc(output)
-        return pred, hidden
-
-
-class RevGradFunc(Function):
-    @staticmethod
-    def forward(ctx, input_, alpha_):
-        ctx.save_for_backward(input_, alpha_)
-        output = input_
-        return output
-
-    @staticmethod
-    def backward(ctx, grad_output):  # pragma: no cover
-        grad_input = None
-        _, alpha_ = ctx.saved_tensors
-        if ctx.needs_input_grad[0]:
-            grad_input = -grad_output * alpha_
-        return grad_input, None
-
-
-class RevGrad(nn.Module):
-    def __init__(self, gamma=0.1, gamma_clip=0.4, *args, **kwargs):
-        """
-        A gradient reversal layer.
-        This layer has no parameters, and simply reverses the gradient
-        in the backward pass.
-        """
-        super().__init__(*args, **kwargs)
-
-        self.gamma = gamma
-        self.gamma_clip = torch.tensor(float(gamma_clip), requires_grad=False)
-        self._alpha = torch.tensor(0, requires_grad=False)
-        self._p = 0
-
-    def step_alpha(self):
-        self._p += 1
-        self._alpha = min(
-            self.gamma_clip, torch.tensor(2 / (1 + math.exp(-self.gamma * self._p)) - 1, requires_grad=False)
-        )
-
-    def forward(self, input_):
-        return RevGradFunc.apply(input_, self._alpha)
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+from __future__ import division
+from __future__ import print_function
+
+
+import copy
+import math
+from typing import Text, Union
+
+import numpy as np
+import pandas as pd
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
+import torch.optim as optim
+from qlib.contrib.model.pytorch_gru import GRUModel
+from qlib.contrib.model.pytorch_lstm import LSTMModel
+from qlib.contrib.model.pytorch_utils import count_parameters
+from qlib.data.dataset import DatasetH
+from qlib.data.dataset.handler import DataHandlerLP
+from qlib.log import get_module_logger
+from qlib.model.base import Model
+from qlib.utils import get_or_create_path
+from torch.autograd import Function
+
+
+class ADD(Model):
+    """ADD Model
+
+    Parameters
+    ----------
+     lr : float
+         learning rate
+     d_feat : int
+         input dimensions for each time step
+     metric : str
+         the evaluation metric used in early stop
+     optimizer : str
+         optimizer name
+     GPU : int
+         the GPU ID used for training
+    """
+
+    def __init__(
+        self,
+        d_feat=6,
+        hidden_size=64,
+        num_layers=2,
+        dropout=0.0,
+        dec_dropout=0.0,
+        n_epochs=200,
+        lr=0.001,
+        metric="mse",
+        batch_size=5000,
+        early_stop=20,
+        base_model="GRU",
+        model_path=None,
+        optimizer="adam",
+        gamma=0.1,
+        gamma_clip=0.4,
+        mu=0.05,
+        GPU=0,
+        seed=None,
+        **kwargs
+    ):
+        # Set logger.
+        self.logger = get_module_logger("ADD")
+        self.logger.info("ADD pytorch version...")
+
+        # set hyper-parameters.
+        self.d_feat = d_feat
+        self.hidden_size = hidden_size
+        self.num_layers = num_layers
+        self.dropout = dropout
+        self.dec_dropout = dec_dropout
+        self.n_epochs = n_epochs
+        self.lr = lr
+        self.metric = metric
+        self.batch_size = batch_size
+        self.early_stop = early_stop
+        self.optimizer = optimizer.lower()
+        self.base_model = base_model
+        self.model_path = model_path
+        self.device = torch.device("cuda:%d" % (GPU) if torch.cuda.is_available() and GPU >= 0 else "cpu")
+        self.seed = seed
+
+        self.gamma = gamma
+        self.gamma_clip = gamma_clip
+        self.mu = mu
+
+        self.logger.info(
+            "ADD parameters setting:"
+            "\nd_feat : {}"
+            "\nhidden_size : {}"
+            "\nnum_layers : {}"
+            "\ndropout : {}"
+            "\ndec_dropout : {}"
+            "\nn_epochs : {}"
+            "\nlr : {}"
+            "\nmetric : {}"
+            "\nbatch_size : {}"
+            "\nearly_stop : {}"
+            "\noptimizer : {}"
+            "\nbase_model : {}"
+            "\nmodel_path : {}"
+            "\ngamma : {}"
+            "\ngamma_clip : {}"
+            "\nmu : {}"
+            "\ndevice : {}"
+            "\nuse_GPU : {}"
+            "\nseed : {}".format(
+                d_feat,
+                hidden_size,
+                num_layers,
+                dropout,
+                dec_dropout,
+                n_epochs,
+                lr,
+                metric,
+                batch_size,
+                early_stop,
+                optimizer.lower(),
+                base_model,
+                model_path,
+                gamma,
+                gamma_clip,
+                mu,
+                self.device,
+                self.use_gpu,
+                seed,
+            )
+        )
+
+        if self.seed is not None:
+            np.random.seed(self.seed)
+            torch.manual_seed(self.seed)
+
+        self.ADD_model = ADDModel(
+            d_feat=self.d_feat,
+            hidden_size=self.hidden_size,
+            num_layers=self.num_layers,
+            dropout=self.dropout,
+            dec_dropout=self.dec_dropout,
+            base_model=self.base_model,
+            gamma=self.gamma,
+            gamma_clip=self.gamma_clip,
+        )
+        self.logger.info("model:\n{:}".format(self.ADD_model))
+        self.logger.info("model size: {:.4f} MB".format(count_parameters(self.ADD_model)))
+
+        if optimizer.lower() == "adam":
+            self.train_optimizer = optim.Adam(self.ADD_model.parameters(), lr=self.lr)
+        elif optimizer.lower() == "gd":
+            self.train_optimizer = optim.SGD(self.ADD_model.parameters(), lr=self.lr)
+        else:
+            raise NotImplementedError("optimizer {} is not supported!".format(optimizer))
+
+        self.fitted = False
+        self.ADD_model.to(self.device)
+
+    @property
+    def use_gpu(self):
+        return self.device != torch.device("cpu")
+
+    def loss_pre_excess(self, pred_excess, label_excess, record=None):
+        mask = ~torch.isnan(label_excess)
+        pre_excess_loss = F.mse_loss(pred_excess[mask], label_excess[mask])
+        if record is not None:
+            record["pre_excess_loss"] = pre_excess_loss.item()
+        return pre_excess_loss
+
+    def loss_pre_market(self, pred_market, label_market, record=None):
+        pre_market_loss = F.cross_entropy(pred_market, label_market)
+        if record is not None:
+            record["pre_market_loss"] = pre_market_loss.item()
+        return pre_market_loss
+
+    def loss_pre(self, pred_excess, label_excess, pred_market, label_market, record=None):
+        pre_loss = self.loss_pre_excess(pred_excess, label_excess, record) + self.loss_pre_market(
+            pred_market, label_market, record
+        )
+        if record is not None:
+            record["pre_loss"] = pre_loss.item()
+        return pre_loss
+
+    def loss_adv_excess(self, adv_excess, label_excess, record=None):
+        mask = ~torch.isnan(label_excess)
+        adv_excess_loss = F.mse_loss(adv_excess.squeeze()[mask], label_excess[mask])
+        if record is not None:
+            record["adv_excess_loss"] = adv_excess_loss.item()
+        return adv_excess_loss
+
+    def loss_adv_market(self, adv_market, label_market, record=None):
+        adv_market_loss = F.cross_entropy(adv_market, label_market)
+        if record is not None:
+            record["adv_market_loss"] = adv_market_loss.item()
+        return adv_market_loss
+
+    def loss_adv(self, adv_excess, label_excess, adv_market, label_market, record=None):
+        adv_loss = self.loss_adv_excess(adv_excess, label_excess, record) + self.loss_adv_market(
+            adv_market, label_market, record
+        )
+        if record is not None:
+            record["adv_loss"] = adv_loss.item()
+        return adv_loss
+
+    def loss_fn(self, x, preds, label_excess, label_market, record=None):
+        loss = (
+            self.loss_pre(preds["excess"], label_excess, preds["market"], label_market, record)
+            + self.loss_adv(preds["adv_excess"], label_excess, preds["adv_market"], label_market, record)
+            + self.mu * self.loss_rec(x, preds["reconstructed_feature"], record)
+        )
+        if record is not None:
+            record["loss"] = loss.item()
+        return loss
+
+    def loss_rec(self, x, rec_x, record=None):
+        x = x.reshape(len(x), self.d_feat, -1)
+        x = x.permute(0, 2, 1)
+        rec_loss = F.mse_loss(x, rec_x)
+        if record is not None:
+            record["rec_loss"] = rec_loss.item()
+        return rec_loss
+
+    def get_daily_inter(self, df, shuffle=False):
+        # organize the train data into daily batches
+        daily_count = df.groupby(level=0).size().values
+        daily_index = np.roll(np.cumsum(daily_count), 1)
+        daily_index[0] = 0
+        if shuffle:
+            # shuffle data
+            daily_shuffle = list(zip(daily_index, daily_count))
+            np.random.shuffle(daily_shuffle)
+            daily_index, daily_count = zip(*daily_shuffle)
+        return daily_index, daily_count
+
+    def cal_ic_metrics(self, pred, label):
+        metrics = {}
+        metrics["mse"] = -F.mse_loss(pred, label).item()
+        metrics["loss"] = metrics["mse"]
+        pred = pd.Series(pred.cpu().detach().numpy())
+        label = pd.Series(label.cpu().detach().numpy())
+        metrics["ic"] = pred.corr(label)
+        metrics["ric"] = pred.corr(label, method="spearman")
+        return metrics
+
+    def test_epoch(self, data_x, data_y, data_m):
+        x_values = data_x.values
+        y_values = np.squeeze(data_y.values)
+        m_values = np.squeeze(data_m.values.astype(int))
+        self.ADD_model.eval()
+
+        metrics_list = []
+
+        daily_index, daily_count = self.get_daily_inter(data_x, shuffle=False)
+
+        for idx, count in zip(daily_index, daily_count):
+            batch = slice(idx, idx + count)
+            feature = torch.from_numpy(x_values[batch]).float().to(self.device)
+            label_excess = torch.from_numpy(y_values[batch]).float().to(self.device)
+            label_market = torch.from_numpy(m_values[batch]).long().to(self.device)
+
+            metrics = {}
+            preds = self.ADD_model(feature)
+            self.loss_fn(feature, preds, label_excess, label_market, metrics)
+            metrics.update(self.cal_ic_metrics(preds["excess"], label_excess))
+            metrics_list.append(metrics)
+        metrics = {}
+        keys = metrics_list[0].keys()
+        for k in keys:
+            vs = [m[k] for m in metrics_list]
+            metrics[k] = sum(vs) / len(vs)
+
+        return metrics
+
+    def train_epoch(self, x_train_values, y_train_values, m_train_values):
+        self.ADD_model.train()
+
+        indices = np.arange(len(x_train_values))
+        np.random.shuffle(indices)
+
+        cur_step = 1
+
+        for i in range(len(indices))[:: self.batch_size]:
+            if len(indices) - i < self.batch_size:
+                break
+            batch = indices[i : i + self.batch_size]
+            feature = torch.from_numpy(x_train_values[batch]).float().to(self.device)
+            label_excess = torch.from_numpy(y_train_values[batch]).float().to(self.device)
+            label_market = torch.from_numpy(m_train_values[batch]).long().to(self.device)
+
+            preds = self.ADD_model(feature)
+
+            loss = self.loss_fn(feature, preds, label_excess, label_market)
+
+            self.train_optimizer.zero_grad()
+            loss.backward()
+            torch.nn.utils.clip_grad_value_(self.ADD_model.parameters(), 3.0)
+            self.train_optimizer.step()
+            cur_step += 1
+
+    def log_metrics(self, mode, metrics):
+        metrics = ["{}/{}: {:.6f}".format(k, mode, v) for k, v in metrics.items()]
+        metrics = ", ".join(metrics)
+        self.logger.info(metrics)
+
+    def bootstrap_fit(self, x_train, y_train, m_train, x_valid, y_valid, m_valid):
+        stop_steps = 0
+        best_score = -np.inf
+        best_epoch = 0
+
+        # train
+        self.logger.info("training...")
+        self.fitted = True
+        x_train_values = x_train.values
+        y_train_values = np.squeeze(y_train.values)
+        m_train_values = np.squeeze(m_train.values.astype(int))
+
+        for step in range(self.n_epochs):
+            self.logger.info("Epoch%d:", step)
+            self.logger.info("training...")
+            self.train_epoch(x_train_values, y_train_values, m_train_values)
+            self.logger.info("evaluating...")
+            train_metrics = self.test_epoch(x_train, y_train, m_train)
+            valid_metrics = self.test_epoch(x_valid, y_valid, m_valid)
+            self.log_metrics("train", train_metrics)
+            self.log_metrics("valid", valid_metrics)
+
+            if self.metric in valid_metrics:
+                val_score = valid_metrics[self.metric]
+            else:
+                raise ValueError("unknown metric name `%s`" % self.metric)
+            if val_score > best_score:
+                best_score = val_score
+                stop_steps = 0
+                best_epoch = step
+                best_param = copy.deepcopy(self.ADD_model.state_dict())
+            else:
+                stop_steps += 1
+                if stop_steps >= self.early_stop:
+                    self.logger.info("early stop")
+                    break
+            self.ADD_model.before_adv_excess.step_alpha()
+            self.ADD_model.before_adv_market.step_alpha()
+        self.logger.info("bootstrap_fit best score: {:.6f} @ {}".format(best_score, best_epoch))
+        self.ADD_model.load_state_dict(best_param)
+        return best_score
+
+    def gen_market_label(self, df, raw_label):
+        market_label = raw_label.groupby("datetime").mean().squeeze()
+        bins = [-np.inf, self.lo, self.hi, np.inf]
+        market_label = pd.cut(market_label, bins, labels=False)
+        market_label.name = ("market_return", "market_return")
+        df = df.join(market_label)
+        return df
+
+    def fit_thresh(self, train_label):
+        market_label = train_label.groupby("datetime").mean().squeeze()
+        self.lo, self.hi = market_label.quantile([1 / 3, 2 / 3])
+
+    def fit(
+        self,
+        dataset: DatasetH,
+        evals_result=dict(),
+        save_path=None,
+    ):
+        label_train, label_valid = dataset.prepare(
+            ["train", "valid"],
+            col_set=["label"],
+            data_key=DataHandlerLP.DK_R,
+        )
+        self.fit_thresh(label_train)
+        df_train, df_valid = dataset.prepare(
+            ["train", "valid"],
+            col_set=["feature", "label"],
+            data_key=DataHandlerLP.DK_L,
+        )
+        df_train = self.gen_market_label(df_train, label_train)
+        df_valid = self.gen_market_label(df_valid, label_valid)
+
+        x_train, y_train, m_train = df_train["feature"], df_train["label"], df_train["market_return"]
+        x_valid, y_valid, m_valid = df_valid["feature"], df_valid["label"], df_valid["market_return"]
+
+        evals_result["train"] = []
+        evals_result["valid"] = []
+        # load pretrained base_model
+
+        if self.base_model == "LSTM":
+            pretrained_model = LSTMModel()
+        elif self.base_model == "GRU":
+            pretrained_model = GRUModel()
+        else:
+            raise ValueError("unknown base model name `%s`" % self.base_model)
+
+        if self.model_path is not None:
+            self.logger.info("Loading pretrained model...")
+            pretrained_model.load_state_dict(torch.load(self.model_path, map_location=self.device))
+
+            model_dict = self.ADD_model.enc_excess.state_dict()
+            pretrained_dict = {k: v for k, v in pretrained_model.rnn.state_dict().items() if k in model_dict}
+            model_dict.update(pretrained_dict)
+            self.ADD_model.enc_excess.load_state_dict(model_dict)
+            model_dict = self.ADD_model.enc_market.state_dict()
+            pretrained_dict = {k: v for k, v in pretrained_model.rnn.state_dict().items() if k in model_dict}
+            model_dict.update(pretrained_dict)
+            self.ADD_model.enc_market.load_state_dict(model_dict)
+            self.logger.info("Loading pretrained model Done...")
+
+        self.bootstrap_fit(x_train, y_train, m_train, x_valid, y_valid, m_valid)
+
+        best_param = copy.deepcopy(self.ADD_model.state_dict())
+        save_path = get_or_create_path(save_path)
+        torch.save(best_param, save_path)
+        if self.use_gpu:
+            torch.cuda.empty_cache()
+
+    def predict(self, dataset: DatasetH, segment: Union[Text, slice] = "test"):
+        x_test = dataset.prepare(segment, col_set="feature", data_key=DataHandlerLP.DK_I)
+        index = x_test.index
+        self.ADD_model.eval()
+        x_values = x_test.values
+        preds = []
+
+        daily_index, daily_count = self.get_daily_inter(x_test, shuffle=False)
+
+        for idx, count in zip(daily_index, daily_count):
+            batch = slice(idx, idx + count)
+            x_batch = torch.from_numpy(x_values[batch]).float().to(self.device)
+
+            with torch.no_grad():
+                pred = self.ADD_model(x_batch)
+                pred = pred["excess"].detach().cpu().numpy()
+
+            preds.append(pred)
+
+        r = pd.Series(np.concatenate(preds), index=index)
+        return r
+
+
+class ADDModel(nn.Module):
+    def __init__(
+        self,
+        d_feat=6,
+        hidden_size=64,
+        num_layers=1,
+        dropout=0.0,
+        dec_dropout=0.5,
+        base_model="GRU",
+        gamma=0.1,
+        gamma_clip=0.4,
+    ):
+        super().__init__()
+        self.d_feat = d_feat
+        self.base_model = base_model
+        if base_model == "GRU":
+            self.enc_excess, self.enc_market = [
+                nn.GRU(
+                    input_size=d_feat,
+                    hidden_size=hidden_size,
+                    num_layers=num_layers,
+                    batch_first=True,
+                    dropout=dropout,
+                )
+                for _ in range(2)
+            ]
+        elif base_model == "LSTM":
+            self.enc_excess, self.enc_market = [
+                nn.LSTM(
+                    input_size=d_feat,
+                    hidden_size=hidden_size,
+                    num_layers=num_layers,
+                    batch_first=True,
+                    dropout=dropout,
+                )
+                for _ in range(2)
+            ]
+        else:
+            raise ValueError("unknown base model name `%s`" % base_model)
+        self.dec = Decoder(d_feat, 2 * hidden_size, num_layers, dec_dropout, base_model)
+
+        ctx_size = hidden_size * num_layers
+        self.pred_excess, self.adv_excess = [
+            nn.Sequential(nn.Linear(ctx_size, ctx_size), nn.BatchNorm1d(ctx_size), nn.Tanh(), nn.Linear(ctx_size, 1))
+            for _ in range(2)
+        ]
+        self.adv_market, self.pred_market = [
+            nn.Sequential(nn.Linear(ctx_size, ctx_size), nn.BatchNorm1d(ctx_size), nn.Tanh(), nn.Linear(ctx_size, 3))
+            for _ in range(2)
+        ]
+        self.before_adv_market, self.before_adv_excess = [RevGrad(gamma, gamma_clip) for _ in range(2)]
+
+    def forward(self, x):
+        x = x.reshape(len(x), self.d_feat, -1)
+        N = x.shape[0]
+        T = x.shape[-1]
+        x = x.permute(0, 2, 1)
+
+        out, hidden_excess = self.enc_excess(x)
+        out, hidden_market = self.enc_market(x)
+        if self.base_model == "LSTM":
+            feature_excess = hidden_excess[0].permute(1, 0, 2).reshape(N, -1)
+            feature_market = hidden_market[0].permute(1, 0, 2).reshape(N, -1)
+        else:
+            feature_excess = hidden_excess.permute(1, 0, 2).reshape(N, -1)
+            feature_market = hidden_market.permute(1, 0, 2).reshape(N, -1)
+        predicts = {}
+        predicts["excess"] = self.pred_excess(feature_excess).squeeze(1)
+        predicts["market"] = self.pred_market(feature_market)
+        predicts["adv_market"] = self.adv_market(self.before_adv_market(feature_excess))
+        predicts["adv_excess"] = self.adv_excess(self.before_adv_excess(feature_market).squeeze(1))
+        if self.base_model == "LSTM":
+            hidden = [torch.cat([hidden_excess[i], hidden_market[i]], -1) for i in range(2)]
+        else:
+            hidden = torch.cat([hidden_excess, hidden_market], -1)
+        x = torch.zeros_like(x[:, 1, :])
+        reconstructed_feature = []
+        for i in range(T):
+            x, hidden = self.dec(x, hidden)
+            reconstructed_feature.append(x)
+        reconstructed_feature = torch.stack(reconstructed_feature, 1)
+        predicts["reconstructed_feature"] = reconstructed_feature
+        return predicts
+
+
+class Decoder(nn.Module):
+    def __init__(self, d_feat=6, hidden_size=128, num_layers=1, dropout=0.5, base_model="GRU"):
+        super().__init__()
+        self.base_model = base_model
+        if base_model == "GRU":
+            self.rnn = nn.GRU(
+                input_size=d_feat,
+                hidden_size=hidden_size,
+                num_layers=num_layers,
+                batch_first=True,
+                dropout=dropout,
+            )
+        elif base_model == "LSTM":
+            self.rnn = nn.LSTM(
+                input_size=d_feat,
+                hidden_size=hidden_size,
+                num_layers=num_layers,
+                batch_first=True,
+                dropout=dropout,
+            )
+        else:
+            raise ValueError("unknown base model name `%s`" % base_model)
+
+        self.fc = nn.Linear(hidden_size, d_feat)
+
+    def forward(self, x, hidden):
+        x = x.unsqueeze(1)
+        output, hidden = self.rnn(x, hidden)
+        output = output.squeeze(1)
+        pred = self.fc(output)
+        return pred, hidden
+
+
+class RevGradFunc(Function):
+    @staticmethod
+    def forward(ctx, input_, alpha_):
+        ctx.save_for_backward(input_, alpha_)
+        output = input_
+        return output
+
+    @staticmethod
+    def backward(ctx, grad_output):  # pragma: no cover
+        grad_input = None
+        _, alpha_ = ctx.saved_tensors
+        if ctx.needs_input_grad[0]:
+            grad_input = -grad_output * alpha_
+        return grad_input, None
+
+
+class RevGrad(nn.Module):
+    def __init__(self, gamma=0.1, gamma_clip=0.4, *args, **kwargs):
+        """
+        A gradient reversal layer.
+        This layer has no parameters, and simply reverses the gradient
+        in the backward pass.
+        """
+        super().__init__(*args, **kwargs)
+
+        self.gamma = gamma
+        self.gamma_clip = torch.tensor(float(gamma_clip), requires_grad=False)
+        self._alpha = torch.tensor(0, requires_grad=False)
+        self._p = 0
+
+    def step_alpha(self):
+        self._p += 1
+        self._alpha = min(
+            self.gamma_clip, torch.tensor(2 / (1 + math.exp(-self.gamma * self._p)) - 1, requires_grad=False)
+        )
+
+    def forward(self, input_):
+        return RevGradFunc.apply(input_, self._alpha)
```

## qlib/contrib/model/pytorch_alstm.py

```diff
@@ -1,351 +1,344 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-
-from __future__ import division
-from __future__ import print_function
-
-import numpy as np
-import pandas as pd
-from typing import Text, Union
-import copy
-from ...utils import get_or_create_path
-from ...log import get_module_logger
-
-import torch
-import torch.nn as nn
-import torch.optim as optim
-
-from .pytorch_utils import count_parameters
-from ...model.base import Model
-from ...data.dataset import DatasetH
-from ...data.dataset.handler import DataHandlerLP
-
-
-class ALSTM(Model):
-    """ALSTM Model
-
-    Parameters
-    ----------
-    d_feat : int
-        input dimension for each time step
-    metric: str
-        the evaluation metric used in early stop
-    optimizer : str
-        optimizer name
-    GPU : int
-        the GPU ID used for training
-    """
-
-    def __init__(
-        self,
-        d_feat=6,
-        hidden_size=64,
-        num_layers=2,
-        dropout=0.0,
-        n_epochs=200,
-        lr=0.001,
-        metric="",
-        batch_size=2000,
-        early_stop=20,
-        loss="mse",
-        optimizer="adam",
-        GPU=0,
-        seed=None,
-        **kwargs
-    ):
-        # Set logger.
-        self.logger = get_module_logger("ALSTM")
-        self.logger.info("ALSTM pytorch version...")
-
-        # set hyper-parameters.
-        self.d_feat = d_feat
-        self.hidden_size = hidden_size
-        self.num_layers = num_layers
-        self.dropout = dropout
-        self.n_epochs = n_epochs
-        self.lr = lr
-        self.metric = metric
-        self.batch_size = batch_size
-        self.early_stop = early_stop
-        self.optimizer = optimizer.lower()
-        self.loss = loss
-        self.device = torch.device("cuda:%d" % (GPU) if torch.cuda.is_available() and GPU >= 0 else "cpu")
-        self.seed = seed
-
-        self.logger.info(
-            "ALSTM parameters setting:"
-            "\nd_feat : {}"
-            "\nhidden_size : {}"
-            "\nnum_layers : {}"
-            "\ndropout : {}"
-            "\nn_epochs : {}"
-            "\nlr : {}"
-            "\nmetric : {}"
-            "\nbatch_size : {}"
-            "\nearly_stop : {}"
-            "\noptimizer : {}"
-            "\nloss_type : {}"
-            "\ndevice : {}"
-            "\nuse_GPU : {}"
-            "\nseed : {}".format(
-                d_feat,
-                hidden_size,
-                num_layers,
-                dropout,
-                n_epochs,
-                lr,
-                metric,
-                batch_size,
-                early_stop,
-                optimizer.lower(),
-                loss,
-                self.device,
-                self.use_gpu,
-                seed,
-            )
-        )
-
-        if self.seed is not None:
-            np.random.seed(self.seed)
-            torch.manual_seed(self.seed)
-
-        self.ALSTM_model = ALSTMModel(
-            d_feat=self.d_feat,
-            hidden_size=self.hidden_size,
-            num_layers=self.num_layers,
-            dropout=self.dropout,
-        )
-        self.logger.info("model:\n{:}".format(self.ALSTM_model))
-        self.logger.info("model size: {:.4f} MB".format(count_parameters(self.ALSTM_model)))
-
-        if optimizer.lower() == "adam":
-            self.train_optimizer = optim.Adam(self.ALSTM_model.parameters(), lr=self.lr)
-        elif optimizer.lower() == "gd":
-            self.train_optimizer = optim.SGD(self.ALSTM_model.parameters(), lr=self.lr)
-        else:
-            raise NotImplementedError("optimizer {} is not supported!".format(optimizer))
-
-        self.fitted = False
-        self.ALSTM_model.to(self.device)
-
-    @property
-    def use_gpu(self):
-        return self.device != torch.device("cpu")
-
-    def mse(self, pred, label):
-        loss = (pred - label) ** 2
-        return torch.mean(loss)
-
-    def loss_fn(self, pred, label):
-        mask = ~torch.isnan(label)
-
-        if self.loss == "mse":
-            return self.mse(pred[mask], label[mask])
-
-        raise ValueError("unknown loss `%s`" % self.loss)
-
-    def metric_fn(self, pred, label):
-
-        mask = torch.isfinite(label)
-
-        if self.metric in ("", "loss"):
-            return -self.loss_fn(pred[mask], label[mask])
-
-        raise ValueError("unknown metric `%s`" % self.metric)
-
-    def train_epoch(self, x_train, y_train):
-
-        x_train_values = x_train.values
-        y_train_values = np.squeeze(y_train.values)
-
-        self.ALSTM_model.train()
-
-        indices = np.arange(len(x_train_values))
-        np.random.shuffle(indices)
-
-        for i in range(len(indices))[:: self.batch_size]:
-
-            if len(indices) - i < self.batch_size:
-                break
-
-            feature = torch.from_numpy(x_train_values[indices[i : i + self.batch_size]]).float().to(self.device)
-            label = torch.from_numpy(y_train_values[indices[i : i + self.batch_size]]).float().to(self.device)
-
-            pred = self.ALSTM_model(feature)
-            loss = self.loss_fn(pred, label)
-
-            self.train_optimizer.zero_grad()
-            loss.backward()
-            torch.nn.utils.clip_grad_value_(self.ALSTM_model.parameters(), 3.0)
-            self.train_optimizer.step()
-
-    def test_epoch(self, data_x, data_y):
-
-        # prepare training data
-        x_values = data_x.values
-        y_values = np.squeeze(data_y.values)
-
-        self.ALSTM_model.eval()
-
-        scores = []
-        losses = []
-
-        indices = np.arange(len(x_values))
-
-        for i in range(len(indices))[:: self.batch_size]:
-
-            if len(indices) - i < self.batch_size:
-                break
-
-            feature = torch.from_numpy(x_values[indices[i : i + self.batch_size]]).float().to(self.device)
-            label = torch.from_numpy(y_values[indices[i : i + self.batch_size]]).float().to(self.device)
-
-            with torch.no_grad():
-                pred = self.ALSTM_model(feature)
-                loss = self.loss_fn(pred, label)
-                losses.append(loss.item())
-
-                score = self.metric_fn(pred, label)
-                scores.append(score.item())
-
-        return np.mean(losses), np.mean(scores)
-
-    def fit(
-        self,
-        dataset: DatasetH,
-        evals_result=dict(),
-        save_path=None,
-    ):
-
-        df_train, df_valid, df_test = dataset.prepare(
-            ["train", "valid", "test"],
-            col_set=["feature", "label"],
-            data_key=DataHandlerLP.DK_L,
-        )
-        if df_train.empty or df_valid.empty:
-            raise ValueError("Empty data from dataset, please check your dataset config.")
-
-        x_train, y_train = df_train["feature"], df_train["label"]
-        x_valid, y_valid = df_valid["feature"], df_valid["label"]
-
-        save_path = get_or_create_path(save_path)
-        stop_steps = 0
-        train_loss = 0
-        best_score = -np.inf
-        best_epoch = 0
-        evals_result["train"] = []
-        evals_result["valid"] = []
-
-        # train
-        self.logger.info("training...")
-        self.fitted = True
-
-        for step in range(self.n_epochs):
-            self.logger.info("Epoch%d:", step)
-            self.logger.info("training...")
-            self.train_epoch(x_train, y_train)
-            self.logger.info("evaluating...")
-            train_loss, train_score = self.test_epoch(x_train, y_train)
-            val_loss, val_score = self.test_epoch(x_valid, y_valid)
-            self.logger.info("train %.6f, valid %.6f" % (train_score, val_score))
-            evals_result["train"].append(train_score)
-            evals_result["valid"].append(val_score)
-
-            if val_score > best_score:
-                best_score = val_score
-                stop_steps = 0
-                best_epoch = step
-                best_param = copy.deepcopy(self.ALSTM_model.state_dict())
-            else:
-                stop_steps += 1
-                if stop_steps >= self.early_stop:
-                    self.logger.info("early stop")
-                    break
-
-        self.logger.info("best score: %.6lf @ %d" % (best_score, best_epoch))
-        self.ALSTM_model.load_state_dict(best_param)
-        torch.save(best_param, save_path)
-
-        if self.use_gpu:
-            torch.cuda.empty_cache()
-
-    def predict(self, dataset: DatasetH, segment: Union[Text, slice] = "test"):
-        if not self.fitted:
-            raise ValueError("model is not fitted yet!")
-
-        x_test = dataset.prepare(segment, col_set="feature", data_key=DataHandlerLP.DK_I)
-        index = x_test.index
-        self.ALSTM_model.eval()
-        x_values = x_test.values
-        sample_num = x_values.shape[0]
-        preds = []
-
-        for begin in range(sample_num)[:: self.batch_size]:
-
-            if sample_num - begin < self.batch_size:
-                end = sample_num
-            else:
-                end = begin + self.batch_size
-
-            x_batch = torch.from_numpy(x_values[begin:end]).float().to(self.device)
-
-            with torch.no_grad():
-                pred = self.ALSTM_model(x_batch).detach().cpu().numpy()
-
-            preds.append(pred)
-
-        return pd.Series(np.concatenate(preds), index=index)
-
-
-class ALSTMModel(nn.Module):
-    def __init__(self, d_feat=6, hidden_size=64, num_layers=2, dropout=0.0, rnn_type="GRU"):
-        super().__init__()
-        self.hid_size = hidden_size
-        self.input_size = d_feat
-        self.dropout = dropout
-        self.rnn_type = rnn_type
-        self.rnn_layer = num_layers
-        self._build_model()
-
-    def _build_model(self):
-        try:
-            klass = getattr(nn, self.rnn_type.upper())
-        except Exception as e:
-            raise ValueError("unknown rnn_type `%s`" % self.rnn_type) from e
-        self.net = nn.Sequential()
-        self.net.add_module("fc_in", nn.Linear(in_features=self.input_size, out_features=self.hid_size))
-        self.net.add_module("act", nn.Tanh())
-        self.rnn = klass(
-            input_size=self.hid_size,
-            hidden_size=self.hid_size,
-            num_layers=self.rnn_layer,
-            batch_first=True,
-            dropout=self.dropout,
-        )
-        self.fc_out = nn.Linear(in_features=self.hid_size * 2, out_features=1)
-        self.att_net = nn.Sequential()
-        self.att_net.add_module(
-            "att_fc_in",
-            nn.Linear(in_features=self.hid_size, out_features=int(self.hid_size / 2)),
-        )
-        self.att_net.add_module("att_dropout", torch.nn.Dropout(self.dropout))
-        self.att_net.add_module("att_act", nn.Tanh())
-        self.att_net.add_module(
-            "att_fc_out",
-            nn.Linear(in_features=int(self.hid_size / 2), out_features=1, bias=False),
-        )
-        self.att_net.add_module("att_softmax", nn.Softmax(dim=1))
-
-    def forward(self, inputs):
-        # inputs: [batch_size, input_size*input_day]
-        inputs = inputs.view(len(inputs), self.input_size, -1)
-        inputs = inputs.permute(0, 2, 1)  # [batch, input_size, seq_len] -> [batch, seq_len, input_size]
-        rnn_out, _ = self.rnn(self.net(inputs))  # [batch, seq_len, num_directions * hidden_size]
-        attention_score = self.att_net(rnn_out)  # [batch, seq_len, 1]
-        out_att = torch.mul(rnn_out, attention_score)
-        out_att = torch.sum(out_att, dim=1)
-        out = self.fc_out(
-            torch.cat((rnn_out[:, -1, :], out_att), dim=1)
-        )  # [batch, seq_len, num_directions * hidden_size] -> [batch, 1]
-        return out[..., 0]
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+
+from __future__ import division
+from __future__ import print_function
+
+import numpy as np
+import pandas as pd
+from typing import Text, Union
+import copy
+from ...utils import get_or_create_path
+from ...log import get_module_logger
+
+import torch
+import torch.nn as nn
+import torch.optim as optim
+
+from .pytorch_utils import count_parameters
+from ...model.base import Model
+from ...data.dataset import DatasetH
+from ...data.dataset.handler import DataHandlerLP
+
+
+class ALSTM(Model):
+    """ALSTM Model
+
+    Parameters
+    ----------
+    d_feat : int
+        input dimension for each time step
+    metric: str
+        the evaluation metric used in early stop
+    optimizer : str
+        optimizer name
+    GPU : int
+        the GPU ID used for training
+    """
+
+    def __init__(
+        self,
+        d_feat=6,
+        hidden_size=64,
+        num_layers=2,
+        dropout=0.0,
+        n_epochs=200,
+        lr=0.001,
+        metric="",
+        batch_size=2000,
+        early_stop=20,
+        loss="mse",
+        optimizer="adam",
+        GPU=0,
+        seed=None,
+        **kwargs
+    ):
+        # Set logger.
+        self.logger = get_module_logger("ALSTM")
+        self.logger.info("ALSTM pytorch version...")
+
+        # set hyper-parameters.
+        self.d_feat = d_feat
+        self.hidden_size = hidden_size
+        self.num_layers = num_layers
+        self.dropout = dropout
+        self.n_epochs = n_epochs
+        self.lr = lr
+        self.metric = metric
+        self.batch_size = batch_size
+        self.early_stop = early_stop
+        self.optimizer = optimizer.lower()
+        self.loss = loss
+        self.device = torch.device("cuda:%d" % (GPU) if torch.cuda.is_available() and GPU >= 0 else "cpu")
+        self.seed = seed
+
+        self.logger.info(
+            "ALSTM parameters setting:"
+            "\nd_feat : {}"
+            "\nhidden_size : {}"
+            "\nnum_layers : {}"
+            "\ndropout : {}"
+            "\nn_epochs : {}"
+            "\nlr : {}"
+            "\nmetric : {}"
+            "\nbatch_size : {}"
+            "\nearly_stop : {}"
+            "\noptimizer : {}"
+            "\nloss_type : {}"
+            "\ndevice : {}"
+            "\nuse_GPU : {}"
+            "\nseed : {}".format(
+                d_feat,
+                hidden_size,
+                num_layers,
+                dropout,
+                n_epochs,
+                lr,
+                metric,
+                batch_size,
+                early_stop,
+                optimizer.lower(),
+                loss,
+                self.device,
+                self.use_gpu,
+                seed,
+            )
+        )
+
+        if self.seed is not None:
+            np.random.seed(self.seed)
+            torch.manual_seed(self.seed)
+
+        self.ALSTM_model = ALSTMModel(
+            d_feat=self.d_feat,
+            hidden_size=self.hidden_size,
+            num_layers=self.num_layers,
+            dropout=self.dropout,
+        )
+        self.logger.info("model:\n{:}".format(self.ALSTM_model))
+        self.logger.info("model size: {:.4f} MB".format(count_parameters(self.ALSTM_model)))
+
+        if optimizer.lower() == "adam":
+            self.train_optimizer = optim.Adam(self.ALSTM_model.parameters(), lr=self.lr)
+        elif optimizer.lower() == "gd":
+            self.train_optimizer = optim.SGD(self.ALSTM_model.parameters(), lr=self.lr)
+        else:
+            raise NotImplementedError("optimizer {} is not supported!".format(optimizer))
+
+        self.fitted = False
+        self.ALSTM_model.to(self.device)
+
+    @property
+    def use_gpu(self):
+        return self.device != torch.device("cpu")
+
+    def mse(self, pred, label):
+        loss = (pred - label) ** 2
+        return torch.mean(loss)
+
+    def loss_fn(self, pred, label):
+        mask = ~torch.isnan(label)
+
+        if self.loss == "mse":
+            return self.mse(pred[mask], label[mask])
+
+        raise ValueError("unknown loss `%s`" % self.loss)
+
+    def metric_fn(self, pred, label):
+        mask = torch.isfinite(label)
+
+        if self.metric in ("", "loss"):
+            return -self.loss_fn(pred[mask], label[mask])
+
+        raise ValueError("unknown metric `%s`" % self.metric)
+
+    def train_epoch(self, x_train, y_train):
+        x_train_values = x_train.values
+        y_train_values = np.squeeze(y_train.values)
+
+        self.ALSTM_model.train()
+
+        indices = np.arange(len(x_train_values))
+        np.random.shuffle(indices)
+
+        for i in range(len(indices))[:: self.batch_size]:
+            if len(indices) - i < self.batch_size:
+                break
+
+            feature = torch.from_numpy(x_train_values[indices[i : i + self.batch_size]]).float().to(self.device)
+            label = torch.from_numpy(y_train_values[indices[i : i + self.batch_size]]).float().to(self.device)
+
+            pred = self.ALSTM_model(feature)
+            loss = self.loss_fn(pred, label)
+
+            self.train_optimizer.zero_grad()
+            loss.backward()
+            torch.nn.utils.clip_grad_value_(self.ALSTM_model.parameters(), 3.0)
+            self.train_optimizer.step()
+
+    def test_epoch(self, data_x, data_y):
+        # prepare training data
+        x_values = data_x.values
+        y_values = np.squeeze(data_y.values)
+
+        self.ALSTM_model.eval()
+
+        scores = []
+        losses = []
+
+        indices = np.arange(len(x_values))
+
+        for i in range(len(indices))[:: self.batch_size]:
+            if len(indices) - i < self.batch_size:
+                break
+
+            feature = torch.from_numpy(x_values[indices[i : i + self.batch_size]]).float().to(self.device)
+            label = torch.from_numpy(y_values[indices[i : i + self.batch_size]]).float().to(self.device)
+
+            with torch.no_grad():
+                pred = self.ALSTM_model(feature)
+                loss = self.loss_fn(pred, label)
+                losses.append(loss.item())
+
+                score = self.metric_fn(pred, label)
+                scores.append(score.item())
+
+        return np.mean(losses), np.mean(scores)
+
+    def fit(
+        self,
+        dataset: DatasetH,
+        evals_result=dict(),
+        save_path=None,
+    ):
+        df_train, df_valid, df_test = dataset.prepare(
+            ["train", "valid", "test"],
+            col_set=["feature", "label"],
+            data_key=DataHandlerLP.DK_L,
+        )
+        if df_train.empty or df_valid.empty:
+            raise ValueError("Empty data from dataset, please check your dataset config.")
+
+        x_train, y_train = df_train["feature"], df_train["label"]
+        x_valid, y_valid = df_valid["feature"], df_valid["label"]
+
+        save_path = get_or_create_path(save_path)
+        stop_steps = 0
+        train_loss = 0
+        best_score = -np.inf
+        best_epoch = 0
+        evals_result["train"] = []
+        evals_result["valid"] = []
+
+        # train
+        self.logger.info("training...")
+        self.fitted = True
+
+        for step in range(self.n_epochs):
+            self.logger.info("Epoch%d:", step)
+            self.logger.info("training...")
+            self.train_epoch(x_train, y_train)
+            self.logger.info("evaluating...")
+            train_loss, train_score = self.test_epoch(x_train, y_train)
+            val_loss, val_score = self.test_epoch(x_valid, y_valid)
+            self.logger.info("train %.6f, valid %.6f" % (train_score, val_score))
+            evals_result["train"].append(train_score)
+            evals_result["valid"].append(val_score)
+
+            if val_score > best_score:
+                best_score = val_score
+                stop_steps = 0
+                best_epoch = step
+                best_param = copy.deepcopy(self.ALSTM_model.state_dict())
+            else:
+                stop_steps += 1
+                if stop_steps >= self.early_stop:
+                    self.logger.info("early stop")
+                    break
+
+        self.logger.info("best score: %.6lf @ %d" % (best_score, best_epoch))
+        self.ALSTM_model.load_state_dict(best_param)
+        torch.save(best_param, save_path)
+
+        if self.use_gpu:
+            torch.cuda.empty_cache()
+
+    def predict(self, dataset: DatasetH, segment: Union[Text, slice] = "test"):
+        if not self.fitted:
+            raise ValueError("model is not fitted yet!")
+
+        x_test = dataset.prepare(segment, col_set="feature", data_key=DataHandlerLP.DK_I)
+        index = x_test.index
+        self.ALSTM_model.eval()
+        x_values = x_test.values
+        sample_num = x_values.shape[0]
+        preds = []
+
+        for begin in range(sample_num)[:: self.batch_size]:
+            if sample_num - begin < self.batch_size:
+                end = sample_num
+            else:
+                end = begin + self.batch_size
+
+            x_batch = torch.from_numpy(x_values[begin:end]).float().to(self.device)
+
+            with torch.no_grad():
+                pred = self.ALSTM_model(x_batch).detach().cpu().numpy()
+
+            preds.append(pred)
+
+        return pd.Series(np.concatenate(preds), index=index)
+
+
+class ALSTMModel(nn.Module):
+    def __init__(self, d_feat=6, hidden_size=64, num_layers=2, dropout=0.0, rnn_type="GRU"):
+        super().__init__()
+        self.hid_size = hidden_size
+        self.input_size = d_feat
+        self.dropout = dropout
+        self.rnn_type = rnn_type
+        self.rnn_layer = num_layers
+        self._build_model()
+
+    def _build_model(self):
+        try:
+            klass = getattr(nn, self.rnn_type.upper())
+        except Exception as e:
+            raise ValueError("unknown rnn_type `%s`" % self.rnn_type) from e
+        self.net = nn.Sequential()
+        self.net.add_module("fc_in", nn.Linear(in_features=self.input_size, out_features=self.hid_size))
+        self.net.add_module("act", nn.Tanh())
+        self.rnn = klass(
+            input_size=self.hid_size,
+            hidden_size=self.hid_size,
+            num_layers=self.rnn_layer,
+            batch_first=True,
+            dropout=self.dropout,
+        )
+        self.fc_out = nn.Linear(in_features=self.hid_size * 2, out_features=1)
+        self.att_net = nn.Sequential()
+        self.att_net.add_module(
+            "att_fc_in",
+            nn.Linear(in_features=self.hid_size, out_features=int(self.hid_size / 2)),
+        )
+        self.att_net.add_module("att_dropout", torch.nn.Dropout(self.dropout))
+        self.att_net.add_module("att_act", nn.Tanh())
+        self.att_net.add_module(
+            "att_fc_out",
+            nn.Linear(in_features=int(self.hid_size / 2), out_features=1, bias=False),
+        )
+        self.att_net.add_module("att_softmax", nn.Softmax(dim=1))
+
+    def forward(self, inputs):
+        # inputs: [batch_size, input_size*input_day]
+        inputs = inputs.view(len(inputs), self.input_size, -1)
+        inputs = inputs.permute(0, 2, 1)  # [batch, input_size, seq_len] -> [batch, seq_len, input_size]
+        rnn_out, _ = self.rnn(self.net(inputs))  # [batch, seq_len, num_directions * hidden_size]
+        attention_score = self.att_net(rnn_out)  # [batch, seq_len, 1]
+        out_att = torch.mul(rnn_out, attention_score)
+        out_att = torch.sum(out_att, dim=1)
+        out = self.fc_out(
+            torch.cat((rnn_out[:, -1, :], out_att), dim=1)
+        )  # [batch, seq_len, num_directions * hidden_size] -> [batch, 1]
+        return out[..., 0]
```

## qlib/contrib/model/pytorch_alstm_ts.py

```diff
@@ -1,356 +1,351 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-
-from __future__ import division
-from __future__ import print_function
-
-import numpy as np
-import pandas as pd
-from typing import Text, Union
-import copy
-from ...utils import get_or_create_path
-from ...log import get_module_logger
-
-import torch
-import torch.nn as nn
-import torch.optim as optim
-from torch.utils.data import DataLoader
-
-from .pytorch_utils import count_parameters
-from ...model.base import Model
-from ...data.dataset import DatasetH
-from ...data.dataset.handler import DataHandlerLP
-from ...model.utils import ConcatDataset
-from ...data.dataset.weight import Reweighter
-
-
-class ALSTM(Model):
-    """ALSTM Model
-
-    Parameters
-    ----------
-    d_feat : int
-        input dimension for each time step
-    metric: str
-        the evaluation metric used in early stop
-    optimizer : str
-        optimizer name
-    GPU : int
-        the GPU ID used for training
-    """
-
-    def __init__(
-        self,
-        d_feat=6,
-        hidden_size=64,
-        num_layers=2,
-        dropout=0.0,
-        n_epochs=200,
-        lr=0.001,
-        metric="",
-        batch_size=2000,
-        early_stop=20,
-        loss="mse",
-        optimizer="adam",
-        n_jobs=10,
-        GPU=0,
-        seed=None,
-        **kwargs
-    ):
-        # Set logger.
-        self.logger = get_module_logger("ALSTM")
-        self.logger.info("ALSTM pytorch version...")
-
-        # set hyper-parameters.
-        self.d_feat = d_feat
-        self.hidden_size = hidden_size
-        self.num_layers = num_layers
-        self.dropout = dropout
-        self.n_epochs = n_epochs
-        self.lr = lr
-        self.metric = metric
-        self.batch_size = batch_size
-        self.early_stop = early_stop
-        self.optimizer = optimizer.lower()
-        self.loss = loss
-        self.device = torch.device("cuda:%d" % (GPU) if torch.cuda.is_available() and GPU >= 0 else "cpu")
-        self.n_jobs = n_jobs
-        self.seed = seed
-
-        self.logger.info(
-            "ALSTM parameters setting:"
-            "\nd_feat : {}"
-            "\nhidden_size : {}"
-            "\nnum_layers : {}"
-            "\ndropout : {}"
-            "\nn_epochs : {}"
-            "\nlr : {}"
-            "\nmetric : {}"
-            "\nbatch_size : {}"
-            "\nearly_stop : {}"
-            "\noptimizer : {}"
-            "\nloss_type : {}"
-            "\ndevice : {}"
-            "\nn_jobs : {}"
-            "\nuse_GPU : {}"
-            "\nseed : {}".format(
-                d_feat,
-                hidden_size,
-                num_layers,
-                dropout,
-                n_epochs,
-                lr,
-                metric,
-                batch_size,
-                early_stop,
-                optimizer.lower(),
-                loss,
-                self.device,
-                n_jobs,
-                self.use_gpu,
-                seed,
-            )
-        )
-
-        if self.seed is not None:
-            np.random.seed(self.seed)
-            torch.manual_seed(self.seed)
-
-        self.ALSTM_model = ALSTMModel(
-            d_feat=self.d_feat,
-            hidden_size=self.hidden_size,
-            num_layers=self.num_layers,
-            dropout=self.dropout,
-        )
-        self.logger.info("model:\n{:}".format(self.ALSTM_model))
-        self.logger.info("model size: {:.4f} MB".format(count_parameters(self.ALSTM_model)))
-
-        if optimizer.lower() == "adam":
-            self.train_optimizer = optim.Adam(self.ALSTM_model.parameters(), lr=self.lr)
-        elif optimizer.lower() == "gd":
-            self.train_optimizer = optim.SGD(self.ALSTM_model.parameters(), lr=self.lr)
-        else:
-            raise NotImplementedError("optimizer {} is not supported!".format(optimizer))
-
-        self.fitted = False
-        self.ALSTM_model.to(self.device)
-
-    @property
-    def use_gpu(self):
-        return self.device != torch.device("cpu")
-
-    def mse(self, pred, label, weight):
-        loss = weight * (pred - label) ** 2
-        return torch.mean(loss)
-
-    def loss_fn(self, pred, label, weight=None):
-        mask = ~torch.isnan(label)
-
-        if weight is None:
-            weight = torch.ones_like(label)
-
-        if self.loss == "mse":
-            return self.mse(pred[mask], label[mask], weight[mask])
-
-        raise ValueError("unknown loss `%s`" % self.loss)
-
-    def metric_fn(self, pred, label):
-
-        mask = torch.isfinite(label)
-
-        if self.metric in ("", "loss"):
-            return -self.loss_fn(pred[mask], label[mask])
-
-        raise ValueError("unknown metric `%s`" % self.metric)
-
-    def train_epoch(self, data_loader):
-
-        self.ALSTM_model.train()
-
-        for (data, weight) in data_loader:
-            feature = data[:, :, 0:-1].to(self.device)
-            label = data[:, -1, -1].to(self.device)
-
-            pred = self.ALSTM_model(feature.float())
-            loss = self.loss_fn(pred, label, weight.to(self.device))
-
-            self.train_optimizer.zero_grad()
-            loss.backward()
-            torch.nn.utils.clip_grad_value_(self.ALSTM_model.parameters(), 3.0)
-            self.train_optimizer.step()
-
-    def test_epoch(self, data_loader):
-
-        self.ALSTM_model.eval()
-
-        scores = []
-        losses = []
-
-        for (data, weight) in data_loader:
-
-            feature = data[:, :, 0:-1].to(self.device)
-            # feature[torch.isnan(feature)] = 0
-            label = data[:, -1, -1].to(self.device)
-
-            with torch.no_grad():
-                pred = self.ALSTM_model(feature.float())
-                loss = self.loss_fn(pred, label, weight.to(self.device))
-                losses.append(loss.item())
-
-                score = self.metric_fn(pred, label)
-                scores.append(score.item())
-
-        return np.mean(losses), np.mean(scores)
-
-    def fit(
-        self,
-        dataset,
-        evals_result=dict(),
-        save_path=None,
-        reweighter=None,
-    ):
-        dl_train = dataset.prepare("train", col_set=["feature", "label"], data_key=DataHandlerLP.DK_L)
-        dl_valid = dataset.prepare("valid", col_set=["feature", "label"], data_key=DataHandlerLP.DK_L)
-        if dl_train.empty or dl_valid.empty:
-            raise ValueError("Empty data from dataset, please check your dataset config.")
-
-        dl_train.config(fillna_type="ffill+bfill")  # process nan brought by dataloader
-        dl_valid.config(fillna_type="ffill+bfill")  # process nan brought by dataloader
-
-        if reweighter is None:
-            wl_train = np.ones(len(dl_train))
-            wl_valid = np.ones(len(dl_valid))
-        elif isinstance(reweighter, Reweighter):
-            wl_train = reweighter.reweight(dl_train)
-            wl_valid = reweighter.reweight(dl_valid)
-        else:
-            raise ValueError("Unsupported reweighter type.")
-
-        train_loader = DataLoader(
-            ConcatDataset(dl_train, wl_train),
-            batch_size=self.batch_size,
-            shuffle=True,
-            num_workers=self.n_jobs,
-            drop_last=True,
-        )
-        valid_loader = DataLoader(
-            ConcatDataset(dl_valid, wl_valid),
-            batch_size=self.batch_size,
-            shuffle=False,
-            num_workers=self.n_jobs,
-            drop_last=True,
-        )
-
-        save_path = get_or_create_path(save_path)
-
-        stop_steps = 0
-        train_loss = 0
-        best_score = -np.inf
-        best_epoch = 0
-        evals_result["train"] = []
-        evals_result["valid"] = []
-
-        # train
-        self.logger.info("training...")
-        self.fitted = True
-
-        for step in range(self.n_epochs):
-            self.logger.info("Epoch%d:", step)
-            self.logger.info("training...")
-            self.train_epoch(train_loader)
-            self.logger.info("evaluating...")
-            train_loss, train_score = self.test_epoch(train_loader)
-            val_loss, val_score = self.test_epoch(valid_loader)
-            self.logger.info("train %.6f, valid %.6f" % (train_score, val_score))
-            evals_result["train"].append(train_score)
-            evals_result["valid"].append(val_score)
-
-            if val_score > best_score:
-                best_score = val_score
-                stop_steps = 0
-                best_epoch = step
-                best_param = copy.deepcopy(self.ALSTM_model.state_dict())
-            else:
-                stop_steps += 1
-                if stop_steps >= self.early_stop:
-                    self.logger.info("early stop")
-                    break
-
-        self.logger.info("best score: %.6lf @ %d" % (best_score, best_epoch))
-        self.ALSTM_model.load_state_dict(best_param)
-        torch.save(best_param, save_path)
-
-        if self.use_gpu:
-            torch.cuda.empty_cache()
-
-    def predict(self, dataset: DatasetH, segment: Union[Text, slice] = "test"):
-        if not self.fitted:
-            raise ValueError("model is not fitted yet!")
-
-        dl_test = dataset.prepare(segment, col_set=["feature", "label"], data_key=DataHandlerLP.DK_I)
-        dl_test.config(fillna_type="ffill+bfill")
-        test_loader = DataLoader(dl_test, batch_size=self.batch_size, num_workers=self.n_jobs)
-        self.ALSTM_model.eval()
-        preds = []
-
-        for data in test_loader:
-
-            feature = data[:, :, 0:-1].to(self.device)
-
-            with torch.no_grad():
-                pred = self.ALSTM_model(feature.float()).detach().cpu().numpy()
-
-            preds.append(pred)
-
-        return pd.Series(np.concatenate(preds), index=dl_test.get_index())
-
-
-class ALSTMModel(nn.Module):
-    def __init__(self, d_feat=6, hidden_size=64, num_layers=2, dropout=0.0, rnn_type="GRU"):
-        super().__init__()
-        self.hid_size = hidden_size
-        self.input_size = d_feat
-        self.dropout = dropout
-        self.rnn_type = rnn_type
-        self.rnn_layer = num_layers
-        self._build_model()
-
-    def _build_model(self):
-        try:
-            klass = getattr(nn, self.rnn_type.upper())
-        except Exception as e:
-            raise ValueError("unknown rnn_type `%s`" % self.rnn_type) from e
-        self.net = nn.Sequential()
-        self.net.add_module("fc_in", nn.Linear(in_features=self.input_size, out_features=self.hid_size))
-        self.net.add_module("act", nn.Tanh())
-        self.rnn = klass(
-            input_size=self.hid_size,
-            hidden_size=self.hid_size,
-            num_layers=self.rnn_layer,
-            batch_first=True,
-            dropout=self.dropout,
-        )
-        self.fc_out = nn.Linear(in_features=self.hid_size * 2, out_features=1)
-        self.att_net = nn.Sequential()
-        self.att_net.add_module(
-            "att_fc_in",
-            nn.Linear(in_features=self.hid_size, out_features=int(self.hid_size / 2)),
-        )
-        self.att_net.add_module("att_dropout", torch.nn.Dropout(self.dropout))
-        self.att_net.add_module("att_act", nn.Tanh())
-        self.att_net.add_module(
-            "att_fc_out",
-            nn.Linear(in_features=int(self.hid_size / 2), out_features=1, bias=False),
-        )
-        self.att_net.add_module("att_softmax", nn.Softmax(dim=1))
-
-    def forward(self, inputs):
-        rnn_out, _ = self.rnn(self.net(inputs))  # [batch, seq_len, num_directions * hidden_size]
-        attention_score = self.att_net(rnn_out)  # [batch, seq_len, 1]
-        out_att = torch.mul(rnn_out, attention_score)
-        out_att = torch.sum(out_att, dim=1)
-        out = self.fc_out(
-            torch.cat((rnn_out[:, -1, :], out_att), dim=1)
-        )  # [batch, seq_len, num_directions * hidden_size] -> [batch, 1]
-        return out[..., 0]
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+
+from __future__ import division
+from __future__ import print_function
+
+import numpy as np
+import pandas as pd
+from typing import Text, Union
+import copy
+from ...utils import get_or_create_path
+from ...log import get_module_logger
+
+import torch
+import torch.nn as nn
+import torch.optim as optim
+from torch.utils.data import DataLoader
+
+from .pytorch_utils import count_parameters
+from ...model.base import Model
+from ...data.dataset import DatasetH
+from ...data.dataset.handler import DataHandlerLP
+from ...model.utils import ConcatDataset
+from ...data.dataset.weight import Reweighter
+
+
+class ALSTM(Model):
+    """ALSTM Model
+
+    Parameters
+    ----------
+    d_feat : int
+        input dimension for each time step
+    metric: str
+        the evaluation metric used in early stop
+    optimizer : str
+        optimizer name
+    GPU : int
+        the GPU ID used for training
+    """
+
+    def __init__(
+        self,
+        d_feat=6,
+        hidden_size=64,
+        num_layers=2,
+        dropout=0.0,
+        n_epochs=200,
+        lr=0.001,
+        metric="",
+        batch_size=2000,
+        early_stop=20,
+        loss="mse",
+        optimizer="adam",
+        n_jobs=10,
+        GPU=0,
+        seed=None,
+        **kwargs
+    ):
+        # Set logger.
+        self.logger = get_module_logger("ALSTM")
+        self.logger.info("ALSTM pytorch version...")
+
+        # set hyper-parameters.
+        self.d_feat = d_feat
+        self.hidden_size = hidden_size
+        self.num_layers = num_layers
+        self.dropout = dropout
+        self.n_epochs = n_epochs
+        self.lr = lr
+        self.metric = metric
+        self.batch_size = batch_size
+        self.early_stop = early_stop
+        self.optimizer = optimizer.lower()
+        self.loss = loss
+        self.device = torch.device("cuda:%d" % (GPU) if torch.cuda.is_available() and GPU >= 0 else "cpu")
+        self.n_jobs = n_jobs
+        self.seed = seed
+
+        self.logger.info(
+            "ALSTM parameters setting:"
+            "\nd_feat : {}"
+            "\nhidden_size : {}"
+            "\nnum_layers : {}"
+            "\ndropout : {}"
+            "\nn_epochs : {}"
+            "\nlr : {}"
+            "\nmetric : {}"
+            "\nbatch_size : {}"
+            "\nearly_stop : {}"
+            "\noptimizer : {}"
+            "\nloss_type : {}"
+            "\ndevice : {}"
+            "\nn_jobs : {}"
+            "\nuse_GPU : {}"
+            "\nseed : {}".format(
+                d_feat,
+                hidden_size,
+                num_layers,
+                dropout,
+                n_epochs,
+                lr,
+                metric,
+                batch_size,
+                early_stop,
+                optimizer.lower(),
+                loss,
+                self.device,
+                n_jobs,
+                self.use_gpu,
+                seed,
+            )
+        )
+
+        if self.seed is not None:
+            np.random.seed(self.seed)
+            torch.manual_seed(self.seed)
+
+        self.ALSTM_model = ALSTMModel(
+            d_feat=self.d_feat,
+            hidden_size=self.hidden_size,
+            num_layers=self.num_layers,
+            dropout=self.dropout,
+        )
+        self.logger.info("model:\n{:}".format(self.ALSTM_model))
+        self.logger.info("model size: {:.4f} MB".format(count_parameters(self.ALSTM_model)))
+
+        if optimizer.lower() == "adam":
+            self.train_optimizer = optim.Adam(self.ALSTM_model.parameters(), lr=self.lr)
+        elif optimizer.lower() == "gd":
+            self.train_optimizer = optim.SGD(self.ALSTM_model.parameters(), lr=self.lr)
+        else:
+            raise NotImplementedError("optimizer {} is not supported!".format(optimizer))
+
+        self.fitted = False
+        self.ALSTM_model.to(self.device)
+
+    @property
+    def use_gpu(self):
+        return self.device != torch.device("cpu")
+
+    def mse(self, pred, label, weight):
+        loss = weight * (pred - label) ** 2
+        return torch.mean(loss)
+
+    def loss_fn(self, pred, label, weight=None):
+        mask = ~torch.isnan(label)
+
+        if weight is None:
+            weight = torch.ones_like(label)
+
+        if self.loss == "mse":
+            return self.mse(pred[mask], label[mask], weight[mask])
+
+        raise ValueError("unknown loss `%s`" % self.loss)
+
+    def metric_fn(self, pred, label):
+        mask = torch.isfinite(label)
+
+        if self.metric in ("", "loss"):
+            return -self.loss_fn(pred[mask], label[mask])
+
+        raise ValueError("unknown metric `%s`" % self.metric)
+
+    def train_epoch(self, data_loader):
+        self.ALSTM_model.train()
+
+        for data, weight in data_loader:
+            feature = data[:, :, 0:-1].to(self.device)
+            label = data[:, -1, -1].to(self.device)
+
+            pred = self.ALSTM_model(feature.float())
+            loss = self.loss_fn(pred, label, weight.to(self.device))
+
+            self.train_optimizer.zero_grad()
+            loss.backward()
+            torch.nn.utils.clip_grad_value_(self.ALSTM_model.parameters(), 3.0)
+            self.train_optimizer.step()
+
+    def test_epoch(self, data_loader):
+        self.ALSTM_model.eval()
+
+        scores = []
+        losses = []
+
+        for data, weight in data_loader:
+            feature = data[:, :, 0:-1].to(self.device)
+            # feature[torch.isnan(feature)] = 0
+            label = data[:, -1, -1].to(self.device)
+
+            with torch.no_grad():
+                pred = self.ALSTM_model(feature.float())
+                loss = self.loss_fn(pred, label, weight.to(self.device))
+                losses.append(loss.item())
+
+                score = self.metric_fn(pred, label)
+                scores.append(score.item())
+
+        return np.mean(losses), np.mean(scores)
+
+    def fit(
+        self,
+        dataset,
+        evals_result=dict(),
+        save_path=None,
+        reweighter=None,
+    ):
+        dl_train = dataset.prepare("train", col_set=["feature", "label"], data_key=DataHandlerLP.DK_L)
+        dl_valid = dataset.prepare("valid", col_set=["feature", "label"], data_key=DataHandlerLP.DK_L)
+        if dl_train.empty or dl_valid.empty:
+            raise ValueError("Empty data from dataset, please check your dataset config.")
+
+        dl_train.config(fillna_type="ffill+bfill")  # process nan brought by dataloader
+        dl_valid.config(fillna_type="ffill+bfill")  # process nan brought by dataloader
+
+        if reweighter is None:
+            wl_train = np.ones(len(dl_train))
+            wl_valid = np.ones(len(dl_valid))
+        elif isinstance(reweighter, Reweighter):
+            wl_train = reweighter.reweight(dl_train)
+            wl_valid = reweighter.reweight(dl_valid)
+        else:
+            raise ValueError("Unsupported reweighter type.")
+
+        train_loader = DataLoader(
+            ConcatDataset(dl_train, wl_train),
+            batch_size=self.batch_size,
+            shuffle=True,
+            num_workers=self.n_jobs,
+            drop_last=True,
+        )
+        valid_loader = DataLoader(
+            ConcatDataset(dl_valid, wl_valid),
+            batch_size=self.batch_size,
+            shuffle=False,
+            num_workers=self.n_jobs,
+            drop_last=True,
+        )
+
+        save_path = get_or_create_path(save_path)
+
+        stop_steps = 0
+        train_loss = 0
+        best_score = -np.inf
+        best_epoch = 0
+        evals_result["train"] = []
+        evals_result["valid"] = []
+
+        # train
+        self.logger.info("training...")
+        self.fitted = True
+
+        for step in range(self.n_epochs):
+            self.logger.info("Epoch%d:", step)
+            self.logger.info("training...")
+            self.train_epoch(train_loader)
+            self.logger.info("evaluating...")
+            train_loss, train_score = self.test_epoch(train_loader)
+            val_loss, val_score = self.test_epoch(valid_loader)
+            self.logger.info("train %.6f, valid %.6f" % (train_score, val_score))
+            evals_result["train"].append(train_score)
+            evals_result["valid"].append(val_score)
+
+            if val_score > best_score:
+                best_score = val_score
+                stop_steps = 0
+                best_epoch = step
+                best_param = copy.deepcopy(self.ALSTM_model.state_dict())
+            else:
+                stop_steps += 1
+                if stop_steps >= self.early_stop:
+                    self.logger.info("early stop")
+                    break
+
+        self.logger.info("best score: %.6lf @ %d" % (best_score, best_epoch))
+        self.ALSTM_model.load_state_dict(best_param)
+        torch.save(best_param, save_path)
+
+        if self.use_gpu:
+            torch.cuda.empty_cache()
+
+    def predict(self, dataset: DatasetH, segment: Union[Text, slice] = "test"):
+        if not self.fitted:
+            raise ValueError("model is not fitted yet!")
+
+        dl_test = dataset.prepare(segment, col_set=["feature", "label"], data_key=DataHandlerLP.DK_I)
+        dl_test.config(fillna_type="ffill+bfill")
+        test_loader = DataLoader(dl_test, batch_size=self.batch_size, num_workers=self.n_jobs)
+        self.ALSTM_model.eval()
+        preds = []
+
+        for data in test_loader:
+            feature = data[:, :, 0:-1].to(self.device)
+
+            with torch.no_grad():
+                pred = self.ALSTM_model(feature.float()).detach().cpu().numpy()
+
+            preds.append(pred)
+
+        return pd.Series(np.concatenate(preds), index=dl_test.get_index())
+
+
+class ALSTMModel(nn.Module):
+    def __init__(self, d_feat=6, hidden_size=64, num_layers=2, dropout=0.0, rnn_type="GRU"):
+        super().__init__()
+        self.hid_size = hidden_size
+        self.input_size = d_feat
+        self.dropout = dropout
+        self.rnn_type = rnn_type
+        self.rnn_layer = num_layers
+        self._build_model()
+
+    def _build_model(self):
+        try:
+            klass = getattr(nn, self.rnn_type.upper())
+        except Exception as e:
+            raise ValueError("unknown rnn_type `%s`" % self.rnn_type) from e
+        self.net = nn.Sequential()
+        self.net.add_module("fc_in", nn.Linear(in_features=self.input_size, out_features=self.hid_size))
+        self.net.add_module("act", nn.Tanh())
+        self.rnn = klass(
+            input_size=self.hid_size,
+            hidden_size=self.hid_size,
+            num_layers=self.rnn_layer,
+            batch_first=True,
+            dropout=self.dropout,
+        )
+        self.fc_out = nn.Linear(in_features=self.hid_size * 2, out_features=1)
+        self.att_net = nn.Sequential()
+        self.att_net.add_module(
+            "att_fc_in",
+            nn.Linear(in_features=self.hid_size, out_features=int(self.hid_size / 2)),
+        )
+        self.att_net.add_module("att_dropout", torch.nn.Dropout(self.dropout))
+        self.att_net.add_module("att_act", nn.Tanh())
+        self.att_net.add_module(
+            "att_fc_out",
+            nn.Linear(in_features=int(self.hid_size / 2), out_features=1, bias=False),
+        )
+        self.att_net.add_module("att_softmax", nn.Softmax(dim=1))
+
+    def forward(self, inputs):
+        rnn_out, _ = self.rnn(self.net(inputs))  # [batch, seq_len, num_directions * hidden_size]
+        attention_score = self.att_net(rnn_out)  # [batch, seq_len, 1]
+        out_att = torch.mul(rnn_out, attention_score)
+        out_att = torch.sum(out_att, dim=1)
+        out = self.fc_out(
+            torch.cat((rnn_out[:, -1, :], out_att), dim=1)
+        )  # [batch, seq_len, num_directions * hidden_size] -> [batch, 1]
+        return out[..., 0]
```

## qlib/contrib/model/pytorch_gats.py

```diff
@@ -1,388 +1,384 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-
-from __future__ import division
-from __future__ import print_function
-
-import numpy as np
-import pandas as pd
-from typing import Text, Union
-import copy
-from ...utils import get_or_create_path
-from ...log import get_module_logger
-import torch
-import torch.nn as nn
-import torch.optim as optim
-
-from .pytorch_utils import count_parameters
-from ...model.base import Model
-from ...data.dataset import DatasetH
-from ...data.dataset.handler import DataHandlerLP
-from ...contrib.model.pytorch_lstm import LSTMModel
-from ...contrib.model.pytorch_gru import GRUModel
-
-
-class GATs(Model):
-    """GATs Model
-
-    Parameters
-    ----------
-    lr : float
-        learning rate
-    d_feat : int
-        input dimensions for each time step
-    metric : str
-        the evaluation metric used in early stop
-    optimizer : str
-        optimizer name
-    GPU : int
-        the GPU ID used for training
-    """
-
-    def __init__(
-        self,
-        d_feat=6,
-        hidden_size=64,
-        num_layers=2,
-        dropout=0.0,
-        n_epochs=200,
-        lr=0.001,
-        metric="",
-        early_stop=20,
-        loss="mse",
-        base_model="GRU",
-        model_path=None,
-        optimizer="adam",
-        GPU=0,
-        seed=None,
-        **kwargs
-    ):
-        # Set logger.
-        self.logger = get_module_logger("GATs")
-        self.logger.info("GATs pytorch version...")
-
-        # set hyper-parameters.
-        self.d_feat = d_feat
-        self.hidden_size = hidden_size
-        self.num_layers = num_layers
-        self.dropout = dropout
-        self.n_epochs = n_epochs
-        self.lr = lr
-        self.metric = metric
-        self.early_stop = early_stop
-        self.optimizer = optimizer.lower()
-        self.loss = loss
-        self.base_model = base_model
-        self.model_path = model_path
-        self.device = torch.device("cuda:%d" % (GPU) if torch.cuda.is_available() and GPU >= 0 else "cpu")
-        self.seed = seed
-
-        self.logger.info(
-            "GATs parameters setting:"
-            "\nd_feat : {}"
-            "\nhidden_size : {}"
-            "\nnum_layers : {}"
-            "\ndropout : {}"
-            "\nn_epochs : {}"
-            "\nlr : {}"
-            "\nmetric : {}"
-            "\nearly_stop : {}"
-            "\noptimizer : {}"
-            "\nloss_type : {}"
-            "\nbase_model : {}"
-            "\nmodel_path : {}"
-            "\ndevice : {}"
-            "\nuse_GPU : {}"
-            "\nseed : {}".format(
-                d_feat,
-                hidden_size,
-                num_layers,
-                dropout,
-                n_epochs,
-                lr,
-                metric,
-                early_stop,
-                optimizer.lower(),
-                loss,
-                base_model,
-                model_path,
-                self.device,
-                self.use_gpu,
-                seed,
-            )
-        )
-
-        if self.seed is not None:
-            np.random.seed(self.seed)
-            torch.manual_seed(self.seed)
-
-        self.GAT_model = GATModel(
-            d_feat=self.d_feat,
-            hidden_size=self.hidden_size,
-            num_layers=self.num_layers,
-            dropout=self.dropout,
-            base_model=self.base_model,
-        )
-        self.logger.info("model:\n{:}".format(self.GAT_model))
-        self.logger.info("model size: {:.4f} MB".format(count_parameters(self.GAT_model)))
-
-        if optimizer.lower() == "adam":
-            self.train_optimizer = optim.Adam(self.GAT_model.parameters(), lr=self.lr)
-        elif optimizer.lower() == "gd":
-            self.train_optimizer = optim.SGD(self.GAT_model.parameters(), lr=self.lr)
-        else:
-            raise NotImplementedError("optimizer {} is not supported!".format(optimizer))
-
-        self.fitted = False
-        self.GAT_model.to(self.device)
-
-    @property
-    def use_gpu(self):
-        return self.device != torch.device("cpu")
-
-    def mse(self, pred, label):
-        loss = (pred - label) ** 2
-        return torch.mean(loss)
-
-    def loss_fn(self, pred, label):
-        mask = ~torch.isnan(label)
-
-        if self.loss == "mse":
-            return self.mse(pred[mask], label[mask])
-
-        raise ValueError("unknown loss `%s`" % self.loss)
-
-    def metric_fn(self, pred, label):
-
-        mask = torch.isfinite(label)
-
-        if self.metric in ("", "loss"):
-            return -self.loss_fn(pred[mask], label[mask])
-
-        raise ValueError("unknown metric `%s`" % self.metric)
-
-    def get_daily_inter(self, df, shuffle=False):
-        # organize the train data into daily batches
-        daily_count = df.groupby(level=0).size().values
-        daily_index = np.roll(np.cumsum(daily_count), 1)
-        daily_index[0] = 0
-        if shuffle:
-            # shuffle data
-            daily_shuffle = list(zip(daily_index, daily_count))
-            np.random.shuffle(daily_shuffle)
-            daily_index, daily_count = zip(*daily_shuffle)
-        return daily_index, daily_count
-
-    def train_epoch(self, x_train, y_train):
-
-        x_train_values = x_train.values
-        y_train_values = np.squeeze(y_train.values)
-        self.GAT_model.train()
-
-        # organize the train data into daily batches
-        daily_index, daily_count = self.get_daily_inter(x_train, shuffle=True)
-
-        for idx, count in zip(daily_index, daily_count):
-            batch = slice(idx, idx + count)
-            feature = torch.from_numpy(x_train_values[batch]).float().to(self.device)
-            label = torch.from_numpy(y_train_values[batch]).float().to(self.device)
-
-            pred = self.GAT_model(feature)
-            loss = self.loss_fn(pred, label)
-
-            self.train_optimizer.zero_grad()
-            loss.backward()
-            torch.nn.utils.clip_grad_value_(self.GAT_model.parameters(), 3.0)
-            self.train_optimizer.step()
-
-    def test_epoch(self, data_x, data_y):
-
-        # prepare training data
-        x_values = data_x.values
-        y_values = np.squeeze(data_y.values)
-
-        self.GAT_model.eval()
-
-        scores = []
-        losses = []
-
-        # organize the test data into daily batches
-        daily_index, daily_count = self.get_daily_inter(data_x, shuffle=False)
-
-        for idx, count in zip(daily_index, daily_count):
-            batch = slice(idx, idx + count)
-            feature = torch.from_numpy(x_values[batch]).float().to(self.device)
-            label = torch.from_numpy(y_values[batch]).float().to(self.device)
-
-            pred = self.GAT_model(feature)
-            loss = self.loss_fn(pred, label)
-            losses.append(loss.item())
-
-            score = self.metric_fn(pred, label)
-            scores.append(score.item())
-
-        return np.mean(losses), np.mean(scores)
-
-    def fit(
-        self,
-        dataset: DatasetH,
-        evals_result=dict(),
-        save_path=None,
-    ):
-
-        df_train, df_valid, df_test = dataset.prepare(
-            ["train", "valid", "test"],
-            col_set=["feature", "label"],
-            data_key=DataHandlerLP.DK_L,
-        )
-        if df_train.empty or df_valid.empty:
-            raise ValueError("Empty data from dataset, please check your dataset config.")
-
-        x_train, y_train = df_train["feature"], df_train["label"]
-        x_valid, y_valid = df_valid["feature"], df_valid["label"]
-
-        save_path = get_or_create_path(save_path)
-        stop_steps = 0
-        best_score = -np.inf
-        best_epoch = 0
-        evals_result["train"] = []
-        evals_result["valid"] = []
-
-        # load pretrained base_model
-        if self.base_model == "LSTM":
-            pretrained_model = LSTMModel()
-        elif self.base_model == "GRU":
-            pretrained_model = GRUModel()
-        else:
-            raise ValueError("unknown base model name `%s`" % self.base_model)
-
-        if self.model_path is not None:
-            self.logger.info("Loading pretrained model...")
-            pretrained_model.load_state_dict(torch.load(self.model_path, map_location=self.device))
-
-        model_dict = self.GAT_model.state_dict()
-        pretrained_dict = {
-            k: v for k, v in pretrained_model.state_dict().items() if k in model_dict  # pylint: disable=E1135
-        }
-        model_dict.update(pretrained_dict)
-        self.GAT_model.load_state_dict(model_dict)
-        self.logger.info("Loading pretrained model Done...")
-
-        # train
-        self.logger.info("training...")
-        self.fitted = True
-
-        for step in range(self.n_epochs):
-            self.logger.info("Epoch%d:", step)
-            self.logger.info("training...")
-            self.train_epoch(x_train, y_train)
-            self.logger.info("evaluating...")
-            train_loss, train_score = self.test_epoch(x_train, y_train)
-            val_loss, val_score = self.test_epoch(x_valid, y_valid)
-            self.logger.info("train %.6f, valid %.6f" % (train_score, val_score))
-            evals_result["train"].append(train_score)
-            evals_result["valid"].append(val_score)
-
-            if val_score > best_score:
-                best_score = val_score
-                stop_steps = 0
-                best_epoch = step
-                best_param = copy.deepcopy(self.GAT_model.state_dict())
-            else:
-                stop_steps += 1
-                if stop_steps >= self.early_stop:
-                    self.logger.info("early stop")
-                    break
-
-        self.logger.info("best score: %.6lf @ %d" % (best_score, best_epoch))
-        self.GAT_model.load_state_dict(best_param)
-        torch.save(best_param, save_path)
-
-        if self.use_gpu:
-            torch.cuda.empty_cache()
-
-    def predict(self, dataset: DatasetH, segment: Union[Text, slice] = "test"):
-        if not self.fitted:
-            raise ValueError("model is not fitted yet!")
-
-        x_test = dataset.prepare(segment, col_set="feature")
-        index = x_test.index
-        self.GAT_model.eval()
-        x_values = x_test.values
-        preds = []
-
-        # organize the data into daily batches
-        daily_index, daily_count = self.get_daily_inter(x_test, shuffle=False)
-
-        for idx, count in zip(daily_index, daily_count):
-            batch = slice(idx, idx + count)
-            x_batch = torch.from_numpy(x_values[batch]).float().to(self.device)
-
-            with torch.no_grad():
-                pred = self.GAT_model(x_batch).detach().cpu().numpy()
-
-            preds.append(pred)
-
-        return pd.Series(np.concatenate(preds), index=index)
-
-
-class GATModel(nn.Module):
-    def __init__(self, d_feat=6, hidden_size=64, num_layers=2, dropout=0.0, base_model="GRU"):
-        super().__init__()
-
-        if base_model == "GRU":
-            self.rnn = nn.GRU(
-                input_size=d_feat,
-                hidden_size=hidden_size,
-                num_layers=num_layers,
-                batch_first=True,
-                dropout=dropout,
-            )
-        elif base_model == "LSTM":
-            self.rnn = nn.LSTM(
-                input_size=d_feat,
-                hidden_size=hidden_size,
-                num_layers=num_layers,
-                batch_first=True,
-                dropout=dropout,
-            )
-        else:
-            raise ValueError("unknown base model name `%s`" % base_model)
-
-        self.hidden_size = hidden_size
-        self.d_feat = d_feat
-        self.transformation = nn.Linear(self.hidden_size, self.hidden_size)
-        self.a = nn.Parameter(torch.randn(self.hidden_size * 2, 1))
-        self.a.requires_grad = True
-        self.fc = nn.Linear(self.hidden_size, self.hidden_size)
-        self.fc_out = nn.Linear(hidden_size, 1)
-        self.leaky_relu = nn.LeakyReLU()
-        self.softmax = nn.Softmax(dim=1)
-
-    def cal_attention(self, x, y):
-        x = self.transformation(x)
-        y = self.transformation(y)
-
-        sample_num = x.shape[0]
-        dim = x.shape[1]
-        e_x = x.expand(sample_num, sample_num, dim)
-        e_y = torch.transpose(e_x, 0, 1)
-        attention_in = torch.cat((e_x, e_y), 2).view(-1, dim * 2)
-        self.a_t = torch.t(self.a)
-        attention_out = self.a_t.mm(torch.t(attention_in)).view(sample_num, sample_num)
-        attention_out = self.leaky_relu(attention_out)
-        att_weight = self.softmax(attention_out)
-        return att_weight
-
-    def forward(self, x):
-        # x: [N, F*T]
-        x = x.reshape(len(x), self.d_feat, -1)  # [N, F, T]
-        x = x.permute(0, 2, 1)  # [N, T, F]
-        out, _ = self.rnn(x)
-        hidden = out[:, -1, :]
-        att_weight = self.cal_attention(hidden, hidden)
-        hidden = att_weight.mm(hidden) + hidden
-        hidden = self.fc(hidden)
-        hidden = self.leaky_relu(hidden)
-        return self.fc_out(hidden).squeeze()
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+
+from __future__ import division
+from __future__ import print_function
+
+import numpy as np
+import pandas as pd
+from typing import Text, Union
+import copy
+from ...utils import get_or_create_path
+from ...log import get_module_logger
+import torch
+import torch.nn as nn
+import torch.optim as optim
+
+from .pytorch_utils import count_parameters
+from ...model.base import Model
+from ...data.dataset import DatasetH
+from ...data.dataset.handler import DataHandlerLP
+from ...contrib.model.pytorch_lstm import LSTMModel
+from ...contrib.model.pytorch_gru import GRUModel
+
+
+class GATs(Model):
+    """GATs Model
+
+    Parameters
+    ----------
+    lr : float
+        learning rate
+    d_feat : int
+        input dimensions for each time step
+    metric : str
+        the evaluation metric used in early stop
+    optimizer : str
+        optimizer name
+    GPU : int
+        the GPU ID used for training
+    """
+
+    def __init__(
+        self,
+        d_feat=6,
+        hidden_size=64,
+        num_layers=2,
+        dropout=0.0,
+        n_epochs=200,
+        lr=0.001,
+        metric="",
+        early_stop=20,
+        loss="mse",
+        base_model="GRU",
+        model_path=None,
+        optimizer="adam",
+        GPU=0,
+        seed=None,
+        **kwargs
+    ):
+        # Set logger.
+        self.logger = get_module_logger("GATs")
+        self.logger.info("GATs pytorch version...")
+
+        # set hyper-parameters.
+        self.d_feat = d_feat
+        self.hidden_size = hidden_size
+        self.num_layers = num_layers
+        self.dropout = dropout
+        self.n_epochs = n_epochs
+        self.lr = lr
+        self.metric = metric
+        self.early_stop = early_stop
+        self.optimizer = optimizer.lower()
+        self.loss = loss
+        self.base_model = base_model
+        self.model_path = model_path
+        self.device = torch.device("cuda:%d" % (GPU) if torch.cuda.is_available() and GPU >= 0 else "cpu")
+        self.seed = seed
+
+        self.logger.info(
+            "GATs parameters setting:"
+            "\nd_feat : {}"
+            "\nhidden_size : {}"
+            "\nnum_layers : {}"
+            "\ndropout : {}"
+            "\nn_epochs : {}"
+            "\nlr : {}"
+            "\nmetric : {}"
+            "\nearly_stop : {}"
+            "\noptimizer : {}"
+            "\nloss_type : {}"
+            "\nbase_model : {}"
+            "\nmodel_path : {}"
+            "\ndevice : {}"
+            "\nuse_GPU : {}"
+            "\nseed : {}".format(
+                d_feat,
+                hidden_size,
+                num_layers,
+                dropout,
+                n_epochs,
+                lr,
+                metric,
+                early_stop,
+                optimizer.lower(),
+                loss,
+                base_model,
+                model_path,
+                self.device,
+                self.use_gpu,
+                seed,
+            )
+        )
+
+        if self.seed is not None:
+            np.random.seed(self.seed)
+            torch.manual_seed(self.seed)
+
+        self.GAT_model = GATModel(
+            d_feat=self.d_feat,
+            hidden_size=self.hidden_size,
+            num_layers=self.num_layers,
+            dropout=self.dropout,
+            base_model=self.base_model,
+        )
+        self.logger.info("model:\n{:}".format(self.GAT_model))
+        self.logger.info("model size: {:.4f} MB".format(count_parameters(self.GAT_model)))
+
+        if optimizer.lower() == "adam":
+            self.train_optimizer = optim.Adam(self.GAT_model.parameters(), lr=self.lr)
+        elif optimizer.lower() == "gd":
+            self.train_optimizer = optim.SGD(self.GAT_model.parameters(), lr=self.lr)
+        else:
+            raise NotImplementedError("optimizer {} is not supported!".format(optimizer))
+
+        self.fitted = False
+        self.GAT_model.to(self.device)
+
+    @property
+    def use_gpu(self):
+        return self.device != torch.device("cpu")
+
+    def mse(self, pred, label):
+        loss = (pred - label) ** 2
+        return torch.mean(loss)
+
+    def loss_fn(self, pred, label):
+        mask = ~torch.isnan(label)
+
+        if self.loss == "mse":
+            return self.mse(pred[mask], label[mask])
+
+        raise ValueError("unknown loss `%s`" % self.loss)
+
+    def metric_fn(self, pred, label):
+        mask = torch.isfinite(label)
+
+        if self.metric in ("", "loss"):
+            return -self.loss_fn(pred[mask], label[mask])
+
+        raise ValueError("unknown metric `%s`" % self.metric)
+
+    def get_daily_inter(self, df, shuffle=False):
+        # organize the train data into daily batches
+        daily_count = df.groupby(level=0).size().values
+        daily_index = np.roll(np.cumsum(daily_count), 1)
+        daily_index[0] = 0
+        if shuffle:
+            # shuffle data
+            daily_shuffle = list(zip(daily_index, daily_count))
+            np.random.shuffle(daily_shuffle)
+            daily_index, daily_count = zip(*daily_shuffle)
+        return daily_index, daily_count
+
+    def train_epoch(self, x_train, y_train):
+        x_train_values = x_train.values
+        y_train_values = np.squeeze(y_train.values)
+        self.GAT_model.train()
+
+        # organize the train data into daily batches
+        daily_index, daily_count = self.get_daily_inter(x_train, shuffle=True)
+
+        for idx, count in zip(daily_index, daily_count):
+            batch = slice(idx, idx + count)
+            feature = torch.from_numpy(x_train_values[batch]).float().to(self.device)
+            label = torch.from_numpy(y_train_values[batch]).float().to(self.device)
+
+            pred = self.GAT_model(feature)
+            loss = self.loss_fn(pred, label)
+
+            self.train_optimizer.zero_grad()
+            loss.backward()
+            torch.nn.utils.clip_grad_value_(self.GAT_model.parameters(), 3.0)
+            self.train_optimizer.step()
+
+    def test_epoch(self, data_x, data_y):
+        # prepare training data
+        x_values = data_x.values
+        y_values = np.squeeze(data_y.values)
+
+        self.GAT_model.eval()
+
+        scores = []
+        losses = []
+
+        # organize the test data into daily batches
+        daily_index, daily_count = self.get_daily_inter(data_x, shuffle=False)
+
+        for idx, count in zip(daily_index, daily_count):
+            batch = slice(idx, idx + count)
+            feature = torch.from_numpy(x_values[batch]).float().to(self.device)
+            label = torch.from_numpy(y_values[batch]).float().to(self.device)
+
+            pred = self.GAT_model(feature)
+            loss = self.loss_fn(pred, label)
+            losses.append(loss.item())
+
+            score = self.metric_fn(pred, label)
+            scores.append(score.item())
+
+        return np.mean(losses), np.mean(scores)
+
+    def fit(
+        self,
+        dataset: DatasetH,
+        evals_result=dict(),
+        save_path=None,
+    ):
+        df_train, df_valid, df_test = dataset.prepare(
+            ["train", "valid", "test"],
+            col_set=["feature", "label"],
+            data_key=DataHandlerLP.DK_L,
+        )
+        if df_train.empty or df_valid.empty:
+            raise ValueError("Empty data from dataset, please check your dataset config.")
+
+        x_train, y_train = df_train["feature"], df_train["label"]
+        x_valid, y_valid = df_valid["feature"], df_valid["label"]
+
+        save_path = get_or_create_path(save_path)
+        stop_steps = 0
+        best_score = -np.inf
+        best_epoch = 0
+        evals_result["train"] = []
+        evals_result["valid"] = []
+
+        # load pretrained base_model
+        if self.base_model == "LSTM":
+            pretrained_model = LSTMModel()
+        elif self.base_model == "GRU":
+            pretrained_model = GRUModel()
+        else:
+            raise ValueError("unknown base model name `%s`" % self.base_model)
+
+        if self.model_path is not None:
+            self.logger.info("Loading pretrained model...")
+            pretrained_model.load_state_dict(torch.load(self.model_path, map_location=self.device))
+
+        model_dict = self.GAT_model.state_dict()
+        pretrained_dict = {
+            k: v for k, v in pretrained_model.state_dict().items() if k in model_dict  # pylint: disable=E1135
+        }
+        model_dict.update(pretrained_dict)
+        self.GAT_model.load_state_dict(model_dict)
+        self.logger.info("Loading pretrained model Done...")
+
+        # train
+        self.logger.info("training...")
+        self.fitted = True
+
+        for step in range(self.n_epochs):
+            self.logger.info("Epoch%d:", step)
+            self.logger.info("training...")
+            self.train_epoch(x_train, y_train)
+            self.logger.info("evaluating...")
+            train_loss, train_score = self.test_epoch(x_train, y_train)
+            val_loss, val_score = self.test_epoch(x_valid, y_valid)
+            self.logger.info("train %.6f, valid %.6f" % (train_score, val_score))
+            evals_result["train"].append(train_score)
+            evals_result["valid"].append(val_score)
+
+            if val_score > best_score:
+                best_score = val_score
+                stop_steps = 0
+                best_epoch = step
+                best_param = copy.deepcopy(self.GAT_model.state_dict())
+            else:
+                stop_steps += 1
+                if stop_steps >= self.early_stop:
+                    self.logger.info("early stop")
+                    break
+
+        self.logger.info("best score: %.6lf @ %d" % (best_score, best_epoch))
+        self.GAT_model.load_state_dict(best_param)
+        torch.save(best_param, save_path)
+
+        if self.use_gpu:
+            torch.cuda.empty_cache()
+
+    def predict(self, dataset: DatasetH, segment: Union[Text, slice] = "test"):
+        if not self.fitted:
+            raise ValueError("model is not fitted yet!")
+
+        x_test = dataset.prepare(segment, col_set="feature")
+        index = x_test.index
+        self.GAT_model.eval()
+        x_values = x_test.values
+        preds = []
+
+        # organize the data into daily batches
+        daily_index, daily_count = self.get_daily_inter(x_test, shuffle=False)
+
+        for idx, count in zip(daily_index, daily_count):
+            batch = slice(idx, idx + count)
+            x_batch = torch.from_numpy(x_values[batch]).float().to(self.device)
+
+            with torch.no_grad():
+                pred = self.GAT_model(x_batch).detach().cpu().numpy()
+
+            preds.append(pred)
+
+        return pd.Series(np.concatenate(preds), index=index)
+
+
+class GATModel(nn.Module):
+    def __init__(self, d_feat=6, hidden_size=64, num_layers=2, dropout=0.0, base_model="GRU"):
+        super().__init__()
+
+        if base_model == "GRU":
+            self.rnn = nn.GRU(
+                input_size=d_feat,
+                hidden_size=hidden_size,
+                num_layers=num_layers,
+                batch_first=True,
+                dropout=dropout,
+            )
+        elif base_model == "LSTM":
+            self.rnn = nn.LSTM(
+                input_size=d_feat,
+                hidden_size=hidden_size,
+                num_layers=num_layers,
+                batch_first=True,
+                dropout=dropout,
+            )
+        else:
+            raise ValueError("unknown base model name `%s`" % base_model)
+
+        self.hidden_size = hidden_size
+        self.d_feat = d_feat
+        self.transformation = nn.Linear(self.hidden_size, self.hidden_size)
+        self.a = nn.Parameter(torch.randn(self.hidden_size * 2, 1))
+        self.a.requires_grad = True
+        self.fc = nn.Linear(self.hidden_size, self.hidden_size)
+        self.fc_out = nn.Linear(hidden_size, 1)
+        self.leaky_relu = nn.LeakyReLU()
+        self.softmax = nn.Softmax(dim=1)
+
+    def cal_attention(self, x, y):
+        x = self.transformation(x)
+        y = self.transformation(y)
+
+        sample_num = x.shape[0]
+        dim = x.shape[1]
+        e_x = x.expand(sample_num, sample_num, dim)
+        e_y = torch.transpose(e_x, 0, 1)
+        attention_in = torch.cat((e_x, e_y), 2).view(-1, dim * 2)
+        self.a_t = torch.t(self.a)
+        attention_out = self.a_t.mm(torch.t(attention_in)).view(sample_num, sample_num)
+        attention_out = self.leaky_relu(attention_out)
+        att_weight = self.softmax(attention_out)
+        return att_weight
+
+    def forward(self, x):
+        # x: [N, F*T]
+        x = x.reshape(len(x), self.d_feat, -1)  # [N, F, T]
+        x = x.permute(0, 2, 1)  # [N, T, F]
+        out, _ = self.rnn(x)
+        hidden = out[:, -1, :]
+        att_weight = self.cal_attention(hidden, hidden)
+        hidden = att_weight.mm(hidden) + hidden
+        hidden = self.fc(hidden)
+        hidden = self.leaky_relu(hidden)
+        return self.fc_out(hidden).squeeze()
```

## qlib/contrib/model/pytorch_gats_ts.py

```diff
@@ -1,399 +1,391 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-
-from __future__ import division
-from __future__ import print_function
-
-import numpy as np
-import pandas as pd
-import copy
-from ...utils import get_or_create_path
-from ...log import get_module_logger
-import torch
-import torch.nn as nn
-import torch.optim as optim
-from torch.utils.data import DataLoader
-from torch.utils.data import Sampler
-
-from .pytorch_utils import count_parameters
-from ...model.base import Model
-from ...data.dataset.handler import DataHandlerLP
-from ...contrib.model.pytorch_lstm import LSTMModel
-from ...contrib.model.pytorch_gru import GRUModel
-
-
-class DailyBatchSampler(Sampler):
-    def __init__(self, data_source):
-        self.data_source = data_source
-        # calculate number of samples in each batch
-        self.daily_count = pd.Series(index=self.data_source.get_index()).groupby("datetime").size().values
-        self.daily_index = np.roll(np.cumsum(self.daily_count), 1)  # calculate begin index of each batch
-        self.daily_index[0] = 0
-
-    def __iter__(self):
-
-        for idx, count in zip(self.daily_index, self.daily_count):
-            yield np.arange(idx, idx + count)
-
-    def __len__(self):
-        return len(self.data_source)
-
-
-class GATs(Model):
-    """GATs Model
-
-    Parameters
-    ----------
-    lr : float
-        learning rate
-    d_feat : int
-        input dimensions for each time step
-    metric : str
-        the evaluation metric used in early stop
-    optimizer : str
-        optimizer name
-    GPU : int
-        the GPU ID used for training
-    """
-
-    def __init__(
-        self,
-        d_feat=20,
-        hidden_size=64,
-        num_layers=2,
-        dropout=0.0,
-        n_epochs=200,
-        lr=0.001,
-        metric="",
-        early_stop=20,
-        loss="mse",
-        base_model="GRU",
-        model_path=None,
-        optimizer="adam",
-        GPU=0,
-        n_jobs=10,
-        seed=None,
-        **kwargs
-    ):
-        # Set logger.
-        self.logger = get_module_logger("GATs")
-        self.logger.info("GATs pytorch version...")
-
-        # set hyper-parameters.
-        self.d_feat = d_feat
-        self.hidden_size = hidden_size
-        self.num_layers = num_layers
-        self.dropout = dropout
-        self.n_epochs = n_epochs
-        self.lr = lr
-        self.metric = metric
-        self.early_stop = early_stop
-        self.optimizer = optimizer.lower()
-        self.loss = loss
-        self.base_model = base_model
-        self.model_path = model_path
-        self.device = torch.device("cuda:%d" % (GPU) if torch.cuda.is_available() and GPU >= 0 else "cpu")
-        self.n_jobs = n_jobs
-        self.seed = seed
-
-        self.logger.info(
-            "GATs parameters setting:"
-            "\nd_feat : {}"
-            "\nhidden_size : {}"
-            "\nnum_layers : {}"
-            "\ndropout : {}"
-            "\nn_epochs : {}"
-            "\nlr : {}"
-            "\nmetric : {}"
-            "\nearly_stop : {}"
-            "\noptimizer : {}"
-            "\nloss_type : {}"
-            "\nbase_model : {}"
-            "\nmodel_path : {}"
-            "\nvisible_GPU : {}"
-            "\nuse_GPU : {}"
-            "\nseed : {}".format(
-                d_feat,
-                hidden_size,
-                num_layers,
-                dropout,
-                n_epochs,
-                lr,
-                metric,
-                early_stop,
-                optimizer.lower(),
-                loss,
-                base_model,
-                model_path,
-                GPU,
-                self.use_gpu,
-                seed,
-            )
-        )
-
-        if self.seed is not None:
-            np.random.seed(self.seed)
-            torch.manual_seed(self.seed)
-
-        self.GAT_model = GATModel(
-            d_feat=self.d_feat,
-            hidden_size=self.hidden_size,
-            num_layers=self.num_layers,
-            dropout=self.dropout,
-            base_model=self.base_model,
-        )
-        self.logger.info("model:\n{:}".format(self.GAT_model))
-        self.logger.info("model size: {:.4f} MB".format(count_parameters(self.GAT_model)))
-
-        if optimizer.lower() == "adam":
-            self.train_optimizer = optim.Adam(self.GAT_model.parameters(), lr=self.lr)
-        elif optimizer.lower() == "gd":
-            self.train_optimizer = optim.SGD(self.GAT_model.parameters(), lr=self.lr)
-        else:
-            raise NotImplementedError("optimizer {} is not supported!".format(optimizer))
-
-        self.fitted = False
-        self.GAT_model.to(self.device)
-
-    @property
-    def use_gpu(self):
-        return self.device != torch.device("cpu")
-
-    def mse(self, pred, label):
-        loss = (pred - label) ** 2
-        return torch.mean(loss)
-
-    def loss_fn(self, pred, label):
-        mask = ~torch.isnan(label)
-
-        if self.loss == "mse":
-            return self.mse(pred[mask], label[mask])
-
-        raise ValueError("unknown loss `%s`" % self.loss)
-
-    def metric_fn(self, pred, label):
-
-        mask = torch.isfinite(label)
-
-        if self.metric in ("", "loss"):
-            return -self.loss_fn(pred[mask], label[mask])
-
-        raise ValueError("unknown metric `%s`" % self.metric)
-
-    def get_daily_inter(self, df, shuffle=False):
-        # organize the train data into daily batches
-        daily_count = df.groupby(level=0).size().values
-        daily_index = np.roll(np.cumsum(daily_count), 1)
-        daily_index[0] = 0
-        if shuffle:
-            # shuffle data
-            daily_shuffle = list(zip(daily_index, daily_count))
-            np.random.shuffle(daily_shuffle)
-            daily_index, daily_count = zip(*daily_shuffle)
-        return daily_index, daily_count
-
-    def train_epoch(self, data_loader):
-
-        self.GAT_model.train()
-
-        for data in data_loader:
-
-            data = data.squeeze()
-            feature = data[:, :, 0:-1].to(self.device)
-            label = data[:, -1, -1].to(self.device)
-
-            pred = self.GAT_model(feature.float())
-            loss = self.loss_fn(pred, label)
-
-            self.train_optimizer.zero_grad()
-            loss.backward()
-            torch.nn.utils.clip_grad_value_(self.GAT_model.parameters(), 3.0)
-            self.train_optimizer.step()
-
-    def test_epoch(self, data_loader):
-
-        self.GAT_model.eval()
-
-        scores = []
-        losses = []
-
-        for data in data_loader:
-
-            data = data.squeeze()
-            feature = data[:, :, 0:-1].to(self.device)
-            # feature[torch.isnan(feature)] = 0
-            label = data[:, -1, -1].to(self.device)
-
-            pred = self.GAT_model(feature.float())
-            loss = self.loss_fn(pred, label)
-            losses.append(loss.item())
-
-            score = self.metric_fn(pred, label)
-            scores.append(score.item())
-
-        return np.mean(losses), np.mean(scores)
-
-    def fit(
-        self,
-        dataset,
-        evals_result=dict(),
-        save_path=None,
-    ):
-
-        dl_train = dataset.prepare("train", col_set=["feature", "label"], data_key=DataHandlerLP.DK_L)
-        dl_valid = dataset.prepare("valid", col_set=["feature", "label"], data_key=DataHandlerLP.DK_L)
-        if dl_train.empty or dl_valid.empty:
-            raise ValueError("Empty data from dataset, please check your dataset config.")
-
-        dl_train.config(fillna_type="ffill+bfill")  # process nan brought by dataloader
-        dl_valid.config(fillna_type="ffill+bfill")  # process nan brought by dataloader
-
-        sampler_train = DailyBatchSampler(dl_train)
-        sampler_valid = DailyBatchSampler(dl_valid)
-
-        train_loader = DataLoader(dl_train, sampler=sampler_train, num_workers=self.n_jobs, drop_last=True)
-        valid_loader = DataLoader(dl_valid, sampler=sampler_valid, num_workers=self.n_jobs, drop_last=True)
-
-        save_path = get_or_create_path(save_path)
-
-        stop_steps = 0
-        train_loss = 0
-        best_score = -np.inf
-        best_epoch = 0
-        evals_result["train"] = []
-        evals_result["valid"] = []
-
-        # load pretrained base_model
-        if self.base_model == "LSTM":
-            pretrained_model = LSTMModel(d_feat=self.d_feat, hidden_size=self.hidden_size, num_layers=self.num_layers)
-        elif self.base_model == "GRU":
-            pretrained_model = GRUModel(d_feat=self.d_feat, hidden_size=self.hidden_size, num_layers=self.num_layers)
-        else:
-            raise ValueError("unknown base model name `%s`" % self.base_model)
-
-        if self.model_path is not None:
-            self.logger.info("Loading pretrained model...")
-            pretrained_model.load_state_dict(torch.load(self.model_path, map_location=self.device))
-
-        model_dict = self.GAT_model.state_dict()
-        pretrained_dict = {
-            k: v for k, v in pretrained_model.state_dict().items() if k in model_dict  # pylint: disable=E1135
-        }
-        model_dict.update(pretrained_dict)
-        self.GAT_model.load_state_dict(model_dict)
-        self.logger.info("Loading pretrained model Done...")
-
-        # train
-        self.logger.info("training...")
-        self.fitted = True
-
-        for step in range(self.n_epochs):
-            self.logger.info("Epoch%d:", step)
-            self.logger.info("training...")
-            self.train_epoch(train_loader)
-            self.logger.info("evaluating...")
-            train_loss, train_score = self.test_epoch(train_loader)
-            val_loss, val_score = self.test_epoch(valid_loader)
-            self.logger.info("train %.6f, valid %.6f" % (train_score, val_score))
-            evals_result["train"].append(train_score)
-            evals_result["valid"].append(val_score)
-
-            if val_score > best_score:
-                best_score = val_score
-                stop_steps = 0
-                best_epoch = step
-                best_param = copy.deepcopy(self.GAT_model.state_dict())
-            else:
-                stop_steps += 1
-                if stop_steps >= self.early_stop:
-                    self.logger.info("early stop")
-                    break
-
-        self.logger.info("best score: %.6lf @ %d" % (best_score, best_epoch))
-        self.GAT_model.load_state_dict(best_param)
-        torch.save(best_param, save_path)
-
-        if self.use_gpu:
-            torch.cuda.empty_cache()
-
-    def predict(self, dataset):
-        if not self.fitted:
-            raise ValueError("model is not fitted yet!")
-
-        dl_test = dataset.prepare("test", col_set=["feature", "label"], data_key=DataHandlerLP.DK_I)
-        dl_test.config(fillna_type="ffill+bfill")
-        sampler_test = DailyBatchSampler(dl_test)
-        test_loader = DataLoader(dl_test, sampler=sampler_test, num_workers=self.n_jobs)
-        self.GAT_model.eval()
-        preds = []
-
-        for data in test_loader:
-
-            data = data.squeeze()
-            feature = data[:, :, 0:-1].to(self.device)
-
-            with torch.no_grad():
-                pred = self.GAT_model(feature.float()).detach().cpu().numpy()
-
-            preds.append(pred)
-
-        return pd.Series(np.concatenate(preds), index=dl_test.get_index())
-
-
-class GATModel(nn.Module):
-    def __init__(self, d_feat=6, hidden_size=64, num_layers=2, dropout=0.0, base_model="GRU"):
-        super().__init__()
-
-        if base_model == "GRU":
-            self.rnn = nn.GRU(
-                input_size=d_feat,
-                hidden_size=hidden_size,
-                num_layers=num_layers,
-                batch_first=True,
-                dropout=dropout,
-            )
-        elif base_model == "LSTM":
-            self.rnn = nn.LSTM(
-                input_size=d_feat,
-                hidden_size=hidden_size,
-                num_layers=num_layers,
-                batch_first=True,
-                dropout=dropout,
-            )
-        else:
-            raise ValueError("unknown base model name `%s`" % base_model)
-
-        self.hidden_size = hidden_size
-        self.d_feat = d_feat
-        self.transformation = nn.Linear(self.hidden_size, self.hidden_size)
-        self.a = nn.Parameter(torch.randn(self.hidden_size * 2, 1))
-        self.a.requires_grad = True
-        self.fc = nn.Linear(self.hidden_size, self.hidden_size)
-        self.fc_out = nn.Linear(hidden_size, 1)
-        self.leaky_relu = nn.LeakyReLU()
-        self.softmax = nn.Softmax(dim=1)
-
-    def cal_attention(self, x, y):
-        x = self.transformation(x)
-        y = self.transformation(y)
-
-        sample_num = x.shape[0]
-        dim = x.shape[1]
-        e_x = x.expand(sample_num, sample_num, dim)
-        e_y = torch.transpose(e_x, 0, 1)
-        attention_in = torch.cat((e_x, e_y), 2).view(-1, dim * 2)
-        self.a_t = torch.t(self.a)
-        attention_out = self.a_t.mm(torch.t(attention_in)).view(sample_num, sample_num)
-        attention_out = self.leaky_relu(attention_out)
-        att_weight = self.softmax(attention_out)
-        return att_weight
-
-    def forward(self, x):
-        out, _ = self.rnn(x)
-        hidden = out[:, -1, :]
-        att_weight = self.cal_attention(hidden, hidden)
-        hidden = att_weight.mm(hidden) + hidden
-        hidden = self.fc(hidden)
-        hidden = self.leaky_relu(hidden)
-        return self.fc_out(hidden).squeeze()
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+
+from __future__ import division
+from __future__ import print_function
+
+import numpy as np
+import pandas as pd
+import copy
+from ...utils import get_or_create_path
+from ...log import get_module_logger
+import torch
+import torch.nn as nn
+import torch.optim as optim
+from torch.utils.data import DataLoader
+from torch.utils.data import Sampler
+
+from .pytorch_utils import count_parameters
+from ...model.base import Model
+from ...data.dataset.handler import DataHandlerLP
+from ...contrib.model.pytorch_lstm import LSTMModel
+from ...contrib.model.pytorch_gru import GRUModel
+
+
+class DailyBatchSampler(Sampler):
+    def __init__(self, data_source):
+        self.data_source = data_source
+        # calculate number of samples in each batch
+        self.daily_count = pd.Series(index=self.data_source.get_index()).groupby("datetime").size().values
+        self.daily_index = np.roll(np.cumsum(self.daily_count), 1)  # calculate begin index of each batch
+        self.daily_index[0] = 0
+
+    def __iter__(self):
+        for idx, count in zip(self.daily_index, self.daily_count):
+            yield np.arange(idx, idx + count)
+
+    def __len__(self):
+        return len(self.data_source)
+
+
+class GATs(Model):
+    """GATs Model
+
+    Parameters
+    ----------
+    lr : float
+        learning rate
+    d_feat : int
+        input dimensions for each time step
+    metric : str
+        the evaluation metric used in early stop
+    optimizer : str
+        optimizer name
+    GPU : int
+        the GPU ID used for training
+    """
+
+    def __init__(
+        self,
+        d_feat=20,
+        hidden_size=64,
+        num_layers=2,
+        dropout=0.0,
+        n_epochs=200,
+        lr=0.001,
+        metric="",
+        early_stop=20,
+        loss="mse",
+        base_model="GRU",
+        model_path=None,
+        optimizer="adam",
+        GPU=0,
+        n_jobs=10,
+        seed=None,
+        **kwargs
+    ):
+        # Set logger.
+        self.logger = get_module_logger("GATs")
+        self.logger.info("GATs pytorch version...")
+
+        # set hyper-parameters.
+        self.d_feat = d_feat
+        self.hidden_size = hidden_size
+        self.num_layers = num_layers
+        self.dropout = dropout
+        self.n_epochs = n_epochs
+        self.lr = lr
+        self.metric = metric
+        self.early_stop = early_stop
+        self.optimizer = optimizer.lower()
+        self.loss = loss
+        self.base_model = base_model
+        self.model_path = model_path
+        self.device = torch.device("cuda:%d" % (GPU) if torch.cuda.is_available() and GPU >= 0 else "cpu")
+        self.n_jobs = n_jobs
+        self.seed = seed
+
+        self.logger.info(
+            "GATs parameters setting:"
+            "\nd_feat : {}"
+            "\nhidden_size : {}"
+            "\nnum_layers : {}"
+            "\ndropout : {}"
+            "\nn_epochs : {}"
+            "\nlr : {}"
+            "\nmetric : {}"
+            "\nearly_stop : {}"
+            "\noptimizer : {}"
+            "\nloss_type : {}"
+            "\nbase_model : {}"
+            "\nmodel_path : {}"
+            "\nvisible_GPU : {}"
+            "\nuse_GPU : {}"
+            "\nseed : {}".format(
+                d_feat,
+                hidden_size,
+                num_layers,
+                dropout,
+                n_epochs,
+                lr,
+                metric,
+                early_stop,
+                optimizer.lower(),
+                loss,
+                base_model,
+                model_path,
+                GPU,
+                self.use_gpu,
+                seed,
+            )
+        )
+
+        if self.seed is not None:
+            np.random.seed(self.seed)
+            torch.manual_seed(self.seed)
+
+        self.GAT_model = GATModel(
+            d_feat=self.d_feat,
+            hidden_size=self.hidden_size,
+            num_layers=self.num_layers,
+            dropout=self.dropout,
+            base_model=self.base_model,
+        )
+        self.logger.info("model:\n{:}".format(self.GAT_model))
+        self.logger.info("model size: {:.4f} MB".format(count_parameters(self.GAT_model)))
+
+        if optimizer.lower() == "adam":
+            self.train_optimizer = optim.Adam(self.GAT_model.parameters(), lr=self.lr)
+        elif optimizer.lower() == "gd":
+            self.train_optimizer = optim.SGD(self.GAT_model.parameters(), lr=self.lr)
+        else:
+            raise NotImplementedError("optimizer {} is not supported!".format(optimizer))
+
+        self.fitted = False
+        self.GAT_model.to(self.device)
+
+    @property
+    def use_gpu(self):
+        return self.device != torch.device("cpu")
+
+    def mse(self, pred, label):
+        loss = (pred - label) ** 2
+        return torch.mean(loss)
+
+    def loss_fn(self, pred, label):
+        mask = ~torch.isnan(label)
+
+        if self.loss == "mse":
+            return self.mse(pred[mask], label[mask])
+
+        raise ValueError("unknown loss `%s`" % self.loss)
+
+    def metric_fn(self, pred, label):
+        mask = torch.isfinite(label)
+
+        if self.metric in ("", "loss"):
+            return -self.loss_fn(pred[mask], label[mask])
+
+        raise ValueError("unknown metric `%s`" % self.metric)
+
+    def get_daily_inter(self, df, shuffle=False):
+        # organize the train data into daily batches
+        daily_count = df.groupby(level=0).size().values
+        daily_index = np.roll(np.cumsum(daily_count), 1)
+        daily_index[0] = 0
+        if shuffle:
+            # shuffle data
+            daily_shuffle = list(zip(daily_index, daily_count))
+            np.random.shuffle(daily_shuffle)
+            daily_index, daily_count = zip(*daily_shuffle)
+        return daily_index, daily_count
+
+    def train_epoch(self, data_loader):
+        self.GAT_model.train()
+
+        for data in data_loader:
+            data = data.squeeze()
+            feature = data[:, :, 0:-1].to(self.device)
+            label = data[:, -1, -1].to(self.device)
+
+            pred = self.GAT_model(feature.float())
+            loss = self.loss_fn(pred, label)
+
+            self.train_optimizer.zero_grad()
+            loss.backward()
+            torch.nn.utils.clip_grad_value_(self.GAT_model.parameters(), 3.0)
+            self.train_optimizer.step()
+
+    def test_epoch(self, data_loader):
+        self.GAT_model.eval()
+
+        scores = []
+        losses = []
+
+        for data in data_loader:
+            data = data.squeeze()
+            feature = data[:, :, 0:-1].to(self.device)
+            # feature[torch.isnan(feature)] = 0
+            label = data[:, -1, -1].to(self.device)
+
+            pred = self.GAT_model(feature.float())
+            loss = self.loss_fn(pred, label)
+            losses.append(loss.item())
+
+            score = self.metric_fn(pred, label)
+            scores.append(score.item())
+
+        return np.mean(losses), np.mean(scores)
+
+    def fit(
+        self,
+        dataset,
+        evals_result=dict(),
+        save_path=None,
+    ):
+        dl_train = dataset.prepare("train", col_set=["feature", "label"], data_key=DataHandlerLP.DK_L)
+        dl_valid = dataset.prepare("valid", col_set=["feature", "label"], data_key=DataHandlerLP.DK_L)
+        if dl_train.empty or dl_valid.empty:
+            raise ValueError("Empty data from dataset, please check your dataset config.")
+
+        dl_train.config(fillna_type="ffill+bfill")  # process nan brought by dataloader
+        dl_valid.config(fillna_type="ffill+bfill")  # process nan brought by dataloader
+
+        sampler_train = DailyBatchSampler(dl_train)
+        sampler_valid = DailyBatchSampler(dl_valid)
+
+        train_loader = DataLoader(dl_train, sampler=sampler_train, num_workers=self.n_jobs, drop_last=True)
+        valid_loader = DataLoader(dl_valid, sampler=sampler_valid, num_workers=self.n_jobs, drop_last=True)
+
+        save_path = get_or_create_path(save_path)
+
+        stop_steps = 0
+        train_loss = 0
+        best_score = -np.inf
+        best_epoch = 0
+        evals_result["train"] = []
+        evals_result["valid"] = []
+
+        # load pretrained base_model
+        if self.base_model == "LSTM":
+            pretrained_model = LSTMModel(d_feat=self.d_feat, hidden_size=self.hidden_size, num_layers=self.num_layers)
+        elif self.base_model == "GRU":
+            pretrained_model = GRUModel(d_feat=self.d_feat, hidden_size=self.hidden_size, num_layers=self.num_layers)
+        else:
+            raise ValueError("unknown base model name `%s`" % self.base_model)
+
+        if self.model_path is not None:
+            self.logger.info("Loading pretrained model...")
+            pretrained_model.load_state_dict(torch.load(self.model_path, map_location=self.device))
+
+        model_dict = self.GAT_model.state_dict()
+        pretrained_dict = {
+            k: v for k, v in pretrained_model.state_dict().items() if k in model_dict  # pylint: disable=E1135
+        }
+        model_dict.update(pretrained_dict)
+        self.GAT_model.load_state_dict(model_dict)
+        self.logger.info("Loading pretrained model Done...")
+
+        # train
+        self.logger.info("training...")
+        self.fitted = True
+
+        for step in range(self.n_epochs):
+            self.logger.info("Epoch%d:", step)
+            self.logger.info("training...")
+            self.train_epoch(train_loader)
+            self.logger.info("evaluating...")
+            train_loss, train_score = self.test_epoch(train_loader)
+            val_loss, val_score = self.test_epoch(valid_loader)
+            self.logger.info("train %.6f, valid %.6f" % (train_score, val_score))
+            evals_result["train"].append(train_score)
+            evals_result["valid"].append(val_score)
+
+            if val_score > best_score:
+                best_score = val_score
+                stop_steps = 0
+                best_epoch = step
+                best_param = copy.deepcopy(self.GAT_model.state_dict())
+            else:
+                stop_steps += 1
+                if stop_steps >= self.early_stop:
+                    self.logger.info("early stop")
+                    break
+
+        self.logger.info("best score: %.6lf @ %d" % (best_score, best_epoch))
+        self.GAT_model.load_state_dict(best_param)
+        torch.save(best_param, save_path)
+
+        if self.use_gpu:
+            torch.cuda.empty_cache()
+
+    def predict(self, dataset):
+        if not self.fitted:
+            raise ValueError("model is not fitted yet!")
+
+        dl_test = dataset.prepare("test", col_set=["feature", "label"], data_key=DataHandlerLP.DK_I)
+        dl_test.config(fillna_type="ffill+bfill")
+        sampler_test = DailyBatchSampler(dl_test)
+        test_loader = DataLoader(dl_test, sampler=sampler_test, num_workers=self.n_jobs)
+        self.GAT_model.eval()
+        preds = []
+
+        for data in test_loader:
+            data = data.squeeze()
+            feature = data[:, :, 0:-1].to(self.device)
+
+            with torch.no_grad():
+                pred = self.GAT_model(feature.float()).detach().cpu().numpy()
+
+            preds.append(pred)
+
+        return pd.Series(np.concatenate(preds), index=dl_test.get_index())
+
+
+class GATModel(nn.Module):
+    def __init__(self, d_feat=6, hidden_size=64, num_layers=2, dropout=0.0, base_model="GRU"):
+        super().__init__()
+
+        if base_model == "GRU":
+            self.rnn = nn.GRU(
+                input_size=d_feat,
+                hidden_size=hidden_size,
+                num_layers=num_layers,
+                batch_first=True,
+                dropout=dropout,
+            )
+        elif base_model == "LSTM":
+            self.rnn = nn.LSTM(
+                input_size=d_feat,
+                hidden_size=hidden_size,
+                num_layers=num_layers,
+                batch_first=True,
+                dropout=dropout,
+            )
+        else:
+            raise ValueError("unknown base model name `%s`" % base_model)
+
+        self.hidden_size = hidden_size
+        self.d_feat = d_feat
+        self.transformation = nn.Linear(self.hidden_size, self.hidden_size)
+        self.a = nn.Parameter(torch.randn(self.hidden_size * 2, 1))
+        self.a.requires_grad = True
+        self.fc = nn.Linear(self.hidden_size, self.hidden_size)
+        self.fc_out = nn.Linear(hidden_size, 1)
+        self.leaky_relu = nn.LeakyReLU()
+        self.softmax = nn.Softmax(dim=1)
+
+    def cal_attention(self, x, y):
+        x = self.transformation(x)
+        y = self.transformation(y)
+
+        sample_num = x.shape[0]
+        dim = x.shape[1]
+        e_x = x.expand(sample_num, sample_num, dim)
+        e_y = torch.transpose(e_x, 0, 1)
+        attention_in = torch.cat((e_x, e_y), 2).view(-1, dim * 2)
+        self.a_t = torch.t(self.a)
+        attention_out = self.a_t.mm(torch.t(attention_in)).view(sample_num, sample_num)
+        attention_out = self.leaky_relu(attention_out)
+        att_weight = self.softmax(attention_out)
+        return att_weight
+
+    def forward(self, x):
+        out, _ = self.rnn(x)
+        hidden = out[:, -1, :]
+        att_weight = self.cal_attention(hidden, hidden)
+        hidden = att_weight.mm(hidden) + hidden
+        hidden = self.fc(hidden)
+        hidden = self.leaky_relu(hidden)
+        return self.fc_out(hidden).squeeze()
```

## qlib/contrib/model/pytorch_gru.py

```diff
@@ -1,321 +1,314 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-
-from __future__ import division
-from __future__ import print_function
-
-import numpy as np
-import pandas as pd
-from typing import Text, Union
-import copy
-from ...utils import get_or_create_path
-from ...log import get_module_logger
-
-import torch
-import torch.nn as nn
-import torch.optim as optim
-
-from .pytorch_utils import count_parameters
-from ...model.base import Model
-from ...data.dataset import DatasetH
-from ...data.dataset.handler import DataHandlerLP
-
-
-class GRU(Model):
-    """GRU Model
-
-    Parameters
-    ----------
-    d_feat : int
-        input dimension for each time step
-    metric: str
-        the evaluation metric used in early stop
-    optimizer : str
-        optimizer name
-    GPU : str
-        the GPU ID(s) used for training
-    """
-
-    def __init__(
-        self,
-        d_feat=6,
-        hidden_size=64,
-        num_layers=2,
-        dropout=0.0,
-        n_epochs=200,
-        lr=0.001,
-        metric="",
-        batch_size=2000,
-        early_stop=20,
-        loss="mse",
-        optimizer="adam",
-        GPU=0,
-        seed=None,
-        **kwargs
-    ):
-        # Set logger.
-        self.logger = get_module_logger("GRU")
-        self.logger.info("GRU pytorch version...")
-
-        # set hyper-parameters.
-        self.d_feat = d_feat
-        self.hidden_size = hidden_size
-        self.num_layers = num_layers
-        self.dropout = dropout
-        self.n_epochs = n_epochs
-        self.lr = lr
-        self.metric = metric
-        self.batch_size = batch_size
-        self.early_stop = early_stop
-        self.optimizer = optimizer.lower()
-        self.loss = loss
-        self.device = torch.device("cuda:%d" % (GPU) if torch.cuda.is_available() and GPU >= 0 else "cpu")
-        self.seed = seed
-
-        self.logger.info(
-            "GRU parameters setting:"
-            "\nd_feat : {}"
-            "\nhidden_size : {}"
-            "\nnum_layers : {}"
-            "\ndropout : {}"
-            "\nn_epochs : {}"
-            "\nlr : {}"
-            "\nmetric : {}"
-            "\nbatch_size : {}"
-            "\nearly_stop : {}"
-            "\noptimizer : {}"
-            "\nloss_type : {}"
-            "\nvisible_GPU : {}"
-            "\nuse_GPU : {}"
-            "\nseed : {}".format(
-                d_feat,
-                hidden_size,
-                num_layers,
-                dropout,
-                n_epochs,
-                lr,
-                metric,
-                batch_size,
-                early_stop,
-                optimizer.lower(),
-                loss,
-                GPU,
-                self.use_gpu,
-                seed,
-            )
-        )
-
-        if self.seed is not None:
-            np.random.seed(self.seed)
-            torch.manual_seed(self.seed)
-
-        self.gru_model = GRUModel(
-            d_feat=self.d_feat,
-            hidden_size=self.hidden_size,
-            num_layers=self.num_layers,
-            dropout=self.dropout,
-        )
-        self.logger.info("model:\n{:}".format(self.gru_model))
-        self.logger.info("model size: {:.4f} MB".format(count_parameters(self.gru_model)))
-
-        if optimizer.lower() == "adam":
-            self.train_optimizer = optim.Adam(self.gru_model.parameters(), lr=self.lr)
-        elif optimizer.lower() == "gd":
-            self.train_optimizer = optim.SGD(self.gru_model.parameters(), lr=self.lr)
-        else:
-            raise NotImplementedError("optimizer {} is not supported!".format(optimizer))
-
-        self.fitted = False
-        self.gru_model.to(self.device)
-
-    @property
-    def use_gpu(self):
-        return self.device != torch.device("cpu")
-
-    def mse(self, pred, label):
-        loss = (pred - label) ** 2
-        return torch.mean(loss)
-
-    def loss_fn(self, pred, label):
-        mask = ~torch.isnan(label)
-
-        if self.loss == "mse":
-            return self.mse(pred[mask], label[mask])
-
-        raise ValueError("unknown loss `%s`" % self.loss)
-
-    def metric_fn(self, pred, label):
-
-        mask = torch.isfinite(label)
-
-        if self.metric in ("", "loss"):
-            return -self.loss_fn(pred[mask], label[mask])
-
-        raise ValueError("unknown metric `%s`" % self.metric)
-
-    def train_epoch(self, x_train, y_train):
-
-        x_train_values = x_train.values
-        y_train_values = np.squeeze(y_train.values)
-
-        self.gru_model.train()
-
-        indices = np.arange(len(x_train_values))
-        np.random.shuffle(indices)
-
-        for i in range(len(indices))[:: self.batch_size]:
-
-            if len(indices) - i < self.batch_size:
-                break
-
-            feature = torch.from_numpy(x_train_values[indices[i : i + self.batch_size]]).float().to(self.device)
-            label = torch.from_numpy(y_train_values[indices[i : i + self.batch_size]]).float().to(self.device)
-
-            pred = self.gru_model(feature)
-            loss = self.loss_fn(pred, label)
-
-            self.train_optimizer.zero_grad()
-            loss.backward()
-            torch.nn.utils.clip_grad_value_(self.gru_model.parameters(), 3.0)
-            self.train_optimizer.step()
-
-    def test_epoch(self, data_x, data_y):
-
-        # prepare training data
-        x_values = data_x.values
-        y_values = np.squeeze(data_y.values)
-
-        self.gru_model.eval()
-
-        scores = []
-        losses = []
-
-        indices = np.arange(len(x_values))
-
-        for i in range(len(indices))[:: self.batch_size]:
-
-            if len(indices) - i < self.batch_size:
-                break
-
-            feature = torch.from_numpy(x_values[indices[i : i + self.batch_size]]).float().to(self.device)
-            label = torch.from_numpy(y_values[indices[i : i + self.batch_size]]).float().to(self.device)
-
-            with torch.no_grad():
-                pred = self.gru_model(feature)
-                loss = self.loss_fn(pred, label)
-                losses.append(loss.item())
-
-                score = self.metric_fn(pred, label)
-                scores.append(score.item())
-
-        return np.mean(losses), np.mean(scores)
-
-    def fit(
-        self,
-        dataset: DatasetH,
-        evals_result=dict(),
-        save_path=None,
-    ):
-
-        df_train, df_valid, df_test = dataset.prepare(
-            ["train", "valid", "test"],
-            col_set=["feature", "label"],
-            data_key=DataHandlerLP.DK_L,
-        )
-        if df_train.empty or df_valid.empty:
-            raise ValueError("Empty data from dataset, please check your dataset config.")
-
-        x_train, y_train = df_train["feature"], df_train["label"]
-        x_valid, y_valid = df_valid["feature"], df_valid["label"]
-
-        save_path = get_or_create_path(save_path)
-        stop_steps = 0
-        train_loss = 0
-        best_score = -np.inf
-        best_epoch = 0
-        evals_result["train"] = []
-        evals_result["valid"] = []
-
-        # train
-        self.logger.info("training...")
-        self.fitted = True
-
-        for step in range(self.n_epochs):
-            self.logger.info("Epoch%d:", step)
-            self.logger.info("training...")
-            self.train_epoch(x_train, y_train)
-            self.logger.info("evaluating...")
-            train_loss, train_score = self.test_epoch(x_train, y_train)
-            val_loss, val_score = self.test_epoch(x_valid, y_valid)
-            self.logger.info("train %.6f, valid %.6f" % (train_score, val_score))
-            evals_result["train"].append(train_score)
-            evals_result["valid"].append(val_score)
-
-            if val_score > best_score:
-                best_score = val_score
-                stop_steps = 0
-                best_epoch = step
-                best_param = copy.deepcopy(self.gru_model.state_dict())
-            else:
-                stop_steps += 1
-                if stop_steps >= self.early_stop:
-                    self.logger.info("early stop")
-                    break
-
-        self.logger.info("best score: %.6lf @ %d" % (best_score, best_epoch))
-        self.gru_model.load_state_dict(best_param)
-        torch.save(best_param, save_path)
-
-        if self.use_gpu:
-            torch.cuda.empty_cache()
-
-    def predict(self, dataset: DatasetH, segment: Union[Text, slice] = "test"):
-        if not self.fitted:
-            raise ValueError("model is not fitted yet!")
-
-        x_test = dataset.prepare(segment, col_set="feature", data_key=DataHandlerLP.DK_I)
-        index = x_test.index
-        self.gru_model.eval()
-        x_values = x_test.values
-        sample_num = x_values.shape[0]
-        preds = []
-
-        for begin in range(sample_num)[:: self.batch_size]:
-
-            if sample_num - begin < self.batch_size:
-                end = sample_num
-            else:
-                end = begin + self.batch_size
-
-            x_batch = torch.from_numpy(x_values[begin:end]).float().to(self.device)
-
-            with torch.no_grad():
-                pred = self.gru_model(x_batch).detach().cpu().numpy()
-
-            preds.append(pred)
-
-        return pd.Series(np.concatenate(preds), index=index)
-
-
-class GRUModel(nn.Module):
-    def __init__(self, d_feat=6, hidden_size=64, num_layers=2, dropout=0.0):
-        super().__init__()
-
-        self.rnn = nn.GRU(
-            input_size=d_feat,
-            hidden_size=hidden_size,
-            num_layers=num_layers,
-            batch_first=True,
-            dropout=dropout,
-        )
-        self.fc_out = nn.Linear(hidden_size, 1)
-
-        self.d_feat = d_feat
-
-    def forward(self, x):
-        # x: [N, F*T]
-        x = x.reshape(len(x), self.d_feat, -1)  # [N, F, T]
-        x = x.permute(0, 2, 1)  # [N, T, F]
-        out, _ = self.rnn(x)
-        return self.fc_out(out[:, -1, :]).squeeze()
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+
+from __future__ import division
+from __future__ import print_function
+
+import numpy as np
+import pandas as pd
+from typing import Text, Union
+import copy
+from ...utils import get_or_create_path
+from ...log import get_module_logger
+
+import torch
+import torch.nn as nn
+import torch.optim as optim
+
+from .pytorch_utils import count_parameters
+from ...model.base import Model
+from ...data.dataset import DatasetH
+from ...data.dataset.handler import DataHandlerLP
+
+
+class GRU(Model):
+    """GRU Model
+
+    Parameters
+    ----------
+    d_feat : int
+        input dimension for each time step
+    metric: str
+        the evaluation metric used in early stop
+    optimizer : str
+        optimizer name
+    GPU : str
+        the GPU ID(s) used for training
+    """
+
+    def __init__(
+        self,
+        d_feat=6,
+        hidden_size=64,
+        num_layers=2,
+        dropout=0.0,
+        n_epochs=200,
+        lr=0.001,
+        metric="",
+        batch_size=2000,
+        early_stop=20,
+        loss="mse",
+        optimizer="adam",
+        GPU=0,
+        seed=None,
+        **kwargs
+    ):
+        # Set logger.
+        self.logger = get_module_logger("GRU")
+        self.logger.info("GRU pytorch version...")
+
+        # set hyper-parameters.
+        self.d_feat = d_feat
+        self.hidden_size = hidden_size
+        self.num_layers = num_layers
+        self.dropout = dropout
+        self.n_epochs = n_epochs
+        self.lr = lr
+        self.metric = metric
+        self.batch_size = batch_size
+        self.early_stop = early_stop
+        self.optimizer = optimizer.lower()
+        self.loss = loss
+        self.device = torch.device("cuda:%d" % (GPU) if torch.cuda.is_available() and GPU >= 0 else "cpu")
+        self.seed = seed
+
+        self.logger.info(
+            "GRU parameters setting:"
+            "\nd_feat : {}"
+            "\nhidden_size : {}"
+            "\nnum_layers : {}"
+            "\ndropout : {}"
+            "\nn_epochs : {}"
+            "\nlr : {}"
+            "\nmetric : {}"
+            "\nbatch_size : {}"
+            "\nearly_stop : {}"
+            "\noptimizer : {}"
+            "\nloss_type : {}"
+            "\nvisible_GPU : {}"
+            "\nuse_GPU : {}"
+            "\nseed : {}".format(
+                d_feat,
+                hidden_size,
+                num_layers,
+                dropout,
+                n_epochs,
+                lr,
+                metric,
+                batch_size,
+                early_stop,
+                optimizer.lower(),
+                loss,
+                GPU,
+                self.use_gpu,
+                seed,
+            )
+        )
+
+        if self.seed is not None:
+            np.random.seed(self.seed)
+            torch.manual_seed(self.seed)
+
+        self.gru_model = GRUModel(
+            d_feat=self.d_feat,
+            hidden_size=self.hidden_size,
+            num_layers=self.num_layers,
+            dropout=self.dropout,
+        )
+        self.logger.info("model:\n{:}".format(self.gru_model))
+        self.logger.info("model size: {:.4f} MB".format(count_parameters(self.gru_model)))
+
+        if optimizer.lower() == "adam":
+            self.train_optimizer = optim.Adam(self.gru_model.parameters(), lr=self.lr)
+        elif optimizer.lower() == "gd":
+            self.train_optimizer = optim.SGD(self.gru_model.parameters(), lr=self.lr)
+        else:
+            raise NotImplementedError("optimizer {} is not supported!".format(optimizer))
+
+        self.fitted = False
+        self.gru_model.to(self.device)
+
+    @property
+    def use_gpu(self):
+        return self.device != torch.device("cpu")
+
+    def mse(self, pred, label):
+        loss = (pred - label) ** 2
+        return torch.mean(loss)
+
+    def loss_fn(self, pred, label):
+        mask = ~torch.isnan(label)
+
+        if self.loss == "mse":
+            return self.mse(pred[mask], label[mask])
+
+        raise ValueError("unknown loss `%s`" % self.loss)
+
+    def metric_fn(self, pred, label):
+        mask = torch.isfinite(label)
+
+        if self.metric in ("", "loss"):
+            return -self.loss_fn(pred[mask], label[mask])
+
+        raise ValueError("unknown metric `%s`" % self.metric)
+
+    def train_epoch(self, x_train, y_train):
+        x_train_values = x_train.values
+        y_train_values = np.squeeze(y_train.values)
+
+        self.gru_model.train()
+
+        indices = np.arange(len(x_train_values))
+        np.random.shuffle(indices)
+
+        for i in range(len(indices))[:: self.batch_size]:
+            if len(indices) - i < self.batch_size:
+                break
+
+            feature = torch.from_numpy(x_train_values[indices[i : i + self.batch_size]]).float().to(self.device)
+            label = torch.from_numpy(y_train_values[indices[i : i + self.batch_size]]).float().to(self.device)
+
+            pred = self.gru_model(feature)
+            loss = self.loss_fn(pred, label)
+
+            self.train_optimizer.zero_grad()
+            loss.backward()
+            torch.nn.utils.clip_grad_value_(self.gru_model.parameters(), 3.0)
+            self.train_optimizer.step()
+
+    def test_epoch(self, data_x, data_y):
+        # prepare training data
+        x_values = data_x.values
+        y_values = np.squeeze(data_y.values)
+
+        self.gru_model.eval()
+
+        scores = []
+        losses = []
+
+        indices = np.arange(len(x_values))
+
+        for i in range(len(indices))[:: self.batch_size]:
+            if len(indices) - i < self.batch_size:
+                break
+
+            feature = torch.from_numpy(x_values[indices[i : i + self.batch_size]]).float().to(self.device)
+            label = torch.from_numpy(y_values[indices[i : i + self.batch_size]]).float().to(self.device)
+
+            with torch.no_grad():
+                pred = self.gru_model(feature)
+                loss = self.loss_fn(pred, label)
+                losses.append(loss.item())
+
+                score = self.metric_fn(pred, label)
+                scores.append(score.item())
+
+        return np.mean(losses), np.mean(scores)
+
+    def fit(
+        self,
+        dataset: DatasetH,
+        evals_result=dict(),
+        save_path=None,
+    ):
+        df_train, df_valid, df_test = dataset.prepare(
+            ["train", "valid", "test"],
+            col_set=["feature", "label"],
+            data_key=DataHandlerLP.DK_L,
+        )
+        if df_train.empty or df_valid.empty:
+            raise ValueError("Empty data from dataset, please check your dataset config.")
+
+        x_train, y_train = df_train["feature"], df_train["label"]
+        x_valid, y_valid = df_valid["feature"], df_valid["label"]
+
+        save_path = get_or_create_path(save_path)
+        stop_steps = 0
+        train_loss = 0
+        best_score = -np.inf
+        best_epoch = 0
+        evals_result["train"] = []
+        evals_result["valid"] = []
+
+        # train
+        self.logger.info("training...")
+        self.fitted = True
+
+        for step in range(self.n_epochs):
+            self.logger.info("Epoch%d:", step)
+            self.logger.info("training...")
+            self.train_epoch(x_train, y_train)
+            self.logger.info("evaluating...")
+            train_loss, train_score = self.test_epoch(x_train, y_train)
+            val_loss, val_score = self.test_epoch(x_valid, y_valid)
+            self.logger.info("train %.6f, valid %.6f" % (train_score, val_score))
+            evals_result["train"].append(train_score)
+            evals_result["valid"].append(val_score)
+
+            if val_score > best_score:
+                best_score = val_score
+                stop_steps = 0
+                best_epoch = step
+                best_param = copy.deepcopy(self.gru_model.state_dict())
+            else:
+                stop_steps += 1
+                if stop_steps >= self.early_stop:
+                    self.logger.info("early stop")
+                    break
+
+        self.logger.info("best score: %.6lf @ %d" % (best_score, best_epoch))
+        self.gru_model.load_state_dict(best_param)
+        torch.save(best_param, save_path)
+
+        if self.use_gpu:
+            torch.cuda.empty_cache()
+
+    def predict(self, dataset: DatasetH, segment: Union[Text, slice] = "test"):
+        if not self.fitted:
+            raise ValueError("model is not fitted yet!")
+
+        x_test = dataset.prepare(segment, col_set="feature", data_key=DataHandlerLP.DK_I)
+        index = x_test.index
+        self.gru_model.eval()
+        x_values = x_test.values
+        sample_num = x_values.shape[0]
+        preds = []
+
+        for begin in range(sample_num)[:: self.batch_size]:
+            if sample_num - begin < self.batch_size:
+                end = sample_num
+            else:
+                end = begin + self.batch_size
+
+            x_batch = torch.from_numpy(x_values[begin:end]).float().to(self.device)
+
+            with torch.no_grad():
+                pred = self.gru_model(x_batch).detach().cpu().numpy()
+
+            preds.append(pred)
+
+        return pd.Series(np.concatenate(preds), index=index)
+
+
+class GRUModel(nn.Module):
+    def __init__(self, d_feat=6, hidden_size=64, num_layers=2, dropout=0.0):
+        super().__init__()
+
+        self.rnn = nn.GRU(
+            input_size=d_feat,
+            hidden_size=hidden_size,
+            num_layers=num_layers,
+            batch_first=True,
+            dropout=dropout,
+        )
+        self.fc_out = nn.Linear(hidden_size, 1)
+
+        self.d_feat = d_feat
+
+    def forward(self, x):
+        # x: [N, F*T]
+        x = x.reshape(len(x), self.d_feat, -1)  # [N, F, T]
+        x = x.permute(0, 2, 1)  # [N, T, F]
+        out, _ = self.rnn(x)
+        return self.fc_out(out[:, -1, :]).squeeze()
```

## qlib/contrib/model/pytorch_gru_ts.py

```diff
@@ -1,324 +1,319 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-
-from __future__ import division
-from __future__ import print_function
-
-import numpy as np
-import pandas as pd
-import copy
-from ...utils import get_or_create_path
-from ...log import get_module_logger
-
-import torch
-import torch.nn as nn
-import torch.optim as optim
-from torch.utils.data import DataLoader
-
-from .pytorch_utils import count_parameters
-from ...model.base import Model
-from ...data.dataset.handler import DataHandlerLP
-from ...model.utils import ConcatDataset
-from ...data.dataset.weight import Reweighter
-
-
-class GRU(Model):
-    """GRU Model
-
-    Parameters
-    ----------
-    d_feat : int
-        input dimension for each time step
-    metric: str
-        the evaluation metric used in early stop
-    optimizer : str
-        optimizer name
-    GPU : str
-        the GPU ID(s) used for training
-    """
-
-    def __init__(
-        self,
-        d_feat=6,
-        hidden_size=64,
-        num_layers=2,
-        dropout=0.0,
-        n_epochs=200,
-        lr=0.001,
-        metric="",
-        batch_size=2000,
-        early_stop=20,
-        loss="mse",
-        optimizer="adam",
-        n_jobs=10,
-        GPU=0,
-        seed=None,
-        **kwargs
-    ):
-        # Set logger.
-        self.logger = get_module_logger("GRU")
-        self.logger.info("GRU pytorch version...")
-
-        # set hyper-parameters.
-        self.d_feat = d_feat
-        self.hidden_size = hidden_size
-        self.num_layers = num_layers
-        self.dropout = dropout
-        self.n_epochs = n_epochs
-        self.lr = lr
-        self.metric = metric
-        self.batch_size = batch_size
-        self.early_stop = early_stop
-        self.optimizer = optimizer.lower()
-        self.loss = loss
-        self.device = torch.device("cuda:%d" % (GPU) if torch.cuda.is_available() and GPU >= 0 else "cpu")
-        self.n_jobs = n_jobs
-        self.seed = seed
-
-        self.logger.info(
-            "GRU parameters setting:"
-            "\nd_feat : {}"
-            "\nhidden_size : {}"
-            "\nnum_layers : {}"
-            "\ndropout : {}"
-            "\nn_epochs : {}"
-            "\nlr : {}"
-            "\nmetric : {}"
-            "\nbatch_size : {}"
-            "\nearly_stop : {}"
-            "\noptimizer : {}"
-            "\nloss_type : {}"
-            "\ndevice : {}"
-            "\nn_jobs : {}"
-            "\nuse_GPU : {}"
-            "\nseed : {}".format(
-                d_feat,
-                hidden_size,
-                num_layers,
-                dropout,
-                n_epochs,
-                lr,
-                metric,
-                batch_size,
-                early_stop,
-                optimizer.lower(),
-                loss,
-                self.device,
-                n_jobs,
-                self.use_gpu,
-                seed,
-            )
-        )
-
-        if self.seed is not None:
-            np.random.seed(self.seed)
-            torch.manual_seed(self.seed)
-
-        self.GRU_model = GRUModel(
-            d_feat=self.d_feat,
-            hidden_size=self.hidden_size,
-            num_layers=self.num_layers,
-            dropout=self.dropout,
-        )
-        self.logger.info("model:\n{:}".format(self.GRU_model))
-        self.logger.info("model size: {:.4f} MB".format(count_parameters(self.GRU_model)))
-
-        if optimizer.lower() == "adam":
-            self.train_optimizer = optim.Adam(self.GRU_model.parameters(), lr=self.lr)
-        elif optimizer.lower() == "gd":
-            self.train_optimizer = optim.SGD(self.GRU_model.parameters(), lr=self.lr)
-        else:
-            raise NotImplementedError("optimizer {} is not supported!".format(optimizer))
-
-        self.fitted = False
-        self.GRU_model.to(self.device)
-
-    @property
-    def use_gpu(self):
-        return self.device != torch.device("cpu")
-
-    def mse(self, pred, label, weight):
-        loss = weight * (pred - label) ** 2
-        return torch.mean(loss)
-
-    def loss_fn(self, pred, label, weight=None):
-        mask = ~torch.isnan(label)
-
-        if weight is None:
-            weight = torch.ones_like(label)
-
-        if self.loss == "mse":
-            return self.mse(pred[mask], label[mask], weight[mask])
-
-        raise ValueError("unknown loss `%s`" % self.loss)
-
-    def metric_fn(self, pred, label):
-
-        mask = torch.isfinite(label)
-
-        if self.metric in ("", "loss"):
-            return -self.loss_fn(pred[mask], label[mask])
-
-        raise ValueError("unknown metric `%s`" % self.metric)
-
-    def train_epoch(self, data_loader):
-
-        self.GRU_model.train()
-
-        for (data, weight) in data_loader:
-            feature = data[:, :, 0:-1].to(self.device)
-            label = data[:, -1, -1].to(self.device)
-
-            pred = self.GRU_model(feature.float())
-            loss = self.loss_fn(pred, label, weight.to(self.device))
-
-            self.train_optimizer.zero_grad()
-            loss.backward()
-            torch.nn.utils.clip_grad_value_(self.GRU_model.parameters(), 3.0)
-            self.train_optimizer.step()
-
-    def test_epoch(self, data_loader):
-
-        self.GRU_model.eval()
-
-        scores = []
-        losses = []
-
-        for (data, weight) in data_loader:
-
-            feature = data[:, :, 0:-1].to(self.device)
-            # feature[torch.isnan(feature)] = 0
-            label = data[:, -1, -1].to(self.device)
-
-            with torch.no_grad():
-                pred = self.GRU_model(feature.float())
-                loss = self.loss_fn(pred, label, weight.to(self.device))
-                losses.append(loss.item())
-
-                score = self.metric_fn(pred, label)
-                scores.append(score.item())
-
-        return np.mean(losses), np.mean(scores)
-
-    def fit(
-        self,
-        dataset,
-        evals_result=dict(),
-        save_path=None,
-        reweighter=None,
-    ):
-        dl_train = dataset.prepare("train", col_set=["feature", "label"], data_key=DataHandlerLP.DK_L)
-        dl_valid = dataset.prepare("valid", col_set=["feature", "label"], data_key=DataHandlerLP.DK_L)
-        if dl_train.empty or dl_valid.empty:
-            raise ValueError("Empty data from dataset, please check your dataset config.")
-
-        dl_train.config(fillna_type="ffill+bfill")  # process nan brought by dataloader
-        dl_valid.config(fillna_type="ffill+bfill")  # process nan brought by dataloader
-
-        if reweighter is None:
-            wl_train = np.ones(len(dl_train))
-            wl_valid = np.ones(len(dl_valid))
-        elif isinstance(reweighter, Reweighter):
-            wl_train = reweighter.reweight(dl_train)
-            wl_valid = reweighter.reweight(dl_valid)
-        else:
-            raise ValueError("Unsupported reweighter type.")
-
-        train_loader = DataLoader(
-            ConcatDataset(dl_train, wl_train),
-            batch_size=self.batch_size,
-            shuffle=True,
-            num_workers=self.n_jobs,
-            drop_last=True,
-        )
-        valid_loader = DataLoader(
-            ConcatDataset(dl_valid, wl_valid),
-            batch_size=self.batch_size,
-            shuffle=False,
-            num_workers=self.n_jobs,
-            drop_last=True,
-        )
-
-        save_path = get_or_create_path(save_path)
-
-        stop_steps = 0
-        train_loss = 0
-        best_score = -np.inf
-        best_epoch = 0
-        evals_result["train"] = []
-        evals_result["valid"] = []
-
-        # train
-        self.logger.info("training...")
-        self.fitted = True
-
-        for step in range(self.n_epochs):
-            self.logger.info("Epoch%d:", step)
-            self.logger.info("training...")
-            self.train_epoch(train_loader)
-            self.logger.info("evaluating...")
-            train_loss, train_score = self.test_epoch(train_loader)
-            val_loss, val_score = self.test_epoch(valid_loader)
-            self.logger.info("train %.6f, valid %.6f" % (train_score, val_score))
-            evals_result["train"].append(train_score)
-            evals_result["valid"].append(val_score)
-
-            if val_score > best_score:
-                best_score = val_score
-                stop_steps = 0
-                best_epoch = step
-                best_param = copy.deepcopy(self.GRU_model.state_dict())
-            else:
-                stop_steps += 1
-                if stop_steps >= self.early_stop:
-                    self.logger.info("early stop")
-                    break
-
-        self.logger.info("best score: %.6lf @ %d" % (best_score, best_epoch))
-        self.GRU_model.load_state_dict(best_param)
-        torch.save(best_param, save_path)
-
-        if self.use_gpu:
-            torch.cuda.empty_cache()
-
-    def predict(self, dataset):
-        if not self.fitted:
-            raise ValueError("model is not fitted yet!")
-
-        dl_test = dataset.prepare("test", col_set=["feature", "label"], data_key=DataHandlerLP.DK_I)
-        dl_test.config(fillna_type="ffill+bfill")
-        test_loader = DataLoader(dl_test, batch_size=self.batch_size, num_workers=self.n_jobs)
-        self.GRU_model.eval()
-        preds = []
-
-        for data in test_loader:
-
-            feature = data[:, :, 0:-1].to(self.device)
-
-            with torch.no_grad():
-                pred = self.GRU_model(feature.float()).detach().cpu().numpy()
-
-            preds.append(pred)
-
-        return pd.Series(np.concatenate(preds), index=dl_test.get_index())
-
-
-class GRUModel(nn.Module):
-    def __init__(self, d_feat=6, hidden_size=64, num_layers=2, dropout=0.0):
-        super().__init__()
-
-        self.rnn = nn.GRU(
-            input_size=d_feat,
-            hidden_size=hidden_size,
-            num_layers=num_layers,
-            batch_first=True,
-            dropout=dropout,
-        )
-        self.fc_out = nn.Linear(hidden_size, 1)
-
-        self.d_feat = d_feat
-
-    def forward(self, x):
-        out, _ = self.rnn(x)
-        return self.fc_out(out[:, -1, :]).squeeze()
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+
+from __future__ import division
+from __future__ import print_function
+
+import numpy as np
+import pandas as pd
+import copy
+from ...utils import get_or_create_path
+from ...log import get_module_logger
+
+import torch
+import torch.nn as nn
+import torch.optim as optim
+from torch.utils.data import DataLoader
+
+from .pytorch_utils import count_parameters
+from ...model.base import Model
+from ...data.dataset.handler import DataHandlerLP
+from ...model.utils import ConcatDataset
+from ...data.dataset.weight import Reweighter
+
+
+class GRU(Model):
+    """GRU Model
+
+    Parameters
+    ----------
+    d_feat : int
+        input dimension for each time step
+    metric: str
+        the evaluation metric used in early stop
+    optimizer : str
+        optimizer name
+    GPU : str
+        the GPU ID(s) used for training
+    """
+
+    def __init__(
+        self,
+        d_feat=6,
+        hidden_size=64,
+        num_layers=2,
+        dropout=0.0,
+        n_epochs=200,
+        lr=0.001,
+        metric="",
+        batch_size=2000,
+        early_stop=20,
+        loss="mse",
+        optimizer="adam",
+        n_jobs=10,
+        GPU=0,
+        seed=None,
+        **kwargs
+    ):
+        # Set logger.
+        self.logger = get_module_logger("GRU")
+        self.logger.info("GRU pytorch version...")
+
+        # set hyper-parameters.
+        self.d_feat = d_feat
+        self.hidden_size = hidden_size
+        self.num_layers = num_layers
+        self.dropout = dropout
+        self.n_epochs = n_epochs
+        self.lr = lr
+        self.metric = metric
+        self.batch_size = batch_size
+        self.early_stop = early_stop
+        self.optimizer = optimizer.lower()
+        self.loss = loss
+        self.device = torch.device("cuda:%d" % (GPU) if torch.cuda.is_available() and GPU >= 0 else "cpu")
+        self.n_jobs = n_jobs
+        self.seed = seed
+
+        self.logger.info(
+            "GRU parameters setting:"
+            "\nd_feat : {}"
+            "\nhidden_size : {}"
+            "\nnum_layers : {}"
+            "\ndropout : {}"
+            "\nn_epochs : {}"
+            "\nlr : {}"
+            "\nmetric : {}"
+            "\nbatch_size : {}"
+            "\nearly_stop : {}"
+            "\noptimizer : {}"
+            "\nloss_type : {}"
+            "\ndevice : {}"
+            "\nn_jobs : {}"
+            "\nuse_GPU : {}"
+            "\nseed : {}".format(
+                d_feat,
+                hidden_size,
+                num_layers,
+                dropout,
+                n_epochs,
+                lr,
+                metric,
+                batch_size,
+                early_stop,
+                optimizer.lower(),
+                loss,
+                self.device,
+                n_jobs,
+                self.use_gpu,
+                seed,
+            )
+        )
+
+        if self.seed is not None:
+            np.random.seed(self.seed)
+            torch.manual_seed(self.seed)
+
+        self.GRU_model = GRUModel(
+            d_feat=self.d_feat,
+            hidden_size=self.hidden_size,
+            num_layers=self.num_layers,
+            dropout=self.dropout,
+        )
+        self.logger.info("model:\n{:}".format(self.GRU_model))
+        self.logger.info("model size: {:.4f} MB".format(count_parameters(self.GRU_model)))
+
+        if optimizer.lower() == "adam":
+            self.train_optimizer = optim.Adam(self.GRU_model.parameters(), lr=self.lr)
+        elif optimizer.lower() == "gd":
+            self.train_optimizer = optim.SGD(self.GRU_model.parameters(), lr=self.lr)
+        else:
+            raise NotImplementedError("optimizer {} is not supported!".format(optimizer))
+
+        self.fitted = False
+        self.GRU_model.to(self.device)
+
+    @property
+    def use_gpu(self):
+        return self.device != torch.device("cpu")
+
+    def mse(self, pred, label, weight):
+        loss = weight * (pred - label) ** 2
+        return torch.mean(loss)
+
+    def loss_fn(self, pred, label, weight=None):
+        mask = ~torch.isnan(label)
+
+        if weight is None:
+            weight = torch.ones_like(label)
+
+        if self.loss == "mse":
+            return self.mse(pred[mask], label[mask], weight[mask])
+
+        raise ValueError("unknown loss `%s`" % self.loss)
+
+    def metric_fn(self, pred, label):
+        mask = torch.isfinite(label)
+
+        if self.metric in ("", "loss"):
+            return -self.loss_fn(pred[mask], label[mask])
+
+        raise ValueError("unknown metric `%s`" % self.metric)
+
+    def train_epoch(self, data_loader):
+        self.GRU_model.train()
+
+        for data, weight in data_loader:
+            feature = data[:, :, 0:-1].to(self.device)
+            label = data[:, -1, -1].to(self.device)
+
+            pred = self.GRU_model(feature.float())
+            loss = self.loss_fn(pred, label, weight.to(self.device))
+
+            self.train_optimizer.zero_grad()
+            loss.backward()
+            torch.nn.utils.clip_grad_value_(self.GRU_model.parameters(), 3.0)
+            self.train_optimizer.step()
+
+    def test_epoch(self, data_loader):
+        self.GRU_model.eval()
+
+        scores = []
+        losses = []
+
+        for data, weight in data_loader:
+            feature = data[:, :, 0:-1].to(self.device)
+            # feature[torch.isnan(feature)] = 0
+            label = data[:, -1, -1].to(self.device)
+
+            with torch.no_grad():
+                pred = self.GRU_model(feature.float())
+                loss = self.loss_fn(pred, label, weight.to(self.device))
+                losses.append(loss.item())
+
+                score = self.metric_fn(pred, label)
+                scores.append(score.item())
+
+        return np.mean(losses), np.mean(scores)
+
+    def fit(
+        self,
+        dataset,
+        evals_result=dict(),
+        save_path=None,
+        reweighter=None,
+    ):
+        dl_train = dataset.prepare("train", col_set=["feature", "label"], data_key=DataHandlerLP.DK_L)
+        dl_valid = dataset.prepare("valid", col_set=["feature", "label"], data_key=DataHandlerLP.DK_L)
+        if dl_train.empty or dl_valid.empty:
+            raise ValueError("Empty data from dataset, please check your dataset config.")
+
+        dl_train.config(fillna_type="ffill+bfill")  # process nan brought by dataloader
+        dl_valid.config(fillna_type="ffill+bfill")  # process nan brought by dataloader
+
+        if reweighter is None:
+            wl_train = np.ones(len(dl_train))
+            wl_valid = np.ones(len(dl_valid))
+        elif isinstance(reweighter, Reweighter):
+            wl_train = reweighter.reweight(dl_train)
+            wl_valid = reweighter.reweight(dl_valid)
+        else:
+            raise ValueError("Unsupported reweighter type.")
+
+        train_loader = DataLoader(
+            ConcatDataset(dl_train, wl_train),
+            batch_size=self.batch_size,
+            shuffle=True,
+            num_workers=self.n_jobs,
+            drop_last=True,
+        )
+        valid_loader = DataLoader(
+            ConcatDataset(dl_valid, wl_valid),
+            batch_size=self.batch_size,
+            shuffle=False,
+            num_workers=self.n_jobs,
+            drop_last=True,
+        )
+
+        save_path = get_or_create_path(save_path)
+
+        stop_steps = 0
+        train_loss = 0
+        best_score = -np.inf
+        best_epoch = 0
+        evals_result["train"] = []
+        evals_result["valid"] = []
+
+        # train
+        self.logger.info("training...")
+        self.fitted = True
+
+        for step in range(self.n_epochs):
+            self.logger.info("Epoch%d:", step)
+            self.logger.info("training...")
+            self.train_epoch(train_loader)
+            self.logger.info("evaluating...")
+            train_loss, train_score = self.test_epoch(train_loader)
+            val_loss, val_score = self.test_epoch(valid_loader)
+            self.logger.info("train %.6f, valid %.6f" % (train_score, val_score))
+            evals_result["train"].append(train_score)
+            evals_result["valid"].append(val_score)
+
+            if val_score > best_score:
+                best_score = val_score
+                stop_steps = 0
+                best_epoch = step
+                best_param = copy.deepcopy(self.GRU_model.state_dict())
+            else:
+                stop_steps += 1
+                if stop_steps >= self.early_stop:
+                    self.logger.info("early stop")
+                    break
+
+        self.logger.info("best score: %.6lf @ %d" % (best_score, best_epoch))
+        self.GRU_model.load_state_dict(best_param)
+        torch.save(best_param, save_path)
+
+        if self.use_gpu:
+            torch.cuda.empty_cache()
+
+    def predict(self, dataset):
+        if not self.fitted:
+            raise ValueError("model is not fitted yet!")
+
+        dl_test = dataset.prepare("test", col_set=["feature", "label"], data_key=DataHandlerLP.DK_I)
+        dl_test.config(fillna_type="ffill+bfill")
+        test_loader = DataLoader(dl_test, batch_size=self.batch_size, num_workers=self.n_jobs)
+        self.GRU_model.eval()
+        preds = []
+
+        for data in test_loader:
+            feature = data[:, :, 0:-1].to(self.device)
+
+            with torch.no_grad():
+                pred = self.GRU_model(feature.float()).detach().cpu().numpy()
+
+            preds.append(pred)
+
+        return pd.Series(np.concatenate(preds), index=dl_test.get_index())
+
+
+class GRUModel(nn.Module):
+    def __init__(self, d_feat=6, hidden_size=64, num_layers=2, dropout=0.0):
+        super().__init__()
+
+        self.rnn = nn.GRU(
+            input_size=d_feat,
+            hidden_size=hidden_size,
+            num_layers=num_layers,
+            batch_first=True,
+            dropout=dropout,
+        )
+        self.fc_out = nn.Linear(hidden_size, 1)
+
+        self.d_feat = d_feat
+
+    def forward(self, x):
+        out, _ = self.rnn(x)
+        return self.fc_out(out[:, -1, :]).squeeze()
```

## qlib/contrib/model/pytorch_hist.py

```diff
@@ -1,503 +1,500 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-
-from __future__ import division
-from __future__ import print_function
-
-import os
-import numpy as np
-import pandas as pd
-from typing import Text, Union
-import urllib.request
-import copy
-from ...utils import get_or_create_path
-from ...log import get_module_logger
-import torch
-import torch.nn as nn
-import torch.optim as optim
-from .pytorch_utils import count_parameters
-from ...model.base import Model
-from ...data.dataset import DatasetH
-from ...data.dataset.handler import DataHandlerLP
-from ...contrib.model.pytorch_lstm import LSTMModel
-from ...contrib.model.pytorch_gru import GRUModel
-
-
-class HIST(Model):
-    """HIST Model
-
-    Parameters
-    ----------
-    lr : float
-        learning rate
-    d_feat : int
-        input dimensions for each time step
-    metric : str
-        the evaluation metric used in early stop
-    optimizer : str
-        optimizer name
-    GPU : str
-        the GPU ID(s) used for training
-    """
-
-    def __init__(
-        self,
-        d_feat=6,
-        hidden_size=64,
-        num_layers=2,
-        dropout=0.0,
-        n_epochs=200,
-        lr=0.001,
-        metric="",
-        early_stop=20,
-        loss="mse",
-        base_model="GRU",
-        model_path=None,
-        stock2concept=None,
-        stock_index=None,
-        optimizer="adam",
-        GPU=0,
-        seed=None,
-        **kwargs
-    ):
-        # Set logger.
-        self.logger = get_module_logger("HIST")
-        self.logger.info("HIST pytorch version...")
-
-        # set hyper-parameters.
-        self.d_feat = d_feat
-        self.hidden_size = hidden_size
-        self.num_layers = num_layers
-        self.dropout = dropout
-        self.n_epochs = n_epochs
-        self.lr = lr
-        self.metric = metric
-        self.early_stop = early_stop
-        self.optimizer = optimizer.lower()
-        self.loss = loss
-        self.base_model = base_model
-        self.model_path = model_path
-        self.stock2concept = stock2concept
-        self.stock_index = stock_index
-        self.device = torch.device("cuda:%d" % (GPU) if torch.cuda.is_available() and GPU >= 0 else "cpu")
-        self.seed = seed
-
-        self.logger.info(
-            "HIST parameters setting:"
-            "\nd_feat : {}"
-            "\nhidden_size : {}"
-            "\nnum_layers : {}"
-            "\ndropout : {}"
-            "\nn_epochs : {}"
-            "\nlr : {}"
-            "\nmetric : {}"
-            "\nearly_stop : {}"
-            "\noptimizer : {}"
-            "\nloss_type : {}"
-            "\nbase_model : {}"
-            "\nmodel_path : {}"
-            "\nstock2concept : {}"
-            "\nstock_index : {}"
-            "\nuse_GPU : {}"
-            "\nseed : {}".format(
-                d_feat,
-                hidden_size,
-                num_layers,
-                dropout,
-                n_epochs,
-                lr,
-                metric,
-                early_stop,
-                optimizer.lower(),
-                loss,
-                base_model,
-                model_path,
-                stock2concept,
-                stock_index,
-                GPU,
-                seed,
-            )
-        )
-
-        if self.seed is not None:
-            np.random.seed(self.seed)
-            torch.manual_seed(self.seed)
-
-        self.HIST_model = HISTModel(
-            d_feat=self.d_feat,
-            hidden_size=self.hidden_size,
-            num_layers=self.num_layers,
-            dropout=self.dropout,
-            base_model=self.base_model,
-        )
-        self.logger.info("model:\n{:}".format(self.HIST_model))
-        self.logger.info("model size: {:.4f} MB".format(count_parameters(self.HIST_model)))
-        if optimizer.lower() == "adam":
-            self.train_optimizer = optim.Adam(self.HIST_model.parameters(), lr=self.lr)
-        elif optimizer.lower() == "gd":
-            self.train_optimizer = optim.SGD(self.HIST_model.parameters(), lr=self.lr)
-        else:
-            raise NotImplementedError("optimizer {} is not supported!".format(optimizer))
-
-        self.fitted = False
-        self.HIST_model.to(self.device)
-
-    @property
-    def use_gpu(self):
-        return self.device != torch.device("cpu")
-
-    def mse(self, pred, label):
-        loss = (pred - label) ** 2
-        return torch.mean(loss)
-
-    def loss_fn(self, pred, label):
-        mask = ~torch.isnan(label)
-
-        if self.loss == "mse":
-            return self.mse(pred[mask], label[mask])
-
-        raise ValueError("unknown loss `%s`" % self.loss)
-
-    def metric_fn(self, pred, label):
-
-        mask = torch.isfinite(label)
-
-        if self.metric == "ic":
-            x = pred[mask]
-            y = label[mask]
-
-            vx = x - torch.mean(x)
-            vy = y - torch.mean(y)
-            return torch.sum(vx * vy) / (torch.sqrt(torch.sum(vx**2)) * torch.sqrt(torch.sum(vy**2)))
-
-        if self.metric == ("", "loss"):
-            return -self.loss_fn(pred[mask], label[mask])
-
-        raise ValueError("unknown metric `%s`" % self.metric)
-
-    def get_daily_inter(self, df, shuffle=False):
-        # organize the train data into daily batches
-        daily_count = df.groupby(level=0).size().values
-        daily_index = np.roll(np.cumsum(daily_count), 1)
-        daily_index[0] = 0
-        if shuffle:
-            # shuffle data
-            daily_shuffle = list(zip(daily_index, daily_count))
-            np.random.shuffle(daily_shuffle)
-            daily_index, daily_count = zip(*daily_shuffle)
-        return daily_index, daily_count
-
-    def train_epoch(self, x_train, y_train, stock_index):
-
-        stock2concept_matrix = np.load(self.stock2concept)
-        x_train_values = x_train.values
-        y_train_values = np.squeeze(y_train.values)
-        stock_index = stock_index.values
-        stock_index[np.isnan(stock_index)] = 733
-        self.HIST_model.train()
-
-        # organize the train data into daily batches
-        daily_index, daily_count = self.get_daily_inter(x_train, shuffle=True)
-
-        for idx, count in zip(daily_index, daily_count):
-            batch = slice(idx, idx + count)
-            feature = torch.from_numpy(x_train_values[batch]).float().to(self.device)
-            concept_matrix = torch.from_numpy(stock2concept_matrix[stock_index[batch]]).float().to(self.device)
-            label = torch.from_numpy(y_train_values[batch]).float().to(self.device)
-            pred = self.HIST_model(feature, concept_matrix)
-            loss = self.loss_fn(pred, label)
-
-            self.train_optimizer.zero_grad()
-            loss.backward()
-            torch.nn.utils.clip_grad_value_(self.HIST_model.parameters(), 3.0)
-            self.train_optimizer.step()
-
-    def test_epoch(self, data_x, data_y, stock_index):
-
-        # prepare training data
-        stock2concept_matrix = np.load(self.stock2concept)
-        x_values = data_x.values
-        y_values = np.squeeze(data_y.values)
-        stock_index = stock_index.values
-        stock_index[np.isnan(stock_index)] = 733
-        self.HIST_model.eval()
-
-        scores = []
-        losses = []
-
-        # organize the test data into daily batches
-        daily_index, daily_count = self.get_daily_inter(data_x, shuffle=False)
-
-        for idx, count in zip(daily_index, daily_count):
-            batch = slice(idx, idx + count)
-            feature = torch.from_numpy(x_values[batch]).float().to(self.device)
-            concept_matrix = torch.from_numpy(stock2concept_matrix[stock_index[batch]]).float().to(self.device)
-            label = torch.from_numpy(y_values[batch]).float().to(self.device)
-            with torch.no_grad():
-                pred = self.HIST_model(feature, concept_matrix)
-                loss = self.loss_fn(pred, label)
-                losses.append(loss.item())
-
-                score = self.metric_fn(pred, label)
-                scores.append(score.item())
-
-        return np.mean(losses), np.mean(scores)
-
-    def fit(
-        self,
-        dataset: DatasetH,
-        evals_result=dict(),
-        save_path=None,
-    ):
-        df_train, df_valid, df_test = dataset.prepare(
-            ["train", "valid", "test"],
-            col_set=["feature", "label"],
-            data_key=DataHandlerLP.DK_L,
-        )
-        if df_train.empty or df_valid.empty:
-            raise ValueError("Empty data from dataset, please check your dataset config.")
-
-        if not os.path.exists(self.stock2concept):
-            url = "http://fintech.msra.cn/stock_data/downloads/qlib_csi300_stock2concept.npy"
-            urllib.request.urlretrieve(url, self.stock2concept)
-
-        stock_index = np.load(self.stock_index, allow_pickle=True).item()
-        df_train["stock_index"] = 733
-        df_train["stock_index"] = df_train.index.get_level_values("instrument").map(stock_index)
-        df_valid["stock_index"] = 733
-        df_valid["stock_index"] = df_valid.index.get_level_values("instrument").map(stock_index)
-
-        x_train, y_train, stock_index_train = df_train["feature"], df_train["label"], df_train["stock_index"]
-        x_valid, y_valid, stock_index_valid = df_valid["feature"], df_valid["label"], df_valid["stock_index"]
-
-        save_path = get_or_create_path(save_path)
-
-        stop_steps = 0
-        best_score = -np.inf
-        best_epoch = 0
-        evals_result["train"] = []
-        evals_result["valid"] = []
-
-        # load pretrained base_model
-        if self.base_model == "LSTM":
-            pretrained_model = LSTMModel()
-        elif self.base_model == "GRU":
-            pretrained_model = GRUModel()
-        else:
-            raise ValueError("unknown base model name `%s`" % self.base_model)
-
-        if self.model_path is not None:
-            self.logger.info("Loading pretrained model...")
-            pretrained_model.load_state_dict(torch.load(self.model_path))
-
-        model_dict = self.HIST_model.state_dict()
-        pretrained_dict = {
-            k: v for k, v in pretrained_model.state_dict().items() if k in model_dict  # pylint: disable=E1135
-        }
-        model_dict.update(pretrained_dict)
-        self.HIST_model.load_state_dict(model_dict)
-        self.logger.info("Loading pretrained model Done...")
-
-        # train
-        self.logger.info("training...")
-        self.fitted = True
-
-        for step in range(self.n_epochs):
-            self.logger.info("Epoch%d:", step)
-            self.logger.info("training...")
-            self.train_epoch(x_train, y_train, stock_index_train)
-
-            self.logger.info("evaluating...")
-            train_loss, train_score = self.test_epoch(x_train, y_train, stock_index_train)
-            val_loss, val_score = self.test_epoch(x_valid, y_valid, stock_index_valid)
-            self.logger.info("train %.6f, valid %.6f" % (train_score, val_score))
-            evals_result["train"].append(train_score)
-            evals_result["valid"].append(val_score)
-
-            if val_score > best_score:
-                best_score = val_score
-                stop_steps = 0
-                best_epoch = step
-                best_param = copy.deepcopy(self.HIST_model.state_dict())
-            else:
-                stop_steps += 1
-                if stop_steps >= self.early_stop:
-                    self.logger.info("early stop")
-                    break
-
-        self.logger.info("best score: %.6lf @ %d" % (best_score, best_epoch))
-        self.HIST_model.load_state_dict(best_param)
-        torch.save(best_param, save_path)
-
-    def predict(self, dataset: DatasetH, segment: Union[Text, slice] = "test"):
-        if not self.fitted:
-            raise ValueError("model is not fitted yet!")
-
-        stock2concept_matrix = np.load(self.stock2concept)
-        stock_index = np.load(self.stock_index, allow_pickle=True).item()
-        df_test = dataset.prepare(segment, col_set="feature", data_key=DataHandlerLP.DK_I)
-        df_test["stock_index"] = 733
-        df_test["stock_index"] = df_test.index.get_level_values("instrument").map(stock_index)
-        stock_index_test = df_test["stock_index"].values
-        stock_index_test[np.isnan(stock_index_test)] = 733
-        stock_index_test = stock_index_test.astype("int")
-        df_test = df_test.drop(["stock_index"], axis=1)
-        index = df_test.index
-
-        self.HIST_model.eval()
-        x_values = df_test.values
-        preds = []
-
-        # organize the data into daily batches
-        daily_index, daily_count = self.get_daily_inter(df_test, shuffle=False)
-
-        for idx, count in zip(daily_index, daily_count):
-            batch = slice(idx, idx + count)
-            x_batch = torch.from_numpy(x_values[batch]).float().to(self.device)
-            concept_matrix = torch.from_numpy(stock2concept_matrix[stock_index_test[batch]]).float().to(self.device)
-
-            with torch.no_grad():
-                pred = self.HIST_model(x_batch, concept_matrix).detach().cpu().numpy()
-
-            preds.append(pred)
-
-        return pd.Series(np.concatenate(preds), index=index)
-
-
-class HISTModel(nn.Module):
-    def __init__(self, d_feat=6, hidden_size=64, num_layers=2, dropout=0.0, base_model="GRU"):
-        super().__init__()
-
-        self.d_feat = d_feat
-        self.hidden_size = hidden_size
-
-        if base_model == "GRU":
-            self.rnn = nn.GRU(
-                input_size=d_feat,
-                hidden_size=hidden_size,
-                num_layers=num_layers,
-                batch_first=True,
-                dropout=dropout,
-            )
-        elif base_model == "LSTM":
-            self.rnn = nn.LSTM(
-                input_size=d_feat,
-                hidden_size=hidden_size,
-                num_layers=num_layers,
-                batch_first=True,
-                dropout=dropout,
-            )
-        else:
-            raise ValueError("unknown base model name `%s`" % base_model)
-
-        self.fc_es = nn.Linear(hidden_size, hidden_size)
-        torch.nn.init.xavier_uniform_(self.fc_es.weight)
-        self.fc_is = nn.Linear(hidden_size, hidden_size)
-        torch.nn.init.xavier_uniform_(self.fc_is.weight)
-
-        self.fc_es_middle = nn.Linear(hidden_size, hidden_size)
-        torch.nn.init.xavier_uniform_(self.fc_es_middle.weight)
-        self.fc_is_middle = nn.Linear(hidden_size, hidden_size)
-        torch.nn.init.xavier_uniform_(self.fc_is_middle.weight)
-
-        self.fc_es_fore = nn.Linear(hidden_size, hidden_size)
-        torch.nn.init.xavier_uniform_(self.fc_es_fore.weight)
-        self.fc_is_fore = nn.Linear(hidden_size, hidden_size)
-        torch.nn.init.xavier_uniform_(self.fc_is_fore.weight)
-        self.fc_indi_fore = nn.Linear(hidden_size, hidden_size)
-        torch.nn.init.xavier_uniform_(self.fc_indi_fore.weight)
-
-        self.fc_es_back = nn.Linear(hidden_size, hidden_size)
-        torch.nn.init.xavier_uniform_(self.fc_es_back.weight)
-        self.fc_is_back = nn.Linear(hidden_size, hidden_size)
-        torch.nn.init.xavier_uniform_(self.fc_is_back.weight)
-        self.fc_indi = nn.Linear(hidden_size, hidden_size)
-        torch.nn.init.xavier_uniform_(self.fc_indi.weight)
-
-        self.leaky_relu = nn.LeakyReLU()
-        self.softmax_s2t = torch.nn.Softmax(dim=0)
-        self.softmax_t2s = torch.nn.Softmax(dim=1)
-
-        self.fc_out_es = nn.Linear(hidden_size, 1)
-        self.fc_out_is = nn.Linear(hidden_size, 1)
-        self.fc_out_indi = nn.Linear(hidden_size, 1)
-        self.fc_out = nn.Linear(hidden_size, 1)
-
-    def cal_cos_similarity(self, x, y):  # the 2nd dimension of x and y are the same
-        xy = x.mm(torch.t(y))
-        x_norm = torch.sqrt(torch.sum(x * x, dim=1)).reshape(-1, 1)
-        y_norm = torch.sqrt(torch.sum(y * y, dim=1)).reshape(-1, 1)
-        cos_similarity = xy / (x_norm.mm(torch.t(y_norm)) + 1e-6)
-        return cos_similarity
-
-    def forward(self, x, concept_matrix):
-        device = torch.device(torch.get_device(x))
-
-        x_hidden = x.reshape(len(x), self.d_feat, -1)  # [N, F, T]
-        x_hidden = x_hidden.permute(0, 2, 1)  # [N, T, F]
-        x_hidden, _ = self.rnn(x_hidden)
-        x_hidden = x_hidden[:, -1, :]
-
-        # Predefined Concept Module
-
-        stock_to_concept = concept_matrix
-
-        stock_to_concept_sum = torch.sum(stock_to_concept, 0).reshape(1, -1).repeat(stock_to_concept.shape[0], 1)
-        stock_to_concept_sum = stock_to_concept_sum.mul(concept_matrix)
-
-        stock_to_concept_sum = stock_to_concept_sum + (
-            torch.ones(stock_to_concept.shape[0], stock_to_concept.shape[1]).to(device)
-        )
-        stock_to_concept = stock_to_concept / stock_to_concept_sum
-        hidden = torch.t(stock_to_concept).mm(x_hidden)
-
-        hidden = hidden[hidden.sum(1) != 0]
-
-        concept_to_stock = self.cal_cos_similarity(x_hidden, hidden)
-        concept_to_stock = self.softmax_t2s(concept_to_stock)
-
-        e_shared_info = concept_to_stock.mm(hidden)
-        e_shared_info = self.fc_es(e_shared_info)
-
-        e_shared_back = self.fc_es_back(e_shared_info)
-        output_es = self.fc_es_fore(e_shared_info)
-        output_es = self.leaky_relu(output_es)
-
-        # Hidden Concept Module
-        i_shared_info = x_hidden - e_shared_back
-        hidden = i_shared_info
-        i_stock_to_concept = self.cal_cos_similarity(i_shared_info, hidden)
-        dim = i_stock_to_concept.shape[0]
-        diag = i_stock_to_concept.diagonal(0)
-        i_stock_to_concept = i_stock_to_concept * (torch.ones(dim, dim) - torch.eye(dim)).to(device)
-        row = torch.linspace(0, dim - 1, dim).to(device).long()
-        column = i_stock_to_concept.max(1)[1].long()
-        value = i_stock_to_concept.max(1)[0]
-        i_stock_to_concept[row, column] = 10
-        i_stock_to_concept[i_stock_to_concept != 10] = 0
-        i_stock_to_concept[row, column] = value
-        i_stock_to_concept = i_stock_to_concept + torch.diag_embed((i_stock_to_concept.sum(0) != 0).float() * diag)
-        hidden = torch.t(i_shared_info).mm(i_stock_to_concept).t()
-        hidden = hidden[hidden.sum(1) != 0]
-
-        i_concept_to_stock = self.cal_cos_similarity(i_shared_info, hidden)
-        i_concept_to_stock = self.softmax_t2s(i_concept_to_stock)
-        i_shared_info = i_concept_to_stock.mm(hidden)
-        i_shared_info = self.fc_is(i_shared_info)
-
-        i_shared_back = self.fc_is_back(i_shared_info)
-        output_is = self.fc_is_fore(i_shared_info)
-        output_is = self.leaky_relu(output_is)
-
-        # Individual Information Module
-        individual_info = x_hidden - e_shared_back - i_shared_back
-        output_indi = individual_info
-        output_indi = self.fc_indi(output_indi)
-        output_indi = self.leaky_relu(output_indi)
-
-        # Stock Trend Prediction
-        all_info = output_es + output_is + output_indi
-        pred_all = self.fc_out(all_info).squeeze()
-
-        return pred_all
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+
+from __future__ import division
+from __future__ import print_function
+
+import os
+import numpy as np
+import pandas as pd
+from typing import Text, Union
+import urllib.request
+import copy
+from ...utils import get_or_create_path
+from ...log import get_module_logger
+import torch
+import torch.nn as nn
+import torch.optim as optim
+from .pytorch_utils import count_parameters
+from ...model.base import Model
+from ...data.dataset import DatasetH
+from ...data.dataset.handler import DataHandlerLP
+from ...contrib.model.pytorch_lstm import LSTMModel
+from ...contrib.model.pytorch_gru import GRUModel
+
+
+class HIST(Model):
+    """HIST Model
+
+    Parameters
+    ----------
+    lr : float
+        learning rate
+    d_feat : int
+        input dimensions for each time step
+    metric : str
+        the evaluation metric used in early stop
+    optimizer : str
+        optimizer name
+    GPU : str
+        the GPU ID(s) used for training
+    """
+
+    def __init__(
+        self,
+        d_feat=6,
+        hidden_size=64,
+        num_layers=2,
+        dropout=0.0,
+        n_epochs=200,
+        lr=0.001,
+        metric="",
+        early_stop=20,
+        loss="mse",
+        base_model="GRU",
+        model_path=None,
+        stock2concept=None,
+        stock_index=None,
+        optimizer="adam",
+        GPU=0,
+        seed=None,
+        **kwargs
+    ):
+        # Set logger.
+        self.logger = get_module_logger("HIST")
+        self.logger.info("HIST pytorch version...")
+
+        # set hyper-parameters.
+        self.d_feat = d_feat
+        self.hidden_size = hidden_size
+        self.num_layers = num_layers
+        self.dropout = dropout
+        self.n_epochs = n_epochs
+        self.lr = lr
+        self.metric = metric
+        self.early_stop = early_stop
+        self.optimizer = optimizer.lower()
+        self.loss = loss
+        self.base_model = base_model
+        self.model_path = model_path
+        self.stock2concept = stock2concept
+        self.stock_index = stock_index
+        self.device = torch.device("cuda:%d" % (GPU) if torch.cuda.is_available() and GPU >= 0 else "cpu")
+        self.seed = seed
+
+        self.logger.info(
+            "HIST parameters setting:"
+            "\nd_feat : {}"
+            "\nhidden_size : {}"
+            "\nnum_layers : {}"
+            "\ndropout : {}"
+            "\nn_epochs : {}"
+            "\nlr : {}"
+            "\nmetric : {}"
+            "\nearly_stop : {}"
+            "\noptimizer : {}"
+            "\nloss_type : {}"
+            "\nbase_model : {}"
+            "\nmodel_path : {}"
+            "\nstock2concept : {}"
+            "\nstock_index : {}"
+            "\nuse_GPU : {}"
+            "\nseed : {}".format(
+                d_feat,
+                hidden_size,
+                num_layers,
+                dropout,
+                n_epochs,
+                lr,
+                metric,
+                early_stop,
+                optimizer.lower(),
+                loss,
+                base_model,
+                model_path,
+                stock2concept,
+                stock_index,
+                GPU,
+                seed,
+            )
+        )
+
+        if self.seed is not None:
+            np.random.seed(self.seed)
+            torch.manual_seed(self.seed)
+
+        self.HIST_model = HISTModel(
+            d_feat=self.d_feat,
+            hidden_size=self.hidden_size,
+            num_layers=self.num_layers,
+            dropout=self.dropout,
+            base_model=self.base_model,
+        )
+        self.logger.info("model:\n{:}".format(self.HIST_model))
+        self.logger.info("model size: {:.4f} MB".format(count_parameters(self.HIST_model)))
+        if optimizer.lower() == "adam":
+            self.train_optimizer = optim.Adam(self.HIST_model.parameters(), lr=self.lr)
+        elif optimizer.lower() == "gd":
+            self.train_optimizer = optim.SGD(self.HIST_model.parameters(), lr=self.lr)
+        else:
+            raise NotImplementedError("optimizer {} is not supported!".format(optimizer))
+
+        self.fitted = False
+        self.HIST_model.to(self.device)
+
+    @property
+    def use_gpu(self):
+        return self.device != torch.device("cpu")
+
+    def mse(self, pred, label):
+        loss = (pred - label) ** 2
+        return torch.mean(loss)
+
+    def loss_fn(self, pred, label):
+        mask = ~torch.isnan(label)
+
+        if self.loss == "mse":
+            return self.mse(pred[mask], label[mask])
+
+        raise ValueError("unknown loss `%s`" % self.loss)
+
+    def metric_fn(self, pred, label):
+        mask = torch.isfinite(label)
+
+        if self.metric == "ic":
+            x = pred[mask]
+            y = label[mask]
+
+            vx = x - torch.mean(x)
+            vy = y - torch.mean(y)
+            return torch.sum(vx * vy) / (torch.sqrt(torch.sum(vx**2)) * torch.sqrt(torch.sum(vy**2)))
+
+        if self.metric == ("", "loss"):
+            return -self.loss_fn(pred[mask], label[mask])
+
+        raise ValueError("unknown metric `%s`" % self.metric)
+
+    def get_daily_inter(self, df, shuffle=False):
+        # organize the train data into daily batches
+        daily_count = df.groupby(level=0).size().values
+        daily_index = np.roll(np.cumsum(daily_count), 1)
+        daily_index[0] = 0
+        if shuffle:
+            # shuffle data
+            daily_shuffle = list(zip(daily_index, daily_count))
+            np.random.shuffle(daily_shuffle)
+            daily_index, daily_count = zip(*daily_shuffle)
+        return daily_index, daily_count
+
+    def train_epoch(self, x_train, y_train, stock_index):
+        stock2concept_matrix = np.load(self.stock2concept)
+        x_train_values = x_train.values
+        y_train_values = np.squeeze(y_train.values)
+        stock_index = stock_index.values
+        stock_index[np.isnan(stock_index)] = 733
+        self.HIST_model.train()
+
+        # organize the train data into daily batches
+        daily_index, daily_count = self.get_daily_inter(x_train, shuffle=True)
+
+        for idx, count in zip(daily_index, daily_count):
+            batch = slice(idx, idx + count)
+            feature = torch.from_numpy(x_train_values[batch]).float().to(self.device)
+            concept_matrix = torch.from_numpy(stock2concept_matrix[stock_index[batch]]).float().to(self.device)
+            label = torch.from_numpy(y_train_values[batch]).float().to(self.device)
+            pred = self.HIST_model(feature, concept_matrix)
+            loss = self.loss_fn(pred, label)
+
+            self.train_optimizer.zero_grad()
+            loss.backward()
+            torch.nn.utils.clip_grad_value_(self.HIST_model.parameters(), 3.0)
+            self.train_optimizer.step()
+
+    def test_epoch(self, data_x, data_y, stock_index):
+        # prepare training data
+        stock2concept_matrix = np.load(self.stock2concept)
+        x_values = data_x.values
+        y_values = np.squeeze(data_y.values)
+        stock_index = stock_index.values
+        stock_index[np.isnan(stock_index)] = 733
+        self.HIST_model.eval()
+
+        scores = []
+        losses = []
+
+        # organize the test data into daily batches
+        daily_index, daily_count = self.get_daily_inter(data_x, shuffle=False)
+
+        for idx, count in zip(daily_index, daily_count):
+            batch = slice(idx, idx + count)
+            feature = torch.from_numpy(x_values[batch]).float().to(self.device)
+            concept_matrix = torch.from_numpy(stock2concept_matrix[stock_index[batch]]).float().to(self.device)
+            label = torch.from_numpy(y_values[batch]).float().to(self.device)
+            with torch.no_grad():
+                pred = self.HIST_model(feature, concept_matrix)
+                loss = self.loss_fn(pred, label)
+                losses.append(loss.item())
+
+                score = self.metric_fn(pred, label)
+                scores.append(score.item())
+
+        return np.mean(losses), np.mean(scores)
+
+    def fit(
+        self,
+        dataset: DatasetH,
+        evals_result=dict(),
+        save_path=None,
+    ):
+        df_train, df_valid, df_test = dataset.prepare(
+            ["train", "valid", "test"],
+            col_set=["feature", "label"],
+            data_key=DataHandlerLP.DK_L,
+        )
+        if df_train.empty or df_valid.empty:
+            raise ValueError("Empty data from dataset, please check your dataset config.")
+
+        if not os.path.exists(self.stock2concept):
+            url = "http://fintech.msra.cn/stock_data/downloads/qlib_csi300_stock2concept.npy"
+            urllib.request.urlretrieve(url, self.stock2concept)
+
+        stock_index = np.load(self.stock_index, allow_pickle=True).item()
+        df_train["stock_index"] = 733
+        df_train["stock_index"] = df_train.index.get_level_values("instrument").map(stock_index)
+        df_valid["stock_index"] = 733
+        df_valid["stock_index"] = df_valid.index.get_level_values("instrument").map(stock_index)
+
+        x_train, y_train, stock_index_train = df_train["feature"], df_train["label"], df_train["stock_index"]
+        x_valid, y_valid, stock_index_valid = df_valid["feature"], df_valid["label"], df_valid["stock_index"]
+
+        save_path = get_or_create_path(save_path)
+
+        stop_steps = 0
+        best_score = -np.inf
+        best_epoch = 0
+        evals_result["train"] = []
+        evals_result["valid"] = []
+
+        # load pretrained base_model
+        if self.base_model == "LSTM":
+            pretrained_model = LSTMModel()
+        elif self.base_model == "GRU":
+            pretrained_model = GRUModel()
+        else:
+            raise ValueError("unknown base model name `%s`" % self.base_model)
+
+        if self.model_path is not None:
+            self.logger.info("Loading pretrained model...")
+            pretrained_model.load_state_dict(torch.load(self.model_path))
+
+        model_dict = self.HIST_model.state_dict()
+        pretrained_dict = {
+            k: v for k, v in pretrained_model.state_dict().items() if k in model_dict  # pylint: disable=E1135
+        }
+        model_dict.update(pretrained_dict)
+        self.HIST_model.load_state_dict(model_dict)
+        self.logger.info("Loading pretrained model Done...")
+
+        # train
+        self.logger.info("training...")
+        self.fitted = True
+
+        for step in range(self.n_epochs):
+            self.logger.info("Epoch%d:", step)
+            self.logger.info("training...")
+            self.train_epoch(x_train, y_train, stock_index_train)
+
+            self.logger.info("evaluating...")
+            train_loss, train_score = self.test_epoch(x_train, y_train, stock_index_train)
+            val_loss, val_score = self.test_epoch(x_valid, y_valid, stock_index_valid)
+            self.logger.info("train %.6f, valid %.6f" % (train_score, val_score))
+            evals_result["train"].append(train_score)
+            evals_result["valid"].append(val_score)
+
+            if val_score > best_score:
+                best_score = val_score
+                stop_steps = 0
+                best_epoch = step
+                best_param = copy.deepcopy(self.HIST_model.state_dict())
+            else:
+                stop_steps += 1
+                if stop_steps >= self.early_stop:
+                    self.logger.info("early stop")
+                    break
+
+        self.logger.info("best score: %.6lf @ %d" % (best_score, best_epoch))
+        self.HIST_model.load_state_dict(best_param)
+        torch.save(best_param, save_path)
+
+    def predict(self, dataset: DatasetH, segment: Union[Text, slice] = "test"):
+        if not self.fitted:
+            raise ValueError("model is not fitted yet!")
+
+        stock2concept_matrix = np.load(self.stock2concept)
+        stock_index = np.load(self.stock_index, allow_pickle=True).item()
+        df_test = dataset.prepare(segment, col_set="feature", data_key=DataHandlerLP.DK_I)
+        df_test["stock_index"] = 733
+        df_test["stock_index"] = df_test.index.get_level_values("instrument").map(stock_index)
+        stock_index_test = df_test["stock_index"].values
+        stock_index_test[np.isnan(stock_index_test)] = 733
+        stock_index_test = stock_index_test.astype("int")
+        df_test = df_test.drop(["stock_index"], axis=1)
+        index = df_test.index
+
+        self.HIST_model.eval()
+        x_values = df_test.values
+        preds = []
+
+        # organize the data into daily batches
+        daily_index, daily_count = self.get_daily_inter(df_test, shuffle=False)
+
+        for idx, count in zip(daily_index, daily_count):
+            batch = slice(idx, idx + count)
+            x_batch = torch.from_numpy(x_values[batch]).float().to(self.device)
+            concept_matrix = torch.from_numpy(stock2concept_matrix[stock_index_test[batch]]).float().to(self.device)
+
+            with torch.no_grad():
+                pred = self.HIST_model(x_batch, concept_matrix).detach().cpu().numpy()
+
+            preds.append(pred)
+
+        return pd.Series(np.concatenate(preds), index=index)
+
+
+class HISTModel(nn.Module):
+    def __init__(self, d_feat=6, hidden_size=64, num_layers=2, dropout=0.0, base_model="GRU"):
+        super().__init__()
+
+        self.d_feat = d_feat
+        self.hidden_size = hidden_size
+
+        if base_model == "GRU":
+            self.rnn = nn.GRU(
+                input_size=d_feat,
+                hidden_size=hidden_size,
+                num_layers=num_layers,
+                batch_first=True,
+                dropout=dropout,
+            )
+        elif base_model == "LSTM":
+            self.rnn = nn.LSTM(
+                input_size=d_feat,
+                hidden_size=hidden_size,
+                num_layers=num_layers,
+                batch_first=True,
+                dropout=dropout,
+            )
+        else:
+            raise ValueError("unknown base model name `%s`" % base_model)
+
+        self.fc_es = nn.Linear(hidden_size, hidden_size)
+        torch.nn.init.xavier_uniform_(self.fc_es.weight)
+        self.fc_is = nn.Linear(hidden_size, hidden_size)
+        torch.nn.init.xavier_uniform_(self.fc_is.weight)
+
+        self.fc_es_middle = nn.Linear(hidden_size, hidden_size)
+        torch.nn.init.xavier_uniform_(self.fc_es_middle.weight)
+        self.fc_is_middle = nn.Linear(hidden_size, hidden_size)
+        torch.nn.init.xavier_uniform_(self.fc_is_middle.weight)
+
+        self.fc_es_fore = nn.Linear(hidden_size, hidden_size)
+        torch.nn.init.xavier_uniform_(self.fc_es_fore.weight)
+        self.fc_is_fore = nn.Linear(hidden_size, hidden_size)
+        torch.nn.init.xavier_uniform_(self.fc_is_fore.weight)
+        self.fc_indi_fore = nn.Linear(hidden_size, hidden_size)
+        torch.nn.init.xavier_uniform_(self.fc_indi_fore.weight)
+
+        self.fc_es_back = nn.Linear(hidden_size, hidden_size)
+        torch.nn.init.xavier_uniform_(self.fc_es_back.weight)
+        self.fc_is_back = nn.Linear(hidden_size, hidden_size)
+        torch.nn.init.xavier_uniform_(self.fc_is_back.weight)
+        self.fc_indi = nn.Linear(hidden_size, hidden_size)
+        torch.nn.init.xavier_uniform_(self.fc_indi.weight)
+
+        self.leaky_relu = nn.LeakyReLU()
+        self.softmax_s2t = torch.nn.Softmax(dim=0)
+        self.softmax_t2s = torch.nn.Softmax(dim=1)
+
+        self.fc_out_es = nn.Linear(hidden_size, 1)
+        self.fc_out_is = nn.Linear(hidden_size, 1)
+        self.fc_out_indi = nn.Linear(hidden_size, 1)
+        self.fc_out = nn.Linear(hidden_size, 1)
+
+    def cal_cos_similarity(self, x, y):  # the 2nd dimension of x and y are the same
+        xy = x.mm(torch.t(y))
+        x_norm = torch.sqrt(torch.sum(x * x, dim=1)).reshape(-1, 1)
+        y_norm = torch.sqrt(torch.sum(y * y, dim=1)).reshape(-1, 1)
+        cos_similarity = xy / (x_norm.mm(torch.t(y_norm)) + 1e-6)
+        return cos_similarity
+
+    def forward(self, x, concept_matrix):
+        device = torch.device(torch.get_device(x))
+
+        x_hidden = x.reshape(len(x), self.d_feat, -1)  # [N, F, T]
+        x_hidden = x_hidden.permute(0, 2, 1)  # [N, T, F]
+        x_hidden, _ = self.rnn(x_hidden)
+        x_hidden = x_hidden[:, -1, :]
+
+        # Predefined Concept Module
+
+        stock_to_concept = concept_matrix
+
+        stock_to_concept_sum = torch.sum(stock_to_concept, 0).reshape(1, -1).repeat(stock_to_concept.shape[0], 1)
+        stock_to_concept_sum = stock_to_concept_sum.mul(concept_matrix)
+
+        stock_to_concept_sum = stock_to_concept_sum + (
+            torch.ones(stock_to_concept.shape[0], stock_to_concept.shape[1]).to(device)
+        )
+        stock_to_concept = stock_to_concept / stock_to_concept_sum
+        hidden = torch.t(stock_to_concept).mm(x_hidden)
+
+        hidden = hidden[hidden.sum(1) != 0]
+
+        concept_to_stock = self.cal_cos_similarity(x_hidden, hidden)
+        concept_to_stock = self.softmax_t2s(concept_to_stock)
+
+        e_shared_info = concept_to_stock.mm(hidden)
+        e_shared_info = self.fc_es(e_shared_info)
+
+        e_shared_back = self.fc_es_back(e_shared_info)
+        output_es = self.fc_es_fore(e_shared_info)
+        output_es = self.leaky_relu(output_es)
+
+        # Hidden Concept Module
+        i_shared_info = x_hidden - e_shared_back
+        hidden = i_shared_info
+        i_stock_to_concept = self.cal_cos_similarity(i_shared_info, hidden)
+        dim = i_stock_to_concept.shape[0]
+        diag = i_stock_to_concept.diagonal(0)
+        i_stock_to_concept = i_stock_to_concept * (torch.ones(dim, dim) - torch.eye(dim)).to(device)
+        row = torch.linspace(0, dim - 1, dim).to(device).long()
+        column = i_stock_to_concept.max(1)[1].long()
+        value = i_stock_to_concept.max(1)[0]
+        i_stock_to_concept[row, column] = 10
+        i_stock_to_concept[i_stock_to_concept != 10] = 0
+        i_stock_to_concept[row, column] = value
+        i_stock_to_concept = i_stock_to_concept + torch.diag_embed((i_stock_to_concept.sum(0) != 0).float() * diag)
+        hidden = torch.t(i_shared_info).mm(i_stock_to_concept).t()
+        hidden = hidden[hidden.sum(1) != 0]
+
+        i_concept_to_stock = self.cal_cos_similarity(i_shared_info, hidden)
+        i_concept_to_stock = self.softmax_t2s(i_concept_to_stock)
+        i_shared_info = i_concept_to_stock.mm(hidden)
+        i_shared_info = self.fc_is(i_shared_info)
+
+        i_shared_back = self.fc_is_back(i_shared_info)
+        output_is = self.fc_is_fore(i_shared_info)
+        output_is = self.leaky_relu(output_is)
+
+        # Individual Information Module
+        individual_info = x_hidden - e_shared_back - i_shared_back
+        output_indi = individual_info
+        output_indi = self.fc_indi(output_indi)
+        output_indi = self.leaky_relu(output_indi)
+
+        # Stock Trend Prediction
+        all_info = output_es + output_is + output_indi
+        pred_all = self.fc_out(all_info).squeeze()
+
+        return pred_all
```

## qlib/contrib/model/pytorch_igmtf.py

```diff
@@ -1,446 +1,442 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-
-from __future__ import division
-from __future__ import print_function
-
-import numpy as np
-import pandas as pd
-from typing import Text, Union
-import copy
-from ...utils import get_or_create_path
-from ...log import get_module_logger
-
-import torch
-import torch.nn as nn
-import torch.optim as optim
-
-from .pytorch_utils import count_parameters
-from ...model.base import Model
-from ...data.dataset import DatasetH
-from ...data.dataset.handler import DataHandlerLP
-from ...contrib.model.pytorch_lstm import LSTMModel
-from ...contrib.model.pytorch_gru import GRUModel
-
-
-class IGMTF(Model):
-    """IGMTF Model
-
-    Parameters
-    ----------
-    d_feat : int
-        input dimension for each time step
-    metric: str
-        the evaluation metric used in early stop
-    optimizer : str
-        optimizer name
-    GPU : str
-        the GPU ID(s) used for training
-    """
-
-    def __init__(
-        self,
-        d_feat=6,
-        hidden_size=64,
-        num_layers=2,
-        dropout=0.0,
-        n_epochs=200,
-        lr=0.001,
-        metric="",
-        early_stop=20,
-        loss="mse",
-        base_model="GRU",
-        model_path=None,
-        optimizer="adam",
-        GPU=0,
-        seed=None,
-        **kwargs
-    ):
-        # Set logger.
-        self.logger = get_module_logger("IGMTF")
-        self.logger.info("IMGTF pytorch version...")
-
-        # set hyper-parameters.
-        self.d_feat = d_feat
-        self.hidden_size = hidden_size
-        self.num_layers = num_layers
-        self.dropout = dropout
-        self.n_epochs = n_epochs
-        self.lr = lr
-        self.metric = metric
-        self.early_stop = early_stop
-        self.optimizer = optimizer.lower()
-        self.loss = loss
-        self.base_model = base_model
-        self.model_path = model_path
-        self.device = torch.device("cuda:%d" % (GPU) if torch.cuda.is_available() and GPU >= 0 else "cpu")
-        self.seed = seed
-
-        self.logger.info(
-            "IGMTF parameters setting:"
-            "\nd_feat : {}"
-            "\nhidden_size : {}"
-            "\nnum_layers : {}"
-            "\ndropout : {}"
-            "\nn_epochs : {}"
-            "\nlr : {}"
-            "\nmetric : {}"
-            "\nearly_stop : {}"
-            "\noptimizer : {}"
-            "\nloss_type : {}"
-            "\nbase_model : {}"
-            "\nmodel_path : {}"
-            "\nvisible_GPU : {}"
-            "\nuse_GPU : {}"
-            "\nseed : {}".format(
-                d_feat,
-                hidden_size,
-                num_layers,
-                dropout,
-                n_epochs,
-                lr,
-                metric,
-                early_stop,
-                optimizer.lower(),
-                loss,
-                base_model,
-                model_path,
-                GPU,
-                self.use_gpu,
-                seed,
-            )
-        )
-
-        if self.seed is not None:
-            np.random.seed(self.seed)
-            torch.manual_seed(self.seed)
-
-        self.igmtf_model = IGMTFModel(
-            d_feat=self.d_feat,
-            hidden_size=self.hidden_size,
-            num_layers=self.num_layers,
-            dropout=self.dropout,
-            base_model=self.base_model,
-        )
-        self.logger.info("model:\n{:}".format(self.igmtf_model))
-        self.logger.info("model size: {:.4f} MB".format(count_parameters(self.igmtf_model)))
-
-        if optimizer.lower() == "adam":
-            self.train_optimizer = optim.Adam(self.igmtf_model.parameters(), lr=self.lr)
-        elif optimizer.lower() == "gd":
-            self.train_optimizer = optim.SGD(self.igmtf_model.parameters(), lr=self.lr)
-        else:
-            raise NotImplementedError("optimizer {} is not supported!".format(optimizer))
-
-        self.fitted = False
-        self.igmtf_model.to(self.device)
-
-    @property
-    def use_gpu(self):
-        return self.device != torch.device("cpu")
-
-    def mse(self, pred, label):
-        loss = (pred - label) ** 2
-        return torch.mean(loss)
-
-    def loss_fn(self, pred, label):
-        mask = ~torch.isnan(label)
-
-        if self.loss == "mse":
-            return self.mse(pred[mask], label[mask])
-
-        raise ValueError("unknown loss `%s`" % self.loss)
-
-    def metric_fn(self, pred, label):
-
-        mask = torch.isfinite(label)
-
-        if self.metric == "ic":
-            x = pred[mask]
-            y = label[mask]
-
-            vx = x - torch.mean(x)
-            vy = y - torch.mean(y)
-            return torch.sum(vx * vy) / (torch.sqrt(torch.sum(vx**2)) * torch.sqrt(torch.sum(vy**2)))
-
-        if self.metric == ("", "loss"):
-            return -self.loss_fn(pred[mask], label[mask])
-
-        raise ValueError("unknown metric `%s`" % self.metric)
-
-    def get_daily_inter(self, df, shuffle=False):
-        # organize the train data into daily batches
-        daily_count = df.groupby(level=0).size().values
-        daily_index = np.roll(np.cumsum(daily_count), 1)
-        daily_index[0] = 0
-        if shuffle:
-            # shuffle data
-            daily_shuffle = list(zip(daily_index, daily_count))
-            np.random.shuffle(daily_shuffle)
-            daily_index, daily_count = zip(*daily_shuffle)
-        return daily_index, daily_count
-
-    def get_train_hidden(self, x_train):
-        x_train_values = x_train.values
-        daily_index, daily_count = self.get_daily_inter(x_train, shuffle=True)
-        self.igmtf_model.eval()
-        train_hidden = []
-        train_hidden_day = []
-
-        for idx, count in zip(daily_index, daily_count):
-            batch = slice(idx, idx + count)
-            feature = torch.from_numpy(x_train_values[batch]).float().to(self.device)
-            out = self.igmtf_model(feature, get_hidden=True)
-            train_hidden.append(out.detach().cpu())
-            train_hidden_day.append(out.detach().cpu().mean(dim=0).unsqueeze(dim=0))
-
-        train_hidden = np.asarray(train_hidden, dtype=object)
-        train_hidden_day = torch.cat(train_hidden_day)
-
-        return train_hidden, train_hidden_day
-
-    def train_epoch(self, x_train, y_train, train_hidden, train_hidden_day):
-
-        x_train_values = x_train.values
-        y_train_values = np.squeeze(y_train.values)
-
-        self.igmtf_model.train()
-
-        daily_index, daily_count = self.get_daily_inter(x_train, shuffle=True)
-
-        for idx, count in zip(daily_index, daily_count):
-            batch = slice(idx, idx + count)
-            feature = torch.from_numpy(x_train_values[batch]).float().to(self.device)
-            label = torch.from_numpy(y_train_values[batch]).float().to(self.device)
-            pred = self.igmtf_model(feature, train_hidden=train_hidden, train_hidden_day=train_hidden_day)
-            loss = self.loss_fn(pred, label)
-
-            self.train_optimizer.zero_grad()
-            loss.backward()
-            torch.nn.utils.clip_grad_value_(self.igmtf_model.parameters(), 3.0)
-            self.train_optimizer.step()
-
-    def test_epoch(self, data_x, data_y, train_hidden, train_hidden_day):
-
-        # prepare training data
-        x_values = data_x.values
-        y_values = np.squeeze(data_y.values)
-
-        self.igmtf_model.eval()
-
-        scores = []
-        losses = []
-
-        daily_index, daily_count = self.get_daily_inter(data_x, shuffle=False)
-
-        for idx, count in zip(daily_index, daily_count):
-            batch = slice(idx, idx + count)
-            feature = torch.from_numpy(x_values[batch]).float().to(self.device)
-            label = torch.from_numpy(y_values[batch]).float().to(self.device)
-
-            pred = self.igmtf_model(feature, train_hidden=train_hidden, train_hidden_day=train_hidden_day)
-            loss = self.loss_fn(pred, label)
-            losses.append(loss.item())
-
-            score = self.metric_fn(pred, label)
-            scores.append(score.item())
-
-        return np.mean(losses), np.mean(scores)
-
-    def fit(
-        self,
-        dataset: DatasetH,
-        evals_result=dict(),
-        save_path=None,
-    ):
-
-        df_train, df_valid = dataset.prepare(
-            ["train", "valid"],
-            col_set=["feature", "label"],
-            data_key=DataHandlerLP.DK_L,
-        )
-        if df_train.empty or df_valid.empty:
-            raise ValueError("Empty data from dataset, please check your dataset config.")
-
-        x_train, y_train = df_train["feature"], df_train["label"]
-        x_valid, y_valid = df_valid["feature"], df_valid["label"]
-
-        save_path = get_or_create_path(save_path)
-        stop_steps = 0
-        train_loss = 0
-        best_score = -np.inf
-        best_epoch = 0
-        evals_result["train"] = []
-        evals_result["valid"] = []
-
-        # load pretrained base_model
-        if self.base_model == "LSTM":
-            pretrained_model = LSTMModel()
-        elif self.base_model == "GRU":
-            pretrained_model = GRUModel()
-        else:
-            raise ValueError("unknown base model name `%s`" % self.base_model)
-
-        if self.model_path is not None:
-            self.logger.info("Loading pretrained model...")
-            pretrained_model.load_state_dict(torch.load(self.model_path, map_location=self.device))
-
-        model_dict = self.igmtf_model.state_dict()
-        pretrained_dict = {
-            k: v for k, v in pretrained_model.state_dict().items() if k in model_dict  # pylint: disable=E1135
-        }
-        model_dict.update(pretrained_dict)
-        self.igmtf_model.load_state_dict(model_dict)
-        self.logger.info("Loading pretrained model Done...")
-
-        # train
-        self.logger.info("training...")
-        self.fitted = True
-
-        for step in range(self.n_epochs):
-            self.logger.info("Epoch%d:", step)
-            self.logger.info("training...")
-            train_hidden, train_hidden_day = self.get_train_hidden(x_train)
-            self.train_epoch(x_train, y_train, train_hidden, train_hidden_day)
-            self.logger.info("evaluating...")
-            train_loss, train_score = self.test_epoch(x_train, y_train, train_hidden, train_hidden_day)
-            val_loss, val_score = self.test_epoch(x_valid, y_valid, train_hidden, train_hidden_day)
-            self.logger.info("train %.6f, valid %.6f" % (train_score, val_score))
-            evals_result["train"].append(train_score)
-            evals_result["valid"].append(val_score)
-
-            if val_score > best_score:
-                best_score = val_score
-                stop_steps = 0
-                best_epoch = step
-                best_param = copy.deepcopy(self.igmtf_model.state_dict())
-            else:
-                stop_steps += 1
-                if stop_steps >= self.early_stop:
-                    self.logger.info("early stop")
-                    break
-
-        self.logger.info("best score: %.6lf @ %d" % (best_score, best_epoch))
-        self.igmtf_model.load_state_dict(best_param)
-        torch.save(best_param, save_path)
-
-        if self.use_gpu:
-            torch.cuda.empty_cache()
-
-    def predict(self, dataset: DatasetH, segment: Union[Text, slice] = "test"):
-        if not self.fitted:
-            raise ValueError("model is not fitted yet!")
-        x_train = dataset.prepare("train", col_set="feature", data_key=DataHandlerLP.DK_L)
-        train_hidden, train_hidden_day = self.get_train_hidden(x_train)
-        x_test = dataset.prepare(segment, col_set="feature", data_key=DataHandlerLP.DK_I)
-        index = x_test.index
-        self.igmtf_model.eval()
-        x_values = x_test.values
-        preds = []
-
-        daily_index, daily_count = self.get_daily_inter(x_test, shuffle=False)
-
-        for idx, count in zip(daily_index, daily_count):
-            batch = slice(idx, idx + count)
-            x_batch = torch.from_numpy(x_values[batch]).float().to(self.device)
-
-            with torch.no_grad():
-                pred = (
-                    self.igmtf_model(x_batch, train_hidden=train_hidden, train_hidden_day=train_hidden_day)
-                    .detach()
-                    .cpu()
-                    .numpy()
-                )
-
-            preds.append(pred)
-
-        return pd.Series(np.concatenate(preds), index=index)
-
-
-class IGMTFModel(nn.Module):
-    def __init__(self, d_feat=6, hidden_size=64, num_layers=2, dropout=0.0, base_model="GRU"):
-        super().__init__()
-
-        if base_model == "GRU":
-            self.rnn = nn.GRU(
-                input_size=d_feat,
-                hidden_size=hidden_size,
-                num_layers=num_layers,
-                batch_first=True,
-                dropout=dropout,
-            )
-        elif base_model == "LSTM":
-            self.rnn = nn.LSTM(
-                input_size=d_feat,
-                hidden_size=hidden_size,
-                num_layers=num_layers,
-                batch_first=True,
-                dropout=dropout,
-            )
-        else:
-            raise ValueError("unknown base model name `%s`" % base_model)
-        self.lins = nn.Sequential()
-        for i in range(2):
-            self.lins.add_module("linear" + str(i), nn.Linear(hidden_size, hidden_size))
-            self.lins.add_module("leakyrelu" + str(i), nn.LeakyReLU())
-        self.fc_output = nn.Linear(hidden_size * 2, hidden_size * 2)
-        self.project1 = nn.Linear(hidden_size, hidden_size, bias=False)
-        self.project2 = nn.Linear(hidden_size, hidden_size, bias=False)
-        self.fc_out_pred = nn.Linear(hidden_size * 2, 1)
-
-        self.leaky_relu = nn.LeakyReLU()
-        self.d_feat = d_feat
-
-    def cal_cos_similarity(self, x, y):  # the 2nd dimension of x and y are the same
-        xy = x.mm(torch.t(y))
-        x_norm = torch.sqrt(torch.sum(x * x, dim=1)).reshape(-1, 1)
-        y_norm = torch.sqrt(torch.sum(y * y, dim=1)).reshape(-1, 1)
-        cos_similarity = xy / (x_norm.mm(torch.t(y_norm)) + 1e-6)
-        return cos_similarity
-
-    def sparse_dense_mul(self, s, d):
-        i = s._indices()
-        v = s._values()
-        dv = d[i[0, :], i[1, :]]  # get values from relevant entries of dense matrix
-        return torch.sparse.FloatTensor(i, v * dv, s.size())
-
-    def forward(self, x, get_hidden=False, train_hidden=None, train_hidden_day=None, k_day=10, n_neighbor=10):
-        # x: [N, F*T]
-        device = x.device
-        x = x.reshape(len(x), self.d_feat, -1)  # [N, F, T]
-        x = x.permute(0, 2, 1)  # [N, T, F]
-        out, _ = self.rnn(x)
-        out = out[:, -1, :]
-        out = self.lins(out)
-        mini_batch_out = out
-        if get_hidden is True:
-            return mini_batch_out
-
-        mini_batch_out_day = torch.mean(mini_batch_out, dim=0).unsqueeze(0)
-        day_similarity = self.cal_cos_similarity(mini_batch_out_day, train_hidden_day.to(device))
-        day_index = torch.topk(day_similarity, k_day, dim=1)[1]
-        sample_train_hidden = train_hidden[day_index.long().cpu()].squeeze()
-        sample_train_hidden = torch.cat(list(sample_train_hidden)).to(device)
-        sample_train_hidden = self.lins(sample_train_hidden)
-        cos_similarity = self.cal_cos_similarity(self.project1(mini_batch_out), self.project2(sample_train_hidden))
-
-        row = (
-            torch.linspace(0, x.shape[0] - 1, x.shape[0])
-            .reshape([-1, 1])
-            .repeat(1, n_neighbor)
-            .reshape(1, -1)
-            .to(device)
-        )
-        column = torch.topk(cos_similarity, n_neighbor, dim=1)[1].reshape(1, -1)
-        mask = torch.sparse_coo_tensor(
-            torch.cat([row, column]),
-            torch.ones([row.shape[1]]).to(device) / n_neighbor,
-            (x.shape[0], sample_train_hidden.shape[0]),
-        )
-        cos_similarity = self.sparse_dense_mul(mask, cos_similarity)
-
-        agg_out = torch.sparse.mm(cos_similarity, self.project2(sample_train_hidden))
-        # out = self.fc_out(out).squeeze()
-        out = self.fc_out_pred(torch.cat([mini_batch_out, agg_out], axis=1)).squeeze()
-        return out
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+
+from __future__ import division
+from __future__ import print_function
+
+import numpy as np
+import pandas as pd
+from typing import Text, Union
+import copy
+from ...utils import get_or_create_path
+from ...log import get_module_logger
+
+import torch
+import torch.nn as nn
+import torch.optim as optim
+
+from .pytorch_utils import count_parameters
+from ...model.base import Model
+from ...data.dataset import DatasetH
+from ...data.dataset.handler import DataHandlerLP
+from ...contrib.model.pytorch_lstm import LSTMModel
+from ...contrib.model.pytorch_gru import GRUModel
+
+
+class IGMTF(Model):
+    """IGMTF Model
+
+    Parameters
+    ----------
+    d_feat : int
+        input dimension for each time step
+    metric: str
+        the evaluation metric used in early stop
+    optimizer : str
+        optimizer name
+    GPU : str
+        the GPU ID(s) used for training
+    """
+
+    def __init__(
+        self,
+        d_feat=6,
+        hidden_size=64,
+        num_layers=2,
+        dropout=0.0,
+        n_epochs=200,
+        lr=0.001,
+        metric="",
+        early_stop=20,
+        loss="mse",
+        base_model="GRU",
+        model_path=None,
+        optimizer="adam",
+        GPU=0,
+        seed=None,
+        **kwargs
+    ):
+        # Set logger.
+        self.logger = get_module_logger("IGMTF")
+        self.logger.info("IMGTF pytorch version...")
+
+        # set hyper-parameters.
+        self.d_feat = d_feat
+        self.hidden_size = hidden_size
+        self.num_layers = num_layers
+        self.dropout = dropout
+        self.n_epochs = n_epochs
+        self.lr = lr
+        self.metric = metric
+        self.early_stop = early_stop
+        self.optimizer = optimizer.lower()
+        self.loss = loss
+        self.base_model = base_model
+        self.model_path = model_path
+        self.device = torch.device("cuda:%d" % (GPU) if torch.cuda.is_available() and GPU >= 0 else "cpu")
+        self.seed = seed
+
+        self.logger.info(
+            "IGMTF parameters setting:"
+            "\nd_feat : {}"
+            "\nhidden_size : {}"
+            "\nnum_layers : {}"
+            "\ndropout : {}"
+            "\nn_epochs : {}"
+            "\nlr : {}"
+            "\nmetric : {}"
+            "\nearly_stop : {}"
+            "\noptimizer : {}"
+            "\nloss_type : {}"
+            "\nbase_model : {}"
+            "\nmodel_path : {}"
+            "\nvisible_GPU : {}"
+            "\nuse_GPU : {}"
+            "\nseed : {}".format(
+                d_feat,
+                hidden_size,
+                num_layers,
+                dropout,
+                n_epochs,
+                lr,
+                metric,
+                early_stop,
+                optimizer.lower(),
+                loss,
+                base_model,
+                model_path,
+                GPU,
+                self.use_gpu,
+                seed,
+            )
+        )
+
+        if self.seed is not None:
+            np.random.seed(self.seed)
+            torch.manual_seed(self.seed)
+
+        self.igmtf_model = IGMTFModel(
+            d_feat=self.d_feat,
+            hidden_size=self.hidden_size,
+            num_layers=self.num_layers,
+            dropout=self.dropout,
+            base_model=self.base_model,
+        )
+        self.logger.info("model:\n{:}".format(self.igmtf_model))
+        self.logger.info("model size: {:.4f} MB".format(count_parameters(self.igmtf_model)))
+
+        if optimizer.lower() == "adam":
+            self.train_optimizer = optim.Adam(self.igmtf_model.parameters(), lr=self.lr)
+        elif optimizer.lower() == "gd":
+            self.train_optimizer = optim.SGD(self.igmtf_model.parameters(), lr=self.lr)
+        else:
+            raise NotImplementedError("optimizer {} is not supported!".format(optimizer))
+
+        self.fitted = False
+        self.igmtf_model.to(self.device)
+
+    @property
+    def use_gpu(self):
+        return self.device != torch.device("cpu")
+
+    def mse(self, pred, label):
+        loss = (pred - label) ** 2
+        return torch.mean(loss)
+
+    def loss_fn(self, pred, label):
+        mask = ~torch.isnan(label)
+
+        if self.loss == "mse":
+            return self.mse(pred[mask], label[mask])
+
+        raise ValueError("unknown loss `%s`" % self.loss)
+
+    def metric_fn(self, pred, label):
+        mask = torch.isfinite(label)
+
+        if self.metric == "ic":
+            x = pred[mask]
+            y = label[mask]
+
+            vx = x - torch.mean(x)
+            vy = y - torch.mean(y)
+            return torch.sum(vx * vy) / (torch.sqrt(torch.sum(vx**2)) * torch.sqrt(torch.sum(vy**2)))
+
+        if self.metric == ("", "loss"):
+            return -self.loss_fn(pred[mask], label[mask])
+
+        raise ValueError("unknown metric `%s`" % self.metric)
+
+    def get_daily_inter(self, df, shuffle=False):
+        # organize the train data into daily batches
+        daily_count = df.groupby(level=0).size().values
+        daily_index = np.roll(np.cumsum(daily_count), 1)
+        daily_index[0] = 0
+        if shuffle:
+            # shuffle data
+            daily_shuffle = list(zip(daily_index, daily_count))
+            np.random.shuffle(daily_shuffle)
+            daily_index, daily_count = zip(*daily_shuffle)
+        return daily_index, daily_count
+
+    def get_train_hidden(self, x_train):
+        x_train_values = x_train.values
+        daily_index, daily_count = self.get_daily_inter(x_train, shuffle=True)
+        self.igmtf_model.eval()
+        train_hidden = []
+        train_hidden_day = []
+
+        for idx, count in zip(daily_index, daily_count):
+            batch = slice(idx, idx + count)
+            feature = torch.from_numpy(x_train_values[batch]).float().to(self.device)
+            out = self.igmtf_model(feature, get_hidden=True)
+            train_hidden.append(out.detach().cpu())
+            train_hidden_day.append(out.detach().cpu().mean(dim=0).unsqueeze(dim=0))
+
+        train_hidden = np.asarray(train_hidden, dtype=object)
+        train_hidden_day = torch.cat(train_hidden_day)
+
+        return train_hidden, train_hidden_day
+
+    def train_epoch(self, x_train, y_train, train_hidden, train_hidden_day):
+        x_train_values = x_train.values
+        y_train_values = np.squeeze(y_train.values)
+
+        self.igmtf_model.train()
+
+        daily_index, daily_count = self.get_daily_inter(x_train, shuffle=True)
+
+        for idx, count in zip(daily_index, daily_count):
+            batch = slice(idx, idx + count)
+            feature = torch.from_numpy(x_train_values[batch]).float().to(self.device)
+            label = torch.from_numpy(y_train_values[batch]).float().to(self.device)
+            pred = self.igmtf_model(feature, train_hidden=train_hidden, train_hidden_day=train_hidden_day)
+            loss = self.loss_fn(pred, label)
+
+            self.train_optimizer.zero_grad()
+            loss.backward()
+            torch.nn.utils.clip_grad_value_(self.igmtf_model.parameters(), 3.0)
+            self.train_optimizer.step()
+
+    def test_epoch(self, data_x, data_y, train_hidden, train_hidden_day):
+        # prepare training data
+        x_values = data_x.values
+        y_values = np.squeeze(data_y.values)
+
+        self.igmtf_model.eval()
+
+        scores = []
+        losses = []
+
+        daily_index, daily_count = self.get_daily_inter(data_x, shuffle=False)
+
+        for idx, count in zip(daily_index, daily_count):
+            batch = slice(idx, idx + count)
+            feature = torch.from_numpy(x_values[batch]).float().to(self.device)
+            label = torch.from_numpy(y_values[batch]).float().to(self.device)
+
+            pred = self.igmtf_model(feature, train_hidden=train_hidden, train_hidden_day=train_hidden_day)
+            loss = self.loss_fn(pred, label)
+            losses.append(loss.item())
+
+            score = self.metric_fn(pred, label)
+            scores.append(score.item())
+
+        return np.mean(losses), np.mean(scores)
+
+    def fit(
+        self,
+        dataset: DatasetH,
+        evals_result=dict(),
+        save_path=None,
+    ):
+        df_train, df_valid = dataset.prepare(
+            ["train", "valid"],
+            col_set=["feature", "label"],
+            data_key=DataHandlerLP.DK_L,
+        )
+        if df_train.empty or df_valid.empty:
+            raise ValueError("Empty data from dataset, please check your dataset config.")
+
+        x_train, y_train = df_train["feature"], df_train["label"]
+        x_valid, y_valid = df_valid["feature"], df_valid["label"]
+
+        save_path = get_or_create_path(save_path)
+        stop_steps = 0
+        train_loss = 0
+        best_score = -np.inf
+        best_epoch = 0
+        evals_result["train"] = []
+        evals_result["valid"] = []
+
+        # load pretrained base_model
+        if self.base_model == "LSTM":
+            pretrained_model = LSTMModel()
+        elif self.base_model == "GRU":
+            pretrained_model = GRUModel()
+        else:
+            raise ValueError("unknown base model name `%s`" % self.base_model)
+
+        if self.model_path is not None:
+            self.logger.info("Loading pretrained model...")
+            pretrained_model.load_state_dict(torch.load(self.model_path, map_location=self.device))
+
+        model_dict = self.igmtf_model.state_dict()
+        pretrained_dict = {
+            k: v for k, v in pretrained_model.state_dict().items() if k in model_dict  # pylint: disable=E1135
+        }
+        model_dict.update(pretrained_dict)
+        self.igmtf_model.load_state_dict(model_dict)
+        self.logger.info("Loading pretrained model Done...")
+
+        # train
+        self.logger.info("training...")
+        self.fitted = True
+
+        for step in range(self.n_epochs):
+            self.logger.info("Epoch%d:", step)
+            self.logger.info("training...")
+            train_hidden, train_hidden_day = self.get_train_hidden(x_train)
+            self.train_epoch(x_train, y_train, train_hidden, train_hidden_day)
+            self.logger.info("evaluating...")
+            train_loss, train_score = self.test_epoch(x_train, y_train, train_hidden, train_hidden_day)
+            val_loss, val_score = self.test_epoch(x_valid, y_valid, train_hidden, train_hidden_day)
+            self.logger.info("train %.6f, valid %.6f" % (train_score, val_score))
+            evals_result["train"].append(train_score)
+            evals_result["valid"].append(val_score)
+
+            if val_score > best_score:
+                best_score = val_score
+                stop_steps = 0
+                best_epoch = step
+                best_param = copy.deepcopy(self.igmtf_model.state_dict())
+            else:
+                stop_steps += 1
+                if stop_steps >= self.early_stop:
+                    self.logger.info("early stop")
+                    break
+
+        self.logger.info("best score: %.6lf @ %d" % (best_score, best_epoch))
+        self.igmtf_model.load_state_dict(best_param)
+        torch.save(best_param, save_path)
+
+        if self.use_gpu:
+            torch.cuda.empty_cache()
+
+    def predict(self, dataset: DatasetH, segment: Union[Text, slice] = "test"):
+        if not self.fitted:
+            raise ValueError("model is not fitted yet!")
+        x_train = dataset.prepare("train", col_set="feature", data_key=DataHandlerLP.DK_L)
+        train_hidden, train_hidden_day = self.get_train_hidden(x_train)
+        x_test = dataset.prepare(segment, col_set="feature", data_key=DataHandlerLP.DK_I)
+        index = x_test.index
+        self.igmtf_model.eval()
+        x_values = x_test.values
+        preds = []
+
+        daily_index, daily_count = self.get_daily_inter(x_test, shuffle=False)
+
+        for idx, count in zip(daily_index, daily_count):
+            batch = slice(idx, idx + count)
+            x_batch = torch.from_numpy(x_values[batch]).float().to(self.device)
+
+            with torch.no_grad():
+                pred = (
+                    self.igmtf_model(x_batch, train_hidden=train_hidden, train_hidden_day=train_hidden_day)
+                    .detach()
+                    .cpu()
+                    .numpy()
+                )
+
+            preds.append(pred)
+
+        return pd.Series(np.concatenate(preds), index=index)
+
+
+class IGMTFModel(nn.Module):
+    def __init__(self, d_feat=6, hidden_size=64, num_layers=2, dropout=0.0, base_model="GRU"):
+        super().__init__()
+
+        if base_model == "GRU":
+            self.rnn = nn.GRU(
+                input_size=d_feat,
+                hidden_size=hidden_size,
+                num_layers=num_layers,
+                batch_first=True,
+                dropout=dropout,
+            )
+        elif base_model == "LSTM":
+            self.rnn = nn.LSTM(
+                input_size=d_feat,
+                hidden_size=hidden_size,
+                num_layers=num_layers,
+                batch_first=True,
+                dropout=dropout,
+            )
+        else:
+            raise ValueError("unknown base model name `%s`" % base_model)
+        self.lins = nn.Sequential()
+        for i in range(2):
+            self.lins.add_module("linear" + str(i), nn.Linear(hidden_size, hidden_size))
+            self.lins.add_module("leakyrelu" + str(i), nn.LeakyReLU())
+        self.fc_output = nn.Linear(hidden_size * 2, hidden_size * 2)
+        self.project1 = nn.Linear(hidden_size, hidden_size, bias=False)
+        self.project2 = nn.Linear(hidden_size, hidden_size, bias=False)
+        self.fc_out_pred = nn.Linear(hidden_size * 2, 1)
+
+        self.leaky_relu = nn.LeakyReLU()
+        self.d_feat = d_feat
+
+    def cal_cos_similarity(self, x, y):  # the 2nd dimension of x and y are the same
+        xy = x.mm(torch.t(y))
+        x_norm = torch.sqrt(torch.sum(x * x, dim=1)).reshape(-1, 1)
+        y_norm = torch.sqrt(torch.sum(y * y, dim=1)).reshape(-1, 1)
+        cos_similarity = xy / (x_norm.mm(torch.t(y_norm)) + 1e-6)
+        return cos_similarity
+
+    def sparse_dense_mul(self, s, d):
+        i = s._indices()
+        v = s._values()
+        dv = d[i[0, :], i[1, :]]  # get values from relevant entries of dense matrix
+        return torch.sparse.FloatTensor(i, v * dv, s.size())
+
+    def forward(self, x, get_hidden=False, train_hidden=None, train_hidden_day=None, k_day=10, n_neighbor=10):
+        # x: [N, F*T]
+        device = x.device
+        x = x.reshape(len(x), self.d_feat, -1)  # [N, F, T]
+        x = x.permute(0, 2, 1)  # [N, T, F]
+        out, _ = self.rnn(x)
+        out = out[:, -1, :]
+        out = self.lins(out)
+        mini_batch_out = out
+        if get_hidden is True:
+            return mini_batch_out
+
+        mini_batch_out_day = torch.mean(mini_batch_out, dim=0).unsqueeze(0)
+        day_similarity = self.cal_cos_similarity(mini_batch_out_day, train_hidden_day.to(device))
+        day_index = torch.topk(day_similarity, k_day, dim=1)[1]
+        sample_train_hidden = train_hidden[day_index.long().cpu()].squeeze()
+        sample_train_hidden = torch.cat(list(sample_train_hidden)).to(device)
+        sample_train_hidden = self.lins(sample_train_hidden)
+        cos_similarity = self.cal_cos_similarity(self.project1(mini_batch_out), self.project2(sample_train_hidden))
+
+        row = (
+            torch.linspace(0, x.shape[0] - 1, x.shape[0])
+            .reshape([-1, 1])
+            .repeat(1, n_neighbor)
+            .reshape(1, -1)
+            .to(device)
+        )
+        column = torch.topk(cos_similarity, n_neighbor, dim=1)[1].reshape(1, -1)
+        mask = torch.sparse_coo_tensor(
+            torch.cat([row, column]),
+            torch.ones([row.shape[1]]).to(device) / n_neighbor,
+            (x.shape[0], sample_train_hidden.shape[0]),
+        )
+        cos_similarity = self.sparse_dense_mul(mask, cos_similarity)
+
+        agg_out = torch.sparse.mm(cos_similarity, self.project2(sample_train_hidden))
+        # out = self.fc_out(out).squeeze()
+        out = self.fc_out_pred(torch.cat([mini_batch_out, agg_out], axis=1)).squeeze()
+        return out
```

## qlib/contrib/model/pytorch_localformer.py

```diff
@@ -42,15 +42,14 @@
         optimizer="adam",
         reg=1e-3,
         n_jobs=10,
         GPU=0,
         seed=None,
         **kwargs
     ):
-
         # set hyper-parameters.
         self.d_model = d_model
         self.dropout = dropout
         self.n_epochs = n_epochs
         self.lr = lr
         self.reg = reg
         self.metric = metric
@@ -92,34 +91,31 @@
 
         if self.loss == "mse":
             return self.mse(pred[mask], label[mask])
 
         raise ValueError("unknown loss `%s`" % self.loss)
 
     def metric_fn(self, pred, label):
-
         mask = torch.isfinite(label)
 
         if self.metric in ("", "loss"):
             return -self.loss_fn(pred[mask], label[mask])
 
         raise ValueError("unknown metric `%s`" % self.metric)
 
     def train_epoch(self, x_train, y_train):
-
         x_train_values = x_train.values
         y_train_values = np.squeeze(y_train.values)
 
         self.model.train()
 
         indices = np.arange(len(x_train_values))
         np.random.shuffle(indices)
 
         for i in range(len(indices))[:: self.batch_size]:
-
             if len(indices) - i < self.batch_size:
                 break
 
             feature = torch.from_numpy(x_train_values[indices[i : i + self.batch_size]]).float().to(self.device)
             label = torch.from_numpy(y_train_values[indices[i : i + self.batch_size]]).float().to(self.device)
 
             pred = self.model(feature)
@@ -127,28 +123,26 @@
 
             self.train_optimizer.zero_grad()
             loss.backward()
             torch.nn.utils.clip_grad_value_(self.model.parameters(), 3.0)
             self.train_optimizer.step()
 
     def test_epoch(self, data_x, data_y):
-
         # prepare training data
         x_values = data_x.values
         y_values = np.squeeze(data_y.values)
 
         self.model.eval()
 
         scores = []
         losses = []
 
         indices = np.arange(len(x_values))
 
         for i in range(len(indices))[:: self.batch_size]:
-
             if len(indices) - i < self.batch_size:
                 break
 
             feature = torch.from_numpy(x_values[indices[i : i + self.batch_size]]).float().to(self.device)
             label = torch.from_numpy(y_values[indices[i : i + self.batch_size]]).float().to(self.device)
 
             with torch.no_grad():
@@ -163,15 +157,14 @@
 
     def fit(
         self,
         dataset: DatasetH,
         evals_result=dict(),
         save_path=None,
     ):
-
         df_train, df_valid, df_test = dataset.prepare(
             ["train", "valid", "test"],
             col_set=["feature", "label"],
             data_key=DataHandlerLP.DK_L,
         )
         if df_train.empty or df_valid.empty:
             raise ValueError("Empty data from dataset, please check your dataset config.")
@@ -228,15 +221,14 @@
         index = x_test.index
         self.model.eval()
         x_values = x_test.values
         sample_num = x_values.shape[0]
         preds = []
 
         for begin in range(sample_num)[:: self.batch_size]:
-
             if sample_num - begin < self.batch_size:
                 end = sample_num
             else:
                 end = begin + self.batch_size
 
             x_batch = torch.from_numpy(x_values[begin:end]).float().to(self.device)
```

## qlib/contrib/model/pytorch_localformer_ts.py

```diff
@@ -40,15 +40,14 @@
         optimizer="adam",
         reg=1e-3,
         n_jobs=10,
         GPU=0,
         seed=None,
         **kwargs
     ):
-
         # set hyper-parameters.
         self.d_model = d_model
         self.dropout = dropout
         self.n_epochs = n_epochs
         self.lr = lr
         self.reg = reg
         self.metric = metric
@@ -92,24 +91,22 @@
 
         if self.loss == "mse":
             return self.mse(pred[mask], label[mask])
 
         raise ValueError("unknown loss `%s`" % self.loss)
 
     def metric_fn(self, pred, label):
-
         mask = torch.isfinite(label)
 
         if self.metric in ("", "loss"):
             return -self.loss_fn(pred[mask], label[mask])
 
         raise ValueError("unknown metric `%s`" % self.metric)
 
     def train_epoch(self, data_loader):
-
         self.model.train()
 
         for data in data_loader:
             feature = data[:, :, 0:-1].to(self.device)
             label = data[:, -1, -1].to(self.device)
 
             pred = self.model(feature.float())  # .float()
@@ -117,22 +114,20 @@
 
             self.train_optimizer.zero_grad()
             loss.backward()
             torch.nn.utils.clip_grad_value_(self.model.parameters(), 3.0)
             self.train_optimizer.step()
 
     def test_epoch(self, data_loader):
-
         self.model.eval()
 
         scores = []
         losses = []
 
         for data in data_loader:
-
             feature = data[:, :, 0:-1].to(self.device)
             label = data[:, -1, -1].to(self.device)
 
             with torch.no_grad():
                 pred = self.model(feature.float())  # .float()
                 loss = self.loss_fn(pred, label)
                 losses.append(loss.item())
@@ -144,15 +139,14 @@
 
     def fit(
         self,
         dataset: DatasetH,
         evals_result=dict(),
         save_path=None,
     ):
-
         dl_train = dataset.prepare("train", col_set=["feature", "label"], data_key=DataHandlerLP.DK_L)
         dl_valid = dataset.prepare("valid", col_set=["feature", "label"], data_key=DataHandlerLP.DK_L)
         if dl_train.empty or dl_valid.empty:
             raise ValueError("Empty data from dataset, please check your dataset config.")
 
         dl_train.config(fillna_type="ffill+bfill")  # process nan brought by dataloader
         dl_valid.config(fillna_type="ffill+bfill")  # process nan brought by dataloader
```

## qlib/contrib/model/pytorch_lstm.py

```diff
@@ -1,312 +1,306 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-
-from __future__ import division
-from __future__ import print_function
-
-import numpy as np
-import pandas as pd
-from typing import Text, Union
-import copy
-from ...utils import get_or_create_path
-from ...log import get_module_logger
-
-import torch
-import torch.nn as nn
-import torch.optim as optim
-
-from ...model.base import Model
-from ...data.dataset import DatasetH
-from ...data.dataset.handler import DataHandlerLP
-
-
-class LSTM(Model):
-    """LSTM Model
-
-    Parameters
-    ----------
-    d_feat : int
-        input dimension for each time step
-    metric: str
-        the evaluation metric used in early stop
-    optimizer : str
-        optimizer name
-    GPU : str
-        the GPU ID(s) used for training
-    """
-
-    def __init__(
-        self,
-        d_feat=6,
-        hidden_size=64,
-        num_layers=2,
-        dropout=0.0,
-        n_epochs=200,
-        lr=0.001,
-        metric="",
-        batch_size=2000,
-        early_stop=20,
-        loss="mse",
-        optimizer="adam",
-        GPU=0,
-        seed=None,
-        **kwargs
-    ):
-        # Set logger.
-        self.logger = get_module_logger("LSTM")
-        self.logger.info("LSTM pytorch version...")
-
-        # set hyper-parameters.
-        self.d_feat = d_feat
-        self.hidden_size = hidden_size
-        self.num_layers = num_layers
-        self.dropout = dropout
-        self.n_epochs = n_epochs
-        self.lr = lr
-        self.metric = metric
-        self.batch_size = batch_size
-        self.early_stop = early_stop
-        self.optimizer = optimizer.lower()
-        self.loss = loss
-        self.device = torch.device("cuda:%d" % (GPU) if torch.cuda.is_available() and GPU >= 0 else "cpu")
-        self.seed = seed
-
-        self.logger.info(
-            "LSTM parameters setting:"
-            "\nd_feat : {}"
-            "\nhidden_size : {}"
-            "\nnum_layers : {}"
-            "\ndropout : {}"
-            "\nn_epochs : {}"
-            "\nlr : {}"
-            "\nmetric : {}"
-            "\nbatch_size : {}"
-            "\nearly_stop : {}"
-            "\noptimizer : {}"
-            "\nloss_type : {}"
-            "\nvisible_GPU : {}"
-            "\nuse_GPU : {}"
-            "\nseed : {}".format(
-                d_feat,
-                hidden_size,
-                num_layers,
-                dropout,
-                n_epochs,
-                lr,
-                metric,
-                batch_size,
-                early_stop,
-                optimizer.lower(),
-                loss,
-                GPU,
-                self.use_gpu,
-                seed,
-            )
-        )
-
-        if self.seed is not None:
-            np.random.seed(self.seed)
-            torch.manual_seed(self.seed)
-
-        self.lstm_model = LSTMModel(
-            d_feat=self.d_feat,
-            hidden_size=self.hidden_size,
-            num_layers=self.num_layers,
-            dropout=self.dropout,
-        )
-        if optimizer.lower() == "adam":
-            self.train_optimizer = optim.Adam(self.lstm_model.parameters(), lr=self.lr)
-        elif optimizer.lower() == "gd":
-            self.train_optimizer = optim.SGD(self.lstm_model.parameters(), lr=self.lr)
-        else:
-            raise NotImplementedError("optimizer {} is not supported!".format(optimizer))
-
-        self.fitted = False
-        self.lstm_model.to(self.device)
-
-    @property
-    def use_gpu(self):
-        return self.device != torch.device("cpu")
-
-    def mse(self, pred, label):
-        loss = (pred - label) ** 2
-        return torch.mean(loss)
-
-    def loss_fn(self, pred, label):
-        mask = ~torch.isnan(label)
-
-        if self.loss == "mse":
-            return self.mse(pred[mask], label[mask])
-
-        raise ValueError("unknown loss `%s`" % self.loss)
-
-    def metric_fn(self, pred, label):
-
-        mask = torch.isfinite(label)
-
-        if self.metric in ("", "loss"):
-            return -self.loss_fn(pred[mask], label[mask])
-
-        raise ValueError("unknown metric `%s`" % self.metric)
-
-    def train_epoch(self, x_train, y_train):
-
-        x_train_values = x_train.values
-        y_train_values = np.squeeze(y_train.values)
-
-        self.lstm_model.train()
-
-        indices = np.arange(len(x_train_values))
-        np.random.shuffle(indices)
-
-        for i in range(len(indices))[:: self.batch_size]:
-
-            if len(indices) - i < self.batch_size:
-                break
-
-            feature = torch.from_numpy(x_train_values[indices[i : i + self.batch_size]]).float().to(self.device)
-            label = torch.from_numpy(y_train_values[indices[i : i + self.batch_size]]).float().to(self.device)
-
-            pred = self.lstm_model(feature)
-            loss = self.loss_fn(pred, label)
-
-            self.train_optimizer.zero_grad()
-            loss.backward()
-            torch.nn.utils.clip_grad_value_(self.lstm_model.parameters(), 3.0)
-            self.train_optimizer.step()
-
-    def test_epoch(self, data_x, data_y):
-
-        # prepare training data
-        x_values = data_x.values
-        y_values = np.squeeze(data_y.values)
-
-        self.lstm_model.eval()
-
-        scores = []
-        losses = []
-
-        indices = np.arange(len(x_values))
-
-        for i in range(len(indices))[:: self.batch_size]:
-
-            if len(indices) - i < self.batch_size:
-                break
-
-            feature = torch.from_numpy(x_values[indices[i : i + self.batch_size]]).float().to(self.device)
-            label = torch.from_numpy(y_values[indices[i : i + self.batch_size]]).float().to(self.device)
-
-            pred = self.lstm_model(feature)
-            loss = self.loss_fn(pred, label)
-            losses.append(loss.item())
-
-            score = self.metric_fn(pred, label)
-            scores.append(score.item())
-
-        return np.mean(losses), np.mean(scores)
-
-    def fit(
-        self,
-        dataset: DatasetH,
-        evals_result=dict(),
-        save_path=None,
-    ):
-
-        df_train, df_valid, df_test = dataset.prepare(
-            ["train", "valid", "test"],
-            col_set=["feature", "label"],
-            data_key=DataHandlerLP.DK_L,
-        )
-        if df_train.empty or df_valid.empty:
-            raise ValueError("Empty data from dataset, please check your dataset config.")
-
-        x_train, y_train = df_train["feature"], df_train["label"]
-        x_valid, y_valid = df_valid["feature"], df_valid["label"]
-
-        save_path = get_or_create_path(save_path)
-        stop_steps = 0
-        train_loss = 0
-        best_score = -np.inf
-        best_epoch = 0
-        evals_result["train"] = []
-        evals_result["valid"] = []
-
-        # train
-        self.logger.info("training...")
-        self.fitted = True
-
-        for step in range(self.n_epochs):
-            self.logger.info("Epoch%d:", step)
-            self.logger.info("training...")
-            self.train_epoch(x_train, y_train)
-            self.logger.info("evaluating...")
-            train_loss, train_score = self.test_epoch(x_train, y_train)
-            val_loss, val_score = self.test_epoch(x_valid, y_valid)
-            self.logger.info("train %.6f, valid %.6f" % (train_score, val_score))
-            evals_result["train"].append(train_score)
-            evals_result["valid"].append(val_score)
-
-            if val_score > best_score:
-                best_score = val_score
-                stop_steps = 0
-                best_epoch = step
-                best_param = copy.deepcopy(self.lstm_model.state_dict())
-            else:
-                stop_steps += 1
-                if stop_steps >= self.early_stop:
-                    self.logger.info("early stop")
-                    break
-
-        self.logger.info("best score: %.6lf @ %d" % (best_score, best_epoch))
-        self.lstm_model.load_state_dict(best_param)
-        torch.save(best_param, save_path)
-
-        if self.use_gpu:
-            torch.cuda.empty_cache()
-
-    def predict(self, dataset: DatasetH, segment: Union[Text, slice] = "test"):
-        if not self.fitted:
-            raise ValueError("model is not fitted yet!")
-
-        x_test = dataset.prepare(segment, col_set="feature", data_key=DataHandlerLP.DK_I)
-        index = x_test.index
-        self.lstm_model.eval()
-        x_values = x_test.values
-        sample_num = x_values.shape[0]
-        preds = []
-
-        for begin in range(sample_num)[:: self.batch_size]:
-            if sample_num - begin < self.batch_size:
-                end = sample_num
-            else:
-                end = begin + self.batch_size
-            x_batch = torch.from_numpy(x_values[begin:end]).float().to(self.device)
-            with torch.no_grad():
-                pred = self.lstm_model(x_batch).detach().cpu().numpy()
-            preds.append(pred)
-
-        return pd.Series(np.concatenate(preds), index=index)
-
-
-class LSTMModel(nn.Module):
-    def __init__(self, d_feat=6, hidden_size=64, num_layers=2, dropout=0.0):
-        super().__init__()
-
-        self.rnn = nn.LSTM(
-            input_size=d_feat,
-            hidden_size=hidden_size,
-            num_layers=num_layers,
-            batch_first=True,
-            dropout=dropout,
-        )
-        self.fc_out = nn.Linear(hidden_size, 1)
-
-        self.d_feat = d_feat
-
-    def forward(self, x):
-        # x: [N, F*T]
-        x = x.reshape(len(x), self.d_feat, -1)  # [N, F, T]
-        x = x.permute(0, 2, 1)  # [N, T, F]
-        out, _ = self.rnn(x)
-        return self.fc_out(out[:, -1, :]).squeeze()
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+
+from __future__ import division
+from __future__ import print_function
+
+import numpy as np
+import pandas as pd
+from typing import Text, Union
+import copy
+from ...utils import get_or_create_path
+from ...log import get_module_logger
+
+import torch
+import torch.nn as nn
+import torch.optim as optim
+
+from ...model.base import Model
+from ...data.dataset import DatasetH
+from ...data.dataset.handler import DataHandlerLP
+
+
+class LSTM(Model):
+    """LSTM Model
+
+    Parameters
+    ----------
+    d_feat : int
+        input dimension for each time step
+    metric: str
+        the evaluation metric used in early stop
+    optimizer : str
+        optimizer name
+    GPU : str
+        the GPU ID(s) used for training
+    """
+
+    def __init__(
+        self,
+        d_feat=6,
+        hidden_size=64,
+        num_layers=2,
+        dropout=0.0,
+        n_epochs=200,
+        lr=0.001,
+        metric="",
+        batch_size=2000,
+        early_stop=20,
+        loss="mse",
+        optimizer="adam",
+        GPU=0,
+        seed=None,
+        **kwargs
+    ):
+        # Set logger.
+        self.logger = get_module_logger("LSTM")
+        self.logger.info("LSTM pytorch version...")
+
+        # set hyper-parameters.
+        self.d_feat = d_feat
+        self.hidden_size = hidden_size
+        self.num_layers = num_layers
+        self.dropout = dropout
+        self.n_epochs = n_epochs
+        self.lr = lr
+        self.metric = metric
+        self.batch_size = batch_size
+        self.early_stop = early_stop
+        self.optimizer = optimizer.lower()
+        self.loss = loss
+        self.device = torch.device("cuda:%d" % (GPU) if torch.cuda.is_available() and GPU >= 0 else "cpu")
+        self.seed = seed
+
+        self.logger.info(
+            "LSTM parameters setting:"
+            "\nd_feat : {}"
+            "\nhidden_size : {}"
+            "\nnum_layers : {}"
+            "\ndropout : {}"
+            "\nn_epochs : {}"
+            "\nlr : {}"
+            "\nmetric : {}"
+            "\nbatch_size : {}"
+            "\nearly_stop : {}"
+            "\noptimizer : {}"
+            "\nloss_type : {}"
+            "\nvisible_GPU : {}"
+            "\nuse_GPU : {}"
+            "\nseed : {}".format(
+                d_feat,
+                hidden_size,
+                num_layers,
+                dropout,
+                n_epochs,
+                lr,
+                metric,
+                batch_size,
+                early_stop,
+                optimizer.lower(),
+                loss,
+                GPU,
+                self.use_gpu,
+                seed,
+            )
+        )
+
+        if self.seed is not None:
+            np.random.seed(self.seed)
+            torch.manual_seed(self.seed)
+
+        self.lstm_model = LSTMModel(
+            d_feat=self.d_feat,
+            hidden_size=self.hidden_size,
+            num_layers=self.num_layers,
+            dropout=self.dropout,
+        )
+        if optimizer.lower() == "adam":
+            self.train_optimizer = optim.Adam(self.lstm_model.parameters(), lr=self.lr)
+        elif optimizer.lower() == "gd":
+            self.train_optimizer = optim.SGD(self.lstm_model.parameters(), lr=self.lr)
+        else:
+            raise NotImplementedError("optimizer {} is not supported!".format(optimizer))
+
+        self.fitted = False
+        self.lstm_model.to(self.device)
+
+    @property
+    def use_gpu(self):
+        return self.device != torch.device("cpu")
+
+    def mse(self, pred, label):
+        loss = (pred - label) ** 2
+        return torch.mean(loss)
+
+    def loss_fn(self, pred, label):
+        mask = ~torch.isnan(label)
+
+        if self.loss == "mse":
+            return self.mse(pred[mask], label[mask])
+
+        raise ValueError("unknown loss `%s`" % self.loss)
+
+    def metric_fn(self, pred, label):
+        mask = torch.isfinite(label)
+
+        if self.metric in ("", "loss"):
+            return -self.loss_fn(pred[mask], label[mask])
+
+        raise ValueError("unknown metric `%s`" % self.metric)
+
+    def train_epoch(self, x_train, y_train):
+        x_train_values = x_train.values
+        y_train_values = np.squeeze(y_train.values)
+
+        self.lstm_model.train()
+
+        indices = np.arange(len(x_train_values))
+        np.random.shuffle(indices)
+
+        for i in range(len(indices))[:: self.batch_size]:
+            if len(indices) - i < self.batch_size:
+                break
+
+            feature = torch.from_numpy(x_train_values[indices[i : i + self.batch_size]]).float().to(self.device)
+            label = torch.from_numpy(y_train_values[indices[i : i + self.batch_size]]).float().to(self.device)
+
+            pred = self.lstm_model(feature)
+            loss = self.loss_fn(pred, label)
+
+            self.train_optimizer.zero_grad()
+            loss.backward()
+            torch.nn.utils.clip_grad_value_(self.lstm_model.parameters(), 3.0)
+            self.train_optimizer.step()
+
+    def test_epoch(self, data_x, data_y):
+        # prepare training data
+        x_values = data_x.values
+        y_values = np.squeeze(data_y.values)
+
+        self.lstm_model.eval()
+
+        scores = []
+        losses = []
+
+        indices = np.arange(len(x_values))
+
+        for i in range(len(indices))[:: self.batch_size]:
+            if len(indices) - i < self.batch_size:
+                break
+
+            feature = torch.from_numpy(x_values[indices[i : i + self.batch_size]]).float().to(self.device)
+            label = torch.from_numpy(y_values[indices[i : i + self.batch_size]]).float().to(self.device)
+
+            pred = self.lstm_model(feature)
+            loss = self.loss_fn(pred, label)
+            losses.append(loss.item())
+
+            score = self.metric_fn(pred, label)
+            scores.append(score.item())
+
+        return np.mean(losses), np.mean(scores)
+
+    def fit(
+        self,
+        dataset: DatasetH,
+        evals_result=dict(),
+        save_path=None,
+    ):
+        df_train, df_valid, df_test = dataset.prepare(
+            ["train", "valid", "test"],
+            col_set=["feature", "label"],
+            data_key=DataHandlerLP.DK_L,
+        )
+        if df_train.empty or df_valid.empty:
+            raise ValueError("Empty data from dataset, please check your dataset config.")
+
+        x_train, y_train = df_train["feature"], df_train["label"]
+        x_valid, y_valid = df_valid["feature"], df_valid["label"]
+
+        save_path = get_or_create_path(save_path)
+        stop_steps = 0
+        train_loss = 0
+        best_score = -np.inf
+        best_epoch = 0
+        evals_result["train"] = []
+        evals_result["valid"] = []
+
+        # train
+        self.logger.info("training...")
+        self.fitted = True
+
+        for step in range(self.n_epochs):
+            self.logger.info("Epoch%d:", step)
+            self.logger.info("training...")
+            self.train_epoch(x_train, y_train)
+            self.logger.info("evaluating...")
+            train_loss, train_score = self.test_epoch(x_train, y_train)
+            val_loss, val_score = self.test_epoch(x_valid, y_valid)
+            self.logger.info("train %.6f, valid %.6f" % (train_score, val_score))
+            evals_result["train"].append(train_score)
+            evals_result["valid"].append(val_score)
+
+            if val_score > best_score:
+                best_score = val_score
+                stop_steps = 0
+                best_epoch = step
+                best_param = copy.deepcopy(self.lstm_model.state_dict())
+            else:
+                stop_steps += 1
+                if stop_steps >= self.early_stop:
+                    self.logger.info("early stop")
+                    break
+
+        self.logger.info("best score: %.6lf @ %d" % (best_score, best_epoch))
+        self.lstm_model.load_state_dict(best_param)
+        torch.save(best_param, save_path)
+
+        if self.use_gpu:
+            torch.cuda.empty_cache()
+
+    def predict(self, dataset: DatasetH, segment: Union[Text, slice] = "test"):
+        if not self.fitted:
+            raise ValueError("model is not fitted yet!")
+
+        x_test = dataset.prepare(segment, col_set="feature", data_key=DataHandlerLP.DK_I)
+        index = x_test.index
+        self.lstm_model.eval()
+        x_values = x_test.values
+        sample_num = x_values.shape[0]
+        preds = []
+
+        for begin in range(sample_num)[:: self.batch_size]:
+            if sample_num - begin < self.batch_size:
+                end = sample_num
+            else:
+                end = begin + self.batch_size
+            x_batch = torch.from_numpy(x_values[begin:end]).float().to(self.device)
+            with torch.no_grad():
+                pred = self.lstm_model(x_batch).detach().cpu().numpy()
+            preds.append(pred)
+
+        return pd.Series(np.concatenate(preds), index=index)
+
+
+class LSTMModel(nn.Module):
+    def __init__(self, d_feat=6, hidden_size=64, num_layers=2, dropout=0.0):
+        super().__init__()
+
+        self.rnn = nn.LSTM(
+            input_size=d_feat,
+            hidden_size=hidden_size,
+            num_layers=num_layers,
+            batch_first=True,
+            dropout=dropout,
+        )
+        self.fc_out = nn.Linear(hidden_size, 1)
+
+        self.d_feat = d_feat
+
+    def forward(self, x):
+        # x: [N, F*T]
+        x = x.reshape(len(x), self.d_feat, -1)  # [N, F, T]
+        x = x.permute(0, 2, 1)  # [N, T, F]
+        out, _ = self.rnn(x)
+        return self.fc_out(out[:, -1, :]).squeeze()
```

## qlib/contrib/model/pytorch_lstm_ts.py

```diff
@@ -1,319 +1,314 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-
-from __future__ import division
-from __future__ import print_function
-
-import numpy as np
-import pandas as pd
-import copy
-from ...utils import get_or_create_path
-from ...log import get_module_logger
-
-import torch
-import torch.nn as nn
-import torch.optim as optim
-from torch.utils.data import DataLoader
-
-from ...model.base import Model
-from ...data.dataset.handler import DataHandlerLP
-from ...model.utils import ConcatDataset
-from ...data.dataset.weight import Reweighter
-
-
-class LSTM(Model):
-    """LSTM Model
-
-    Parameters
-    ----------
-    d_feat : int
-        input dimension for each time step
-    metric: str
-        the evaluation metric used in early stop
-    optimizer : str
-        optimizer name
-    GPU : str
-        the GPU ID(s) used for training
-    """
-
-    def __init__(
-        self,
-        d_feat=6,
-        hidden_size=64,
-        num_layers=2,
-        dropout=0.0,
-        n_epochs=200,
-        lr=0.001,
-        metric="",
-        batch_size=2000,
-        early_stop=20,
-        loss="mse",
-        optimizer="adam",
-        n_jobs=10,
-        GPU=0,
-        seed=None,
-        **kwargs
-    ):
-        # Set logger.
-        self.logger = get_module_logger("LSTM")
-        self.logger.info("LSTM pytorch version...")
-
-        # set hyper-parameters.
-        self.d_feat = d_feat
-        self.hidden_size = hidden_size
-        self.num_layers = num_layers
-        self.dropout = dropout
-        self.n_epochs = n_epochs
-        self.lr = lr
-        self.metric = metric
-        self.batch_size = batch_size
-        self.early_stop = early_stop
-        self.optimizer = optimizer.lower()
-        self.loss = loss
-        self.device = torch.device("cuda:%d" % (GPU) if torch.cuda.is_available() and GPU >= 0 else "cpu")
-        self.n_jobs = n_jobs
-        self.seed = seed
-
-        self.logger.info(
-            "LSTM parameters setting:"
-            "\nd_feat : {}"
-            "\nhidden_size : {}"
-            "\nnum_layers : {}"
-            "\ndropout : {}"
-            "\nn_epochs : {}"
-            "\nlr : {}"
-            "\nmetric : {}"
-            "\nbatch_size : {}"
-            "\nearly_stop : {}"
-            "\noptimizer : {}"
-            "\nloss_type : {}"
-            "\ndevice : {}"
-            "\nn_jobs : {}"
-            "\nuse_GPU : {}"
-            "\nseed : {}".format(
-                d_feat,
-                hidden_size,
-                num_layers,
-                dropout,
-                n_epochs,
-                lr,
-                metric,
-                batch_size,
-                early_stop,
-                optimizer.lower(),
-                loss,
-                self.device,
-                n_jobs,
-                self.use_gpu,
-                seed,
-            )
-        )
-
-        if self.seed is not None:
-            np.random.seed(self.seed)
-            torch.manual_seed(self.seed)
-
-        self.LSTM_model = LSTMModel(
-            d_feat=self.d_feat,
-            hidden_size=self.hidden_size,
-            num_layers=self.num_layers,
-            dropout=self.dropout,
-        ).to(self.device)
-        if optimizer.lower() == "adam":
-            self.train_optimizer = optim.Adam(self.LSTM_model.parameters(), lr=self.lr)
-        elif optimizer.lower() == "gd":
-            self.train_optimizer = optim.SGD(self.LSTM_model.parameters(), lr=self.lr)
-        else:
-            raise NotImplementedError("optimizer {} is not supported!".format(optimizer))
-
-        self.fitted = False
-        self.LSTM_model.to(self.device)
-
-    @property
-    def use_gpu(self):
-        return self.device != torch.device("cpu")
-
-    def mse(self, pred, label, weight):
-        loss = weight * (pred - label) ** 2
-        return torch.mean(loss)
-
-    def loss_fn(self, pred, label, weight):
-        mask = ~torch.isnan(label)
-
-        if weight is None:
-            weight = torch.ones_like(label)
-
-        if self.loss == "mse":
-            return self.mse(pred[mask], label[mask], weight[mask])
-
-        raise ValueError("unknown loss `%s`" % self.loss)
-
-    def metric_fn(self, pred, label):
-
-        mask = torch.isfinite(label)
-
-        if self.metric in ("", "loss"):
-            return -self.loss_fn(pred[mask], label[mask], weight=None)
-
-        raise ValueError("unknown metric `%s`" % self.metric)
-
-    def train_epoch(self, data_loader):
-
-        self.LSTM_model.train()
-
-        for (data, weight) in data_loader:
-            feature = data[:, :, 0:-1].to(self.device)
-            label = data[:, -1, -1].to(self.device)
-
-            pred = self.LSTM_model(feature.float())
-            loss = self.loss_fn(pred, label, weight.to(self.device))
-
-            self.train_optimizer.zero_grad()
-            loss.backward()
-            torch.nn.utils.clip_grad_value_(self.LSTM_model.parameters(), 3.0)
-            self.train_optimizer.step()
-
-    def test_epoch(self, data_loader):
-
-        self.LSTM_model.eval()
-
-        scores = []
-        losses = []
-
-        for (data, weight) in data_loader:
-
-            feature = data[:, :, 0:-1].to(self.device)
-            # feature[torch.isnan(feature)] = 0
-            label = data[:, -1, -1].to(self.device)
-
-            pred = self.LSTM_model(feature.float())
-            loss = self.loss_fn(pred, label, weight.to(self.device))
-            losses.append(loss.item())
-
-            score = self.metric_fn(pred, label)
-            scores.append(score.item())
-
-        return np.mean(losses), np.mean(scores)
-
-    def fit(
-        self,
-        dataset,
-        evals_result=dict(),
-        save_path=None,
-        reweighter=None,
-    ):
-        dl_train = dataset.prepare("train", col_set=["feature", "label"], data_key=DataHandlerLP.DK_L)
-        dl_valid = dataset.prepare("valid", col_set=["feature", "label"], data_key=DataHandlerLP.DK_L)
-        if dl_train.empty or dl_valid.empty:
-            raise ValueError("Empty data from dataset, please check your dataset config.")
-
-        dl_train.config(fillna_type="ffill+bfill")  # process nan brought by dataloader
-        dl_valid.config(fillna_type="ffill+bfill")  # process nan brought by dataloader
-
-        if reweighter is None:
-            wl_train = np.ones(len(dl_train))
-            wl_valid = np.ones(len(dl_valid))
-        elif isinstance(reweighter, Reweighter):
-            wl_train = reweighter.reweight(dl_train)
-            wl_valid = reweighter.reweight(dl_valid)
-        else:
-            raise ValueError("Unsupported reweighter type.")
-
-        train_loader = DataLoader(
-            ConcatDataset(dl_train, wl_train),
-            batch_size=self.batch_size,
-            shuffle=True,
-            num_workers=self.n_jobs,
-            drop_last=True,
-        )
-        valid_loader = DataLoader(
-            ConcatDataset(dl_valid, wl_valid),
-            batch_size=self.batch_size,
-            shuffle=False,
-            num_workers=self.n_jobs,
-            drop_last=True,
-        )
-
-        save_path = get_or_create_path(save_path)
-
-        stop_steps = 0
-        train_loss = 0
-        best_score = -np.inf
-        best_epoch = 0
-        evals_result["train"] = []
-        evals_result["valid"] = []
-
-        # train
-        self.logger.info("training...")
-        self.fitted = True
-
-        for step in range(self.n_epochs):
-            self.logger.info("Epoch%d:", step)
-            self.logger.info("training...")
-            self.train_epoch(train_loader)
-            self.logger.info("evaluating...")
-            train_loss, train_score = self.test_epoch(train_loader)
-            val_loss, val_score = self.test_epoch(valid_loader)
-            self.logger.info("train %.6f, valid %.6f" % (train_score, val_score))
-            evals_result["train"].append(train_score)
-            evals_result["valid"].append(val_score)
-
-            if val_score > best_score:
-                best_score = val_score
-                stop_steps = 0
-                best_epoch = step
-                best_param = copy.deepcopy(self.LSTM_model.state_dict())
-            else:
-                stop_steps += 1
-                if stop_steps >= self.early_stop:
-                    self.logger.info("early stop")
-                    break
-
-        self.logger.info("best score: %.6lf @ %d" % (best_score, best_epoch))
-        self.LSTM_model.load_state_dict(best_param)
-        torch.save(best_param, save_path)
-
-        if self.use_gpu:
-            torch.cuda.empty_cache()
-
-    def predict(self, dataset):
-        if not self.fitted:
-            raise ValueError("model is not fitted yet!")
-
-        dl_test = dataset.prepare("test", col_set=["feature", "label"], data_key=DataHandlerLP.DK_I)
-        dl_test.config(fillna_type="ffill+bfill")
-        test_loader = DataLoader(dl_test, batch_size=self.batch_size, num_workers=self.n_jobs)
-        self.LSTM_model.eval()
-        preds = []
-
-        for data in test_loader:
-
-            feature = data[:, :, 0:-1].to(self.device)
-
-            with torch.no_grad():
-                pred = self.LSTM_model(feature.float()).detach().cpu().numpy()
-
-            preds.append(pred)
-
-        return pd.Series(np.concatenate(preds), index=dl_test.get_index())
-
-
-class LSTMModel(nn.Module):
-    def __init__(self, d_feat=6, hidden_size=64, num_layers=2, dropout=0.0):
-        super().__init__()
-
-        self.rnn = nn.LSTM(
-            input_size=d_feat,
-            hidden_size=hidden_size,
-            num_layers=num_layers,
-            batch_first=True,
-            dropout=dropout,
-        )
-        self.fc_out = nn.Linear(hidden_size, 1)
-
-        self.d_feat = d_feat
-
-    def forward(self, x):
-        out, _ = self.rnn(x)
-        return self.fc_out(out[:, -1, :]).squeeze()
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+
+from __future__ import division
+from __future__ import print_function
+
+import numpy as np
+import pandas as pd
+import copy
+from ...utils import get_or_create_path
+from ...log import get_module_logger
+
+import torch
+import torch.nn as nn
+import torch.optim as optim
+from torch.utils.data import DataLoader
+
+from ...model.base import Model
+from ...data.dataset.handler import DataHandlerLP
+from ...model.utils import ConcatDataset
+from ...data.dataset.weight import Reweighter
+
+
+class LSTM(Model):
+    """LSTM Model
+
+    Parameters
+    ----------
+    d_feat : int
+        input dimension for each time step
+    metric: str
+        the evaluation metric used in early stop
+    optimizer : str
+        optimizer name
+    GPU : str
+        the GPU ID(s) used for training
+    """
+
+    def __init__(
+        self,
+        d_feat=6,
+        hidden_size=64,
+        num_layers=2,
+        dropout=0.0,
+        n_epochs=200,
+        lr=0.001,
+        metric="",
+        batch_size=2000,
+        early_stop=20,
+        loss="mse",
+        optimizer="adam",
+        n_jobs=10,
+        GPU=0,
+        seed=None,
+        **kwargs
+    ):
+        # Set logger.
+        self.logger = get_module_logger("LSTM")
+        self.logger.info("LSTM pytorch version...")
+
+        # set hyper-parameters.
+        self.d_feat = d_feat
+        self.hidden_size = hidden_size
+        self.num_layers = num_layers
+        self.dropout = dropout
+        self.n_epochs = n_epochs
+        self.lr = lr
+        self.metric = metric
+        self.batch_size = batch_size
+        self.early_stop = early_stop
+        self.optimizer = optimizer.lower()
+        self.loss = loss
+        self.device = torch.device("cuda:%d" % (GPU) if torch.cuda.is_available() and GPU >= 0 else "cpu")
+        self.n_jobs = n_jobs
+        self.seed = seed
+
+        self.logger.info(
+            "LSTM parameters setting:"
+            "\nd_feat : {}"
+            "\nhidden_size : {}"
+            "\nnum_layers : {}"
+            "\ndropout : {}"
+            "\nn_epochs : {}"
+            "\nlr : {}"
+            "\nmetric : {}"
+            "\nbatch_size : {}"
+            "\nearly_stop : {}"
+            "\noptimizer : {}"
+            "\nloss_type : {}"
+            "\ndevice : {}"
+            "\nn_jobs : {}"
+            "\nuse_GPU : {}"
+            "\nseed : {}".format(
+                d_feat,
+                hidden_size,
+                num_layers,
+                dropout,
+                n_epochs,
+                lr,
+                metric,
+                batch_size,
+                early_stop,
+                optimizer.lower(),
+                loss,
+                self.device,
+                n_jobs,
+                self.use_gpu,
+                seed,
+            )
+        )
+
+        if self.seed is not None:
+            np.random.seed(self.seed)
+            torch.manual_seed(self.seed)
+
+        self.LSTM_model = LSTMModel(
+            d_feat=self.d_feat,
+            hidden_size=self.hidden_size,
+            num_layers=self.num_layers,
+            dropout=self.dropout,
+        ).to(self.device)
+        if optimizer.lower() == "adam":
+            self.train_optimizer = optim.Adam(self.LSTM_model.parameters(), lr=self.lr)
+        elif optimizer.lower() == "gd":
+            self.train_optimizer = optim.SGD(self.LSTM_model.parameters(), lr=self.lr)
+        else:
+            raise NotImplementedError("optimizer {} is not supported!".format(optimizer))
+
+        self.fitted = False
+        self.LSTM_model.to(self.device)
+
+    @property
+    def use_gpu(self):
+        return self.device != torch.device("cpu")
+
+    def mse(self, pred, label, weight):
+        loss = weight * (pred - label) ** 2
+        return torch.mean(loss)
+
+    def loss_fn(self, pred, label, weight):
+        mask = ~torch.isnan(label)
+
+        if weight is None:
+            weight = torch.ones_like(label)
+
+        if self.loss == "mse":
+            return self.mse(pred[mask], label[mask], weight[mask])
+
+        raise ValueError("unknown loss `%s`" % self.loss)
+
+    def metric_fn(self, pred, label):
+        mask = torch.isfinite(label)
+
+        if self.metric in ("", "loss"):
+            return -self.loss_fn(pred[mask], label[mask], weight=None)
+
+        raise ValueError("unknown metric `%s`" % self.metric)
+
+    def train_epoch(self, data_loader):
+        self.LSTM_model.train()
+
+        for data, weight in data_loader:
+            feature = data[:, :, 0:-1].to(self.device)
+            label = data[:, -1, -1].to(self.device)
+
+            pred = self.LSTM_model(feature.float())
+            loss = self.loss_fn(pred, label, weight.to(self.device))
+
+            self.train_optimizer.zero_grad()
+            loss.backward()
+            torch.nn.utils.clip_grad_value_(self.LSTM_model.parameters(), 3.0)
+            self.train_optimizer.step()
+
+    def test_epoch(self, data_loader):
+        self.LSTM_model.eval()
+
+        scores = []
+        losses = []
+
+        for data, weight in data_loader:
+            feature = data[:, :, 0:-1].to(self.device)
+            # feature[torch.isnan(feature)] = 0
+            label = data[:, -1, -1].to(self.device)
+
+            pred = self.LSTM_model(feature.float())
+            loss = self.loss_fn(pred, label, weight.to(self.device))
+            losses.append(loss.item())
+
+            score = self.metric_fn(pred, label)
+            scores.append(score.item())
+
+        return np.mean(losses), np.mean(scores)
+
+    def fit(
+        self,
+        dataset,
+        evals_result=dict(),
+        save_path=None,
+        reweighter=None,
+    ):
+        dl_train = dataset.prepare("train", col_set=["feature", "label"], data_key=DataHandlerLP.DK_L)
+        dl_valid = dataset.prepare("valid", col_set=["feature", "label"], data_key=DataHandlerLP.DK_L)
+        if dl_train.empty or dl_valid.empty:
+            raise ValueError("Empty data from dataset, please check your dataset config.")
+
+        dl_train.config(fillna_type="ffill+bfill")  # process nan brought by dataloader
+        dl_valid.config(fillna_type="ffill+bfill")  # process nan brought by dataloader
+
+        if reweighter is None:
+            wl_train = np.ones(len(dl_train))
+            wl_valid = np.ones(len(dl_valid))
+        elif isinstance(reweighter, Reweighter):
+            wl_train = reweighter.reweight(dl_train)
+            wl_valid = reweighter.reweight(dl_valid)
+        else:
+            raise ValueError("Unsupported reweighter type.")
+
+        train_loader = DataLoader(
+            ConcatDataset(dl_train, wl_train),
+            batch_size=self.batch_size,
+            shuffle=True,
+            num_workers=self.n_jobs,
+            drop_last=True,
+        )
+        valid_loader = DataLoader(
+            ConcatDataset(dl_valid, wl_valid),
+            batch_size=self.batch_size,
+            shuffle=False,
+            num_workers=self.n_jobs,
+            drop_last=True,
+        )
+
+        save_path = get_or_create_path(save_path)
+
+        stop_steps = 0
+        train_loss = 0
+        best_score = -np.inf
+        best_epoch = 0
+        evals_result["train"] = []
+        evals_result["valid"] = []
+
+        # train
+        self.logger.info("training...")
+        self.fitted = True
+
+        for step in range(self.n_epochs):
+            self.logger.info("Epoch%d:", step)
+            self.logger.info("training...")
+            self.train_epoch(train_loader)
+            self.logger.info("evaluating...")
+            train_loss, train_score = self.test_epoch(train_loader)
+            val_loss, val_score = self.test_epoch(valid_loader)
+            self.logger.info("train %.6f, valid %.6f" % (train_score, val_score))
+            evals_result["train"].append(train_score)
+            evals_result["valid"].append(val_score)
+
+            if val_score > best_score:
+                best_score = val_score
+                stop_steps = 0
+                best_epoch = step
+                best_param = copy.deepcopy(self.LSTM_model.state_dict())
+            else:
+                stop_steps += 1
+                if stop_steps >= self.early_stop:
+                    self.logger.info("early stop")
+                    break
+
+        self.logger.info("best score: %.6lf @ %d" % (best_score, best_epoch))
+        self.LSTM_model.load_state_dict(best_param)
+        torch.save(best_param, save_path)
+
+        if self.use_gpu:
+            torch.cuda.empty_cache()
+
+    def predict(self, dataset):
+        if not self.fitted:
+            raise ValueError("model is not fitted yet!")
+
+        dl_test = dataset.prepare("test", col_set=["feature", "label"], data_key=DataHandlerLP.DK_I)
+        dl_test.config(fillna_type="ffill+bfill")
+        test_loader = DataLoader(dl_test, batch_size=self.batch_size, num_workers=self.n_jobs)
+        self.LSTM_model.eval()
+        preds = []
+
+        for data in test_loader:
+            feature = data[:, :, 0:-1].to(self.device)
+
+            with torch.no_grad():
+                pred = self.LSTM_model(feature.float()).detach().cpu().numpy()
+
+            preds.append(pred)
+
+        return pd.Series(np.concatenate(preds), index=dl_test.get_index())
+
+
+class LSTMModel(nn.Module):
+    def __init__(self, d_feat=6, hidden_size=64, num_layers=2, dropout=0.0):
+        super().__init__()
+
+        self.rnn = nn.LSTM(
+            input_size=d_feat,
+            hidden_size=hidden_size,
+            num_layers=num_layers,
+            batch_first=True,
+            dropout=dropout,
+        )
+        self.fc_out = nn.Linear(hidden_size, 1)
+
+        self.d_feat = d_feat
+
+    def forward(self, x):
+        out, _ = self.rnn(x)
+        return self.fc_out(out[:, -1, :]).squeeze()
```

## qlib/contrib/model/pytorch_nn.py

 * *Ordering differences only*

```diff
@@ -1,447 +1,447 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-
-from __future__ import division
-from __future__ import print_function
-from collections import defaultdict
-
-import os
-import gc
-import numpy as np
-import pandas as pd
-from typing import Callable, Optional, Text, Union
-from sklearn.metrics import roc_auc_score, mean_squared_error
-
-import torch
-import torch.nn as nn
-import torch.optim as optim
-
-from .pytorch_utils import count_parameters
-from ...model.base import Model
-from ...data.dataset import DatasetH
-from ...data.dataset.handler import DataHandlerLP
-from ...data.dataset.weight import Reweighter
-from ...utils import (
-    auto_filter_kwargs,
-    init_instance_by_config,
-    unpack_archive_with_buffer,
-    save_multiple_parts_file,
-    get_or_create_path,
-)
-from ...log import get_module_logger
-from ...workflow import R
-from qlib.contrib.meta.data_selection.utils import ICLoss
-from torch.nn import DataParallel
-
-
-class DNNModelPytorch(Model):
-    """DNN Model
-    Parameters
-    ----------
-    input_dim : int
-        input dimension
-    output_dim : int
-        output dimension
-    layers : tuple
-        layer sizes
-    lr : float
-        learning rate
-    optimizer : str
-        optimizer name
-    GPU : int
-        the GPU ID used for training
-    """
-
-    def __init__(
-        self,
-        lr=0.001,
-        max_steps=300,
-        batch_size=2000,
-        early_stop_rounds=50,
-        eval_steps=20,
-        optimizer="gd",
-        loss="mse",
-        GPU=0,
-        seed=None,
-        weight_decay=0.0,
-        data_parall=False,
-        scheduler: Optional[Union[Callable]] = "default",  # when it is Callable, it accept one argument named optimizer
-        init_model=None,
-        eval_train_metric=False,
-        pt_model_uri="qlib.contrib.model.pytorch_nn.Net",
-        pt_model_kwargs={
-            "input_dim": 360,
-            "layers": (256,),
-        },
-        valid_key=DataHandlerLP.DK_L,
-        # TODO: Infer Key is a more reasonable key. But it requires more detailed processing on label processing
-    ):
-        # Set logger.
-        self.logger = get_module_logger("DNNModelPytorch")
-        self.logger.info("DNN pytorch version...")
-
-        # set hyper-parameters.
-        self.lr = lr
-        self.max_steps = max_steps
-        self.batch_size = batch_size
-        self.early_stop_rounds = early_stop_rounds
-        self.eval_steps = eval_steps
-        self.optimizer = optimizer.lower()
-        self.loss_type = loss
-        if isinstance(GPU, str):
-            self.device = torch.device(GPU)
-        else:
-            self.device = torch.device("cuda:%d" % (GPU) if torch.cuda.is_available() and GPU >= 0 else "cpu")
-        self.seed = seed
-        self.weight_decay = weight_decay
-        self.data_parall = data_parall
-        self.eval_train_metric = eval_train_metric
-        self.valid_key = valid_key
-
-        self.best_step = None
-
-        self.logger.info(
-            "DNN parameters setting:"
-            f"\nlr : {lr}"
-            f"\nmax_steps : {max_steps}"
-            f"\nbatch_size : {batch_size}"
-            f"\nearly_stop_rounds : {early_stop_rounds}"
-            f"\neval_steps : {eval_steps}"
-            f"\noptimizer : {optimizer}"
-            f"\nloss_type : {loss}"
-            f"\nseed : {seed}"
-            f"\ndevice : {self.device}"
-            f"\nuse_GPU : {self.use_gpu}"
-            f"\nweight_decay : {weight_decay}"
-            f"\nenable data parall : {self.data_parall}"
-            f"\npt_model_uri: {pt_model_uri}"
-            f"\npt_model_kwargs: {pt_model_kwargs}"
-        )
-
-        if self.seed is not None:
-            np.random.seed(self.seed)
-            torch.manual_seed(self.seed)
-
-        if loss not in {"mse", "binary"}:
-            raise NotImplementedError("loss {} is not supported!".format(loss))
-        self._scorer = mean_squared_error if loss == "mse" else roc_auc_score
-
-        if init_model is None:
-            self.dnn_model = init_instance_by_config({"class": pt_model_uri, "kwargs": pt_model_kwargs})
-
-            if self.data_parall:
-                self.dnn_model = DataParallel(self.dnn_model).to(self.device)
-        else:
-            self.dnn_model = init_model
-
-        self.logger.info("model:\n{:}".format(self.dnn_model))
-        self.logger.info("model size: {:.4f} MB".format(count_parameters(self.dnn_model)))
-
-        if optimizer.lower() == "adam":
-            self.train_optimizer = optim.Adam(self.dnn_model.parameters(), lr=self.lr, weight_decay=self.weight_decay)
-        elif optimizer.lower() == "gd":
-            self.train_optimizer = optim.SGD(self.dnn_model.parameters(), lr=self.lr, weight_decay=self.weight_decay)
-        else:
-            raise NotImplementedError("optimizer {} is not supported!".format(optimizer))
-
-        if scheduler == "default":
-            # Reduce learning rate when loss has stopped decrease
-            self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
-                self.train_optimizer,
-                mode="min",
-                factor=0.5,
-                patience=10,
-                verbose=True,
-                threshold=0.0001,
-                threshold_mode="rel",
-                cooldown=0,
-                min_lr=0.00001,
-                eps=1e-08,
-            )
-        elif scheduler is None:
-            self.scheduler = None
-        else:
-            self.scheduler = scheduler(optimizer=self.train_optimizer)
-
-        self.fitted = False
-        self.dnn_model.to(self.device)
-
-    @property
-    def use_gpu(self):
-        return self.device != torch.device("cpu")
-
-    def fit(
-        self,
-        dataset: DatasetH,
-        evals_result=dict(),
-        verbose=True,
-        save_path=None,
-        reweighter=None,
-    ):
-        has_valid = "valid" in dataset.segments
-        segments = ["train", "valid"]
-        vars = ["x", "y", "w"]
-        all_df = defaultdict(dict)  # x_train, x_valid y_train, y_valid w_train, w_valid
-        all_t = defaultdict(dict)  # tensors
-        for seg in segments:
-            if seg in dataset.segments:
-                # df_train df_valid
-                df = dataset.prepare(
-                    seg, col_set=["feature", "label"], data_key=self.valid_key if seg == "valid" else DataHandlerLP.DK_L
-                )
-                all_df["x"][seg] = df["feature"]
-                all_df["y"][seg] = df["label"].copy()  # We have to use copy to remove the reference to release mem
-                if reweighter is None:
-                    all_df["w"][seg] = pd.DataFrame(np.ones_like(all_df["y"][seg].values), index=df.index)
-                elif isinstance(reweighter, Reweighter):
-                    all_df["w"][seg] = pd.DataFrame(reweighter.reweight(df))
-                else:
-                    raise ValueError("Unsupported reweighter type.")
-
-                # get tensors
-                for v in vars:
-                    all_t[v][seg] = torch.from_numpy(all_df[v][seg].values).float()
-                    # if seg == "valid": # accelerate the eval of validation
-                    all_t[v][seg] = all_t[v][seg].to(self.device)  # This will consume a lot of memory !!!!
-
-                evals_result[seg] = []
-                # free memory
-                del df
-                del all_df["x"]
-                gc.collect()
-
-        save_path = get_or_create_path(save_path)
-        stop_steps = 0
-        train_loss = 0
-        best_loss = np.inf
-        # train
-        self.logger.info("training...")
-        self.fitted = True
-        # return
-        # prepare training data
-        train_num = all_t["y"]["train"].shape[0]
-
-        for step in range(1, self.max_steps + 1):
-            if stop_steps >= self.early_stop_rounds:
-                if verbose:
-                    self.logger.info("\tearly stop")
-                break
-            loss = AverageMeter()
-            self.dnn_model.train()
-            self.train_optimizer.zero_grad()
-            choice = np.random.choice(train_num, self.batch_size)
-            x_batch_auto = all_t["x"]["train"][choice].to(self.device)
-            y_batch_auto = all_t["y"]["train"][choice].to(self.device)
-            w_batch_auto = all_t["w"]["train"][choice].to(self.device)
-
-            # forward
-            preds = self.dnn_model(x_batch_auto)
-            cur_loss = self.get_loss(preds, w_batch_auto, y_batch_auto, self.loss_type)
-            cur_loss.backward()
-            self.train_optimizer.step()
-            loss.update(cur_loss.item())
-            R.log_metrics(train_loss=loss.avg, step=step)
-
-            # validation
-            train_loss += loss.val
-            # for evert `eval_steps` steps or at the last steps, we will evaluate the model.
-            if step % self.eval_steps == 0 or step == self.max_steps:
-                if has_valid:
-                    stop_steps += 1
-                    train_loss /= self.eval_steps
-
-                    with torch.no_grad():
-                        self.dnn_model.eval()
-
-                        # forward
-                        preds = self._nn_predict(all_t["x"]["valid"], return_cpu=False)
-                        cur_loss_val = self.get_loss(preds, all_t["w"]["valid"], all_t["y"]["valid"], self.loss_type)
-                        loss_val = cur_loss_val.item()
-                        metric_val = (
-                            self.get_metric(
-                                preds.reshape(-1), all_t["y"]["valid"].reshape(-1), all_df["y"]["valid"].index
-                            )
-                            .detach()
-                            .cpu()
-                            .numpy()
-                            .item()
-                        )
-                        R.log_metrics(val_loss=loss_val, step=step)
-                        R.log_metrics(val_metric=metric_val, step=step)
-
-                        if self.eval_train_metric:
-                            metric_train = (
-                                self.get_metric(
-                                    self._nn_predict(all_t["x"]["train"], return_cpu=False),
-                                    all_t["y"]["train"].reshape(-1),
-                                    all_df["y"]["train"].index,
-                                )
-                                .detach()
-                                .cpu()
-                                .numpy()
-                                .item()
-                            )
-                            R.log_metrics(train_metric=metric_train, step=step)
-                        else:
-                            metric_train = np.nan
-                    if verbose:
-                        self.logger.info(
-                            f"[Step {step}]: train_loss {train_loss:.6f}, valid_loss {loss_val:.6f}, train_metric {metric_train:.6f}, valid_metric {metric_val:.6f}"
-                        )
-                    evals_result["train"].append(train_loss)
-                    evals_result["valid"].append(loss_val)
-                    if loss_val < best_loss:
-                        if verbose:
-                            self.logger.info(
-                                "\tvalid loss update from {:.6f} to {:.6f}, save checkpoint.".format(
-                                    best_loss, loss_val
-                                )
-                            )
-                        best_loss = loss_val
-                        self.best_step = step
-                        R.log_metrics(best_step=self.best_step, step=step)
-                        stop_steps = 0
-                        torch.save(self.dnn_model.state_dict(), save_path)
-                    train_loss = 0
-                    # update learning rate
-                    if self.scheduler is not None:
-                        auto_filter_kwargs(self.scheduler.step, warning=False)(metrics=cur_loss_val, epoch=step)
-                    R.log_metrics(lr=self.get_lr(), step=step)
-                else:
-                    # retraining mode
-                    if self.scheduler is not None:
-                        self.scheduler.step(epoch=step)
-
-        if has_valid:
-            # restore the optimal parameters after training
-            self.dnn_model.load_state_dict(torch.load(save_path, map_location=self.device))
-        if self.use_gpu:
-            torch.cuda.empty_cache()
-
-    def get_lr(self):
-        assert len(self.train_optimizer.param_groups) == 1
-        return self.train_optimizer.param_groups[0]["lr"]
-
-    def get_loss(self, pred, w, target, loss_type):
-        pred, w, target = pred.reshape(-1), w.reshape(-1), target.reshape(-1)
-        if loss_type == "mse":
-            sqr_loss = torch.mul(pred - target, pred - target)
-            loss = torch.mul(sqr_loss, w).mean()
-            return loss
-        elif loss_type == "binary":
-            loss = nn.BCEWithLogitsLoss(weight=w)
-            return loss(pred, target)
-        else:
-            raise NotImplementedError("loss {} is not supported!".format(loss_type))
-
-    def get_metric(self, pred, target, index):
-        # NOTE: the order of the index must follow <datetime, instrument> sorted order
-        return -ICLoss()(pred, target, index)  # pylint: disable=E1130
-
-    def _nn_predict(self, data, return_cpu=True):
-        """Reusing predicting NN.
-        Scenarios
-        1) test inference (data may come from CPU and expect the output data is on CPU)
-        2) evaluation on training (data may come from GPU)
-        """
-        if not isinstance(data, torch.Tensor):
-            if isinstance(data, pd.DataFrame):
-                data = data.values
-            data = torch.Tensor(data)
-        data = data.to(self.device)
-        preds = []
-        self.dnn_model.eval()
-        with torch.no_grad():
-            batch_size = 8096
-            for i in range(0, len(data), batch_size):
-                x = data[i : i + batch_size]
-                preds.append(self.dnn_model(x.to(self.device)).detach().reshape(-1))
-        if return_cpu:
-            preds = np.concatenate([pr.cpu().numpy() for pr in preds])
-        else:
-            preds = torch.cat(preds, axis=0)
-        return preds
-
-    def predict(self, dataset: DatasetH, segment: Union[Text, slice] = "test"):
-        if not self.fitted:
-            raise ValueError("model is not fitted yet!")
-        x_test_pd = dataset.prepare(segment, col_set="feature", data_key=DataHandlerLP.DK_I)
-        preds = self._nn_predict(x_test_pd)
-        return pd.Series(preds.reshape(-1), index=x_test_pd.index)
-
-    def save(self, filename, **kwargs):
-        with save_multiple_parts_file(filename) as model_dir:
-            model_path = os.path.join(model_dir, os.path.split(model_dir)[-1])
-            # Save model
-            torch.save(self.dnn_model.state_dict(), model_path)
-
-    def load(self, buffer, **kwargs):
-        with unpack_archive_with_buffer(buffer) as model_dir:
-            # Get model name
-            _model_name = os.path.splitext(list(filter(lambda x: x.startswith("model.bin"), os.listdir(model_dir)))[0])[
-                0
-            ]
-            _model_path = os.path.join(model_dir, _model_name)
-            # Load model
-            self.dnn_model.load_state_dict(torch.load(_model_path, map_location=self.device))
-        self.fitted = True
-
-
-class AverageMeter:
-    """Computes and stores the average and current value"""
-
-    def __init__(self):
-        self.reset()
-
-    def reset(self):
-        self.val = 0
-        self.avg = 0
-        self.sum = 0
-        self.count = 0
-
-    def update(self, val, n=1):
-        self.val = val
-        self.sum += val * n
-        self.count += n
-        self.avg = self.sum / self.count
-
-
-class Net(nn.Module):
-    def __init__(self, input_dim, output_dim=1, layers=(256,), act="LeakyReLU"):
-        super(Net, self).__init__()
-
-        layers = [input_dim] + list(layers)
-        dnn_layers = []
-        drop_input = nn.Dropout(0.05)
-        dnn_layers.append(drop_input)
-        hidden_units = input_dim
-        for i, (_input_dim, hidden_units) in enumerate(zip(layers[:-1], layers[1:])):
-            fc = nn.Linear(_input_dim, hidden_units)
-            if act == "LeakyReLU":
-                activation = nn.LeakyReLU(negative_slope=0.1, inplace=False)
-            elif act == "SiLU":
-                activation = nn.SiLU()
-            else:
-                raise NotImplementedError(f"This type of input is not supported")
-            bn = nn.BatchNorm1d(hidden_units)
-            seq = nn.Sequential(fc, bn, activation)
-            dnn_layers.append(seq)
-        drop_input = nn.Dropout(0.05)
-        dnn_layers.append(drop_input)
-        fc = nn.Linear(hidden_units, output_dim)
-        dnn_layers.append(fc)
-        # optimizer  # pylint: disable=W0631
-        self.dnn_layers = nn.ModuleList(dnn_layers)
-        self._weight_init()
-
-    def _weight_init(self):
-        for m in self.modules():
-            if isinstance(m, nn.Linear):
-                nn.init.kaiming_normal_(m.weight, a=0.1, mode="fan_in", nonlinearity="leaky_relu")
-
-    def forward(self, x):
-        cur_output = x
-        for i, now_layer in enumerate(self.dnn_layers):
-            cur_output = now_layer(cur_output)
-        return cur_output
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+
+from __future__ import division
+from __future__ import print_function
+from collections import defaultdict
+
+import os
+import gc
+import numpy as np
+import pandas as pd
+from typing import Callable, Optional, Text, Union
+from sklearn.metrics import roc_auc_score, mean_squared_error
+
+import torch
+import torch.nn as nn
+import torch.optim as optim
+
+from .pytorch_utils import count_parameters
+from ...model.base import Model
+from ...data.dataset import DatasetH
+from ...data.dataset.handler import DataHandlerLP
+from ...data.dataset.weight import Reweighter
+from ...utils import (
+    auto_filter_kwargs,
+    init_instance_by_config,
+    unpack_archive_with_buffer,
+    save_multiple_parts_file,
+    get_or_create_path,
+)
+from ...log import get_module_logger
+from ...workflow import R
+from qlib.contrib.meta.data_selection.utils import ICLoss
+from torch.nn import DataParallel
+
+
+class DNNModelPytorch(Model):
+    """DNN Model
+    Parameters
+    ----------
+    input_dim : int
+        input dimension
+    output_dim : int
+        output dimension
+    layers : tuple
+        layer sizes
+    lr : float
+        learning rate
+    optimizer : str
+        optimizer name
+    GPU : int
+        the GPU ID used for training
+    """
+
+    def __init__(
+        self,
+        lr=0.001,
+        max_steps=300,
+        batch_size=2000,
+        early_stop_rounds=50,
+        eval_steps=20,
+        optimizer="gd",
+        loss="mse",
+        GPU=0,
+        seed=None,
+        weight_decay=0.0,
+        data_parall=False,
+        scheduler: Optional[Union[Callable]] = "default",  # when it is Callable, it accept one argument named optimizer
+        init_model=None,
+        eval_train_metric=False,
+        pt_model_uri="qlib.contrib.model.pytorch_nn.Net",
+        pt_model_kwargs={
+            "input_dim": 360,
+            "layers": (256,),
+        },
+        valid_key=DataHandlerLP.DK_L,
+        # TODO: Infer Key is a more reasonable key. But it requires more detailed processing on label processing
+    ):
+        # Set logger.
+        self.logger = get_module_logger("DNNModelPytorch")
+        self.logger.info("DNN pytorch version...")
+
+        # set hyper-parameters.
+        self.lr = lr
+        self.max_steps = max_steps
+        self.batch_size = batch_size
+        self.early_stop_rounds = early_stop_rounds
+        self.eval_steps = eval_steps
+        self.optimizer = optimizer.lower()
+        self.loss_type = loss
+        if isinstance(GPU, str):
+            self.device = torch.device(GPU)
+        else:
+            self.device = torch.device("cuda:%d" % (GPU) if torch.cuda.is_available() and GPU >= 0 else "cpu")
+        self.seed = seed
+        self.weight_decay = weight_decay
+        self.data_parall = data_parall
+        self.eval_train_metric = eval_train_metric
+        self.valid_key = valid_key
+
+        self.best_step = None
+
+        self.logger.info(
+            "DNN parameters setting:"
+            f"\nlr : {lr}"
+            f"\nmax_steps : {max_steps}"
+            f"\nbatch_size : {batch_size}"
+            f"\nearly_stop_rounds : {early_stop_rounds}"
+            f"\neval_steps : {eval_steps}"
+            f"\noptimizer : {optimizer}"
+            f"\nloss_type : {loss}"
+            f"\nseed : {seed}"
+            f"\ndevice : {self.device}"
+            f"\nuse_GPU : {self.use_gpu}"
+            f"\nweight_decay : {weight_decay}"
+            f"\nenable data parall : {self.data_parall}"
+            f"\npt_model_uri: {pt_model_uri}"
+            f"\npt_model_kwargs: {pt_model_kwargs}"
+        )
+
+        if self.seed is not None:
+            np.random.seed(self.seed)
+            torch.manual_seed(self.seed)
+
+        if loss not in {"mse", "binary"}:
+            raise NotImplementedError("loss {} is not supported!".format(loss))
+        self._scorer = mean_squared_error if loss == "mse" else roc_auc_score
+
+        if init_model is None:
+            self.dnn_model = init_instance_by_config({"class": pt_model_uri, "kwargs": pt_model_kwargs})
+
+            if self.data_parall:
+                self.dnn_model = DataParallel(self.dnn_model).to(self.device)
+        else:
+            self.dnn_model = init_model
+
+        self.logger.info("model:\n{:}".format(self.dnn_model))
+        self.logger.info("model size: {:.4f} MB".format(count_parameters(self.dnn_model)))
+
+        if optimizer.lower() == "adam":
+            self.train_optimizer = optim.Adam(self.dnn_model.parameters(), lr=self.lr, weight_decay=self.weight_decay)
+        elif optimizer.lower() == "gd":
+            self.train_optimizer = optim.SGD(self.dnn_model.parameters(), lr=self.lr, weight_decay=self.weight_decay)
+        else:
+            raise NotImplementedError("optimizer {} is not supported!".format(optimizer))
+
+        if scheduler == "default":
+            # Reduce learning rate when loss has stopped decrease
+            self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
+                self.train_optimizer,
+                mode="min",
+                factor=0.5,
+                patience=10,
+                verbose=True,
+                threshold=0.0001,
+                threshold_mode="rel",
+                cooldown=0,
+                min_lr=0.00001,
+                eps=1e-08,
+            )
+        elif scheduler is None:
+            self.scheduler = None
+        else:
+            self.scheduler = scheduler(optimizer=self.train_optimizer)
+
+        self.fitted = False
+        self.dnn_model.to(self.device)
+
+    @property
+    def use_gpu(self):
+        return self.device != torch.device("cpu")
+
+    def fit(
+        self,
+        dataset: DatasetH,
+        evals_result=dict(),
+        verbose=True,
+        save_path=None,
+        reweighter=None,
+    ):
+        has_valid = "valid" in dataset.segments
+        segments = ["train", "valid"]
+        vars = ["x", "y", "w"]
+        all_df = defaultdict(dict)  # x_train, x_valid y_train, y_valid w_train, w_valid
+        all_t = defaultdict(dict)  # tensors
+        for seg in segments:
+            if seg in dataset.segments:
+                # df_train df_valid
+                df = dataset.prepare(
+                    seg, col_set=["feature", "label"], data_key=self.valid_key if seg == "valid" else DataHandlerLP.DK_L
+                )
+                all_df["x"][seg] = df["feature"]
+                all_df["y"][seg] = df["label"].copy()  # We have to use copy to remove the reference to release mem
+                if reweighter is None:
+                    all_df["w"][seg] = pd.DataFrame(np.ones_like(all_df["y"][seg].values), index=df.index)
+                elif isinstance(reweighter, Reweighter):
+                    all_df["w"][seg] = pd.DataFrame(reweighter.reweight(df))
+                else:
+                    raise ValueError("Unsupported reweighter type.")
+
+                # get tensors
+                for v in vars:
+                    all_t[v][seg] = torch.from_numpy(all_df[v][seg].values).float()
+                    # if seg == "valid": # accelerate the eval of validation
+                    all_t[v][seg] = all_t[v][seg].to(self.device)  # This will consume a lot of memory !!!!
+
+                evals_result[seg] = []
+                # free memory
+                del df
+                del all_df["x"]
+                gc.collect()
+
+        save_path = get_or_create_path(save_path)
+        stop_steps = 0
+        train_loss = 0
+        best_loss = np.inf
+        # train
+        self.logger.info("training...")
+        self.fitted = True
+        # return
+        # prepare training data
+        train_num = all_t["y"]["train"].shape[0]
+
+        for step in range(1, self.max_steps + 1):
+            if stop_steps >= self.early_stop_rounds:
+                if verbose:
+                    self.logger.info("\tearly stop")
+                break
+            loss = AverageMeter()
+            self.dnn_model.train()
+            self.train_optimizer.zero_grad()
+            choice = np.random.choice(train_num, self.batch_size)
+            x_batch_auto = all_t["x"]["train"][choice].to(self.device)
+            y_batch_auto = all_t["y"]["train"][choice].to(self.device)
+            w_batch_auto = all_t["w"]["train"][choice].to(self.device)
+
+            # forward
+            preds = self.dnn_model(x_batch_auto)
+            cur_loss = self.get_loss(preds, w_batch_auto, y_batch_auto, self.loss_type)
+            cur_loss.backward()
+            self.train_optimizer.step()
+            loss.update(cur_loss.item())
+            R.log_metrics(train_loss=loss.avg, step=step)
+
+            # validation
+            train_loss += loss.val
+            # for evert `eval_steps` steps or at the last steps, we will evaluate the model.
+            if step % self.eval_steps == 0 or step == self.max_steps:
+                if has_valid:
+                    stop_steps += 1
+                    train_loss /= self.eval_steps
+
+                    with torch.no_grad():
+                        self.dnn_model.eval()
+
+                        # forward
+                        preds = self._nn_predict(all_t["x"]["valid"], return_cpu=False)
+                        cur_loss_val = self.get_loss(preds, all_t["w"]["valid"], all_t["y"]["valid"], self.loss_type)
+                        loss_val = cur_loss_val.item()
+                        metric_val = (
+                            self.get_metric(
+                                preds.reshape(-1), all_t["y"]["valid"].reshape(-1), all_df["y"]["valid"].index
+                            )
+                            .detach()
+                            .cpu()
+                            .numpy()
+                            .item()
+                        )
+                        R.log_metrics(val_loss=loss_val, step=step)
+                        R.log_metrics(val_metric=metric_val, step=step)
+
+                        if self.eval_train_metric:
+                            metric_train = (
+                                self.get_metric(
+                                    self._nn_predict(all_t["x"]["train"], return_cpu=False),
+                                    all_t["y"]["train"].reshape(-1),
+                                    all_df["y"]["train"].index,
+                                )
+                                .detach()
+                                .cpu()
+                                .numpy()
+                                .item()
+                            )
+                            R.log_metrics(train_metric=metric_train, step=step)
+                        else:
+                            metric_train = np.nan
+                    if verbose:
+                        self.logger.info(
+                            f"[Step {step}]: train_loss {train_loss:.6f}, valid_loss {loss_val:.6f}, train_metric {metric_train:.6f}, valid_metric {metric_val:.6f}"
+                        )
+                    evals_result["train"].append(train_loss)
+                    evals_result["valid"].append(loss_val)
+                    if loss_val < best_loss:
+                        if verbose:
+                            self.logger.info(
+                                "\tvalid loss update from {:.6f} to {:.6f}, save checkpoint.".format(
+                                    best_loss, loss_val
+                                )
+                            )
+                        best_loss = loss_val
+                        self.best_step = step
+                        R.log_metrics(best_step=self.best_step, step=step)
+                        stop_steps = 0
+                        torch.save(self.dnn_model.state_dict(), save_path)
+                    train_loss = 0
+                    # update learning rate
+                    if self.scheduler is not None:
+                        auto_filter_kwargs(self.scheduler.step, warning=False)(metrics=cur_loss_val, epoch=step)
+                    R.log_metrics(lr=self.get_lr(), step=step)
+                else:
+                    # retraining mode
+                    if self.scheduler is not None:
+                        self.scheduler.step(epoch=step)
+
+        if has_valid:
+            # restore the optimal parameters after training
+            self.dnn_model.load_state_dict(torch.load(save_path, map_location=self.device))
+        if self.use_gpu:
+            torch.cuda.empty_cache()
+
+    def get_lr(self):
+        assert len(self.train_optimizer.param_groups) == 1
+        return self.train_optimizer.param_groups[0]["lr"]
+
+    def get_loss(self, pred, w, target, loss_type):
+        pred, w, target = pred.reshape(-1), w.reshape(-1), target.reshape(-1)
+        if loss_type == "mse":
+            sqr_loss = torch.mul(pred - target, pred - target)
+            loss = torch.mul(sqr_loss, w).mean()
+            return loss
+        elif loss_type == "binary":
+            loss = nn.BCEWithLogitsLoss(weight=w)
+            return loss(pred, target)
+        else:
+            raise NotImplementedError("loss {} is not supported!".format(loss_type))
+
+    def get_metric(self, pred, target, index):
+        # NOTE: the order of the index must follow <datetime, instrument> sorted order
+        return -ICLoss()(pred, target, index)  # pylint: disable=E1130
+
+    def _nn_predict(self, data, return_cpu=True):
+        """Reusing predicting NN.
+        Scenarios
+        1) test inference (data may come from CPU and expect the output data is on CPU)
+        2) evaluation on training (data may come from GPU)
+        """
+        if not isinstance(data, torch.Tensor):
+            if isinstance(data, pd.DataFrame):
+                data = data.values
+            data = torch.Tensor(data)
+        data = data.to(self.device)
+        preds = []
+        self.dnn_model.eval()
+        with torch.no_grad():
+            batch_size = 8096
+            for i in range(0, len(data), batch_size):
+                x = data[i : i + batch_size]
+                preds.append(self.dnn_model(x.to(self.device)).detach().reshape(-1))
+        if return_cpu:
+            preds = np.concatenate([pr.cpu().numpy() for pr in preds])
+        else:
+            preds = torch.cat(preds, axis=0)
+        return preds
+
+    def predict(self, dataset: DatasetH, segment: Union[Text, slice] = "test"):
+        if not self.fitted:
+            raise ValueError("model is not fitted yet!")
+        x_test_pd = dataset.prepare(segment, col_set="feature", data_key=DataHandlerLP.DK_I)
+        preds = self._nn_predict(x_test_pd)
+        return pd.Series(preds.reshape(-1), index=x_test_pd.index)
+
+    def save(self, filename, **kwargs):
+        with save_multiple_parts_file(filename) as model_dir:
+            model_path = os.path.join(model_dir, os.path.split(model_dir)[-1])
+            # Save model
+            torch.save(self.dnn_model.state_dict(), model_path)
+
+    def load(self, buffer, **kwargs):
+        with unpack_archive_with_buffer(buffer) as model_dir:
+            # Get model name
+            _model_name = os.path.splitext(list(filter(lambda x: x.startswith("model.bin"), os.listdir(model_dir)))[0])[
+                0
+            ]
+            _model_path = os.path.join(model_dir, _model_name)
+            # Load model
+            self.dnn_model.load_state_dict(torch.load(_model_path, map_location=self.device))
+        self.fitted = True
+
+
+class AverageMeter:
+    """Computes and stores the average and current value"""
+
+    def __init__(self):
+        self.reset()
+
+    def reset(self):
+        self.val = 0
+        self.avg = 0
+        self.sum = 0
+        self.count = 0
+
+    def update(self, val, n=1):
+        self.val = val
+        self.sum += val * n
+        self.count += n
+        self.avg = self.sum / self.count
+
+
+class Net(nn.Module):
+    def __init__(self, input_dim, output_dim=1, layers=(256,), act="LeakyReLU"):
+        super(Net, self).__init__()
+
+        layers = [input_dim] + list(layers)
+        dnn_layers = []
+        drop_input = nn.Dropout(0.05)
+        dnn_layers.append(drop_input)
+        hidden_units = input_dim
+        for i, (_input_dim, hidden_units) in enumerate(zip(layers[:-1], layers[1:])):
+            fc = nn.Linear(_input_dim, hidden_units)
+            if act == "LeakyReLU":
+                activation = nn.LeakyReLU(negative_slope=0.1, inplace=False)
+            elif act == "SiLU":
+                activation = nn.SiLU()
+            else:
+                raise NotImplementedError(f"This type of input is not supported")
+            bn = nn.BatchNorm1d(hidden_units)
+            seq = nn.Sequential(fc, bn, activation)
+            dnn_layers.append(seq)
+        drop_input = nn.Dropout(0.05)
+        dnn_layers.append(drop_input)
+        fc = nn.Linear(hidden_units, output_dim)
+        dnn_layers.append(fc)
+        # optimizer  # pylint: disable=W0631
+        self.dnn_layers = nn.ModuleList(dnn_layers)
+        self._weight_init()
+
+    def _weight_init(self):
+        for m in self.modules():
+            if isinstance(m, nn.Linear):
+                nn.init.kaiming_normal_(m.weight, a=0.1, mode="fan_in", nonlinearity="leaky_relu")
+
+    def forward(self, x):
+        cur_output = x
+        for i, now_layer in enumerate(self.dnn_layers):
+            cur_output = now_layer(cur_output)
+        return cur_output
```

## qlib/contrib/model/pytorch_sandwich.py

 * *Ordering differences only*

```diff
@@ -1,381 +1,381 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-
-from __future__ import division
-from __future__ import print_function
-
-import numpy as np
-import pandas as pd
-from typing import Text, Union
-import copy
-from ...utils import get_or_create_path
-from ...log import get_module_logger
-
-import torch
-import torch.nn as nn
-import torch.optim as optim
-
-from ...model.base import Model
-from ...data.dataset import DatasetH
-from ...data.dataset.handler import DataHandlerLP
-from .pytorch_krnn import CNNKRNNEncoder
-
-
-class SandwichModel(nn.Module):
-    def __init__(
-        self,
-        fea_dim,
-        cnn_dim_1,
-        cnn_dim_2,
-        cnn_kernel_size,
-        rnn_dim_1,
-        rnn_dim_2,
-        rnn_dups,
-        rnn_layers,
-        dropout,
-        device,
-        **params
-    ):
-        """Build a Sandwich model
-
-        Parameters
-        ----------
-        fea_dim : int
-            The feature dimension
-        cnn_dim_1 : int
-            The hidden dimension of the first CNN
-        cnn_dim_2 : int
-            The hidden dimension of the second CNN
-        cnn_kernel_size : int
-            The size of convolutional kernels
-        rnn_dim_1 : int
-            The hidden dimension of the first KRNN
-        rnn_dim_2 : int
-            The hidden dimension of the second KRNN
-        rnn_dups : int
-            The number of parallel duplicates
-        rnn_layers: int
-            The number of RNN layers
-        """
-        super().__init__()
-
-        self.first_encoder = CNNKRNNEncoder(
-            cnn_input_dim=fea_dim,
-            cnn_output_dim=cnn_dim_1,
-            cnn_kernel_size=cnn_kernel_size,
-            rnn_output_dim=rnn_dim_1,
-            rnn_dup_num=rnn_dups,
-            rnn_layers=rnn_layers,
-            dropout=dropout,
-            device=device,
-        )
-
-        self.second_encoder = CNNKRNNEncoder(
-            cnn_input_dim=rnn_dim_1,
-            cnn_output_dim=cnn_dim_2,
-            cnn_kernel_size=cnn_kernel_size,
-            rnn_output_dim=rnn_dim_2,
-            rnn_dup_num=rnn_dups,
-            rnn_layers=rnn_layers,
-            dropout=dropout,
-            device=device,
-        )
-
-        self.out_fc = nn.Linear(rnn_dim_2, 1)
-        self.device = device
-
-    def forward(self, x):
-        # x: [batch_size, node_num, seq_len, input_dim]
-        encode = self.first_encoder(x)
-        encode = self.second_encoder(encode)
-        out = self.out_fc(encode[:, -1, :]).squeeze().to(self.device)
-
-        return out
-
-
-class Sandwich(Model):
-    """Sandwich Model
-
-    Parameters
-    ----------
-    d_feat : int
-        input dimension for each time step
-    metric: str
-        the evaluation metric used in early stop
-    optimizer : str
-        optimizer name
-    GPU : str
-        the GPU ID(s) used for training
-    """
-
-    def __init__(
-        self,
-        fea_dim=6,
-        cnn_dim_1=64,
-        cnn_dim_2=32,
-        cnn_kernel_size=3,
-        rnn_dim_1=16,
-        rnn_dim_2=8,
-        rnn_dups=3,
-        rnn_layers=2,
-        dropout=0,
-        n_epochs=200,
-        lr=0.001,
-        metric="",
-        batch_size=2000,
-        early_stop=20,
-        loss="mse",
-        optimizer="adam",
-        GPU=0,
-        seed=None,
-        **kwargs
-    ):
-        # Set logger.
-        self.logger = get_module_logger("Sandwich")
-        self.logger.info("Sandwich pytorch version...")
-
-        # set hyper-parameters.
-        self.fea_dim = fea_dim
-        self.cnn_dim_1 = cnn_dim_1
-        self.cnn_dim_2 = cnn_dim_2
-        self.cnn_kernel_size = cnn_kernel_size
-        self.rnn_dim_1 = rnn_dim_1
-        self.rnn_dim_2 = rnn_dim_2
-        self.rnn_dups = rnn_dups
-        self.rnn_layers = rnn_layers
-        self.dropout = dropout
-        self.n_epochs = n_epochs
-        self.lr = lr
-        self.metric = metric
-        self.batch_size = batch_size
-        self.early_stop = early_stop
-        self.optimizer = optimizer.lower()
-        self.loss = loss
-        self.device = torch.device("cuda:%d" % (GPU) if torch.cuda.is_available() and GPU >= 0 else "cpu")
-        self.seed = seed
-
-        self.logger.info(
-            "Sandwich parameters setting:"
-            "\nfea_dim : {}"
-            "\ncnn_dim_1 : {}"
-            "\ncnn_dim_2 : {}"
-            "\ncnn_kernel_size : {}"
-            "\nrnn_dim_1 : {}"
-            "\nrnn_dim_2 : {}"
-            "\nrnn_dups : {}"
-            "\nrnn_layers : {}"
-            "\ndropout : {}"
-            "\nn_epochs : {}"
-            "\nlr : {}"
-            "\nmetric : {}"
-            "\nbatch_size: {}"
-            "\nearly_stop : {}"
-            "\noptimizer : {}"
-            "\nloss_type : {}"
-            "\nvisible_GPU : {}"
-            "\nuse_GPU : {}"
-            "\nseed : {}".format(
-                fea_dim,
-                cnn_dim_1,
-                cnn_dim_2,
-                cnn_kernel_size,
-                rnn_dim_1,
-                rnn_dim_2,
-                rnn_dups,
-                rnn_layers,
-                dropout,
-                n_epochs,
-                lr,
-                metric,
-                batch_size,
-                early_stop,
-                optimizer.lower(),
-                loss,
-                GPU,
-                self.use_gpu,
-                seed,
-            )
-        )
-
-        if self.seed is not None:
-            np.random.seed(self.seed)
-            torch.manual_seed(self.seed)
-
-        self.sandwich_model = SandwichModel(
-            fea_dim=self.fea_dim,
-            cnn_dim_1=self.cnn_dim_1,
-            cnn_dim_2=self.cnn_dim_2,
-            cnn_kernel_size=self.cnn_kernel_size,
-            rnn_dim_1=self.rnn_dim_1,
-            rnn_dim_2=self.rnn_dim_2,
-            rnn_dups=self.rnn_dups,
-            rnn_layers=self.rnn_layers,
-            dropout=self.dropout,
-            device=self.device,
-        )
-        if optimizer.lower() == "adam":
-            self.train_optimizer = optim.Adam(self.sandwich_model.parameters(), lr=self.lr)
-        elif optimizer.lower() == "gd":
-            self.train_optimizer = optim.SGD(self.sandwich_model.parameters(), lr=self.lr)
-        else:
-            raise NotImplementedError("optimizer {} is not supported!".format(optimizer))
-
-        self.fitted = False
-        self.sandwich_model.to(self.device)
-
-    @property
-    def use_gpu(self):
-        return self.device != torch.device("cpu")
-
-    def mse(self, pred, label):
-        loss = (pred - label) ** 2
-        return torch.mean(loss)
-
-    def loss_fn(self, pred, label):
-        mask = ~torch.isnan(label)
-
-        if self.loss == "mse":
-            return self.mse(pred[mask], label[mask])
-
-        raise ValueError("unknown loss `%s`" % self.loss)
-
-    def metric_fn(self, pred, label):
-        mask = torch.isfinite(label)
-
-        if self.metric in ("", "loss"):
-            return -self.loss_fn(pred[mask], label[mask])
-
-        raise ValueError("unknown metric `%s`" % self.metric)
-
-    def train_epoch(self, x_train, y_train):
-        x_train_values = x_train.values
-        y_train_values = np.squeeze(y_train.values)
-        self.sandwich_model.train()
-
-        indices = np.arange(len(x_train_values))
-        np.random.shuffle(indices)
-
-        for i in range(len(indices))[:: self.batch_size]:
-            if len(indices) - i < self.batch_size:
-                break
-
-            feature = torch.from_numpy(x_train_values[indices[i : i + self.batch_size]]).float().to(self.device)
-            label = torch.from_numpy(y_train_values[indices[i : i + self.batch_size]]).float().to(self.device)
-
-            pred = self.sandwich_model(feature)
-            loss = self.loss_fn(pred, label)
-
-            self.train_optimizer.zero_grad()
-            loss.backward()
-            torch.nn.utils.clip_grad_value_(self.sandwich_model.parameters(), 3.0)
-            self.train_optimizer.step()
-
-    def test_epoch(self, data_x, data_y):
-        # prepare training data
-        x_values = data_x.values
-        y_values = np.squeeze(data_y.values)
-
-        self.sandwich_model.eval()
-
-        scores = []
-        losses = []
-
-        indices = np.arange(len(x_values))
-
-        for i in range(len(indices))[:: self.batch_size]:
-            if len(indices) - i < self.batch_size:
-                break
-
-            feature = torch.from_numpy(x_values[indices[i : i + self.batch_size]]).float().to(self.device)
-            label = torch.from_numpy(y_values[indices[i : i + self.batch_size]]).float().to(self.device)
-
-            pred = self.sandwich_model(feature)
-            loss = self.loss_fn(pred, label)
-            losses.append(loss.item())
-
-            score = self.metric_fn(pred, label)
-            scores.append(score.item())
-
-        return np.mean(losses), np.mean(scores)
-
-    def fit(
-        self,
-        dataset: DatasetH,
-        evals_result=dict(),
-        save_path=None,
-    ):
-        df_train, df_valid, df_test = dataset.prepare(
-            ["train", "valid", "test"],
-            col_set=["feature", "label"],
-            data_key=DataHandlerLP.DK_L,
-        )
-        if df_train.empty or df_valid.empty:
-            raise ValueError("Empty data from dataset, please check your dataset config.")
-
-        x_train, y_train = df_train["feature"], df_train["label"]
-        x_valid, y_valid = df_valid["feature"], df_valid["label"]
-
-        save_path = get_or_create_path(save_path)
-        stop_steps = 0
-        train_loss = 0
-        best_score = -np.inf
-        best_epoch = 0
-        evals_result["train"] = []
-        evals_result["valid"] = []
-
-        # train
-        self.logger.info("training...")
-        self.fitted = True
-
-        for step in range(self.n_epochs):
-            self.logger.info("Epoch%d:", step)
-            self.logger.info("training...")
-            self.train_epoch(x_train, y_train)
-            self.logger.info("evaluating...")
-            train_loss, train_score = self.test_epoch(x_train, y_train)
-            val_loss, val_score = self.test_epoch(x_valid, y_valid)
-            self.logger.info("train %.6f, valid %.6f" % (train_score, val_score))
-            evals_result["train"].append(train_score)
-            evals_result["valid"].append(val_score)
-
-            if val_score > best_score:
-                best_score = val_score
-                stop_steps = 0
-                best_epoch = step
-                best_param = copy.deepcopy(self.sandwich_model.state_dict())
-            else:
-                stop_steps += 1
-                if stop_steps >= self.early_stop:
-                    self.logger.info("early stop")
-                    break
-
-        self.logger.info("best score: %.6lf @ %d" % (best_score, best_epoch))
-        self.sandwich_model.load_state_dict(best_param)
-        torch.save(best_param, save_path)
-
-        if self.use_gpu:
-            torch.cuda.empty_cache()
-
-    def predict(self, dataset: DatasetH, segment: Union[Text, slice] = "test"):
-        if not self.fitted:
-            raise ValueError("model is not fitted yet!")
-
-        x_test = dataset.prepare(segment, col_set="feature", data_key=DataHandlerLP.DK_I)
-        index = x_test.index
-        self.sandwich_model.eval()
-        x_values = x_test.values
-        sample_num = x_values.shape[0]
-        preds = []
-
-        for begin in range(sample_num)[:: self.batch_size]:
-            if sample_num - begin < self.batch_size:
-                end = sample_num
-            else:
-                end = begin + self.batch_size
-            x_batch = torch.from_numpy(x_values[begin:end]).float().to(self.device)
-            with torch.no_grad():
-                pred = self.sandwich_model(x_batch).detach().cpu().numpy()
-            preds.append(pred)
-
-        return pd.Series(np.concatenate(preds), index=index)
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+
+from __future__ import division
+from __future__ import print_function
+
+import numpy as np
+import pandas as pd
+from typing import Text, Union
+import copy
+from ...utils import get_or_create_path
+from ...log import get_module_logger
+
+import torch
+import torch.nn as nn
+import torch.optim as optim
+
+from ...model.base import Model
+from ...data.dataset import DatasetH
+from ...data.dataset.handler import DataHandlerLP
+from .pytorch_krnn import CNNKRNNEncoder
+
+
+class SandwichModel(nn.Module):
+    def __init__(
+        self,
+        fea_dim,
+        cnn_dim_1,
+        cnn_dim_2,
+        cnn_kernel_size,
+        rnn_dim_1,
+        rnn_dim_2,
+        rnn_dups,
+        rnn_layers,
+        dropout,
+        device,
+        **params
+    ):
+        """Build a Sandwich model
+
+        Parameters
+        ----------
+        fea_dim : int
+            The feature dimension
+        cnn_dim_1 : int
+            The hidden dimension of the first CNN
+        cnn_dim_2 : int
+            The hidden dimension of the second CNN
+        cnn_kernel_size : int
+            The size of convolutional kernels
+        rnn_dim_1 : int
+            The hidden dimension of the first KRNN
+        rnn_dim_2 : int
+            The hidden dimension of the second KRNN
+        rnn_dups : int
+            The number of parallel duplicates
+        rnn_layers: int
+            The number of RNN layers
+        """
+        super().__init__()
+
+        self.first_encoder = CNNKRNNEncoder(
+            cnn_input_dim=fea_dim,
+            cnn_output_dim=cnn_dim_1,
+            cnn_kernel_size=cnn_kernel_size,
+            rnn_output_dim=rnn_dim_1,
+            rnn_dup_num=rnn_dups,
+            rnn_layers=rnn_layers,
+            dropout=dropout,
+            device=device,
+        )
+
+        self.second_encoder = CNNKRNNEncoder(
+            cnn_input_dim=rnn_dim_1,
+            cnn_output_dim=cnn_dim_2,
+            cnn_kernel_size=cnn_kernel_size,
+            rnn_output_dim=rnn_dim_2,
+            rnn_dup_num=rnn_dups,
+            rnn_layers=rnn_layers,
+            dropout=dropout,
+            device=device,
+        )
+
+        self.out_fc = nn.Linear(rnn_dim_2, 1)
+        self.device = device
+
+    def forward(self, x):
+        # x: [batch_size, node_num, seq_len, input_dim]
+        encode = self.first_encoder(x)
+        encode = self.second_encoder(encode)
+        out = self.out_fc(encode[:, -1, :]).squeeze().to(self.device)
+
+        return out
+
+
+class Sandwich(Model):
+    """Sandwich Model
+
+    Parameters
+    ----------
+    d_feat : int
+        input dimension for each time step
+    metric: str
+        the evaluation metric used in early stop
+    optimizer : str
+        optimizer name
+    GPU : str
+        the GPU ID(s) used for training
+    """
+
+    def __init__(
+        self,
+        fea_dim=6,
+        cnn_dim_1=64,
+        cnn_dim_2=32,
+        cnn_kernel_size=3,
+        rnn_dim_1=16,
+        rnn_dim_2=8,
+        rnn_dups=3,
+        rnn_layers=2,
+        dropout=0,
+        n_epochs=200,
+        lr=0.001,
+        metric="",
+        batch_size=2000,
+        early_stop=20,
+        loss="mse",
+        optimizer="adam",
+        GPU=0,
+        seed=None,
+        **kwargs
+    ):
+        # Set logger.
+        self.logger = get_module_logger("Sandwich")
+        self.logger.info("Sandwich pytorch version...")
+
+        # set hyper-parameters.
+        self.fea_dim = fea_dim
+        self.cnn_dim_1 = cnn_dim_1
+        self.cnn_dim_2 = cnn_dim_2
+        self.cnn_kernel_size = cnn_kernel_size
+        self.rnn_dim_1 = rnn_dim_1
+        self.rnn_dim_2 = rnn_dim_2
+        self.rnn_dups = rnn_dups
+        self.rnn_layers = rnn_layers
+        self.dropout = dropout
+        self.n_epochs = n_epochs
+        self.lr = lr
+        self.metric = metric
+        self.batch_size = batch_size
+        self.early_stop = early_stop
+        self.optimizer = optimizer.lower()
+        self.loss = loss
+        self.device = torch.device("cuda:%d" % (GPU) if torch.cuda.is_available() and GPU >= 0 else "cpu")
+        self.seed = seed
+
+        self.logger.info(
+            "Sandwich parameters setting:"
+            "\nfea_dim : {}"
+            "\ncnn_dim_1 : {}"
+            "\ncnn_dim_2 : {}"
+            "\ncnn_kernel_size : {}"
+            "\nrnn_dim_1 : {}"
+            "\nrnn_dim_2 : {}"
+            "\nrnn_dups : {}"
+            "\nrnn_layers : {}"
+            "\ndropout : {}"
+            "\nn_epochs : {}"
+            "\nlr : {}"
+            "\nmetric : {}"
+            "\nbatch_size: {}"
+            "\nearly_stop : {}"
+            "\noptimizer : {}"
+            "\nloss_type : {}"
+            "\nvisible_GPU : {}"
+            "\nuse_GPU : {}"
+            "\nseed : {}".format(
+                fea_dim,
+                cnn_dim_1,
+                cnn_dim_2,
+                cnn_kernel_size,
+                rnn_dim_1,
+                rnn_dim_2,
+                rnn_dups,
+                rnn_layers,
+                dropout,
+                n_epochs,
+                lr,
+                metric,
+                batch_size,
+                early_stop,
+                optimizer.lower(),
+                loss,
+                GPU,
+                self.use_gpu,
+                seed,
+            )
+        )
+
+        if self.seed is not None:
+            np.random.seed(self.seed)
+            torch.manual_seed(self.seed)
+
+        self.sandwich_model = SandwichModel(
+            fea_dim=self.fea_dim,
+            cnn_dim_1=self.cnn_dim_1,
+            cnn_dim_2=self.cnn_dim_2,
+            cnn_kernel_size=self.cnn_kernel_size,
+            rnn_dim_1=self.rnn_dim_1,
+            rnn_dim_2=self.rnn_dim_2,
+            rnn_dups=self.rnn_dups,
+            rnn_layers=self.rnn_layers,
+            dropout=self.dropout,
+            device=self.device,
+        )
+        if optimizer.lower() == "adam":
+            self.train_optimizer = optim.Adam(self.sandwich_model.parameters(), lr=self.lr)
+        elif optimizer.lower() == "gd":
+            self.train_optimizer = optim.SGD(self.sandwich_model.parameters(), lr=self.lr)
+        else:
+            raise NotImplementedError("optimizer {} is not supported!".format(optimizer))
+
+        self.fitted = False
+        self.sandwich_model.to(self.device)
+
+    @property
+    def use_gpu(self):
+        return self.device != torch.device("cpu")
+
+    def mse(self, pred, label):
+        loss = (pred - label) ** 2
+        return torch.mean(loss)
+
+    def loss_fn(self, pred, label):
+        mask = ~torch.isnan(label)
+
+        if self.loss == "mse":
+            return self.mse(pred[mask], label[mask])
+
+        raise ValueError("unknown loss `%s`" % self.loss)
+
+    def metric_fn(self, pred, label):
+        mask = torch.isfinite(label)
+
+        if self.metric in ("", "loss"):
+            return -self.loss_fn(pred[mask], label[mask])
+
+        raise ValueError("unknown metric `%s`" % self.metric)
+
+    def train_epoch(self, x_train, y_train):
+        x_train_values = x_train.values
+        y_train_values = np.squeeze(y_train.values)
+        self.sandwich_model.train()
+
+        indices = np.arange(len(x_train_values))
+        np.random.shuffle(indices)
+
+        for i in range(len(indices))[:: self.batch_size]:
+            if len(indices) - i < self.batch_size:
+                break
+
+            feature = torch.from_numpy(x_train_values[indices[i : i + self.batch_size]]).float().to(self.device)
+            label = torch.from_numpy(y_train_values[indices[i : i + self.batch_size]]).float().to(self.device)
+
+            pred = self.sandwich_model(feature)
+            loss = self.loss_fn(pred, label)
+
+            self.train_optimizer.zero_grad()
+            loss.backward()
+            torch.nn.utils.clip_grad_value_(self.sandwich_model.parameters(), 3.0)
+            self.train_optimizer.step()
+
+    def test_epoch(self, data_x, data_y):
+        # prepare training data
+        x_values = data_x.values
+        y_values = np.squeeze(data_y.values)
+
+        self.sandwich_model.eval()
+
+        scores = []
+        losses = []
+
+        indices = np.arange(len(x_values))
+
+        for i in range(len(indices))[:: self.batch_size]:
+            if len(indices) - i < self.batch_size:
+                break
+
+            feature = torch.from_numpy(x_values[indices[i : i + self.batch_size]]).float().to(self.device)
+            label = torch.from_numpy(y_values[indices[i : i + self.batch_size]]).float().to(self.device)
+
+            pred = self.sandwich_model(feature)
+            loss = self.loss_fn(pred, label)
+            losses.append(loss.item())
+
+            score = self.metric_fn(pred, label)
+            scores.append(score.item())
+
+        return np.mean(losses), np.mean(scores)
+
+    def fit(
+        self,
+        dataset: DatasetH,
+        evals_result=dict(),
+        save_path=None,
+    ):
+        df_train, df_valid, df_test = dataset.prepare(
+            ["train", "valid", "test"],
+            col_set=["feature", "label"],
+            data_key=DataHandlerLP.DK_L,
+        )
+        if df_train.empty or df_valid.empty:
+            raise ValueError("Empty data from dataset, please check your dataset config.")
+
+        x_train, y_train = df_train["feature"], df_train["label"]
+        x_valid, y_valid = df_valid["feature"], df_valid["label"]
+
+        save_path = get_or_create_path(save_path)
+        stop_steps = 0
+        train_loss = 0
+        best_score = -np.inf
+        best_epoch = 0
+        evals_result["train"] = []
+        evals_result["valid"] = []
+
+        # train
+        self.logger.info("training...")
+        self.fitted = True
+
+        for step in range(self.n_epochs):
+            self.logger.info("Epoch%d:", step)
+            self.logger.info("training...")
+            self.train_epoch(x_train, y_train)
+            self.logger.info("evaluating...")
+            train_loss, train_score = self.test_epoch(x_train, y_train)
+            val_loss, val_score = self.test_epoch(x_valid, y_valid)
+            self.logger.info("train %.6f, valid %.6f" % (train_score, val_score))
+            evals_result["train"].append(train_score)
+            evals_result["valid"].append(val_score)
+
+            if val_score > best_score:
+                best_score = val_score
+                stop_steps = 0
+                best_epoch = step
+                best_param = copy.deepcopy(self.sandwich_model.state_dict())
+            else:
+                stop_steps += 1
+                if stop_steps >= self.early_stop:
+                    self.logger.info("early stop")
+                    break
+
+        self.logger.info("best score: %.6lf @ %d" % (best_score, best_epoch))
+        self.sandwich_model.load_state_dict(best_param)
+        torch.save(best_param, save_path)
+
+        if self.use_gpu:
+            torch.cuda.empty_cache()
+
+    def predict(self, dataset: DatasetH, segment: Union[Text, slice] = "test"):
+        if not self.fitted:
+            raise ValueError("model is not fitted yet!")
+
+        x_test = dataset.prepare(segment, col_set="feature", data_key=DataHandlerLP.DK_I)
+        index = x_test.index
+        self.sandwich_model.eval()
+        x_values = x_test.values
+        sample_num = x_values.shape[0]
+        preds = []
+
+        for begin in range(sample_num)[:: self.batch_size]:
+            if sample_num - begin < self.batch_size:
+                end = sample_num
+            else:
+                end = begin + self.batch_size
+            x_batch = torch.from_numpy(x_values[begin:end]).float().to(self.device)
+            with torch.no_grad():
+                pred = self.sandwich_model(x_batch).detach().cpu().numpy()
+            preds.append(pred)
+
+        return pd.Series(np.concatenate(preds), index=index)
```

## qlib/contrib/model/pytorch_sfm.py

```diff
@@ -1,485 +1,479 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-from __future__ import division
-from __future__ import print_function
-
-import numpy as np
-import pandas as pd
-from typing import Text, Union
-import copy
-from ...utils import get_or_create_path
-from ...log import get_module_logger
-
-import torch
-import torch.nn as nn
-import torch.nn.init as init
-import torch.optim as optim
-
-from .pytorch_utils import count_parameters
-from ...model.base import Model
-from ...data.dataset import DatasetH
-from ...data.dataset.handler import DataHandlerLP
-
-
-class SFM_Model(nn.Module):
-    def __init__(
-        self,
-        d_feat=6,
-        output_dim=1,
-        freq_dim=10,
-        hidden_size=64,
-        dropout_W=0.0,
-        dropout_U=0.0,
-        device="cpu",
-    ):
-        super().__init__()
-
-        self.input_dim = d_feat
-        self.output_dim = output_dim
-        self.freq_dim = freq_dim
-        self.hidden_dim = hidden_size
-        self.device = device
-
-        self.W_i = nn.Parameter(init.xavier_uniform_(torch.empty((self.input_dim, self.hidden_dim))))
-        self.U_i = nn.Parameter(init.orthogonal_(torch.empty(self.hidden_dim, self.hidden_dim)))
-        self.b_i = nn.Parameter(torch.zeros(self.hidden_dim))
-
-        self.W_ste = nn.Parameter(init.xavier_uniform_(torch.empty(self.input_dim, self.hidden_dim)))
-        self.U_ste = nn.Parameter(init.orthogonal_(torch.empty(self.hidden_dim, self.hidden_dim)))
-        self.b_ste = nn.Parameter(torch.ones(self.hidden_dim))
-
-        self.W_fre = nn.Parameter(init.xavier_uniform_(torch.empty(self.input_dim, self.freq_dim)))
-        self.U_fre = nn.Parameter(init.orthogonal_(torch.empty(self.hidden_dim, self.freq_dim)))
-        self.b_fre = nn.Parameter(torch.ones(self.freq_dim))
-
-        self.W_c = nn.Parameter(init.xavier_uniform_(torch.empty(self.input_dim, self.hidden_dim)))
-        self.U_c = nn.Parameter(init.orthogonal_(torch.empty(self.hidden_dim, self.hidden_dim)))
-        self.b_c = nn.Parameter(torch.zeros(self.hidden_dim))
-
-        self.W_o = nn.Parameter(init.xavier_uniform_(torch.empty(self.input_dim, self.hidden_dim)))
-        self.U_o = nn.Parameter(init.orthogonal_(torch.empty(self.hidden_dim, self.hidden_dim)))
-        self.b_o = nn.Parameter(torch.zeros(self.hidden_dim))
-
-        self.U_a = nn.Parameter(init.orthogonal_(torch.empty(self.freq_dim, 1)))
-        self.b_a = nn.Parameter(torch.zeros(self.hidden_dim))
-
-        self.W_p = nn.Parameter(init.xavier_uniform_(torch.empty(self.hidden_dim, self.output_dim)))
-        self.b_p = nn.Parameter(torch.zeros(self.output_dim))
-
-        self.activation = nn.Tanh()
-        self.inner_activation = nn.Hardsigmoid()
-        self.dropout_W, self.dropout_U = (dropout_W, dropout_U)
-        self.fc_out = nn.Linear(self.output_dim, 1)
-
-        self.states = []
-
-    def forward(self, input):
-        input = input.reshape(len(input), self.input_dim, -1)  # [N, F, T]
-        input = input.permute(0, 2, 1)  # [N, T, F]
-        time_step = input.shape[1]
-
-        for ts in range(time_step):
-            x = input[:, ts, :]
-            if len(self.states) == 0:  # hasn't initialized yet
-                self.init_states(x)
-            self.get_constants(x)
-            p_tm1 = self.states[0]  # noqa: F841
-            h_tm1 = self.states[1]
-            S_re_tm1 = self.states[2]
-            S_im_tm1 = self.states[3]
-            time_tm1 = self.states[4]
-            B_U = self.states[5]
-            B_W = self.states[6]
-            frequency = self.states[7]
-
-            x_i = torch.matmul(x * B_W[0], self.W_i) + self.b_i
-            x_ste = torch.matmul(x * B_W[0], self.W_ste) + self.b_ste
-            x_fre = torch.matmul(x * B_W[0], self.W_fre) + self.b_fre
-            x_c = torch.matmul(x * B_W[0], self.W_c) + self.b_c
-            x_o = torch.matmul(x * B_W[0], self.W_o) + self.b_o
-
-            i = self.inner_activation(x_i + torch.matmul(h_tm1 * B_U[0], self.U_i))
-            ste = self.inner_activation(x_ste + torch.matmul(h_tm1 * B_U[0], self.U_ste))
-            fre = self.inner_activation(x_fre + torch.matmul(h_tm1 * B_U[0], self.U_fre))
-
-            ste = torch.reshape(ste, (-1, self.hidden_dim, 1))
-            fre = torch.reshape(fre, (-1, 1, self.freq_dim))
-
-            f = ste * fre
-
-            c = i * self.activation(x_c + torch.matmul(h_tm1 * B_U[0], self.U_c))
-
-            time = time_tm1 + 1
-
-            omega = torch.tensor(2 * np.pi) * time * frequency
-
-            re = torch.cos(omega)
-            im = torch.sin(omega)
-
-            c = torch.reshape(c, (-1, self.hidden_dim, 1))
-
-            S_re = f * S_re_tm1 + c * re
-            S_im = f * S_im_tm1 + c * im
-
-            A = torch.square(S_re) + torch.square(S_im)
-
-            A = torch.reshape(A, (-1, self.freq_dim)).float()
-            A_a = torch.matmul(A * B_U[0], self.U_a)
-            A_a = torch.reshape(A_a, (-1, self.hidden_dim))
-            a = self.activation(A_a + self.b_a)
-
-            o = self.inner_activation(x_o + torch.matmul(h_tm1 * B_U[0], self.U_o))
-
-            h = o * a
-            p = torch.matmul(h, self.W_p) + self.b_p
-
-            self.states = [p, h, S_re, S_im, time, None, None, None]
-        self.states = []
-        return self.fc_out(p).squeeze()
-
-    def init_states(self, x):
-        reducer_f = torch.zeros((self.hidden_dim, self.freq_dim)).to(self.device)
-        reducer_p = torch.zeros((self.hidden_dim, self.output_dim)).to(self.device)
-
-        init_state_h = torch.zeros(self.hidden_dim).to(self.device)
-        init_state_p = torch.matmul(init_state_h, reducer_p)
-
-        init_state = torch.zeros_like(init_state_h).to(self.device)
-        init_freq = torch.matmul(init_state_h, reducer_f)
-
-        init_state = torch.reshape(init_state, (-1, self.hidden_dim, 1))
-        init_freq = torch.reshape(init_freq, (-1, 1, self.freq_dim))
-
-        init_state_S_re = init_state * init_freq
-        init_state_S_im = init_state * init_freq
-
-        init_state_time = torch.tensor(0).to(self.device)
-
-        self.states = [
-            init_state_p,
-            init_state_h,
-            init_state_S_re,
-            init_state_S_im,
-            init_state_time,
-            None,
-            None,
-            None,
-        ]
-
-    def get_constants(self, x):
-        constants = []
-        constants.append([torch.tensor(1.0).to(self.device) for _ in range(6)])
-        constants.append([torch.tensor(1.0).to(self.device) for _ in range(7)])
-        array = np.array([float(ii) / self.freq_dim for ii in range(self.freq_dim)])
-        constants.append(torch.tensor(array).to(self.device))
-
-        self.states[5:] = constants
-
-
-class SFM(Model):
-    """SFM Model
-
-    Parameters
-    ----------
-    input_dim : int
-        input dimension
-    output_dim : int
-        output dimension
-    lr : float
-        learning rate
-    optimizer : str
-        optimizer name
-    GPU : int
-        the GPU ID used for training
-    """
-
-    def __init__(
-        self,
-        d_feat=6,
-        hidden_size=64,
-        output_dim=1,
-        freq_dim=10,
-        dropout_W=0.0,
-        dropout_U=0.0,
-        n_epochs=200,
-        lr=0.001,
-        metric="",
-        batch_size=2000,
-        early_stop=20,
-        eval_steps=5,
-        loss="mse",
-        optimizer="gd",
-        GPU=0,
-        seed=None,
-        **kwargs
-    ):
-        # Set logger.
-        self.logger = get_module_logger("SFM")
-        self.logger.info("SFM pytorch version...")
-
-        # set hyper-parameters.
-        self.d_feat = d_feat
-        self.hidden_size = hidden_size
-        self.output_dim = output_dim
-        self.freq_dim = freq_dim
-        self.dropout_W = dropout_W
-        self.dropout_U = dropout_U
-        self.n_epochs = n_epochs
-        self.lr = lr
-        self.metric = metric
-        self.batch_size = batch_size
-        self.early_stop = early_stop
-        self.eval_steps = eval_steps
-        self.optimizer = optimizer.lower()
-        self.loss = loss
-        self.device = torch.device("cuda:%d" % (GPU) if torch.cuda.is_available() and GPU >= 0 else "cpu")
-        self.seed = seed
-
-        self.logger.info(
-            "SFM parameters setting:"
-            "\nd_feat : {}"
-            "\nhidden_size : {}"
-            "\noutput_size : {}"
-            "\nfrequency_dimension : {}"
-            "\ndropout_W: {}"
-            "\ndropout_U: {}"
-            "\nn_epochs : {}"
-            "\nlr : {}"
-            "\nmetric : {}"
-            "\nbatch_size : {}"
-            "\nearly_stop : {}"
-            "\neval_steps : {}"
-            "\noptimizer : {}"
-            "\nloss_type : {}"
-            "\ndevice : {}"
-            "\nuse_GPU : {}"
-            "\nseed : {}".format(
-                d_feat,
-                hidden_size,
-                output_dim,
-                freq_dim,
-                dropout_W,
-                dropout_U,
-                n_epochs,
-                lr,
-                metric,
-                batch_size,
-                early_stop,
-                eval_steps,
-                optimizer.lower(),
-                loss,
-                self.device,
-                self.use_gpu,
-                seed,
-            )
-        )
-
-        if self.seed is not None:
-            np.random.seed(self.seed)
-            torch.manual_seed(self.seed)
-
-        self.sfm_model = SFM_Model(
-            d_feat=self.d_feat,
-            output_dim=self.output_dim,
-            hidden_size=self.hidden_size,
-            freq_dim=self.freq_dim,
-            dropout_W=self.dropout_W,
-            dropout_U=self.dropout_U,
-            device=self.device,
-        )
-        self.logger.info("model:\n{:}".format(self.sfm_model))
-        self.logger.info("model size: {:.4f} MB".format(count_parameters(self.sfm_model)))
-
-        if optimizer.lower() == "adam":
-            self.train_optimizer = optim.Adam(self.sfm_model.parameters(), lr=self.lr)
-        elif optimizer.lower() == "gd":
-            self.train_optimizer = optim.SGD(self.sfm_model.parameters(), lr=self.lr)
-        else:
-            raise NotImplementedError("optimizer {} is not supported!".format(optimizer))
-
-        self.fitted = False
-        self.sfm_model.to(self.device)
-
-    @property
-    def use_gpu(self):
-        return self.device != torch.device("cpu")
-
-    def test_epoch(self, data_x, data_y):
-
-        # prepare training data
-        x_values = data_x.values
-        y_values = np.squeeze(data_y.values)
-
-        self.sfm_model.eval()
-
-        scores = []
-        losses = []
-
-        indices = np.arange(len(x_values))
-
-        for i in range(len(indices))[:: self.batch_size]:
-
-            if len(indices) - i < self.batch_size:
-                break
-
-            feature = torch.from_numpy(x_values[indices[i : i + self.batch_size]]).float().to(self.device)
-            label = torch.from_numpy(y_values[indices[i : i + self.batch_size]]).float().to(self.device)
-
-            pred = self.sfm_model(feature)
-            loss = self.loss_fn(pred, label)
-            losses.append(loss.item())
-
-            score = self.metric_fn(pred, label)
-            scores.append(score.item())
-
-        return np.mean(losses), np.mean(scores)
-
-    def train_epoch(self, x_train, y_train):
-
-        x_train_values = x_train.values
-        y_train_values = np.squeeze(y_train.values)
-
-        self.sfm_model.train()
-
-        indices = np.arange(len(x_train_values))
-        np.random.shuffle(indices)
-
-        for i in range(len(indices))[:: self.batch_size]:
-
-            if len(indices) - i < self.batch_size:
-                break
-
-            feature = torch.from_numpy(x_train_values[indices[i : i + self.batch_size]]).float().to(self.device)
-            label = torch.from_numpy(y_train_values[indices[i : i + self.batch_size]]).float().to(self.device)
-
-            pred = self.sfm_model(feature)
-            loss = self.loss_fn(pred, label)
-
-            self.train_optimizer.zero_grad()
-            loss.backward()
-            torch.nn.utils.clip_grad_value_(self.sfm_model.parameters(), 3.0)
-            self.train_optimizer.step()
-
-    def fit(
-        self,
-        dataset: DatasetH,
-        evals_result=dict(),
-        save_path=None,
-    ):
-
-        df_train, df_valid = dataset.prepare(
-            ["train", "valid"],
-            col_set=["feature", "label"],
-            data_key=DataHandlerLP.DK_L,
-        )
-        if df_train.empty or df_valid.empty:
-            raise ValueError("Empty data from dataset, please check your dataset config.")
-        x_train, y_train = df_train["feature"], df_train["label"]
-        x_valid, y_valid = df_valid["feature"], df_valid["label"]
-
-        save_path = get_or_create_path(save_path)
-        stop_steps = 0
-        train_loss = 0
-        best_score = -np.inf
-        best_epoch = 0
-        evals_result["train"] = []
-        evals_result["valid"] = []
-
-        # train
-        self.logger.info("training...")
-        self.fitted = True
-
-        for step in range(self.n_epochs):
-            self.logger.info("Epoch%d:", step)
-            self.logger.info("training...")
-            self.train_epoch(x_train, y_train)
-            self.logger.info("evaluating...")
-            train_loss, train_score = self.test_epoch(x_train, y_train)
-            val_loss, val_score = self.test_epoch(x_valid, y_valid)
-            self.logger.info("train %.6f, valid %.6f" % (train_score, val_score))
-            evals_result["train"].append(train_score)
-            evals_result["valid"].append(val_score)
-
-            if val_score > best_score:
-                best_score = val_score
-                stop_steps = 0
-                best_epoch = step
-                best_param = copy.deepcopy(self.sfm_model.state_dict())
-            else:
-                stop_steps += 1
-                if stop_steps >= self.early_stop:
-                    self.logger.info("early stop")
-                    break
-
-        self.logger.info("best score: %.6lf @ %d" % (best_score, best_epoch))
-        self.sfm_model.load_state_dict(best_param)
-        torch.save(best_param, save_path)
-        if self.device != "cpu":
-            torch.cuda.empty_cache()
-
-    def mse(self, pred, label):
-        loss = (pred - label) ** 2
-        return torch.mean(loss)
-
-    def loss_fn(self, pred, label):
-        mask = ~torch.isnan(label)
-
-        if self.loss == "mse":
-            return self.mse(pred[mask], label[mask])
-
-        raise ValueError("unknown loss `%s`" % self.loss)
-
-    def metric_fn(self, pred, label):
-
-        mask = torch.isfinite(label)
-
-        if self.metric in ("", "loss"):
-            return -self.loss_fn(pred[mask], label[mask])
-
-        raise ValueError("unknown metric `%s`" % self.metric)
-
-    def predict(self, dataset: DatasetH, segment: Union[Text, slice] = "test"):
-        if not self.fitted:
-            raise ValueError("model is not fitted yet!")
-
-        x_test = dataset.prepare(segment, col_set="feature", data_key=DataHandlerLP.DK_I)
-        index = x_test.index
-        self.sfm_model.eval()
-        x_values = x_test.values
-        sample_num = x_values.shape[0]
-        preds = []
-
-        for begin in range(sample_num)[:: self.batch_size]:
-            if sample_num - begin < self.batch_size:
-                end = sample_num
-            else:
-                end = begin + self.batch_size
-
-            x_batch = torch.from_numpy(x_values[begin:end]).float().to(self.device)
-
-            with torch.no_grad():
-                pred = self.sfm_model(x_batch).detach().cpu().numpy()
-
-            preds.append(pred)
-
-        return pd.Series(np.concatenate(preds), index=index)
-
-
-class AverageMeter:
-    """Computes and stores the average and current value"""
-
-    def __init__(self):
-        self.reset()
-
-    def reset(self):
-        self.val = 0
-        self.avg = 0
-        self.sum = 0
-        self.count = 0
-
-    def update(self, val, n=1):
-        self.val = val
-        self.sum += val * n
-        self.count += n
-        self.avg = self.sum / self.count
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+from __future__ import division
+from __future__ import print_function
+
+import numpy as np
+import pandas as pd
+from typing import Text, Union
+import copy
+from ...utils import get_or_create_path
+from ...log import get_module_logger
+
+import torch
+import torch.nn as nn
+import torch.nn.init as init
+import torch.optim as optim
+
+from .pytorch_utils import count_parameters
+from ...model.base import Model
+from ...data.dataset import DatasetH
+from ...data.dataset.handler import DataHandlerLP
+
+
+class SFM_Model(nn.Module):
+    def __init__(
+        self,
+        d_feat=6,
+        output_dim=1,
+        freq_dim=10,
+        hidden_size=64,
+        dropout_W=0.0,
+        dropout_U=0.0,
+        device="cpu",
+    ):
+        super().__init__()
+
+        self.input_dim = d_feat
+        self.output_dim = output_dim
+        self.freq_dim = freq_dim
+        self.hidden_dim = hidden_size
+        self.device = device
+
+        self.W_i = nn.Parameter(init.xavier_uniform_(torch.empty((self.input_dim, self.hidden_dim))))
+        self.U_i = nn.Parameter(init.orthogonal_(torch.empty(self.hidden_dim, self.hidden_dim)))
+        self.b_i = nn.Parameter(torch.zeros(self.hidden_dim))
+
+        self.W_ste = nn.Parameter(init.xavier_uniform_(torch.empty(self.input_dim, self.hidden_dim)))
+        self.U_ste = nn.Parameter(init.orthogonal_(torch.empty(self.hidden_dim, self.hidden_dim)))
+        self.b_ste = nn.Parameter(torch.ones(self.hidden_dim))
+
+        self.W_fre = nn.Parameter(init.xavier_uniform_(torch.empty(self.input_dim, self.freq_dim)))
+        self.U_fre = nn.Parameter(init.orthogonal_(torch.empty(self.hidden_dim, self.freq_dim)))
+        self.b_fre = nn.Parameter(torch.ones(self.freq_dim))
+
+        self.W_c = nn.Parameter(init.xavier_uniform_(torch.empty(self.input_dim, self.hidden_dim)))
+        self.U_c = nn.Parameter(init.orthogonal_(torch.empty(self.hidden_dim, self.hidden_dim)))
+        self.b_c = nn.Parameter(torch.zeros(self.hidden_dim))
+
+        self.W_o = nn.Parameter(init.xavier_uniform_(torch.empty(self.input_dim, self.hidden_dim)))
+        self.U_o = nn.Parameter(init.orthogonal_(torch.empty(self.hidden_dim, self.hidden_dim)))
+        self.b_o = nn.Parameter(torch.zeros(self.hidden_dim))
+
+        self.U_a = nn.Parameter(init.orthogonal_(torch.empty(self.freq_dim, 1)))
+        self.b_a = nn.Parameter(torch.zeros(self.hidden_dim))
+
+        self.W_p = nn.Parameter(init.xavier_uniform_(torch.empty(self.hidden_dim, self.output_dim)))
+        self.b_p = nn.Parameter(torch.zeros(self.output_dim))
+
+        self.activation = nn.Tanh()
+        self.inner_activation = nn.Hardsigmoid()
+        self.dropout_W, self.dropout_U = (dropout_W, dropout_U)
+        self.fc_out = nn.Linear(self.output_dim, 1)
+
+        self.states = []
+
+    def forward(self, input):
+        input = input.reshape(len(input), self.input_dim, -1)  # [N, F, T]
+        input = input.permute(0, 2, 1)  # [N, T, F]
+        time_step = input.shape[1]
+
+        for ts in range(time_step):
+            x = input[:, ts, :]
+            if len(self.states) == 0:  # hasn't initialized yet
+                self.init_states(x)
+            self.get_constants(x)
+            p_tm1 = self.states[0]  # noqa: F841
+            h_tm1 = self.states[1]
+            S_re_tm1 = self.states[2]
+            S_im_tm1 = self.states[3]
+            time_tm1 = self.states[4]
+            B_U = self.states[5]
+            B_W = self.states[6]
+            frequency = self.states[7]
+
+            x_i = torch.matmul(x * B_W[0], self.W_i) + self.b_i
+            x_ste = torch.matmul(x * B_W[0], self.W_ste) + self.b_ste
+            x_fre = torch.matmul(x * B_W[0], self.W_fre) + self.b_fre
+            x_c = torch.matmul(x * B_W[0], self.W_c) + self.b_c
+            x_o = torch.matmul(x * B_W[0], self.W_o) + self.b_o
+
+            i = self.inner_activation(x_i + torch.matmul(h_tm1 * B_U[0], self.U_i))
+            ste = self.inner_activation(x_ste + torch.matmul(h_tm1 * B_U[0], self.U_ste))
+            fre = self.inner_activation(x_fre + torch.matmul(h_tm1 * B_U[0], self.U_fre))
+
+            ste = torch.reshape(ste, (-1, self.hidden_dim, 1))
+            fre = torch.reshape(fre, (-1, 1, self.freq_dim))
+
+            f = ste * fre
+
+            c = i * self.activation(x_c + torch.matmul(h_tm1 * B_U[0], self.U_c))
+
+            time = time_tm1 + 1
+
+            omega = torch.tensor(2 * np.pi) * time * frequency
+
+            re = torch.cos(omega)
+            im = torch.sin(omega)
+
+            c = torch.reshape(c, (-1, self.hidden_dim, 1))
+
+            S_re = f * S_re_tm1 + c * re
+            S_im = f * S_im_tm1 + c * im
+
+            A = torch.square(S_re) + torch.square(S_im)
+
+            A = torch.reshape(A, (-1, self.freq_dim)).float()
+            A_a = torch.matmul(A * B_U[0], self.U_a)
+            A_a = torch.reshape(A_a, (-1, self.hidden_dim))
+            a = self.activation(A_a + self.b_a)
+
+            o = self.inner_activation(x_o + torch.matmul(h_tm1 * B_U[0], self.U_o))
+
+            h = o * a
+            p = torch.matmul(h, self.W_p) + self.b_p
+
+            self.states = [p, h, S_re, S_im, time, None, None, None]
+        self.states = []
+        return self.fc_out(p).squeeze()
+
+    def init_states(self, x):
+        reducer_f = torch.zeros((self.hidden_dim, self.freq_dim)).to(self.device)
+        reducer_p = torch.zeros((self.hidden_dim, self.output_dim)).to(self.device)
+
+        init_state_h = torch.zeros(self.hidden_dim).to(self.device)
+        init_state_p = torch.matmul(init_state_h, reducer_p)
+
+        init_state = torch.zeros_like(init_state_h).to(self.device)
+        init_freq = torch.matmul(init_state_h, reducer_f)
+
+        init_state = torch.reshape(init_state, (-1, self.hidden_dim, 1))
+        init_freq = torch.reshape(init_freq, (-1, 1, self.freq_dim))
+
+        init_state_S_re = init_state * init_freq
+        init_state_S_im = init_state * init_freq
+
+        init_state_time = torch.tensor(0).to(self.device)
+
+        self.states = [
+            init_state_p,
+            init_state_h,
+            init_state_S_re,
+            init_state_S_im,
+            init_state_time,
+            None,
+            None,
+            None,
+        ]
+
+    def get_constants(self, x):
+        constants = []
+        constants.append([torch.tensor(1.0).to(self.device) for _ in range(6)])
+        constants.append([torch.tensor(1.0).to(self.device) for _ in range(7)])
+        array = np.array([float(ii) / self.freq_dim for ii in range(self.freq_dim)])
+        constants.append(torch.tensor(array).to(self.device))
+
+        self.states[5:] = constants
+
+
+class SFM(Model):
+    """SFM Model
+
+    Parameters
+    ----------
+    input_dim : int
+        input dimension
+    output_dim : int
+        output dimension
+    lr : float
+        learning rate
+    optimizer : str
+        optimizer name
+    GPU : int
+        the GPU ID used for training
+    """
+
+    def __init__(
+        self,
+        d_feat=6,
+        hidden_size=64,
+        output_dim=1,
+        freq_dim=10,
+        dropout_W=0.0,
+        dropout_U=0.0,
+        n_epochs=200,
+        lr=0.001,
+        metric="",
+        batch_size=2000,
+        early_stop=20,
+        eval_steps=5,
+        loss="mse",
+        optimizer="gd",
+        GPU=0,
+        seed=None,
+        **kwargs
+    ):
+        # Set logger.
+        self.logger = get_module_logger("SFM")
+        self.logger.info("SFM pytorch version...")
+
+        # set hyper-parameters.
+        self.d_feat = d_feat
+        self.hidden_size = hidden_size
+        self.output_dim = output_dim
+        self.freq_dim = freq_dim
+        self.dropout_W = dropout_W
+        self.dropout_U = dropout_U
+        self.n_epochs = n_epochs
+        self.lr = lr
+        self.metric = metric
+        self.batch_size = batch_size
+        self.early_stop = early_stop
+        self.eval_steps = eval_steps
+        self.optimizer = optimizer.lower()
+        self.loss = loss
+        self.device = torch.device("cuda:%d" % (GPU) if torch.cuda.is_available() and GPU >= 0 else "cpu")
+        self.seed = seed
+
+        self.logger.info(
+            "SFM parameters setting:"
+            "\nd_feat : {}"
+            "\nhidden_size : {}"
+            "\noutput_size : {}"
+            "\nfrequency_dimension : {}"
+            "\ndropout_W: {}"
+            "\ndropout_U: {}"
+            "\nn_epochs : {}"
+            "\nlr : {}"
+            "\nmetric : {}"
+            "\nbatch_size : {}"
+            "\nearly_stop : {}"
+            "\neval_steps : {}"
+            "\noptimizer : {}"
+            "\nloss_type : {}"
+            "\ndevice : {}"
+            "\nuse_GPU : {}"
+            "\nseed : {}".format(
+                d_feat,
+                hidden_size,
+                output_dim,
+                freq_dim,
+                dropout_W,
+                dropout_U,
+                n_epochs,
+                lr,
+                metric,
+                batch_size,
+                early_stop,
+                eval_steps,
+                optimizer.lower(),
+                loss,
+                self.device,
+                self.use_gpu,
+                seed,
+            )
+        )
+
+        if self.seed is not None:
+            np.random.seed(self.seed)
+            torch.manual_seed(self.seed)
+
+        self.sfm_model = SFM_Model(
+            d_feat=self.d_feat,
+            output_dim=self.output_dim,
+            hidden_size=self.hidden_size,
+            freq_dim=self.freq_dim,
+            dropout_W=self.dropout_W,
+            dropout_U=self.dropout_U,
+            device=self.device,
+        )
+        self.logger.info("model:\n{:}".format(self.sfm_model))
+        self.logger.info("model size: {:.4f} MB".format(count_parameters(self.sfm_model)))
+
+        if optimizer.lower() == "adam":
+            self.train_optimizer = optim.Adam(self.sfm_model.parameters(), lr=self.lr)
+        elif optimizer.lower() == "gd":
+            self.train_optimizer = optim.SGD(self.sfm_model.parameters(), lr=self.lr)
+        else:
+            raise NotImplementedError("optimizer {} is not supported!".format(optimizer))
+
+        self.fitted = False
+        self.sfm_model.to(self.device)
+
+    @property
+    def use_gpu(self):
+        return self.device != torch.device("cpu")
+
+    def test_epoch(self, data_x, data_y):
+        # prepare training data
+        x_values = data_x.values
+        y_values = np.squeeze(data_y.values)
+
+        self.sfm_model.eval()
+
+        scores = []
+        losses = []
+
+        indices = np.arange(len(x_values))
+
+        for i in range(len(indices))[:: self.batch_size]:
+            if len(indices) - i < self.batch_size:
+                break
+
+            feature = torch.from_numpy(x_values[indices[i : i + self.batch_size]]).float().to(self.device)
+            label = torch.from_numpy(y_values[indices[i : i + self.batch_size]]).float().to(self.device)
+
+            pred = self.sfm_model(feature)
+            loss = self.loss_fn(pred, label)
+            losses.append(loss.item())
+
+            score = self.metric_fn(pred, label)
+            scores.append(score.item())
+
+        return np.mean(losses), np.mean(scores)
+
+    def train_epoch(self, x_train, y_train):
+        x_train_values = x_train.values
+        y_train_values = np.squeeze(y_train.values)
+
+        self.sfm_model.train()
+
+        indices = np.arange(len(x_train_values))
+        np.random.shuffle(indices)
+
+        for i in range(len(indices))[:: self.batch_size]:
+            if len(indices) - i < self.batch_size:
+                break
+
+            feature = torch.from_numpy(x_train_values[indices[i : i + self.batch_size]]).float().to(self.device)
+            label = torch.from_numpy(y_train_values[indices[i : i + self.batch_size]]).float().to(self.device)
+
+            pred = self.sfm_model(feature)
+            loss = self.loss_fn(pred, label)
+
+            self.train_optimizer.zero_grad()
+            loss.backward()
+            torch.nn.utils.clip_grad_value_(self.sfm_model.parameters(), 3.0)
+            self.train_optimizer.step()
+
+    def fit(
+        self,
+        dataset: DatasetH,
+        evals_result=dict(),
+        save_path=None,
+    ):
+        df_train, df_valid = dataset.prepare(
+            ["train", "valid"],
+            col_set=["feature", "label"],
+            data_key=DataHandlerLP.DK_L,
+        )
+        if df_train.empty or df_valid.empty:
+            raise ValueError("Empty data from dataset, please check your dataset config.")
+        x_train, y_train = df_train["feature"], df_train["label"]
+        x_valid, y_valid = df_valid["feature"], df_valid["label"]
+
+        save_path = get_or_create_path(save_path)
+        stop_steps = 0
+        train_loss = 0
+        best_score = -np.inf
+        best_epoch = 0
+        evals_result["train"] = []
+        evals_result["valid"] = []
+
+        # train
+        self.logger.info("training...")
+        self.fitted = True
+
+        for step in range(self.n_epochs):
+            self.logger.info("Epoch%d:", step)
+            self.logger.info("training...")
+            self.train_epoch(x_train, y_train)
+            self.logger.info("evaluating...")
+            train_loss, train_score = self.test_epoch(x_train, y_train)
+            val_loss, val_score = self.test_epoch(x_valid, y_valid)
+            self.logger.info("train %.6f, valid %.6f" % (train_score, val_score))
+            evals_result["train"].append(train_score)
+            evals_result["valid"].append(val_score)
+
+            if val_score > best_score:
+                best_score = val_score
+                stop_steps = 0
+                best_epoch = step
+                best_param = copy.deepcopy(self.sfm_model.state_dict())
+            else:
+                stop_steps += 1
+                if stop_steps >= self.early_stop:
+                    self.logger.info("early stop")
+                    break
+
+        self.logger.info("best score: %.6lf @ %d" % (best_score, best_epoch))
+        self.sfm_model.load_state_dict(best_param)
+        torch.save(best_param, save_path)
+        if self.device != "cpu":
+            torch.cuda.empty_cache()
+
+    def mse(self, pred, label):
+        loss = (pred - label) ** 2
+        return torch.mean(loss)
+
+    def loss_fn(self, pred, label):
+        mask = ~torch.isnan(label)
+
+        if self.loss == "mse":
+            return self.mse(pred[mask], label[mask])
+
+        raise ValueError("unknown loss `%s`" % self.loss)
+
+    def metric_fn(self, pred, label):
+        mask = torch.isfinite(label)
+
+        if self.metric in ("", "loss"):
+            return -self.loss_fn(pred[mask], label[mask])
+
+        raise ValueError("unknown metric `%s`" % self.metric)
+
+    def predict(self, dataset: DatasetH, segment: Union[Text, slice] = "test"):
+        if not self.fitted:
+            raise ValueError("model is not fitted yet!")
+
+        x_test = dataset.prepare(segment, col_set="feature", data_key=DataHandlerLP.DK_I)
+        index = x_test.index
+        self.sfm_model.eval()
+        x_values = x_test.values
+        sample_num = x_values.shape[0]
+        preds = []
+
+        for begin in range(sample_num)[:: self.batch_size]:
+            if sample_num - begin < self.batch_size:
+                end = sample_num
+            else:
+                end = begin + self.batch_size
+
+            x_batch = torch.from_numpy(x_values[begin:end]).float().to(self.device)
+
+            with torch.no_grad():
+                pred = self.sfm_model(x_batch).detach().cpu().numpy()
+
+            preds.append(pred)
+
+        return pd.Series(np.concatenate(preds), index=index)
+
+
+class AverageMeter:
+    """Computes and stores the average and current value"""
+
+    def __init__(self):
+        self.reset()
+
+    def reset(self):
+        self.val = 0
+        self.avg = 0
+        self.sum = 0
+        self.count = 0
+
+    def update(self, val, n=1):
+        self.val = val
+        self.sum += val * n
+        self.count += n
+        self.avg = self.sum / self.count
```

## qlib/contrib/model/pytorch_tabnet.py

```diff
@@ -1,647 +1,643 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-from __future__ import division
-from __future__ import print_function
-
-import numpy as np
-import pandas as pd
-from typing import Text, Union
-import copy
-from ...utils import get_or_create_path
-from ...log import get_module_logger
-
-import torch
-import torch.nn as nn
-import torch.optim as optim
-import torch.nn.functional as F
-from torch.autograd import Function
-
-from .pytorch_utils import count_parameters
-from ...model.base import Model
-from ...data.dataset import DatasetH
-from ...data.dataset.handler import DataHandlerLP
-
-
-class TabnetModel(Model):
-    def __init__(
-        self,
-        d_feat=158,
-        out_dim=64,
-        final_out_dim=1,
-        batch_size=4096,
-        n_d=64,
-        n_a=64,
-        n_shared=2,
-        n_ind=2,
-        n_steps=5,
-        n_epochs=100,
-        pretrain_n_epochs=50,
-        relax=1.3,
-        vbs=2048,
-        seed=993,
-        optimizer="adam",
-        loss="mse",
-        metric="",
-        early_stop=20,
-        GPU=0,
-        pretrain_loss="custom",
-        ps=0.3,
-        lr=0.01,
-        pretrain=True,
-        pretrain_file=None,
-    ):
-        """
-        TabNet model for Qlib
-
-        Args:
-        ps: probability to generate the bernoulli mask
-        """
-        # set hyper-parameters.
-        self.d_feat = d_feat
-        self.out_dim = out_dim
-        self.final_out_dim = final_out_dim
-        self.lr = lr
-        self.batch_size = batch_size
-        self.optimizer = optimizer.lower()
-        self.pretrain_loss = pretrain_loss
-        self.seed = seed
-        self.ps = ps
-        self.n_epochs = n_epochs
-        self.logger = get_module_logger("TabNet")
-        self.pretrain_n_epochs = pretrain_n_epochs
-        self.device = "cuda:%s" % (GPU) if torch.cuda.is_available() and GPU >= 0 else "cpu"
-        self.loss = loss
-        self.metric = metric
-        self.early_stop = early_stop
-        self.pretrain = pretrain
-        self.pretrain_file = get_or_create_path(pretrain_file)
-        self.logger.info(
-            "TabNet:"
-            "\nbatch_size : {}"
-            "\nvirtual bs : {}"
-            "\ndevice : {}"
-            "\npretrain: {}".format(self.batch_size, vbs, self.device, self.pretrain)
-        )
-        self.fitted = False
-        np.random.seed(self.seed)
-        torch.manual_seed(self.seed)
-
-        self.tabnet_model = TabNet(inp_dim=self.d_feat, out_dim=self.out_dim, vbs=vbs, relax=relax).to(self.device)
-        self.tabnet_decoder = TabNet_Decoder(self.out_dim, self.d_feat, n_shared, n_ind, vbs, n_steps).to(self.device)
-        self.logger.info("model:\n{:}\n{:}".format(self.tabnet_model, self.tabnet_decoder))
-        self.logger.info("model size: {:.4f} MB".format(count_parameters([self.tabnet_model, self.tabnet_decoder])))
-
-        if optimizer.lower() == "adam":
-            self.pretrain_optimizer = optim.Adam(
-                list(self.tabnet_model.parameters()) + list(self.tabnet_decoder.parameters()), lr=self.lr
-            )
-            self.train_optimizer = optim.Adam(self.tabnet_model.parameters(), lr=self.lr)
-
-        elif optimizer.lower() == "gd":
-            self.pretrain_optimizer = optim.SGD(
-                list(self.tabnet_model.parameters()) + list(self.tabnet_decoder.parameters()), lr=self.lr
-            )
-            self.train_optimizer = optim.SGD(self.tabnet_model.parameters(), lr=self.lr)
-        else:
-            raise NotImplementedError("optimizer {} is not supported!".format(optimizer))
-
-    @property
-    def use_gpu(self):
-        return self.device != torch.device("cpu")
-
-    def pretrain_fn(self, dataset=DatasetH, pretrain_file="./pretrain/best.model"):
-        get_or_create_path(pretrain_file)
-
-        [df_train, df_valid] = dataset.prepare(
-            ["pretrain", "pretrain_validation"],
-            col_set=["feature", "label"],
-            data_key=DataHandlerLP.DK_L,
-        )
-
-        df_train.fillna(df_train.mean(), inplace=True)
-        df_valid.fillna(df_valid.mean(), inplace=True)
-
-        x_train = df_train["feature"]
-        x_valid = df_valid["feature"]
-
-        # Early stop setup
-        stop_steps = 0
-        train_loss = 0
-        best_loss = np.inf
-
-        for epoch_idx in range(self.pretrain_n_epochs):
-            self.logger.info("epoch: %s" % (epoch_idx))
-            self.logger.info("pre-training...")
-            self.pretrain_epoch(x_train)
-            self.logger.info("evaluating...")
-            train_loss = self.pretrain_test_epoch(x_train)
-            valid_loss = self.pretrain_test_epoch(x_valid)
-            self.logger.info("train %.6f, valid %.6f" % (train_loss, valid_loss))
-
-            if valid_loss < best_loss:
-                self.logger.info("Save Model...")
-                torch.save(self.tabnet_model.state_dict(), pretrain_file)
-                best_loss = valid_loss
-            else:
-                stop_steps += 1
-                if stop_steps >= self.early_stop:
-                    self.logger.info("early stop")
-                    break
-
-    def fit(
-        self,
-        dataset: DatasetH,
-        evals_result=dict(),
-        save_path=None,
-    ):
-        if self.pretrain:
-            # there is a  pretrained model, load the model
-            self.logger.info("Pretrain...")
-            self.pretrain_fn(dataset, self.pretrain_file)
-            self.logger.info("Load Pretrain model")
-            self.tabnet_model.load_state_dict(torch.load(self.pretrain_file, map_location=self.device))
-
-        # adding one more linear layer to fit the final output dimension
-        self.tabnet_model = FinetuneModel(self.out_dim, self.final_out_dim, self.tabnet_model).to(self.device)
-        df_train, df_valid = dataset.prepare(
-            ["train", "valid"],
-            col_set=["feature", "label"],
-            data_key=DataHandlerLP.DK_L,
-        )
-        if df_train.empty or df_valid.empty:
-            raise ValueError("Empty data from dataset, please check your dataset config.")
-        df_train.fillna(df_train.mean(), inplace=True)
-        x_train, y_train = df_train["feature"], df_train["label"]
-        x_valid, y_valid = df_valid["feature"], df_valid["label"]
-        save_path = get_or_create_path(save_path)
-
-        stop_steps = 0
-        train_loss = 0
-        best_score = -np.inf
-        best_epoch = 0
-        evals_result["train"] = []
-        evals_result["valid"] = []
-
-        self.logger.info("training...")
-        self.fitted = True
-
-        for epoch_idx in range(self.n_epochs):
-            self.logger.info("epoch: %s" % (epoch_idx))
-            self.logger.info("training...")
-            self.train_epoch(x_train, y_train)
-            self.logger.info("evaluating...")
-            train_loss, train_score = self.test_epoch(x_train, y_train)
-            valid_loss, val_score = self.test_epoch(x_valid, y_valid)
-            self.logger.info("train %.6f, valid %.6f" % (train_score, val_score))
-            evals_result["train"].append(train_score)
-            evals_result["valid"].append(val_score)
-
-            if val_score > best_score:
-                best_score = val_score
-                stop_steps = 0
-                best_epoch = epoch_idx
-                best_param = copy.deepcopy(self.tabnet_model.state_dict())
-            else:
-                stop_steps += 1
-                if stop_steps >= self.early_stop:
-                    self.logger.info("early stop")
-                    break
-
-        self.logger.info("best score: %.6lf @ %d" % (best_score, best_epoch))
-        self.tabnet_model.load_state_dict(best_param)
-        torch.save(best_param, save_path)
-
-        if self.use_gpu:
-            torch.cuda.empty_cache()
-
-    def predict(self, dataset: DatasetH, segment: Union[Text, slice] = "test"):
-        if not self.fitted:
-            raise ValueError("model is not fitted yet!")
-
-        x_test = dataset.prepare(segment, col_set="feature", data_key=DataHandlerLP.DK_I)
-        index = x_test.index
-        self.tabnet_model.eval()
-        x_values = torch.from_numpy(x_test.values)
-        x_values[torch.isnan(x_values)] = 0
-        sample_num = x_values.shape[0]
-        preds = []
-
-        for begin in range(sample_num)[:: self.batch_size]:
-            if sample_num - begin < self.batch_size:
-                end = sample_num
-            else:
-                end = begin + self.batch_size
-
-            x_batch = x_values[begin:end].float().to(self.device)
-            priors = torch.ones(end - begin, self.d_feat).to(self.device)
-
-            with torch.no_grad():
-                pred = self.tabnet_model(x_batch, priors).detach().cpu().numpy()
-
-            preds.append(pred)
-
-        return pd.Series(np.concatenate(preds), index=index)
-
-    def test_epoch(self, data_x, data_y):
-        # prepare training data
-        x_values = torch.from_numpy(data_x.values)
-        y_values = torch.from_numpy(np.squeeze(data_y.values))
-        x_values[torch.isnan(x_values)] = 0
-        y_values[torch.isnan(y_values)] = 0
-        self.tabnet_model.eval()
-
-        scores = []
-        losses = []
-
-        indices = np.arange(len(x_values))
-
-        for i in range(len(indices))[:: self.batch_size]:
-
-            if len(indices) - i < self.batch_size:
-                break
-            feature = x_values[indices[i : i + self.batch_size]].float().to(self.device)
-            label = y_values[indices[i : i + self.batch_size]].float().to(self.device)
-            priors = torch.ones(self.batch_size, self.d_feat).to(self.device)
-            with torch.no_grad():
-                pred = self.tabnet_model(feature, priors)
-                loss = self.loss_fn(pred, label)
-                losses.append(loss.item())
-
-                score = self.metric_fn(pred, label)
-                scores.append(score.item())
-
-        return np.mean(losses), np.mean(scores)
-
-    def train_epoch(self, x_train, y_train):
-        x_train_values = torch.from_numpy(x_train.values)
-        y_train_values = torch.from_numpy(np.squeeze(y_train.values))
-        x_train_values[torch.isnan(x_train_values)] = 0
-        y_train_values[torch.isnan(y_train_values)] = 0
-        self.tabnet_model.train()
-
-        indices = np.arange(len(x_train_values))
-        np.random.shuffle(indices)
-
-        for i in range(len(indices))[:: self.batch_size]:
-
-            if len(indices) - i < self.batch_size:
-                break
-
-            feature = x_train_values[indices[i : i + self.batch_size]].float().to(self.device)
-            label = y_train_values[indices[i : i + self.batch_size]].float().to(self.device)
-            priors = torch.ones(self.batch_size, self.d_feat).to(self.device)
-            pred = self.tabnet_model(feature, priors)
-            loss = self.loss_fn(pred, label)
-
-            self.train_optimizer.zero_grad()
-            loss.backward()
-            torch.nn.utils.clip_grad_value_(self.tabnet_model.parameters(), 3.0)
-            self.train_optimizer.step()
-
-    def pretrain_epoch(self, x_train):
-        train_set = torch.from_numpy(x_train.values)
-        train_set[torch.isnan(train_set)] = 0
-        indices = np.arange(len(train_set))
-        np.random.shuffle(indices)
-
-        self.tabnet_model.train()
-        self.tabnet_decoder.train()
-
-        for i in range(len(indices))[:: self.batch_size]:
-
-            if len(indices) - i < self.batch_size:
-                break
-
-            S_mask = torch.bernoulli(torch.empty(self.batch_size, self.d_feat).fill_(self.ps))
-            x_train_values = train_set[indices[i : i + self.batch_size]] * (1 - S_mask)
-            y_train_values = train_set[indices[i : i + self.batch_size]] * (S_mask)
-
-            S_mask = S_mask.to(self.device)
-            feature = x_train_values.float().to(self.device)
-            label = y_train_values.float().to(self.device)
-            priors = 1 - S_mask
-            (vec, sparse_loss) = self.tabnet_model(feature, priors)
-            f = self.tabnet_decoder(vec)
-            loss = self.pretrain_loss_fn(label, f, S_mask)
-
-            self.pretrain_optimizer.zero_grad()
-            loss.backward()
-            self.pretrain_optimizer.step()
-
-    def pretrain_test_epoch(self, x_train):
-        train_set = torch.from_numpy(x_train.values)
-        train_set[torch.isnan(train_set)] = 0
-        indices = np.arange(len(train_set))
-
-        self.tabnet_model.eval()
-        self.tabnet_decoder.eval()
-
-        losses = []
-
-        for i in range(len(indices))[:: self.batch_size]:
-
-            if len(indices) - i < self.batch_size:
-                break
-
-            S_mask = torch.bernoulli(torch.empty(self.batch_size, self.d_feat).fill_(self.ps))
-            x_train_values = train_set[indices[i : i + self.batch_size]] * (1 - S_mask)
-            y_train_values = train_set[indices[i : i + self.batch_size]] * (S_mask)
-
-            feature = x_train_values.float().to(self.device)
-            label = y_train_values.float().to(self.device)
-            S_mask = S_mask.to(self.device)
-            priors = 1 - S_mask
-            with torch.no_grad():
-                (vec, sparse_loss) = self.tabnet_model(feature, priors)
-                f = self.tabnet_decoder(vec)
-
-                loss = self.pretrain_loss_fn(label, f, S_mask)
-            losses.append(loss.item())
-
-        return np.mean(losses)
-
-    def pretrain_loss_fn(self, f_hat, f, S):
-        """
-        Pretrain loss function defined in the original paper, read "Tabular self-supervised learning" in https://arxiv.org/pdf/1908.07442.pdf
-        """
-        down_mean = torch.mean(f, dim=0)
-        down = torch.sqrt(torch.sum(torch.square(f - down_mean), dim=0))
-        up = (f_hat - f) * S
-        return torch.sum(torch.square(up / down))
-
-    def loss_fn(self, pred, label):
-        mask = ~torch.isnan(label)
-        if self.loss == "mse":
-            return self.mse(pred[mask], label[mask])
-        raise ValueError("unknown loss `%s`" % self.loss)
-
-    def metric_fn(self, pred, label):
-        mask = torch.isfinite(label)
-        if self.metric in ("", "loss"):
-            return -self.loss_fn(pred[mask], label[mask])
-        raise ValueError("unknown metric `%s`" % self.metric)
-
-    def mse(self, pred, label):
-        loss = (pred - label) ** 2
-        return torch.mean(loss)
-
-
-class FinetuneModel(nn.Module):
-    """
-    FinuetuneModel for adding a layer by the end
-    """
-
-    def __init__(self, input_dim, output_dim, trained_model):
-        super().__init__()
-        self.model = trained_model
-        self.fc = nn.Linear(input_dim, output_dim)
-
-    def forward(self, x, priors):
-        return self.fc(self.model(x, priors)[0]).squeeze()  # take the vec out
-
-
-class DecoderStep(nn.Module):
-    def __init__(self, inp_dim, out_dim, shared, n_ind, vbs):
-        super().__init__()
-        self.fea_tran = FeatureTransformer(inp_dim, out_dim, shared, n_ind, vbs)
-        self.fc = nn.Linear(out_dim, out_dim)
-
-    def forward(self, x):
-        x = self.fea_tran(x)
-        return self.fc(x)
-
-
-class TabNet_Decoder(nn.Module):
-    def __init__(self, inp_dim, out_dim, n_shared, n_ind, vbs, n_steps):
-        """
-        TabNet decoder that is used in pre-training
-        """
-        super().__init__()
-        self.out_dim = out_dim
-        if n_shared > 0:
-            self.shared = nn.ModuleList()
-            self.shared.append(nn.Linear(inp_dim, 2 * out_dim))
-            for x in range(n_shared - 1):
-                self.shared.append(nn.Linear(out_dim, 2 * out_dim))  # preset the linear function we will use
-        else:
-            self.shared = None
-        self.n_steps = n_steps
-        self.steps = nn.ModuleList()
-        for x in range(n_steps):
-            self.steps.append(DecoderStep(inp_dim, out_dim, self.shared, n_ind, vbs))
-
-    def forward(self, x):
-        out = torch.zeros(x.size(0), self.out_dim).to(x.device)
-        for step in self.steps:
-            out += step(x)
-        return out
-
-
-class TabNet(nn.Module):
-    def __init__(self, inp_dim=6, out_dim=6, n_d=64, n_a=64, n_shared=2, n_ind=2, n_steps=5, relax=1.2, vbs=1024):
-        """
-        TabNet AKA the original encoder
-
-        Args:
-            n_d: dimension of the features used to calculate the final results
-            n_a: dimension of the features input to the attention transformer of the next step
-            n_shared: numbr of shared steps in feature transformer(optional)
-            n_ind: number of independent steps in feature transformer
-            n_steps: number of steps of pass through tabbet
-            relax coefficient:
-            virtual batch size:
-        """
-        super().__init__()
-
-        # set the number of shared step in feature transformer
-        if n_shared > 0:
-            self.shared = nn.ModuleList()
-            self.shared.append(nn.Linear(inp_dim, 2 * (n_d + n_a)))
-            for x in range(n_shared - 1):
-                self.shared.append(nn.Linear(n_d + n_a, 2 * (n_d + n_a)))  # preset the linear function we will use
-        else:
-            self.shared = None
-
-        self.first_step = FeatureTransformer(inp_dim, n_d + n_a, self.shared, n_ind, vbs)
-        self.steps = nn.ModuleList()
-        for x in range(n_steps - 1):
-            self.steps.append(DecisionStep(inp_dim, n_d, n_a, self.shared, n_ind, relax, vbs))
-        self.fc = nn.Linear(n_d, out_dim)
-        self.bn = nn.BatchNorm1d(inp_dim, momentum=0.01)
-        self.n_d = n_d
-
-    def forward(self, x, priors):
-        assert not torch.isnan(x).any()
-        x = self.bn(x)
-        x_a = self.first_step(x)[:, self.n_d :]
-        sparse_loss = []
-        out = torch.zeros(x.size(0), self.n_d).to(x.device)
-        for step in self.steps:
-            x_te, loss = step(x, x_a, priors)
-            out += F.relu(x_te[:, : self.n_d])  # split the feature from feat_transformer
-            x_a = x_te[:, self.n_d :]
-            sparse_loss.append(loss)
-        return self.fc(out), sum(sparse_loss)
-
-
-class GBN(nn.Module):
-    """
-    Ghost Batch Normalization
-    an efficient way of doing batch normalization
-
-    Args:
-        vbs: virtual batch size
-    """
-
-    def __init__(self, inp, vbs=1024, momentum=0.01):
-        super().__init__()
-        self.bn = nn.BatchNorm1d(inp, momentum=momentum)
-        self.vbs = vbs
-
-    def forward(self, x):
-        if x.size(0) <= self.vbs:  # can not be chunked
-            return self.bn(x)
-        else:
-            chunk = torch.chunk(x, x.size(0) // self.vbs, 0)
-            res = [self.bn(y) for y in chunk]
-            return torch.cat(res, 0)
-
-
-class GLU(nn.Module):
-    """
-    GLU block that extracts only the most essential information
-
-    Args:
-        vbs: virtual batch size
-    """
-
-    def __init__(self, inp_dim, out_dim, fc=None, vbs=1024):
-        super().__init__()
-        if fc:
-            self.fc = fc
-        else:
-            self.fc = nn.Linear(inp_dim, out_dim * 2)
-        self.bn = GBN(out_dim * 2, vbs=vbs)
-        self.od = out_dim
-
-    def forward(self, x):
-        x = self.bn(self.fc(x))
-        return torch.mul(x[:, : self.od], torch.sigmoid(x[:, self.od :]))
-
-
-class AttentionTransformer(nn.Module):
-    """
-    Args:
-        relax: relax coefficient. The greater it is, we can
-        use the same features more. When it is set to 1
-        we can use every feature only once
-    """
-
-    def __init__(self, d_a, inp_dim, relax, vbs=1024):
-        super().__init__()
-        self.fc = nn.Linear(d_a, inp_dim)
-        self.bn = GBN(inp_dim, vbs=vbs)
-        self.r = relax
-
-    # a:feature from previous decision step
-    def forward(self, a, priors):
-        a = self.bn(self.fc(a))
-        mask = SparsemaxFunction.apply(a * priors)
-        priors = priors * (self.r - mask)  # updating the prior
-        return mask
-
-
-class FeatureTransformer(nn.Module):
-    def __init__(self, inp_dim, out_dim, shared, n_ind, vbs):
-        super().__init__()
-        first = True
-        self.shared = nn.ModuleList()
-        if shared:
-            self.shared.append(GLU(inp_dim, out_dim, shared[0], vbs=vbs))
-            first = False
-            for fc in shared[1:]:
-                self.shared.append(GLU(out_dim, out_dim, fc, vbs=vbs))
-        else:
-            self.shared = None
-        self.independ = nn.ModuleList()
-        if first:
-            self.independ.append(GLU(inp_dim, out_dim, vbs=vbs))
-        for x in range(first, n_ind):
-            self.independ.append(GLU(out_dim, out_dim, vbs=vbs))
-        self.scale = float(np.sqrt(0.5))
-
-    def forward(self, x):
-        if self.shared:
-            x = self.shared[0](x)
-            for glu in self.shared[1:]:
-                x = torch.add(x, glu(x))
-                x = x * self.scale
-        for glu in self.independ:
-            x = torch.add(x, glu(x))
-            x = x * self.scale
-        return x
-
-
-class DecisionStep(nn.Module):
-    """
-    One step for the TabNet
-    """
-
-    def __init__(self, inp_dim, n_d, n_a, shared, n_ind, relax, vbs):
-        super().__init__()
-        self.atten_tran = AttentionTransformer(n_a, inp_dim, relax, vbs)
-        self.fea_tran = FeatureTransformer(inp_dim, n_d + n_a, shared, n_ind, vbs)
-
-    def forward(self, x, a, priors):
-        mask = self.atten_tran(a, priors)
-        sparse_loss = ((-1) * mask * torch.log(mask + 1e-10)).mean()
-        x = self.fea_tran(x * mask)
-        return x, sparse_loss
-
-
-def make_ix_like(input, dim=0):
-    d = input.size(dim)
-    rho = torch.arange(1, d + 1, device=input.device, dtype=input.dtype)
-    view = [1] * input.dim()
-    view[0] = -1
-    return rho.view(view).transpose(0, dim)
-
-
-class SparsemaxFunction(Function):
-    """
-    SparseMax function for replacing reLU
-    """
-
-    @staticmethod
-    def forward(ctx, input, dim=-1):
-        ctx.dim = dim
-        max_val, _ = input.max(dim=dim, keepdim=True)
-        input -= max_val  # same numerical stability trick as for softmax
-        tau, supp_size = SparsemaxFunction.threshold_and_support(input, dim=dim)
-        output = torch.clamp(input - tau, min=0)
-        ctx.save_for_backward(supp_size, output)
-        return output
-
-    @staticmethod
-    def backward(ctx, grad_output):
-        supp_size, output = ctx.saved_tensors
-        dim = ctx.dim
-        grad_input = grad_output.clone()
-        grad_input[output == 0] = 0
-
-        v_hat = grad_input.sum(dim=dim) / supp_size.to(output.dtype).squeeze()
-        v_hat = v_hat.unsqueeze(dim)
-        grad_input = torch.where(output != 0, grad_input - v_hat, grad_input)
-        return grad_input, None
-
-    @staticmethod
-    def threshold_and_support(input, dim=-1):
-        input_srt, _ = torch.sort(input, descending=True, dim=dim)
-        input_cumsum = input_srt.cumsum(dim) - 1
-        rhos = make_ix_like(input, dim)
-        support = rhos * input_srt > input_cumsum
-
-        support_size = support.sum(dim=dim).unsqueeze(dim)
-        tau = input_cumsum.gather(dim, support_size - 1)
-        tau /= support_size.to(input.dtype)
-        return tau, support_size
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+from __future__ import division
+from __future__ import print_function
+
+import numpy as np
+import pandas as pd
+from typing import Text, Union
+import copy
+from ...utils import get_or_create_path
+from ...log import get_module_logger
+
+import torch
+import torch.nn as nn
+import torch.optim as optim
+import torch.nn.functional as F
+from torch.autograd import Function
+
+from .pytorch_utils import count_parameters
+from ...model.base import Model
+from ...data.dataset import DatasetH
+from ...data.dataset.handler import DataHandlerLP
+
+
+class TabnetModel(Model):
+    def __init__(
+        self,
+        d_feat=158,
+        out_dim=64,
+        final_out_dim=1,
+        batch_size=4096,
+        n_d=64,
+        n_a=64,
+        n_shared=2,
+        n_ind=2,
+        n_steps=5,
+        n_epochs=100,
+        pretrain_n_epochs=50,
+        relax=1.3,
+        vbs=2048,
+        seed=993,
+        optimizer="adam",
+        loss="mse",
+        metric="",
+        early_stop=20,
+        GPU=0,
+        pretrain_loss="custom",
+        ps=0.3,
+        lr=0.01,
+        pretrain=True,
+        pretrain_file=None,
+    ):
+        """
+        TabNet model for Qlib
+
+        Args:
+        ps: probability to generate the bernoulli mask
+        """
+        # set hyper-parameters.
+        self.d_feat = d_feat
+        self.out_dim = out_dim
+        self.final_out_dim = final_out_dim
+        self.lr = lr
+        self.batch_size = batch_size
+        self.optimizer = optimizer.lower()
+        self.pretrain_loss = pretrain_loss
+        self.seed = seed
+        self.ps = ps
+        self.n_epochs = n_epochs
+        self.logger = get_module_logger("TabNet")
+        self.pretrain_n_epochs = pretrain_n_epochs
+        self.device = "cuda:%s" % (GPU) if torch.cuda.is_available() and GPU >= 0 else "cpu"
+        self.loss = loss
+        self.metric = metric
+        self.early_stop = early_stop
+        self.pretrain = pretrain
+        self.pretrain_file = get_or_create_path(pretrain_file)
+        self.logger.info(
+            "TabNet:"
+            "\nbatch_size : {}"
+            "\nvirtual bs : {}"
+            "\ndevice : {}"
+            "\npretrain: {}".format(self.batch_size, vbs, self.device, self.pretrain)
+        )
+        self.fitted = False
+        np.random.seed(self.seed)
+        torch.manual_seed(self.seed)
+
+        self.tabnet_model = TabNet(inp_dim=self.d_feat, out_dim=self.out_dim, vbs=vbs, relax=relax).to(self.device)
+        self.tabnet_decoder = TabNet_Decoder(self.out_dim, self.d_feat, n_shared, n_ind, vbs, n_steps).to(self.device)
+        self.logger.info("model:\n{:}\n{:}".format(self.tabnet_model, self.tabnet_decoder))
+        self.logger.info("model size: {:.4f} MB".format(count_parameters([self.tabnet_model, self.tabnet_decoder])))
+
+        if optimizer.lower() == "adam":
+            self.pretrain_optimizer = optim.Adam(
+                list(self.tabnet_model.parameters()) + list(self.tabnet_decoder.parameters()), lr=self.lr
+            )
+            self.train_optimizer = optim.Adam(self.tabnet_model.parameters(), lr=self.lr)
+
+        elif optimizer.lower() == "gd":
+            self.pretrain_optimizer = optim.SGD(
+                list(self.tabnet_model.parameters()) + list(self.tabnet_decoder.parameters()), lr=self.lr
+            )
+            self.train_optimizer = optim.SGD(self.tabnet_model.parameters(), lr=self.lr)
+        else:
+            raise NotImplementedError("optimizer {} is not supported!".format(optimizer))
+
+    @property
+    def use_gpu(self):
+        return self.device != torch.device("cpu")
+
+    def pretrain_fn(self, dataset=DatasetH, pretrain_file="./pretrain/best.model"):
+        get_or_create_path(pretrain_file)
+
+        [df_train, df_valid] = dataset.prepare(
+            ["pretrain", "pretrain_validation"],
+            col_set=["feature", "label"],
+            data_key=DataHandlerLP.DK_L,
+        )
+
+        df_train.fillna(df_train.mean(), inplace=True)
+        df_valid.fillna(df_valid.mean(), inplace=True)
+
+        x_train = df_train["feature"]
+        x_valid = df_valid["feature"]
+
+        # Early stop setup
+        stop_steps = 0
+        train_loss = 0
+        best_loss = np.inf
+
+        for epoch_idx in range(self.pretrain_n_epochs):
+            self.logger.info("epoch: %s" % (epoch_idx))
+            self.logger.info("pre-training...")
+            self.pretrain_epoch(x_train)
+            self.logger.info("evaluating...")
+            train_loss = self.pretrain_test_epoch(x_train)
+            valid_loss = self.pretrain_test_epoch(x_valid)
+            self.logger.info("train %.6f, valid %.6f" % (train_loss, valid_loss))
+
+            if valid_loss < best_loss:
+                self.logger.info("Save Model...")
+                torch.save(self.tabnet_model.state_dict(), pretrain_file)
+                best_loss = valid_loss
+            else:
+                stop_steps += 1
+                if stop_steps >= self.early_stop:
+                    self.logger.info("early stop")
+                    break
+
+    def fit(
+        self,
+        dataset: DatasetH,
+        evals_result=dict(),
+        save_path=None,
+    ):
+        if self.pretrain:
+            # there is a  pretrained model, load the model
+            self.logger.info("Pretrain...")
+            self.pretrain_fn(dataset, self.pretrain_file)
+            self.logger.info("Load Pretrain model")
+            self.tabnet_model.load_state_dict(torch.load(self.pretrain_file, map_location=self.device))
+
+        # adding one more linear layer to fit the final output dimension
+        self.tabnet_model = FinetuneModel(self.out_dim, self.final_out_dim, self.tabnet_model).to(self.device)
+        df_train, df_valid = dataset.prepare(
+            ["train", "valid"],
+            col_set=["feature", "label"],
+            data_key=DataHandlerLP.DK_L,
+        )
+        if df_train.empty or df_valid.empty:
+            raise ValueError("Empty data from dataset, please check your dataset config.")
+        df_train.fillna(df_train.mean(), inplace=True)
+        x_train, y_train = df_train["feature"], df_train["label"]
+        x_valid, y_valid = df_valid["feature"], df_valid["label"]
+        save_path = get_or_create_path(save_path)
+
+        stop_steps = 0
+        train_loss = 0
+        best_score = -np.inf
+        best_epoch = 0
+        evals_result["train"] = []
+        evals_result["valid"] = []
+
+        self.logger.info("training...")
+        self.fitted = True
+
+        for epoch_idx in range(self.n_epochs):
+            self.logger.info("epoch: %s" % (epoch_idx))
+            self.logger.info("training...")
+            self.train_epoch(x_train, y_train)
+            self.logger.info("evaluating...")
+            train_loss, train_score = self.test_epoch(x_train, y_train)
+            valid_loss, val_score = self.test_epoch(x_valid, y_valid)
+            self.logger.info("train %.6f, valid %.6f" % (train_score, val_score))
+            evals_result["train"].append(train_score)
+            evals_result["valid"].append(val_score)
+
+            if val_score > best_score:
+                best_score = val_score
+                stop_steps = 0
+                best_epoch = epoch_idx
+                best_param = copy.deepcopy(self.tabnet_model.state_dict())
+            else:
+                stop_steps += 1
+                if stop_steps >= self.early_stop:
+                    self.logger.info("early stop")
+                    break
+
+        self.logger.info("best score: %.6lf @ %d" % (best_score, best_epoch))
+        self.tabnet_model.load_state_dict(best_param)
+        torch.save(best_param, save_path)
+
+        if self.use_gpu:
+            torch.cuda.empty_cache()
+
+    def predict(self, dataset: DatasetH, segment: Union[Text, slice] = "test"):
+        if not self.fitted:
+            raise ValueError("model is not fitted yet!")
+
+        x_test = dataset.prepare(segment, col_set="feature", data_key=DataHandlerLP.DK_I)
+        index = x_test.index
+        self.tabnet_model.eval()
+        x_values = torch.from_numpy(x_test.values)
+        x_values[torch.isnan(x_values)] = 0
+        sample_num = x_values.shape[0]
+        preds = []
+
+        for begin in range(sample_num)[:: self.batch_size]:
+            if sample_num - begin < self.batch_size:
+                end = sample_num
+            else:
+                end = begin + self.batch_size
+
+            x_batch = x_values[begin:end].float().to(self.device)
+            priors = torch.ones(end - begin, self.d_feat).to(self.device)
+
+            with torch.no_grad():
+                pred = self.tabnet_model(x_batch, priors).detach().cpu().numpy()
+
+            preds.append(pred)
+
+        return pd.Series(np.concatenate(preds), index=index)
+
+    def test_epoch(self, data_x, data_y):
+        # prepare training data
+        x_values = torch.from_numpy(data_x.values)
+        y_values = torch.from_numpy(np.squeeze(data_y.values))
+        x_values[torch.isnan(x_values)] = 0
+        y_values[torch.isnan(y_values)] = 0
+        self.tabnet_model.eval()
+
+        scores = []
+        losses = []
+
+        indices = np.arange(len(x_values))
+
+        for i in range(len(indices))[:: self.batch_size]:
+            if len(indices) - i < self.batch_size:
+                break
+            feature = x_values[indices[i : i + self.batch_size]].float().to(self.device)
+            label = y_values[indices[i : i + self.batch_size]].float().to(self.device)
+            priors = torch.ones(self.batch_size, self.d_feat).to(self.device)
+            with torch.no_grad():
+                pred = self.tabnet_model(feature, priors)
+                loss = self.loss_fn(pred, label)
+                losses.append(loss.item())
+
+                score = self.metric_fn(pred, label)
+                scores.append(score.item())
+
+        return np.mean(losses), np.mean(scores)
+
+    def train_epoch(self, x_train, y_train):
+        x_train_values = torch.from_numpy(x_train.values)
+        y_train_values = torch.from_numpy(np.squeeze(y_train.values))
+        x_train_values[torch.isnan(x_train_values)] = 0
+        y_train_values[torch.isnan(y_train_values)] = 0
+        self.tabnet_model.train()
+
+        indices = np.arange(len(x_train_values))
+        np.random.shuffle(indices)
+
+        for i in range(len(indices))[:: self.batch_size]:
+            if len(indices) - i < self.batch_size:
+                break
+
+            feature = x_train_values[indices[i : i + self.batch_size]].float().to(self.device)
+            label = y_train_values[indices[i : i + self.batch_size]].float().to(self.device)
+            priors = torch.ones(self.batch_size, self.d_feat).to(self.device)
+            pred = self.tabnet_model(feature, priors)
+            loss = self.loss_fn(pred, label)
+
+            self.train_optimizer.zero_grad()
+            loss.backward()
+            torch.nn.utils.clip_grad_value_(self.tabnet_model.parameters(), 3.0)
+            self.train_optimizer.step()
+
+    def pretrain_epoch(self, x_train):
+        train_set = torch.from_numpy(x_train.values)
+        train_set[torch.isnan(train_set)] = 0
+        indices = np.arange(len(train_set))
+        np.random.shuffle(indices)
+
+        self.tabnet_model.train()
+        self.tabnet_decoder.train()
+
+        for i in range(len(indices))[:: self.batch_size]:
+            if len(indices) - i < self.batch_size:
+                break
+
+            S_mask = torch.bernoulli(torch.empty(self.batch_size, self.d_feat).fill_(self.ps))
+            x_train_values = train_set[indices[i : i + self.batch_size]] * (1 - S_mask)
+            y_train_values = train_set[indices[i : i + self.batch_size]] * (S_mask)
+
+            S_mask = S_mask.to(self.device)
+            feature = x_train_values.float().to(self.device)
+            label = y_train_values.float().to(self.device)
+            priors = 1 - S_mask
+            (vec, sparse_loss) = self.tabnet_model(feature, priors)
+            f = self.tabnet_decoder(vec)
+            loss = self.pretrain_loss_fn(label, f, S_mask)
+
+            self.pretrain_optimizer.zero_grad()
+            loss.backward()
+            self.pretrain_optimizer.step()
+
+    def pretrain_test_epoch(self, x_train):
+        train_set = torch.from_numpy(x_train.values)
+        train_set[torch.isnan(train_set)] = 0
+        indices = np.arange(len(train_set))
+
+        self.tabnet_model.eval()
+        self.tabnet_decoder.eval()
+
+        losses = []
+
+        for i in range(len(indices))[:: self.batch_size]:
+            if len(indices) - i < self.batch_size:
+                break
+
+            S_mask = torch.bernoulli(torch.empty(self.batch_size, self.d_feat).fill_(self.ps))
+            x_train_values = train_set[indices[i : i + self.batch_size]] * (1 - S_mask)
+            y_train_values = train_set[indices[i : i + self.batch_size]] * (S_mask)
+
+            feature = x_train_values.float().to(self.device)
+            label = y_train_values.float().to(self.device)
+            S_mask = S_mask.to(self.device)
+            priors = 1 - S_mask
+            with torch.no_grad():
+                (vec, sparse_loss) = self.tabnet_model(feature, priors)
+                f = self.tabnet_decoder(vec)
+
+                loss = self.pretrain_loss_fn(label, f, S_mask)
+            losses.append(loss.item())
+
+        return np.mean(losses)
+
+    def pretrain_loss_fn(self, f_hat, f, S):
+        """
+        Pretrain loss function defined in the original paper, read "Tabular self-supervised learning" in https://arxiv.org/pdf/1908.07442.pdf
+        """
+        down_mean = torch.mean(f, dim=0)
+        down = torch.sqrt(torch.sum(torch.square(f - down_mean), dim=0))
+        up = (f_hat - f) * S
+        return torch.sum(torch.square(up / down))
+
+    def loss_fn(self, pred, label):
+        mask = ~torch.isnan(label)
+        if self.loss == "mse":
+            return self.mse(pred[mask], label[mask])
+        raise ValueError("unknown loss `%s`" % self.loss)
+
+    def metric_fn(self, pred, label):
+        mask = torch.isfinite(label)
+        if self.metric in ("", "loss"):
+            return -self.loss_fn(pred[mask], label[mask])
+        raise ValueError("unknown metric `%s`" % self.metric)
+
+    def mse(self, pred, label):
+        loss = (pred - label) ** 2
+        return torch.mean(loss)
+
+
+class FinetuneModel(nn.Module):
+    """
+    FinuetuneModel for adding a layer by the end
+    """
+
+    def __init__(self, input_dim, output_dim, trained_model):
+        super().__init__()
+        self.model = trained_model
+        self.fc = nn.Linear(input_dim, output_dim)
+
+    def forward(self, x, priors):
+        return self.fc(self.model(x, priors)[0]).squeeze()  # take the vec out
+
+
+class DecoderStep(nn.Module):
+    def __init__(self, inp_dim, out_dim, shared, n_ind, vbs):
+        super().__init__()
+        self.fea_tran = FeatureTransformer(inp_dim, out_dim, shared, n_ind, vbs)
+        self.fc = nn.Linear(out_dim, out_dim)
+
+    def forward(self, x):
+        x = self.fea_tran(x)
+        return self.fc(x)
+
+
+class TabNet_Decoder(nn.Module):
+    def __init__(self, inp_dim, out_dim, n_shared, n_ind, vbs, n_steps):
+        """
+        TabNet decoder that is used in pre-training
+        """
+        super().__init__()
+        self.out_dim = out_dim
+        if n_shared > 0:
+            self.shared = nn.ModuleList()
+            self.shared.append(nn.Linear(inp_dim, 2 * out_dim))
+            for x in range(n_shared - 1):
+                self.shared.append(nn.Linear(out_dim, 2 * out_dim))  # preset the linear function we will use
+        else:
+            self.shared = None
+        self.n_steps = n_steps
+        self.steps = nn.ModuleList()
+        for x in range(n_steps):
+            self.steps.append(DecoderStep(inp_dim, out_dim, self.shared, n_ind, vbs))
+
+    def forward(self, x):
+        out = torch.zeros(x.size(0), self.out_dim).to(x.device)
+        for step in self.steps:
+            out += step(x)
+        return out
+
+
+class TabNet(nn.Module):
+    def __init__(self, inp_dim=6, out_dim=6, n_d=64, n_a=64, n_shared=2, n_ind=2, n_steps=5, relax=1.2, vbs=1024):
+        """
+        TabNet AKA the original encoder
+
+        Args:
+            n_d: dimension of the features used to calculate the final results
+            n_a: dimension of the features input to the attention transformer of the next step
+            n_shared: numbr of shared steps in feature transformer(optional)
+            n_ind: number of independent steps in feature transformer
+            n_steps: number of steps of pass through tabbet
+            relax coefficient:
+            virtual batch size:
+        """
+        super().__init__()
+
+        # set the number of shared step in feature transformer
+        if n_shared > 0:
+            self.shared = nn.ModuleList()
+            self.shared.append(nn.Linear(inp_dim, 2 * (n_d + n_a)))
+            for x in range(n_shared - 1):
+                self.shared.append(nn.Linear(n_d + n_a, 2 * (n_d + n_a)))  # preset the linear function we will use
+        else:
+            self.shared = None
+
+        self.first_step = FeatureTransformer(inp_dim, n_d + n_a, self.shared, n_ind, vbs)
+        self.steps = nn.ModuleList()
+        for x in range(n_steps - 1):
+            self.steps.append(DecisionStep(inp_dim, n_d, n_a, self.shared, n_ind, relax, vbs))
+        self.fc = nn.Linear(n_d, out_dim)
+        self.bn = nn.BatchNorm1d(inp_dim, momentum=0.01)
+        self.n_d = n_d
+
+    def forward(self, x, priors):
+        assert not torch.isnan(x).any()
+        x = self.bn(x)
+        x_a = self.first_step(x)[:, self.n_d :]
+        sparse_loss = []
+        out = torch.zeros(x.size(0), self.n_d).to(x.device)
+        for step in self.steps:
+            x_te, loss = step(x, x_a, priors)
+            out += F.relu(x_te[:, : self.n_d])  # split the feature from feat_transformer
+            x_a = x_te[:, self.n_d :]
+            sparse_loss.append(loss)
+        return self.fc(out), sum(sparse_loss)
+
+
+class GBN(nn.Module):
+    """
+    Ghost Batch Normalization
+    an efficient way of doing batch normalization
+
+    Args:
+        vbs: virtual batch size
+    """
+
+    def __init__(self, inp, vbs=1024, momentum=0.01):
+        super().__init__()
+        self.bn = nn.BatchNorm1d(inp, momentum=momentum)
+        self.vbs = vbs
+
+    def forward(self, x):
+        if x.size(0) <= self.vbs:  # can not be chunked
+            return self.bn(x)
+        else:
+            chunk = torch.chunk(x, x.size(0) // self.vbs, 0)
+            res = [self.bn(y) for y in chunk]
+            return torch.cat(res, 0)
+
+
+class GLU(nn.Module):
+    """
+    GLU block that extracts only the most essential information
+
+    Args:
+        vbs: virtual batch size
+    """
+
+    def __init__(self, inp_dim, out_dim, fc=None, vbs=1024):
+        super().__init__()
+        if fc:
+            self.fc = fc
+        else:
+            self.fc = nn.Linear(inp_dim, out_dim * 2)
+        self.bn = GBN(out_dim * 2, vbs=vbs)
+        self.od = out_dim
+
+    def forward(self, x):
+        x = self.bn(self.fc(x))
+        return torch.mul(x[:, : self.od], torch.sigmoid(x[:, self.od :]))
+
+
+class AttentionTransformer(nn.Module):
+    """
+    Args:
+        relax: relax coefficient. The greater it is, we can
+        use the same features more. When it is set to 1
+        we can use every feature only once
+    """
+
+    def __init__(self, d_a, inp_dim, relax, vbs=1024):
+        super().__init__()
+        self.fc = nn.Linear(d_a, inp_dim)
+        self.bn = GBN(inp_dim, vbs=vbs)
+        self.r = relax
+
+    # a:feature from previous decision step
+    def forward(self, a, priors):
+        a = self.bn(self.fc(a))
+        mask = SparsemaxFunction.apply(a * priors)
+        priors = priors * (self.r - mask)  # updating the prior
+        return mask
+
+
+class FeatureTransformer(nn.Module):
+    def __init__(self, inp_dim, out_dim, shared, n_ind, vbs):
+        super().__init__()
+        first = True
+        self.shared = nn.ModuleList()
+        if shared:
+            self.shared.append(GLU(inp_dim, out_dim, shared[0], vbs=vbs))
+            first = False
+            for fc in shared[1:]:
+                self.shared.append(GLU(out_dim, out_dim, fc, vbs=vbs))
+        else:
+            self.shared = None
+        self.independ = nn.ModuleList()
+        if first:
+            self.independ.append(GLU(inp_dim, out_dim, vbs=vbs))
+        for x in range(first, n_ind):
+            self.independ.append(GLU(out_dim, out_dim, vbs=vbs))
+        self.scale = float(np.sqrt(0.5))
+
+    def forward(self, x):
+        if self.shared:
+            x = self.shared[0](x)
+            for glu in self.shared[1:]:
+                x = torch.add(x, glu(x))
+                x = x * self.scale
+        for glu in self.independ:
+            x = torch.add(x, glu(x))
+            x = x * self.scale
+        return x
+
+
+class DecisionStep(nn.Module):
+    """
+    One step for the TabNet
+    """
+
+    def __init__(self, inp_dim, n_d, n_a, shared, n_ind, relax, vbs):
+        super().__init__()
+        self.atten_tran = AttentionTransformer(n_a, inp_dim, relax, vbs)
+        self.fea_tran = FeatureTransformer(inp_dim, n_d + n_a, shared, n_ind, vbs)
+
+    def forward(self, x, a, priors):
+        mask = self.atten_tran(a, priors)
+        sparse_loss = ((-1) * mask * torch.log(mask + 1e-10)).mean()
+        x = self.fea_tran(x * mask)
+        return x, sparse_loss
+
+
+def make_ix_like(input, dim=0):
+    d = input.size(dim)
+    rho = torch.arange(1, d + 1, device=input.device, dtype=input.dtype)
+    view = [1] * input.dim()
+    view[0] = -1
+    return rho.view(view).transpose(0, dim)
+
+
+class SparsemaxFunction(Function):
+    """
+    SparseMax function for replacing reLU
+    """
+
+    @staticmethod
+    def forward(ctx, input, dim=-1):
+        ctx.dim = dim
+        max_val, _ = input.max(dim=dim, keepdim=True)
+        input -= max_val  # same numerical stability trick as for softmax
+        tau, supp_size = SparsemaxFunction.threshold_and_support(input, dim=dim)
+        output = torch.clamp(input - tau, min=0)
+        ctx.save_for_backward(supp_size, output)
+        return output
+
+    @staticmethod
+    def backward(ctx, grad_output):
+        supp_size, output = ctx.saved_tensors
+        dim = ctx.dim
+        grad_input = grad_output.clone()
+        grad_input[output == 0] = 0
+
+        v_hat = grad_input.sum(dim=dim) / supp_size.to(output.dtype).squeeze()
+        v_hat = v_hat.unsqueeze(dim)
+        grad_input = torch.where(output != 0, grad_input - v_hat, grad_input)
+        return grad_input, None
+
+    @staticmethod
+    def threshold_and_support(input, dim=-1):
+        input_srt, _ = torch.sort(input, descending=True, dim=dim)
+        input_cumsum = input_srt.cumsum(dim) - 1
+        rhos = make_ix_like(input, dim)
+        support = rhos * input_srt > input_cumsum
+
+        support_size = support.sum(dim=dim).unsqueeze(dim)
+        tau = input_cumsum.gather(dim, support_size - 1)
+        tau /= support_size.to(input.dtype)
+        return tau, support_size
```

## qlib/contrib/model/pytorch_tcn.py

```diff
@@ -1,316 +1,310 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-
-from __future__ import division
-from __future__ import print_function
-
-import numpy as np
-import pandas as pd
-from typing import Text, Union
-import copy
-from ...utils import get_or_create_path
-from ...log import get_module_logger
-
-import torch
-import torch.nn as nn
-import torch.optim as optim
-
-from .pytorch_utils import count_parameters
-from ...model.base import Model
-from ...data.dataset import DatasetH
-from ...data.dataset.handler import DataHandlerLP
-from .tcn import TemporalConvNet
-
-
-class TCN(Model):
-    """TCN Model
-
-    Parameters
-    ----------
-    d_feat : int
-        input dimension for each time step
-    n_chans: int
-        number of channels
-    metric: str
-        the evaluation metric used in early stop
-    optimizer : str
-        optimizer name
-    GPU : str
-        the GPU ID(s) used for training
-    """
-
-    def __init__(
-        self,
-        d_feat=6,
-        n_chans=128,
-        kernel_size=5,
-        num_layers=5,
-        dropout=0.5,
-        n_epochs=200,
-        lr=0.0001,
-        metric="",
-        batch_size=2000,
-        early_stop=20,
-        loss="mse",
-        optimizer="adam",
-        GPU=0,
-        seed=None,
-        **kwargs
-    ):
-        # Set logger.
-        self.logger = get_module_logger("TCN")
-        self.logger.info("TCN pytorch version...")
-
-        # set hyper-parameters.
-        self.d_feat = d_feat
-        self.n_chans = n_chans
-        self.kernel_size = kernel_size
-        self.num_layers = num_layers
-        self.dropout = dropout
-        self.n_epochs = n_epochs
-        self.lr = lr
-        self.metric = metric
-        self.batch_size = batch_size
-        self.early_stop = early_stop
-        self.optimizer = optimizer.lower()
-        self.loss = loss
-        self.device = torch.device("cuda:%d" % (GPU) if torch.cuda.is_available() and GPU >= 0 else "cpu")
-        self.seed = seed
-
-        self.logger.info(
-            "TCN parameters setting:"
-            "\nd_feat : {}"
-            "\nn_chans : {}"
-            "\nkernel_size : {}"
-            "\nnum_layers : {}"
-            "\ndropout : {}"
-            "\nn_epochs : {}"
-            "\nlr : {}"
-            "\nmetric : {}"
-            "\nbatch_size : {}"
-            "\nearly_stop : {}"
-            "\noptimizer : {}"
-            "\nloss_type : {}"
-            "\nvisible_GPU : {}"
-            "\nuse_GPU : {}"
-            "\nseed : {}".format(
-                d_feat,
-                n_chans,
-                kernel_size,
-                num_layers,
-                dropout,
-                n_epochs,
-                lr,
-                metric,
-                batch_size,
-                early_stop,
-                optimizer.lower(),
-                loss,
-                GPU,
-                self.use_gpu,
-                seed,
-            )
-        )
-
-        if self.seed is not None:
-            np.random.seed(self.seed)
-            torch.manual_seed(self.seed)
-
-        self.tcn_model = TCNModel(
-            num_input=self.d_feat,
-            output_size=1,
-            num_channels=[self.n_chans] * self.num_layers,
-            kernel_size=self.kernel_size,
-            dropout=self.dropout,
-        )
-        self.logger.info("model:\n{:}".format(self.tcn_model))
-        self.logger.info("model size: {:.4f} MB".format(count_parameters(self.tcn_model)))
-
-        if optimizer.lower() == "adam":
-            self.train_optimizer = optim.Adam(self.tcn_model.parameters(), lr=self.lr)
-        elif optimizer.lower() == "gd":
-            self.train_optimizer = optim.SGD(self.tcn_model.parameters(), lr=self.lr)
-        else:
-            raise NotImplementedError("optimizer {} is not supported!".format(optimizer))
-
-        self.fitted = False
-        self.tcn_model.to(self.device)
-
-    @property
-    def use_gpu(self):
-        return self.device != torch.device("cpu")
-
-    def mse(self, pred, label):
-        loss = (pred - label) ** 2
-        return torch.mean(loss)
-
-    def loss_fn(self, pred, label):
-        mask = ~torch.isnan(label)
-
-        if self.loss == "mse":
-            return self.mse(pred[mask], label[mask])
-
-        raise ValueError("unknown loss `%s`" % self.loss)
-
-    def metric_fn(self, pred, label):
-
-        mask = torch.isfinite(label)
-
-        if self.metric in ("", "loss"):
-            return -self.loss_fn(pred[mask], label[mask])
-
-        raise ValueError("unknown metric `%s`" % self.metric)
-
-    def train_epoch(self, x_train, y_train):
-
-        x_train_values = x_train.values
-        y_train_values = np.squeeze(y_train.values)
-
-        self.tcn_model.train()
-
-        indices = np.arange(len(x_train_values))
-        np.random.shuffle(indices)
-
-        for i in range(len(indices))[:: self.batch_size]:
-
-            if len(indices) - i < self.batch_size:
-                break
-
-            feature = torch.from_numpy(x_train_values[indices[i : i + self.batch_size]]).float().to(self.device)
-            label = torch.from_numpy(y_train_values[indices[i : i + self.batch_size]]).float().to(self.device)
-
-            pred = self.tcn_model(feature)
-            loss = self.loss_fn(pred, label)
-
-            self.train_optimizer.zero_grad()
-            loss.backward()
-            torch.nn.utils.clip_grad_value_(self.tcn_model.parameters(), 3.0)
-            self.train_optimizer.step()
-
-    def test_epoch(self, data_x, data_y):
-        x_values = data_x.values
-        y_values = np.squeeze(data_y.values)
-
-        self.tcn_model.eval()
-
-        scores = []
-        losses = []
-
-        indices = np.arange(len(x_values))
-
-        for i in range(len(indices))[:: self.batch_size]:
-
-            if len(indices) - i < self.batch_size:
-                break
-
-            feature = torch.from_numpy(x_values[indices[i : i + self.batch_size]]).float().to(self.device)
-            label = torch.from_numpy(y_values[indices[i : i + self.batch_size]]).float().to(self.device)
-
-            with torch.no_grad():
-                pred = self.tcn_model(feature)
-                loss = self.loss_fn(pred, label)
-                losses.append(loss.item())
-
-                score = self.metric_fn(pred, label)
-                scores.append(score.item())
-
-        return np.mean(losses), np.mean(scores)
-
-    def fit(
-        self,
-        dataset: DatasetH,
-        evals_result=dict(),
-        save_path=None,
-    ):
-
-        df_train, df_valid, df_test = dataset.prepare(
-            ["train", "valid", "test"],
-            col_set=["feature", "label"],
-            data_key=DataHandlerLP.DK_L,
-        )
-
-        x_train, y_train = df_train["feature"], df_train["label"]
-        x_valid, y_valid = df_valid["feature"], df_valid["label"]
-
-        save_path = get_or_create_path(save_path)
-        stop_steps = 0
-        train_loss = 0
-        best_score = -np.inf
-        best_epoch = 0
-        evals_result["train"] = []
-        evals_result["valid"] = []
-
-        # train
-        self.logger.info("training...")
-        self.fitted = True
-
-        for step in range(self.n_epochs):
-            self.logger.info("Epoch%d:", step)
-            self.logger.info("training...")
-            self.train_epoch(x_train, y_train)
-            self.logger.info("evaluating...")
-            train_loss, train_score = self.test_epoch(x_train, y_train)
-            val_loss, val_score = self.test_epoch(x_valid, y_valid)
-            self.logger.info("train %.6f, valid %.6f" % (train_score, val_score))
-            evals_result["train"].append(train_score)
-            evals_result["valid"].append(val_score)
-
-            if val_score > best_score:
-                best_score = val_score
-                stop_steps = 0
-                best_epoch = step
-                best_param = copy.deepcopy(self.tcn_model.state_dict())
-            else:
-                stop_steps += 1
-                if stop_steps >= self.early_stop:
-                    self.logger.info("early stop")
-                    break
-
-        self.logger.info("best score: %.6lf @ %d" % (best_score, best_epoch))
-        self.tcn_model.load_state_dict(best_param)
-        torch.save(best_param, save_path)
-
-        if self.use_gpu:
-            torch.cuda.empty_cache()
-
-    def predict(self, dataset: DatasetH, segment: Union[Text, slice] = "test"):
-        if not self.fitted:
-            raise ValueError("model is not fitted yet!")
-
-        x_test = dataset.prepare(segment, col_set="feature", data_key=DataHandlerLP.DK_I)
-        index = x_test.index
-        self.tcn_model.eval()
-        x_values = x_test.values
-        sample_num = x_values.shape[0]
-        preds = []
-
-        for begin in range(sample_num)[:: self.batch_size]:
-
-            if sample_num - begin < self.batch_size:
-                end = sample_num
-            else:
-                end = begin + self.batch_size
-
-            x_batch = torch.from_numpy(x_values[begin:end]).float().to(self.device)
-
-            with torch.no_grad():
-                pred = self.tcn_model(x_batch).detach().cpu().numpy()
-
-            preds.append(pred)
-
-        return pd.Series(np.concatenate(preds), index=index)
-
-
-class TCNModel(nn.Module):
-    def __init__(self, num_input, output_size, num_channels, kernel_size, dropout):
-        super().__init__()
-        self.num_input = num_input
-        self.tcn = TemporalConvNet(num_input, num_channels, kernel_size, dropout=dropout)
-        self.linear = nn.Linear(num_channels[-1], output_size)
-
-    def forward(self, x):
-        x = x.reshape(x.shape[0], self.num_input, -1)
-        output = self.tcn(x)
-        output = self.linear(output[:, :, -1])
-        return output.squeeze()
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+
+from __future__ import division
+from __future__ import print_function
+
+import numpy as np
+import pandas as pd
+from typing import Text, Union
+import copy
+from ...utils import get_or_create_path
+from ...log import get_module_logger
+
+import torch
+import torch.nn as nn
+import torch.optim as optim
+
+from .pytorch_utils import count_parameters
+from ...model.base import Model
+from ...data.dataset import DatasetH
+from ...data.dataset.handler import DataHandlerLP
+from .tcn import TemporalConvNet
+
+
+class TCN(Model):
+    """TCN Model
+
+    Parameters
+    ----------
+    d_feat : int
+        input dimension for each time step
+    n_chans: int
+        number of channels
+    metric: str
+        the evaluation metric used in early stop
+    optimizer : str
+        optimizer name
+    GPU : str
+        the GPU ID(s) used for training
+    """
+
+    def __init__(
+        self,
+        d_feat=6,
+        n_chans=128,
+        kernel_size=5,
+        num_layers=5,
+        dropout=0.5,
+        n_epochs=200,
+        lr=0.0001,
+        metric="",
+        batch_size=2000,
+        early_stop=20,
+        loss="mse",
+        optimizer="adam",
+        GPU=0,
+        seed=None,
+        **kwargs
+    ):
+        # Set logger.
+        self.logger = get_module_logger("TCN")
+        self.logger.info("TCN pytorch version...")
+
+        # set hyper-parameters.
+        self.d_feat = d_feat
+        self.n_chans = n_chans
+        self.kernel_size = kernel_size
+        self.num_layers = num_layers
+        self.dropout = dropout
+        self.n_epochs = n_epochs
+        self.lr = lr
+        self.metric = metric
+        self.batch_size = batch_size
+        self.early_stop = early_stop
+        self.optimizer = optimizer.lower()
+        self.loss = loss
+        self.device = torch.device("cuda:%d" % (GPU) if torch.cuda.is_available() and GPU >= 0 else "cpu")
+        self.seed = seed
+
+        self.logger.info(
+            "TCN parameters setting:"
+            "\nd_feat : {}"
+            "\nn_chans : {}"
+            "\nkernel_size : {}"
+            "\nnum_layers : {}"
+            "\ndropout : {}"
+            "\nn_epochs : {}"
+            "\nlr : {}"
+            "\nmetric : {}"
+            "\nbatch_size : {}"
+            "\nearly_stop : {}"
+            "\noptimizer : {}"
+            "\nloss_type : {}"
+            "\nvisible_GPU : {}"
+            "\nuse_GPU : {}"
+            "\nseed : {}".format(
+                d_feat,
+                n_chans,
+                kernel_size,
+                num_layers,
+                dropout,
+                n_epochs,
+                lr,
+                metric,
+                batch_size,
+                early_stop,
+                optimizer.lower(),
+                loss,
+                GPU,
+                self.use_gpu,
+                seed,
+            )
+        )
+
+        if self.seed is not None:
+            np.random.seed(self.seed)
+            torch.manual_seed(self.seed)
+
+        self.tcn_model = TCNModel(
+            num_input=self.d_feat,
+            output_size=1,
+            num_channels=[self.n_chans] * self.num_layers,
+            kernel_size=self.kernel_size,
+            dropout=self.dropout,
+        )
+        self.logger.info("model:\n{:}".format(self.tcn_model))
+        self.logger.info("model size: {:.4f} MB".format(count_parameters(self.tcn_model)))
+
+        if optimizer.lower() == "adam":
+            self.train_optimizer = optim.Adam(self.tcn_model.parameters(), lr=self.lr)
+        elif optimizer.lower() == "gd":
+            self.train_optimizer = optim.SGD(self.tcn_model.parameters(), lr=self.lr)
+        else:
+            raise NotImplementedError("optimizer {} is not supported!".format(optimizer))
+
+        self.fitted = False
+        self.tcn_model.to(self.device)
+
+    @property
+    def use_gpu(self):
+        return self.device != torch.device("cpu")
+
+    def mse(self, pred, label):
+        loss = (pred - label) ** 2
+        return torch.mean(loss)
+
+    def loss_fn(self, pred, label):
+        mask = ~torch.isnan(label)
+
+        if self.loss == "mse":
+            return self.mse(pred[mask], label[mask])
+
+        raise ValueError("unknown loss `%s`" % self.loss)
+
+    def metric_fn(self, pred, label):
+        mask = torch.isfinite(label)
+
+        if self.metric in ("", "loss"):
+            return -self.loss_fn(pred[mask], label[mask])
+
+        raise ValueError("unknown metric `%s`" % self.metric)
+
+    def train_epoch(self, x_train, y_train):
+        x_train_values = x_train.values
+        y_train_values = np.squeeze(y_train.values)
+
+        self.tcn_model.train()
+
+        indices = np.arange(len(x_train_values))
+        np.random.shuffle(indices)
+
+        for i in range(len(indices))[:: self.batch_size]:
+            if len(indices) - i < self.batch_size:
+                break
+
+            feature = torch.from_numpy(x_train_values[indices[i : i + self.batch_size]]).float().to(self.device)
+            label = torch.from_numpy(y_train_values[indices[i : i + self.batch_size]]).float().to(self.device)
+
+            pred = self.tcn_model(feature)
+            loss = self.loss_fn(pred, label)
+
+            self.train_optimizer.zero_grad()
+            loss.backward()
+            torch.nn.utils.clip_grad_value_(self.tcn_model.parameters(), 3.0)
+            self.train_optimizer.step()
+
+    def test_epoch(self, data_x, data_y):
+        x_values = data_x.values
+        y_values = np.squeeze(data_y.values)
+
+        self.tcn_model.eval()
+
+        scores = []
+        losses = []
+
+        indices = np.arange(len(x_values))
+
+        for i in range(len(indices))[:: self.batch_size]:
+            if len(indices) - i < self.batch_size:
+                break
+
+            feature = torch.from_numpy(x_values[indices[i : i + self.batch_size]]).float().to(self.device)
+            label = torch.from_numpy(y_values[indices[i : i + self.batch_size]]).float().to(self.device)
+
+            with torch.no_grad():
+                pred = self.tcn_model(feature)
+                loss = self.loss_fn(pred, label)
+                losses.append(loss.item())
+
+                score = self.metric_fn(pred, label)
+                scores.append(score.item())
+
+        return np.mean(losses), np.mean(scores)
+
+    def fit(
+        self,
+        dataset: DatasetH,
+        evals_result=dict(),
+        save_path=None,
+    ):
+        df_train, df_valid, df_test = dataset.prepare(
+            ["train", "valid", "test"],
+            col_set=["feature", "label"],
+            data_key=DataHandlerLP.DK_L,
+        )
+
+        x_train, y_train = df_train["feature"], df_train["label"]
+        x_valid, y_valid = df_valid["feature"], df_valid["label"]
+
+        save_path = get_or_create_path(save_path)
+        stop_steps = 0
+        train_loss = 0
+        best_score = -np.inf
+        best_epoch = 0
+        evals_result["train"] = []
+        evals_result["valid"] = []
+
+        # train
+        self.logger.info("training...")
+        self.fitted = True
+
+        for step in range(self.n_epochs):
+            self.logger.info("Epoch%d:", step)
+            self.logger.info("training...")
+            self.train_epoch(x_train, y_train)
+            self.logger.info("evaluating...")
+            train_loss, train_score = self.test_epoch(x_train, y_train)
+            val_loss, val_score = self.test_epoch(x_valid, y_valid)
+            self.logger.info("train %.6f, valid %.6f" % (train_score, val_score))
+            evals_result["train"].append(train_score)
+            evals_result["valid"].append(val_score)
+
+            if val_score > best_score:
+                best_score = val_score
+                stop_steps = 0
+                best_epoch = step
+                best_param = copy.deepcopy(self.tcn_model.state_dict())
+            else:
+                stop_steps += 1
+                if stop_steps >= self.early_stop:
+                    self.logger.info("early stop")
+                    break
+
+        self.logger.info("best score: %.6lf @ %d" % (best_score, best_epoch))
+        self.tcn_model.load_state_dict(best_param)
+        torch.save(best_param, save_path)
+
+        if self.use_gpu:
+            torch.cuda.empty_cache()
+
+    def predict(self, dataset: DatasetH, segment: Union[Text, slice] = "test"):
+        if not self.fitted:
+            raise ValueError("model is not fitted yet!")
+
+        x_test = dataset.prepare(segment, col_set="feature", data_key=DataHandlerLP.DK_I)
+        index = x_test.index
+        self.tcn_model.eval()
+        x_values = x_test.values
+        sample_num = x_values.shape[0]
+        preds = []
+
+        for begin in range(sample_num)[:: self.batch_size]:
+            if sample_num - begin < self.batch_size:
+                end = sample_num
+            else:
+                end = begin + self.batch_size
+
+            x_batch = torch.from_numpy(x_values[begin:end]).float().to(self.device)
+
+            with torch.no_grad():
+                pred = self.tcn_model(x_batch).detach().cpu().numpy()
+
+            preds.append(pred)
+
+        return pd.Series(np.concatenate(preds), index=index)
+
+
+class TCNModel(nn.Module):
+    def __init__(self, num_input, output_size, num_channels, kernel_size, dropout):
+        super().__init__()
+        self.num_input = num_input
+        self.tcn = TemporalConvNet(num_input, num_channels, kernel_size, dropout=dropout)
+        self.linear = nn.Linear(num_channels[-1], output_size)
+
+    def forward(self, x):
+        x = x.reshape(x.shape[0], self.num_input, -1)
+        output = self.tcn(x)
+        output = self.linear(output[:, :, -1])
+        return output.squeeze()
```

## qlib/contrib/model/pytorch_tcn_ts.py

```diff
@@ -1,301 +1,297 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-
-from __future__ import division
-from __future__ import print_function
-
-import numpy as np
-import pandas as pd
-import copy
-from ...utils import get_or_create_path
-from ...log import get_module_logger
-
-import torch
-import torch.nn as nn
-import torch.optim as optim
-from torch.utils.data import DataLoader
-
-from .pytorch_utils import count_parameters
-from ...model.base import Model
-from ...data.dataset.handler import DataHandlerLP
-from .tcn import TemporalConvNet
-
-
-class TCN(Model):
-    """TCN Model
-
-    Parameters
-    ----------
-    d_feat : int
-        input dimension for each time step
-    metric: str
-        the evaluation metric used in early stop
-    optimizer : str
-        optimizer name
-    GPU : str
-        the GPU ID(s) used for training
-    """
-
-    def __init__(
-        self,
-        d_feat=6,
-        n_chans=128,
-        kernel_size=5,
-        num_layers=2,
-        dropout=0.0,
-        n_epochs=200,
-        lr=0.001,
-        metric="",
-        batch_size=2000,
-        early_stop=20,
-        loss="mse",
-        optimizer="adam",
-        n_jobs=10,
-        GPU=0,
-        seed=None,
-        **kwargs
-    ):
-        # Set logger.
-        self.logger = get_module_logger("TCN")
-        self.logger.info("TCN pytorch version...")
-
-        # set hyper-parameters.
-        self.d_feat = d_feat
-        self.n_chans = n_chans
-        self.kernel_size = kernel_size
-        self.num_layers = num_layers
-        self.dropout = dropout
-        self.n_epochs = n_epochs
-        self.lr = lr
-        self.metric = metric
-        self.batch_size = batch_size
-        self.early_stop = early_stop
-        self.optimizer = optimizer.lower()
-        self.loss = loss
-        self.device = torch.device("cuda:%d" % (GPU) if torch.cuda.is_available() and GPU >= 0 else "cpu")
-        self.n_jobs = n_jobs
-        self.seed = seed
-
-        self.logger.info(
-            "TCN parameters setting:"
-            "\nd_feat : {}"
-            "\nn_chans : {}"
-            "\nkernel_size : {}"
-            "\nnum_layers : {}"
-            "\ndropout : {}"
-            "\nn_epochs : {}"
-            "\nlr : {}"
-            "\nmetric : {}"
-            "\nbatch_size : {}"
-            "\nearly_stop : {}"
-            "\noptimizer : {}"
-            "\nloss_type : {}"
-            "\ndevice : {}"
-            "\nn_jobs : {}"
-            "\nuse_GPU : {}"
-            "\nseed : {}".format(
-                d_feat,
-                n_chans,
-                kernel_size,
-                num_layers,
-                dropout,
-                n_epochs,
-                lr,
-                metric,
-                batch_size,
-                early_stop,
-                optimizer.lower(),
-                loss,
-                self.device,
-                n_jobs,
-                self.use_gpu,
-                seed,
-            )
-        )
-
-        if self.seed is not None:
-            np.random.seed(self.seed)
-            torch.manual_seed(self.seed)
-
-        self.TCN_model = TCNModel(
-            num_input=self.d_feat,
-            output_size=1,
-            num_channels=[self.n_chans] * self.num_layers,
-            kernel_size=self.kernel_size,
-            dropout=self.dropout,
-        )
-        self.logger.info("model:\n{:}".format(self.TCN_model))
-        self.logger.info("model size: {:.4f} MB".format(count_parameters(self.TCN_model)))
-
-        if optimizer.lower() == "adam":
-            self.train_optimizer = optim.Adam(self.TCN_model.parameters(), lr=self.lr)
-        elif optimizer.lower() == "gd":
-            self.train_optimizer = optim.SGD(self.TCN_model.parameters(), lr=self.lr)
-        else:
-            raise NotImplementedError("optimizer {} is not supported!".format(optimizer))
-
-        self.fitted = False
-        self.TCN_model.to(self.device)
-
-    @property
-    def use_gpu(self):
-        return self.device != torch.device("cpu")
-
-    def mse(self, pred, label):
-        loss = (pred - label) ** 2
-        return torch.mean(loss)
-
-    def loss_fn(self, pred, label):
-        mask = ~torch.isnan(label)
-
-        if self.loss == "mse":
-            return self.mse(pred[mask], label[mask])
-
-        raise ValueError("unknown loss `%s`" % self.loss)
-
-    def metric_fn(self, pred, label):
-
-        mask = torch.isfinite(label)
-
-        if self.metric in ("", "loss"):
-            return -self.loss_fn(pred[mask], label[mask])
-
-        raise ValueError("unknown metric `%s`" % self.metric)
-
-    def train_epoch(self, data_loader):
-
-        self.TCN_model.train()
-
-        for data in data_loader:
-            data = torch.transpose(data, 1, 2)
-            feature = data[:, 0:-1, :].to(self.device)
-            label = data[:, -1, -1].to(self.device)
-
-            pred = self.TCN_model(feature.float())
-            loss = self.loss_fn(pred, label)
-
-            self.train_optimizer.zero_grad()
-            loss.backward()
-            torch.nn.utils.clip_grad_value_(self.TCN_model.parameters(), 3.0)
-            self.train_optimizer.step()
-
-    def test_epoch(self, data_loader):
-
-        self.TCN_model.eval()
-
-        scores = []
-        losses = []
-
-        for data in data_loader:
-            data = torch.transpose(data, 1, 2)
-            feature = data[:, 0:-1, :].to(self.device)
-            # feature[torch.isnan(feature)] = 0
-            label = data[:, -1, -1].to(self.device)
-
-            with torch.no_grad():
-                pred = self.TCN_model(feature.float())
-                loss = self.loss_fn(pred, label)
-                losses.append(loss.item())
-
-                score = self.metric_fn(pred, label)
-                scores.append(score.item())
-
-        return np.mean(losses), np.mean(scores)
-
-    def fit(
-        self,
-        dataset,
-        evals_result=dict(),
-        save_path=None,
-    ):
-        dl_train = dataset.prepare("train", col_set=["feature", "label"], data_key=DataHandlerLP.DK_L)
-        dl_valid = dataset.prepare("valid", col_set=["feature", "label"], data_key=DataHandlerLP.DK_L)
-
-        # process nan brought by dataloader
-        dl_train.config(fillna_type="ffill+bfill")
-        # process nan brought by dataloader
-        dl_valid.config(fillna_type="ffill+bfill")
-
-        train_loader = DataLoader(
-            dl_train, batch_size=self.batch_size, shuffle=True, num_workers=self.n_jobs, drop_last=True
-        )
-        valid_loader = DataLoader(
-            dl_valid, batch_size=self.batch_size, shuffle=False, num_workers=self.n_jobs, drop_last=True
-        )
-
-        save_path = get_or_create_path(save_path)
-
-        stop_steps = 0
-        train_loss = 0
-        best_score = -np.inf
-        best_epoch = 0
-        evals_result["train"] = []
-        evals_result["valid"] = []
-
-        # train
-        self.logger.info("training...")
-        self.fitted = True
-
-        for step in range(self.n_epochs):
-            self.logger.info("Epoch%d:", step)
-            self.logger.info("training...")
-            self.train_epoch(train_loader)
-            self.logger.info("evaluating...")
-            train_loss, train_score = self.test_epoch(train_loader)
-            val_loss, val_score = self.test_epoch(valid_loader)
-            self.logger.info("train %.6f, valid %.6f" % (train_score, val_score))
-            evals_result["train"].append(train_score)
-            evals_result["valid"].append(val_score)
-
-            if val_score > best_score:
-                best_score = val_score
-                stop_steps = 0
-                best_epoch = step
-                best_param = copy.deepcopy(self.TCN_model.state_dict())
-            else:
-                stop_steps += 1
-                if stop_steps >= self.early_stop:
-                    self.logger.info("early stop")
-                    break
-
-        self.logger.info("best score: %.6lf @ %d" % (best_score, best_epoch))
-        self.TCN_model.load_state_dict(best_param)
-        torch.save(best_param, save_path)
-
-        if self.use_gpu:
-            torch.cuda.empty_cache()
-
-    def predict(self, dataset):
-        if not self.fitted:
-            raise ValueError("model is not fitted yet!")
-
-        dl_test = dataset.prepare("test", col_set=["feature", "label"], data_key=DataHandlerLP.DK_I)
-        dl_test.config(fillna_type="ffill+bfill")
-        test_loader = DataLoader(dl_test, batch_size=self.batch_size, num_workers=self.n_jobs)
-        self.TCN_model.eval()
-        preds = []
-
-        for data in test_loader:
-
-            feature = data[:, :, 0:-1].to(self.device)
-
-            with torch.no_grad():
-                pred = self.TCN_model(feature.float()).detach().cpu().numpy()
-
-            preds.append(pred)
-
-        return pd.Series(np.concatenate(preds), index=dl_test.get_index())
-
-
-class TCNModel(nn.Module):
-    def __init__(self, num_input, output_size, num_channels, kernel_size, dropout):
-        super().__init__()
-        self.num_input = num_input
-        self.tcn = TemporalConvNet(num_input, num_channels, kernel_size, dropout=dropout)
-        self.linear = nn.Linear(num_channels[-1], output_size)
-
-    def forward(self, x):
-        output = self.tcn(x)
-        output = self.linear(output[:, :, -1])
-        return output.squeeze()
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+
+from __future__ import division
+from __future__ import print_function
+
+import numpy as np
+import pandas as pd
+import copy
+from ...utils import get_or_create_path
+from ...log import get_module_logger
+
+import torch
+import torch.nn as nn
+import torch.optim as optim
+from torch.utils.data import DataLoader
+
+from .pytorch_utils import count_parameters
+from ...model.base import Model
+from ...data.dataset.handler import DataHandlerLP
+from .tcn import TemporalConvNet
+
+
+class TCN(Model):
+    """TCN Model
+
+    Parameters
+    ----------
+    d_feat : int
+        input dimension for each time step
+    metric: str
+        the evaluation metric used in early stop
+    optimizer : str
+        optimizer name
+    GPU : str
+        the GPU ID(s) used for training
+    """
+
+    def __init__(
+        self,
+        d_feat=6,
+        n_chans=128,
+        kernel_size=5,
+        num_layers=2,
+        dropout=0.0,
+        n_epochs=200,
+        lr=0.001,
+        metric="",
+        batch_size=2000,
+        early_stop=20,
+        loss="mse",
+        optimizer="adam",
+        n_jobs=10,
+        GPU=0,
+        seed=None,
+        **kwargs
+    ):
+        # Set logger.
+        self.logger = get_module_logger("TCN")
+        self.logger.info("TCN pytorch version...")
+
+        # set hyper-parameters.
+        self.d_feat = d_feat
+        self.n_chans = n_chans
+        self.kernel_size = kernel_size
+        self.num_layers = num_layers
+        self.dropout = dropout
+        self.n_epochs = n_epochs
+        self.lr = lr
+        self.metric = metric
+        self.batch_size = batch_size
+        self.early_stop = early_stop
+        self.optimizer = optimizer.lower()
+        self.loss = loss
+        self.device = torch.device("cuda:%d" % (GPU) if torch.cuda.is_available() and GPU >= 0 else "cpu")
+        self.n_jobs = n_jobs
+        self.seed = seed
+
+        self.logger.info(
+            "TCN parameters setting:"
+            "\nd_feat : {}"
+            "\nn_chans : {}"
+            "\nkernel_size : {}"
+            "\nnum_layers : {}"
+            "\ndropout : {}"
+            "\nn_epochs : {}"
+            "\nlr : {}"
+            "\nmetric : {}"
+            "\nbatch_size : {}"
+            "\nearly_stop : {}"
+            "\noptimizer : {}"
+            "\nloss_type : {}"
+            "\ndevice : {}"
+            "\nn_jobs : {}"
+            "\nuse_GPU : {}"
+            "\nseed : {}".format(
+                d_feat,
+                n_chans,
+                kernel_size,
+                num_layers,
+                dropout,
+                n_epochs,
+                lr,
+                metric,
+                batch_size,
+                early_stop,
+                optimizer.lower(),
+                loss,
+                self.device,
+                n_jobs,
+                self.use_gpu,
+                seed,
+            )
+        )
+
+        if self.seed is not None:
+            np.random.seed(self.seed)
+            torch.manual_seed(self.seed)
+
+        self.TCN_model = TCNModel(
+            num_input=self.d_feat,
+            output_size=1,
+            num_channels=[self.n_chans] * self.num_layers,
+            kernel_size=self.kernel_size,
+            dropout=self.dropout,
+        )
+        self.logger.info("model:\n{:}".format(self.TCN_model))
+        self.logger.info("model size: {:.4f} MB".format(count_parameters(self.TCN_model)))
+
+        if optimizer.lower() == "adam":
+            self.train_optimizer = optim.Adam(self.TCN_model.parameters(), lr=self.lr)
+        elif optimizer.lower() == "gd":
+            self.train_optimizer = optim.SGD(self.TCN_model.parameters(), lr=self.lr)
+        else:
+            raise NotImplementedError("optimizer {} is not supported!".format(optimizer))
+
+        self.fitted = False
+        self.TCN_model.to(self.device)
+
+    @property
+    def use_gpu(self):
+        return self.device != torch.device("cpu")
+
+    def mse(self, pred, label):
+        loss = (pred - label) ** 2
+        return torch.mean(loss)
+
+    def loss_fn(self, pred, label):
+        mask = ~torch.isnan(label)
+
+        if self.loss == "mse":
+            return self.mse(pred[mask], label[mask])
+
+        raise ValueError("unknown loss `%s`" % self.loss)
+
+    def metric_fn(self, pred, label):
+        mask = torch.isfinite(label)
+
+        if self.metric in ("", "loss"):
+            return -self.loss_fn(pred[mask], label[mask])
+
+        raise ValueError("unknown metric `%s`" % self.metric)
+
+    def train_epoch(self, data_loader):
+        self.TCN_model.train()
+
+        for data in data_loader:
+            data = torch.transpose(data, 1, 2)
+            feature = data[:, 0:-1, :].to(self.device)
+            label = data[:, -1, -1].to(self.device)
+
+            pred = self.TCN_model(feature.float())
+            loss = self.loss_fn(pred, label)
+
+            self.train_optimizer.zero_grad()
+            loss.backward()
+            torch.nn.utils.clip_grad_value_(self.TCN_model.parameters(), 3.0)
+            self.train_optimizer.step()
+
+    def test_epoch(self, data_loader):
+        self.TCN_model.eval()
+
+        scores = []
+        losses = []
+
+        for data in data_loader:
+            data = torch.transpose(data, 1, 2)
+            feature = data[:, 0:-1, :].to(self.device)
+            # feature[torch.isnan(feature)] = 0
+            label = data[:, -1, -1].to(self.device)
+
+            with torch.no_grad():
+                pred = self.TCN_model(feature.float())
+                loss = self.loss_fn(pred, label)
+                losses.append(loss.item())
+
+                score = self.metric_fn(pred, label)
+                scores.append(score.item())
+
+        return np.mean(losses), np.mean(scores)
+
+    def fit(
+        self,
+        dataset,
+        evals_result=dict(),
+        save_path=None,
+    ):
+        dl_train = dataset.prepare("train", col_set=["feature", "label"], data_key=DataHandlerLP.DK_L)
+        dl_valid = dataset.prepare("valid", col_set=["feature", "label"], data_key=DataHandlerLP.DK_L)
+
+        # process nan brought by dataloader
+        dl_train.config(fillna_type="ffill+bfill")
+        # process nan brought by dataloader
+        dl_valid.config(fillna_type="ffill+bfill")
+
+        train_loader = DataLoader(
+            dl_train, batch_size=self.batch_size, shuffle=True, num_workers=self.n_jobs, drop_last=True
+        )
+        valid_loader = DataLoader(
+            dl_valid, batch_size=self.batch_size, shuffle=False, num_workers=self.n_jobs, drop_last=True
+        )
+
+        save_path = get_or_create_path(save_path)
+
+        stop_steps = 0
+        train_loss = 0
+        best_score = -np.inf
+        best_epoch = 0
+        evals_result["train"] = []
+        evals_result["valid"] = []
+
+        # train
+        self.logger.info("training...")
+        self.fitted = True
+
+        for step in range(self.n_epochs):
+            self.logger.info("Epoch%d:", step)
+            self.logger.info("training...")
+            self.train_epoch(train_loader)
+            self.logger.info("evaluating...")
+            train_loss, train_score = self.test_epoch(train_loader)
+            val_loss, val_score = self.test_epoch(valid_loader)
+            self.logger.info("train %.6f, valid %.6f" % (train_score, val_score))
+            evals_result["train"].append(train_score)
+            evals_result["valid"].append(val_score)
+
+            if val_score > best_score:
+                best_score = val_score
+                stop_steps = 0
+                best_epoch = step
+                best_param = copy.deepcopy(self.TCN_model.state_dict())
+            else:
+                stop_steps += 1
+                if stop_steps >= self.early_stop:
+                    self.logger.info("early stop")
+                    break
+
+        self.logger.info("best score: %.6lf @ %d" % (best_score, best_epoch))
+        self.TCN_model.load_state_dict(best_param)
+        torch.save(best_param, save_path)
+
+        if self.use_gpu:
+            torch.cuda.empty_cache()
+
+    def predict(self, dataset):
+        if not self.fitted:
+            raise ValueError("model is not fitted yet!")
+
+        dl_test = dataset.prepare("test", col_set=["feature", "label"], data_key=DataHandlerLP.DK_I)
+        dl_test.config(fillna_type="ffill+bfill")
+        test_loader = DataLoader(dl_test, batch_size=self.batch_size, num_workers=self.n_jobs)
+        self.TCN_model.eval()
+        preds = []
+
+        for data in test_loader:
+            feature = data[:, :, 0:-1].to(self.device)
+
+            with torch.no_grad():
+                pred = self.TCN_model(feature.float()).detach().cpu().numpy()
+
+            preds.append(pred)
+
+        return pd.Series(np.concatenate(preds), index=dl_test.get_index())
+
+
+class TCNModel(nn.Module):
+    def __init__(self, num_input, output_size, num_channels, kernel_size, dropout):
+        super().__init__()
+        self.num_input = num_input
+        self.tcn = TemporalConvNet(num_input, num_channels, kernel_size, dropout=dropout)
+        self.linear = nn.Linear(num_channels[-1], output_size)
+
+    def forward(self, x):
+        output = self.tcn(x)
+        output = self.linear(output[:, :, -1])
+        return output.squeeze()
```

## qlib/contrib/model/pytorch_tcts.py

```diff
@@ -1,431 +1,424 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-
-from __future__ import division
-from __future__ import print_function
-
-import numpy as np
-import pandas as pd
-import copy
-import random
-from ...utils import get_or_create_path
-from ...log import get_module_logger
-
-import torch
-import torch.nn as nn
-import torch.optim as optim
-
-from ...model.base import Model
-from ...data.dataset import DatasetH
-from ...data.dataset.handler import DataHandlerLP
-
-
-class TCTS(Model):
-    """TCTS Model
-
-    Parameters
-    ----------
-    d_feat : int
-        input dimension for each time step
-    metric: str
-        the evaluation metric used in early stop
-    optimizer : str
-        optimizer name
-    GPU : str
-        the GPU ID(s) used for training
-    """
-
-    def __init__(
-        self,
-        d_feat=6,
-        hidden_size=64,
-        num_layers=2,
-        dropout=0.0,
-        n_epochs=200,
-        batch_size=2000,
-        early_stop=20,
-        loss="mse",
-        fore_optimizer="adam",
-        weight_optimizer="adam",
-        input_dim=360,
-        output_dim=5,
-        fore_lr=5e-7,
-        weight_lr=5e-7,
-        steps=3,
-        GPU=0,
-        target_label=0,
-        mode="soft",
-        seed=None,
-        lowest_valid_performance=0.993,
-        **kwargs
-    ):
-        # Set logger.
-        self.logger = get_module_logger("TCTS")
-        self.logger.info("TCTS pytorch version...")
-
-        # set hyper-parameters.
-        self.d_feat = d_feat
-        self.hidden_size = hidden_size
-        self.num_layers = num_layers
-        self.dropout = dropout
-        self.n_epochs = n_epochs
-        self.batch_size = batch_size
-        self.early_stop = early_stop
-        self.loss = loss
-        self.device = torch.device("cuda:%d" % (GPU) if torch.cuda.is_available() else "cpu")
-        self.use_gpu = torch.cuda.is_available()
-        self.seed = seed
-        self.input_dim = input_dim
-        self.output_dim = output_dim
-        self.fore_lr = fore_lr
-        self.weight_lr = weight_lr
-        self.steps = steps
-        self.target_label = target_label
-        self.mode = mode
-        self.lowest_valid_performance = lowest_valid_performance
-        self._fore_optimizer = fore_optimizer
-        self._weight_optimizer = weight_optimizer
-
-        self.logger.info(
-            "TCTS parameters setting:"
-            "\nd_feat : {}"
-            "\nhidden_size : {}"
-            "\nnum_layers : {}"
-            "\ndropout : {}"
-            "\nn_epochs : {}"
-            "\nbatch_size : {}"
-            "\nearly_stop : {}"
-            "\ntarget_label : {}"
-            "\nmode : {}"
-            "\nloss_type : {}"
-            "\nvisible_GPU : {}"
-            "\nuse_GPU : {}"
-            "\nseed : {}".format(
-                d_feat,
-                hidden_size,
-                num_layers,
-                dropout,
-                n_epochs,
-                batch_size,
-                early_stop,
-                target_label,
-                mode,
-                loss,
-                GPU,
-                self.use_gpu,
-                seed,
-            )
-        )
-
-    def loss_fn(self, pred, label, weight):
-
-        if self.mode == "hard":
-            loc = torch.argmax(weight, 1)
-            loss = (pred - label[np.arange(weight.shape[0]), loc]) ** 2
-            return torch.mean(loss)
-
-        elif self.mode == "soft":
-            loss = (pred - label.transpose(0, 1)) ** 2
-            return torch.mean(loss * weight.transpose(0, 1))
-
-        else:
-            raise NotImplementedError("mode {} is not supported!".format(self.mode))
-
-    def train_epoch(self, x_train, y_train, x_valid, y_valid):
-        x_train_values = x_train.values
-        y_train_values = np.squeeze(y_train.values)
-
-        indices = np.arange(len(x_train_values))
-        np.random.shuffle(indices)
-
-        task_embedding = torch.zeros([self.batch_size, self.output_dim])
-        task_embedding[:, self.target_label] = 1
-        task_embedding = task_embedding.to(self.device)
-
-        init_fore_model = copy.deepcopy(self.fore_model)
-        for p in init_fore_model.parameters():
-            p.requires_grad = False
-
-        self.fore_model.train()
-        self.weight_model.train()
-
-        for p in self.weight_model.parameters():
-            p.requires_grad = False
-        for p in self.fore_model.parameters():
-            p.requires_grad = True
-
-        for i in range(self.steps):
-            for i in range(len(indices))[:: self.batch_size]:
-
-                if len(indices) - i < self.batch_size:
-                    break
-
-                feature = torch.from_numpy(x_train_values[indices[i : i + self.batch_size]]).float().to(self.device)
-                label = torch.from_numpy(y_train_values[indices[i : i + self.batch_size]]).float().to(self.device)
-
-                init_pred = init_fore_model(feature)
-                pred = self.fore_model(feature)
-                dis = init_pred - label.transpose(0, 1)
-                weight_feature = torch.cat(
-                    (feature, dis.transpose(0, 1), label, init_pred.view(-1, 1), task_embedding), 1
-                )
-                weight = self.weight_model(weight_feature)
-
-                loss = self.loss_fn(pred, label, weight)
-
-                self.fore_optimizer.zero_grad()
-                loss.backward()
-                torch.nn.utils.clip_grad_value_(self.fore_model.parameters(), 3.0)
-                self.fore_optimizer.step()
-
-        x_valid_values = x_valid.values
-        y_valid_values = np.squeeze(y_valid.values)
-
-        indices = np.arange(len(x_valid_values))
-        np.random.shuffle(indices)
-        for p in self.weight_model.parameters():
-            p.requires_grad = True
-        for p in self.fore_model.parameters():
-            p.requires_grad = False
-
-        # fix forecasting model and valid weight model
-        for i in range(len(indices))[:: self.batch_size]:
-
-            if len(indices) - i < self.batch_size:
-                break
-
-            feature = torch.from_numpy(x_valid_values[indices[i : i + self.batch_size]]).float().to(self.device)
-            label = torch.from_numpy(y_valid_values[indices[i : i + self.batch_size]]).float().to(self.device)
-
-            pred = self.fore_model(feature)
-            dis = pred - label.transpose(0, 1)
-            weight_feature = torch.cat((feature, dis.transpose(0, 1), label, pred.view(-1, 1), task_embedding), 1)
-            weight = self.weight_model(weight_feature)
-            loc = torch.argmax(weight, 1)
-            valid_loss = torch.mean((pred - label[:, abs(self.target_label)]) ** 2)
-            loss = torch.mean(valid_loss * torch.log(weight[np.arange(weight.shape[0]), loc]))
-
-            self.weight_optimizer.zero_grad()
-            loss.backward()
-            torch.nn.utils.clip_grad_value_(self.weight_model.parameters(), 3.0)
-            self.weight_optimizer.step()
-
-    def test_epoch(self, data_x, data_y):
-
-        # prepare training data
-        x_values = data_x.values
-        y_values = np.squeeze(data_y.values)
-
-        self.fore_model.eval()
-
-        losses = []
-
-        indices = np.arange(len(x_values))
-
-        for i in range(len(indices))[:: self.batch_size]:
-
-            if len(indices) - i < self.batch_size:
-                break
-
-            feature = torch.from_numpy(x_values[indices[i : i + self.batch_size]]).float().to(self.device)
-            label = torch.from_numpy(y_values[indices[i : i + self.batch_size]]).float().to(self.device)
-
-            pred = self.fore_model(feature)
-            loss = torch.mean((pred - label[:, abs(self.target_label)]) ** 2)
-            losses.append(loss.item())
-
-        return np.mean(losses)
-
-    def fit(
-        self,
-        dataset: DatasetH,
-        verbose=True,
-        save_path=None,
-    ):
-        df_train, df_valid, df_test = dataset.prepare(
-            ["train", "valid", "test"],
-            col_set=["feature", "label"],
-            data_key=DataHandlerLP.DK_L,
-        )
-        if df_train.empty or df_valid.empty:
-            raise ValueError("Empty data from dataset, please check your dataset config.")
-
-        x_train, y_train = df_train["feature"], df_train["label"]
-        x_valid, y_valid = df_valid["feature"], df_valid["label"]
-        x_test, y_test = df_test["feature"], df_test["label"]
-
-        if save_path is None:
-            save_path = get_or_create_path(save_path)
-        best_loss = np.inf
-        while best_loss > self.lowest_valid_performance:
-            if best_loss < np.inf:
-                print("Failed! Start retraining.")
-                self.seed = random.randint(0, 1000)  # reset random seed
-
-            if self.seed is not None:
-                np.random.seed(self.seed)
-                torch.manual_seed(self.seed)
-
-            best_loss = self.training(
-                x_train, y_train, x_valid, y_valid, x_test, y_test, verbose=verbose, save_path=save_path
-            )
-
-    def training(
-        self,
-        x_train,
-        y_train,
-        x_valid,
-        y_valid,
-        x_test,
-        y_test,
-        verbose=True,
-        save_path=None,
-    ):
-
-        self.fore_model = GRUModel(
-            d_feat=self.d_feat,
-            hidden_size=self.hidden_size,
-            num_layers=self.num_layers,
-            dropout=self.dropout,
-        )
-        self.weight_model = MLPModel(
-            d_feat=self.input_dim + 3 * self.output_dim + 1,
-            hidden_size=self.hidden_size,
-            num_layers=self.num_layers,
-            dropout=self.dropout,
-            output_dim=self.output_dim,
-        )
-        if self._fore_optimizer.lower() == "adam":
-            self.fore_optimizer = optim.Adam(self.fore_model.parameters(), lr=self.fore_lr)
-        elif self._fore_optimizer.lower() == "gd":
-            self.fore_optimizer = optim.SGD(self.fore_model.parameters(), lr=self.fore_lr)
-        else:
-            raise NotImplementedError("optimizer {} is not supported!".format(self._fore_optimizer))
-        if self._weight_optimizer.lower() == "adam":
-            self.weight_optimizer = optim.Adam(self.weight_model.parameters(), lr=self.weight_lr)
-        elif self._weight_optimizer.lower() == "gd":
-            self.weight_optimizer = optim.SGD(self.weight_model.parameters(), lr=self.weight_lr)
-        else:
-            raise NotImplementedError("optimizer {} is not supported!".format(self._weight_optimizer))
-
-        self.fitted = False
-        self.fore_model.to(self.device)
-        self.weight_model.to(self.device)
-
-        best_loss = np.inf
-        best_epoch = 0
-        stop_round = 0
-
-        for epoch in range(self.n_epochs):
-            print("Epoch:", epoch)
-
-            print("training...")
-            self.train_epoch(x_train, y_train, x_valid, y_valid)
-            print("evaluating...")
-            val_loss = self.test_epoch(x_valid, y_valid)
-            test_loss = self.test_epoch(x_test, y_test)
-
-            if verbose:
-                print("valid %.6f, test %.6f" % (val_loss, test_loss))
-
-            if val_loss < best_loss:
-                best_loss = val_loss
-                stop_round = 0
-                best_epoch = epoch
-                torch.save(copy.deepcopy(self.fore_model.state_dict()), save_path + "_fore_model.bin")
-                torch.save(copy.deepcopy(self.weight_model.state_dict()), save_path + "_weight_model.bin")
-
-            else:
-                stop_round += 1
-                if stop_round >= self.early_stop:
-                    print("early stop")
-                    break
-
-        print("best loss:", best_loss, "@", best_epoch)
-        best_param = torch.load(save_path + "_fore_model.bin", map_location=self.device)
-        self.fore_model.load_state_dict(best_param)
-        best_param = torch.load(save_path + "_weight_model.bin", map_location=self.device)
-        self.weight_model.load_state_dict(best_param)
-        self.fitted = True
-
-        if self.use_gpu:
-            torch.cuda.empty_cache()
-
-        return best_loss
-
-    def predict(self, dataset):
-        if not self.fitted:
-            raise ValueError("model is not fitted yet!")
-
-        x_test = dataset.prepare("test", col_set="feature")
-        index = x_test.index
-        self.fore_model.eval()
-        x_values = x_test.values
-        sample_num = x_values.shape[0]
-        preds = []
-
-        for begin in range(sample_num)[:: self.batch_size]:
-
-            if sample_num - begin < self.batch_size:
-                end = sample_num
-            else:
-                end = begin + self.batch_size
-
-            x_batch = torch.from_numpy(x_values[begin:end]).float().to(self.device)
-
-            with torch.no_grad():
-                if self.use_gpu:
-                    pred = self.fore_model(x_batch).detach().cpu().numpy()
-                else:
-                    pred = self.fore_model(x_batch).detach().numpy()
-
-            preds.append(pred)
-
-        return pd.Series(np.concatenate(preds), index=index)
-
-
-class MLPModel(nn.Module):
-    def __init__(self, d_feat, hidden_size=256, num_layers=3, dropout=0.0, output_dim=1):
-        super().__init__()
-
-        self.mlp = nn.Sequential()
-        self.softmax = nn.Softmax(dim=1)
-
-        for i in range(num_layers):
-            if i > 0:
-                self.mlp.add_module("drop_%d" % i, nn.Dropout(dropout))
-            self.mlp.add_module("fc_%d" % i, nn.Linear(d_feat if i == 0 else hidden_size, hidden_size))
-            self.mlp.add_module("relu_%d" % i, nn.ReLU())
-
-        self.mlp.add_module("fc_out", nn.Linear(hidden_size, output_dim))
-
-    def forward(self, x):
-        # feature
-        # [N, F]
-        out = self.mlp(x).squeeze()
-        out = self.softmax(out)
-        return out
-
-
-class GRUModel(nn.Module):
-    def __init__(self, d_feat=6, hidden_size=64, num_layers=2, dropout=0.0):
-        super().__init__()
-
-        self.rnn = nn.GRU(
-            input_size=d_feat,
-            hidden_size=hidden_size,
-            num_layers=num_layers,
-            batch_first=True,
-            dropout=dropout,
-        )
-        self.fc_out = nn.Linear(hidden_size, 1)
-
-        self.d_feat = d_feat
-
-    def forward(self, x):
-        # x: [N, F*T]
-        x = x.reshape(len(x), self.d_feat, -1)  # [N, F, T]
-        x = x.permute(0, 2, 1)  # [N, T, F]
-        out, _ = self.rnn(x)
-        return self.fc_out(out[:, -1, :]).squeeze()
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+
+from __future__ import division
+from __future__ import print_function
+
+import numpy as np
+import pandas as pd
+import copy
+import random
+from ...utils import get_or_create_path
+from ...log import get_module_logger
+
+import torch
+import torch.nn as nn
+import torch.optim as optim
+
+from ...model.base import Model
+from ...data.dataset import DatasetH
+from ...data.dataset.handler import DataHandlerLP
+
+
+class TCTS(Model):
+    """TCTS Model
+
+    Parameters
+    ----------
+    d_feat : int
+        input dimension for each time step
+    metric: str
+        the evaluation metric used in early stop
+    optimizer : str
+        optimizer name
+    GPU : str
+        the GPU ID(s) used for training
+    """
+
+    def __init__(
+        self,
+        d_feat=6,
+        hidden_size=64,
+        num_layers=2,
+        dropout=0.0,
+        n_epochs=200,
+        batch_size=2000,
+        early_stop=20,
+        loss="mse",
+        fore_optimizer="adam",
+        weight_optimizer="adam",
+        input_dim=360,
+        output_dim=5,
+        fore_lr=5e-7,
+        weight_lr=5e-7,
+        steps=3,
+        GPU=0,
+        target_label=0,
+        mode="soft",
+        seed=None,
+        lowest_valid_performance=0.993,
+        **kwargs
+    ):
+        # Set logger.
+        self.logger = get_module_logger("TCTS")
+        self.logger.info("TCTS pytorch version...")
+
+        # set hyper-parameters.
+        self.d_feat = d_feat
+        self.hidden_size = hidden_size
+        self.num_layers = num_layers
+        self.dropout = dropout
+        self.n_epochs = n_epochs
+        self.batch_size = batch_size
+        self.early_stop = early_stop
+        self.loss = loss
+        self.device = torch.device("cuda:%d" % (GPU) if torch.cuda.is_available() else "cpu")
+        self.use_gpu = torch.cuda.is_available()
+        self.seed = seed
+        self.input_dim = input_dim
+        self.output_dim = output_dim
+        self.fore_lr = fore_lr
+        self.weight_lr = weight_lr
+        self.steps = steps
+        self.target_label = target_label
+        self.mode = mode
+        self.lowest_valid_performance = lowest_valid_performance
+        self._fore_optimizer = fore_optimizer
+        self._weight_optimizer = weight_optimizer
+
+        self.logger.info(
+            "TCTS parameters setting:"
+            "\nd_feat : {}"
+            "\nhidden_size : {}"
+            "\nnum_layers : {}"
+            "\ndropout : {}"
+            "\nn_epochs : {}"
+            "\nbatch_size : {}"
+            "\nearly_stop : {}"
+            "\ntarget_label : {}"
+            "\nmode : {}"
+            "\nloss_type : {}"
+            "\nvisible_GPU : {}"
+            "\nuse_GPU : {}"
+            "\nseed : {}".format(
+                d_feat,
+                hidden_size,
+                num_layers,
+                dropout,
+                n_epochs,
+                batch_size,
+                early_stop,
+                target_label,
+                mode,
+                loss,
+                GPU,
+                self.use_gpu,
+                seed,
+            )
+        )
+
+    def loss_fn(self, pred, label, weight):
+        if self.mode == "hard":
+            loc = torch.argmax(weight, 1)
+            loss = (pred - label[np.arange(weight.shape[0]), loc]) ** 2
+            return torch.mean(loss)
+
+        elif self.mode == "soft":
+            loss = (pred - label.transpose(0, 1)) ** 2
+            return torch.mean(loss * weight.transpose(0, 1))
+
+        else:
+            raise NotImplementedError("mode {} is not supported!".format(self.mode))
+
+    def train_epoch(self, x_train, y_train, x_valid, y_valid):
+        x_train_values = x_train.values
+        y_train_values = np.squeeze(y_train.values)
+
+        indices = np.arange(len(x_train_values))
+        np.random.shuffle(indices)
+
+        task_embedding = torch.zeros([self.batch_size, self.output_dim])
+        task_embedding[:, self.target_label] = 1
+        task_embedding = task_embedding.to(self.device)
+
+        init_fore_model = copy.deepcopy(self.fore_model)
+        for p in init_fore_model.parameters():
+            p.requires_grad = False
+
+        self.fore_model.train()
+        self.weight_model.train()
+
+        for p in self.weight_model.parameters():
+            p.requires_grad = False
+        for p in self.fore_model.parameters():
+            p.requires_grad = True
+
+        for i in range(self.steps):
+            for i in range(len(indices))[:: self.batch_size]:
+                if len(indices) - i < self.batch_size:
+                    break
+
+                feature = torch.from_numpy(x_train_values[indices[i : i + self.batch_size]]).float().to(self.device)
+                label = torch.from_numpy(y_train_values[indices[i : i + self.batch_size]]).float().to(self.device)
+
+                init_pred = init_fore_model(feature)
+                pred = self.fore_model(feature)
+                dis = init_pred - label.transpose(0, 1)
+                weight_feature = torch.cat(
+                    (feature, dis.transpose(0, 1), label, init_pred.view(-1, 1), task_embedding), 1
+                )
+                weight = self.weight_model(weight_feature)
+
+                loss = self.loss_fn(pred, label, weight)
+
+                self.fore_optimizer.zero_grad()
+                loss.backward()
+                torch.nn.utils.clip_grad_value_(self.fore_model.parameters(), 3.0)
+                self.fore_optimizer.step()
+
+        x_valid_values = x_valid.values
+        y_valid_values = np.squeeze(y_valid.values)
+
+        indices = np.arange(len(x_valid_values))
+        np.random.shuffle(indices)
+        for p in self.weight_model.parameters():
+            p.requires_grad = True
+        for p in self.fore_model.parameters():
+            p.requires_grad = False
+
+        # fix forecasting model and valid weight model
+        for i in range(len(indices))[:: self.batch_size]:
+            if len(indices) - i < self.batch_size:
+                break
+
+            feature = torch.from_numpy(x_valid_values[indices[i : i + self.batch_size]]).float().to(self.device)
+            label = torch.from_numpy(y_valid_values[indices[i : i + self.batch_size]]).float().to(self.device)
+
+            pred = self.fore_model(feature)
+            dis = pred - label.transpose(0, 1)
+            weight_feature = torch.cat((feature, dis.transpose(0, 1), label, pred.view(-1, 1), task_embedding), 1)
+            weight = self.weight_model(weight_feature)
+            loc = torch.argmax(weight, 1)
+            valid_loss = torch.mean((pred - label[:, abs(self.target_label)]) ** 2)
+            loss = torch.mean(valid_loss * torch.log(weight[np.arange(weight.shape[0]), loc]))
+
+            self.weight_optimizer.zero_grad()
+            loss.backward()
+            torch.nn.utils.clip_grad_value_(self.weight_model.parameters(), 3.0)
+            self.weight_optimizer.step()
+
+    def test_epoch(self, data_x, data_y):
+        # prepare training data
+        x_values = data_x.values
+        y_values = np.squeeze(data_y.values)
+
+        self.fore_model.eval()
+
+        losses = []
+
+        indices = np.arange(len(x_values))
+
+        for i in range(len(indices))[:: self.batch_size]:
+            if len(indices) - i < self.batch_size:
+                break
+
+            feature = torch.from_numpy(x_values[indices[i : i + self.batch_size]]).float().to(self.device)
+            label = torch.from_numpy(y_values[indices[i : i + self.batch_size]]).float().to(self.device)
+
+            pred = self.fore_model(feature)
+            loss = torch.mean((pred - label[:, abs(self.target_label)]) ** 2)
+            losses.append(loss.item())
+
+        return np.mean(losses)
+
+    def fit(
+        self,
+        dataset: DatasetH,
+        verbose=True,
+        save_path=None,
+    ):
+        df_train, df_valid, df_test = dataset.prepare(
+            ["train", "valid", "test"],
+            col_set=["feature", "label"],
+            data_key=DataHandlerLP.DK_L,
+        )
+        if df_train.empty or df_valid.empty:
+            raise ValueError("Empty data from dataset, please check your dataset config.")
+
+        x_train, y_train = df_train["feature"], df_train["label"]
+        x_valid, y_valid = df_valid["feature"], df_valid["label"]
+        x_test, y_test = df_test["feature"], df_test["label"]
+
+        if save_path is None:
+            save_path = get_or_create_path(save_path)
+        best_loss = np.inf
+        while best_loss > self.lowest_valid_performance:
+            if best_loss < np.inf:
+                print("Failed! Start retraining.")
+                self.seed = random.randint(0, 1000)  # reset random seed
+
+            if self.seed is not None:
+                np.random.seed(self.seed)
+                torch.manual_seed(self.seed)
+
+            best_loss = self.training(
+                x_train, y_train, x_valid, y_valid, x_test, y_test, verbose=verbose, save_path=save_path
+            )
+
+    def training(
+        self,
+        x_train,
+        y_train,
+        x_valid,
+        y_valid,
+        x_test,
+        y_test,
+        verbose=True,
+        save_path=None,
+    ):
+        self.fore_model = GRUModel(
+            d_feat=self.d_feat,
+            hidden_size=self.hidden_size,
+            num_layers=self.num_layers,
+            dropout=self.dropout,
+        )
+        self.weight_model = MLPModel(
+            d_feat=self.input_dim + 3 * self.output_dim + 1,
+            hidden_size=self.hidden_size,
+            num_layers=self.num_layers,
+            dropout=self.dropout,
+            output_dim=self.output_dim,
+        )
+        if self._fore_optimizer.lower() == "adam":
+            self.fore_optimizer = optim.Adam(self.fore_model.parameters(), lr=self.fore_lr)
+        elif self._fore_optimizer.lower() == "gd":
+            self.fore_optimizer = optim.SGD(self.fore_model.parameters(), lr=self.fore_lr)
+        else:
+            raise NotImplementedError("optimizer {} is not supported!".format(self._fore_optimizer))
+        if self._weight_optimizer.lower() == "adam":
+            self.weight_optimizer = optim.Adam(self.weight_model.parameters(), lr=self.weight_lr)
+        elif self._weight_optimizer.lower() == "gd":
+            self.weight_optimizer = optim.SGD(self.weight_model.parameters(), lr=self.weight_lr)
+        else:
+            raise NotImplementedError("optimizer {} is not supported!".format(self._weight_optimizer))
+
+        self.fitted = False
+        self.fore_model.to(self.device)
+        self.weight_model.to(self.device)
+
+        best_loss = np.inf
+        best_epoch = 0
+        stop_round = 0
+
+        for epoch in range(self.n_epochs):
+            print("Epoch:", epoch)
+
+            print("training...")
+            self.train_epoch(x_train, y_train, x_valid, y_valid)
+            print("evaluating...")
+            val_loss = self.test_epoch(x_valid, y_valid)
+            test_loss = self.test_epoch(x_test, y_test)
+
+            if verbose:
+                print("valid %.6f, test %.6f" % (val_loss, test_loss))
+
+            if val_loss < best_loss:
+                best_loss = val_loss
+                stop_round = 0
+                best_epoch = epoch
+                torch.save(copy.deepcopy(self.fore_model.state_dict()), save_path + "_fore_model.bin")
+                torch.save(copy.deepcopy(self.weight_model.state_dict()), save_path + "_weight_model.bin")
+
+            else:
+                stop_round += 1
+                if stop_round >= self.early_stop:
+                    print("early stop")
+                    break
+
+        print("best loss:", best_loss, "@", best_epoch)
+        best_param = torch.load(save_path + "_fore_model.bin", map_location=self.device)
+        self.fore_model.load_state_dict(best_param)
+        best_param = torch.load(save_path + "_weight_model.bin", map_location=self.device)
+        self.weight_model.load_state_dict(best_param)
+        self.fitted = True
+
+        if self.use_gpu:
+            torch.cuda.empty_cache()
+
+        return best_loss
+
+    def predict(self, dataset):
+        if not self.fitted:
+            raise ValueError("model is not fitted yet!")
+
+        x_test = dataset.prepare("test", col_set="feature")
+        index = x_test.index
+        self.fore_model.eval()
+        x_values = x_test.values
+        sample_num = x_values.shape[0]
+        preds = []
+
+        for begin in range(sample_num)[:: self.batch_size]:
+            if sample_num - begin < self.batch_size:
+                end = sample_num
+            else:
+                end = begin + self.batch_size
+
+            x_batch = torch.from_numpy(x_values[begin:end]).float().to(self.device)
+
+            with torch.no_grad():
+                if self.use_gpu:
+                    pred = self.fore_model(x_batch).detach().cpu().numpy()
+                else:
+                    pred = self.fore_model(x_batch).detach().numpy()
+
+            preds.append(pred)
+
+        return pd.Series(np.concatenate(preds), index=index)
+
+
+class MLPModel(nn.Module):
+    def __init__(self, d_feat, hidden_size=256, num_layers=3, dropout=0.0, output_dim=1):
+        super().__init__()
+
+        self.mlp = nn.Sequential()
+        self.softmax = nn.Softmax(dim=1)
+
+        for i in range(num_layers):
+            if i > 0:
+                self.mlp.add_module("drop_%d" % i, nn.Dropout(dropout))
+            self.mlp.add_module("fc_%d" % i, nn.Linear(d_feat if i == 0 else hidden_size, hidden_size))
+            self.mlp.add_module("relu_%d" % i, nn.ReLU())
+
+        self.mlp.add_module("fc_out", nn.Linear(hidden_size, output_dim))
+
+    def forward(self, x):
+        # feature
+        # [N, F]
+        out = self.mlp(x).squeeze()
+        out = self.softmax(out)
+        return out
+
+
+class GRUModel(nn.Module):
+    def __init__(self, d_feat=6, hidden_size=64, num_layers=2, dropout=0.0):
+        super().__init__()
+
+        self.rnn = nn.GRU(
+            input_size=d_feat,
+            hidden_size=hidden_size,
+            num_layers=num_layers,
+            batch_first=True,
+            dropout=dropout,
+        )
+        self.fc_out = nn.Linear(hidden_size, 1)
+
+        self.d_feat = d_feat
+
+    def forward(self, x):
+        # x: [N, F*T]
+        x = x.reshape(len(x), self.d_feat, -1)  # [N, F, T]
+        x = x.permute(0, 2, 1)  # [N, T, F]
+        out, _ = self.rnn(x)
+        return self.fc_out(out[:, -1, :]).squeeze()
```

## qlib/contrib/model/pytorch_tra.py

```diff
@@ -1,943 +1,933 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-import io
-import os
-import copy
-import math
-import json
-import numpy as np
-import pandas as pd
-import matplotlib.pyplot as plt
-
-import torch
-import torch.nn as nn
-import torch.optim as optim
-import torch.nn.functional as F
-
-try:
-    from torch.utils.tensorboard import SummaryWriter
-except ImportError:
-    SummaryWriter = None
-
-from tqdm import tqdm
-
-from qlib.constant import EPS
-from qlib.log import get_module_logger
-from qlib.model.base import Model
-from qlib.contrib.data.dataset import MTSDatasetH
-
-device = "cuda" if torch.cuda.is_available() else "cpu"
-
-
-class TRAModel(Model):
-    """
-    TRA Model
-
-    Args:
-        model_config (dict): model config (will be used by RNN or Transformer)
-        tra_config (dict): TRA config (will be used by TRA)
-        model_type (str): which backbone model to use (RNN/Transformer)
-        lr (float): learning rate
-        n_epochs (int): number of total epochs
-        early_stop (int): early stop when performance not improved at this step
-        update_freq (int): gradient update frequency
-        max_steps_per_epoch (int): maximum number of steps in one epoch
-        lamb (float): regularization parameter
-        rho (float): exponential decay rate for `lamb`
-        alpha (float): fusion parameter for calculating transport loss matrix
-        seed (int): random seed
-        logdir (str): local log directory
-        eval_train (bool): whether evaluate train set between epochs
-        eval_test (bool): whether evaluate test set between epochs
-        pretrain (bool): whether pretrain the backbone model before training TRA.
-            Note that only TRA will be optimized after pretraining
-        init_state (str): model init state path
-        freeze_model (bool): whether freeze backbone model parameters
-        freeze_predictors (bool): whether freeze predictors parameters
-        transport_method (str): transport method, can be none/router/oracle
-        memory_mode (str): memory mode, the same argument for MTSDatasetH
-    """
-
-    def __init__(
-        self,
-        model_config,
-        tra_config,
-        model_type="RNN",
-        lr=1e-3,
-        n_epochs=500,
-        early_stop=50,
-        update_freq=1,
-        max_steps_per_epoch=None,
-        lamb=0.0,
-        rho=0.99,
-        alpha=1.0,
-        seed=None,
-        logdir=None,
-        eval_train=False,
-        eval_test=False,
-        pretrain=False,
-        init_state=None,
-        reset_router=False,
-        freeze_model=False,
-        freeze_predictors=False,
-        transport_method="none",
-        memory_mode="sample",
-    ):
-
-        self.logger = get_module_logger("TRA")
-
-        assert memory_mode in ["sample", "daily"], "invalid memory mode"
-        assert transport_method in ["none", "router", "oracle"], f"invalid transport method {transport_method}"
-        assert transport_method == "none" or tra_config["num_states"] > 1, "optimal transport requires `num_states` > 1"
-        assert (
-            memory_mode != "daily" or tra_config["src_info"] == "TPE"
-        ), "daily transport can only support TPE as `src_info`"
-
-        if transport_method == "router" and not eval_train:
-            self.logger.warning("`eval_train` will be ignored when using TRA.router")
-
-        if seed is not None:
-            np.random.seed(seed)
-            torch.manual_seed(seed)
-
-        self.model_config = model_config
-        self.tra_config = tra_config
-        self.model_type = model_type
-        self.lr = lr
-        self.n_epochs = n_epochs
-        self.early_stop = early_stop
-        self.update_freq = update_freq
-        self.max_steps_per_epoch = max_steps_per_epoch
-        self.lamb = lamb
-        self.rho = rho
-        self.alpha = alpha
-        self.seed = seed
-        self.logdir = logdir
-        self.eval_train = eval_train
-        self.eval_test = eval_test
-        self.pretrain = pretrain
-        self.init_state = init_state
-        self.reset_router = reset_router
-        self.freeze_model = freeze_model
-        self.freeze_predictors = freeze_predictors
-        self.transport_method = transport_method
-        self.use_daily_transport = memory_mode == "daily"
-        self.transport_fn = transport_daily if self.use_daily_transport else transport_sample
-
-        self._writer = None
-        if self.logdir is not None:
-            if os.path.exists(self.logdir):
-                self.logger.warning(f"logdir {self.logdir} is not empty")
-            os.makedirs(self.logdir, exist_ok=True)
-            if SummaryWriter is not None:
-                self._writer = SummaryWriter(log_dir=self.logdir)
-
-        self._init_model()
-
-    def _init_model(self):
-
-        self.logger.info("init TRAModel...")
-
-        self.model = eval(self.model_type)(**self.model_config).to(device)
-        print(self.model)
-
-        self.tra = TRA(self.model.output_size, **self.tra_config).to(device)
-        print(self.tra)
-
-        if self.init_state:
-            self.logger.warning(f"load state dict from `init_state`")
-            state_dict = torch.load(self.init_state, map_location="cpu")
-            self.model.load_state_dict(state_dict["model"])
-            res = load_state_dict_unsafe(self.tra, state_dict["tra"])
-            self.logger.warning(str(res))
-
-        if self.reset_router:
-            self.logger.warning(f"reset TRA.router parameters")
-            self.tra.fc.reset_parameters()
-            self.tra.router.reset_parameters()
-
-        if self.freeze_model:
-            self.logger.warning(f"freeze model parameters")
-            for param in self.model.parameters():
-                param.requires_grad_(False)
-
-        if self.freeze_predictors:
-            self.logger.warning(f"freeze TRA.predictors parameters")
-            for param in self.tra.predictors.parameters():
-                param.requires_grad_(False)
-
-        self.logger.info("# model params: %d" % sum(p.numel() for p in self.model.parameters() if p.requires_grad))
-        self.logger.info("# tra params: %d" % sum(p.numel() for p in self.tra.parameters() if p.requires_grad))
-
-        self.optimizer = optim.Adam(list(self.model.parameters()) + list(self.tra.parameters()), lr=self.lr)
-
-        self.fitted = False
-        self.global_step = -1
-
-    def train_epoch(self, epoch, data_set, is_pretrain=False):
-
-        self.model.train()
-        self.tra.train()
-        data_set.train()
-        self.optimizer.zero_grad()
-
-        P_all = []
-        prob_all = []
-        choice_all = []
-        max_steps = len(data_set)
-        if self.max_steps_per_epoch is not None:
-            if epoch == 0 and self.max_steps_per_epoch < max_steps:
-                self.logger.info(f"max steps updated from {max_steps} to {self.max_steps_per_epoch}")
-            max_steps = min(self.max_steps_per_epoch, max_steps)
-
-        cur_step = 0
-        total_loss = 0
-        total_count = 0
-        for batch in tqdm(data_set, total=max_steps):
-            cur_step += 1
-            if cur_step > max_steps:
-                break
-
-            if not is_pretrain:
-                self.global_step += 1
-
-            data, state, label, count = batch["data"], batch["state"], batch["label"], batch["daily_count"]
-            index = batch["daily_index"] if self.use_daily_transport else batch["index"]
-
-            with torch.set_grad_enabled(not self.freeze_model):
-                hidden = self.model(data)
-
-            all_preds, choice, prob = self.tra(hidden, state)
-
-            if is_pretrain or self.transport_method != "none":
-                # NOTE: use oracle transport for pre-training
-                loss, pred, L, P = self.transport_fn(
-                    all_preds,
-                    label,
-                    choice,
-                    prob,
-                    state.mean(dim=1),
-                    count,
-                    self.transport_method if not is_pretrain else "oracle",
-                    self.alpha,
-                    training=True,
-                )
-                data_set.assign_data(index, L)  # save loss to memory
-                if self.use_daily_transport:  # only save for daily transport
-                    P_all.append(pd.DataFrame(P.detach().cpu().numpy(), index=index))
-                    prob_all.append(pd.DataFrame(prob.detach().cpu().numpy(), index=index))
-                    choice_all.append(pd.DataFrame(choice.detach().cpu().numpy(), index=index))
-                decay = self.rho ** (self.global_step // 100)  # decay every 100 steps
-                lamb = 0 if is_pretrain else self.lamb * decay
-                reg = prob.log().mul(P).sum(dim=1).mean()  # train router to predict TO assignment
-                if self._writer is not None and not is_pretrain:
-                    self._writer.add_scalar("training/router_loss", -reg.item(), self.global_step)
-                    self._writer.add_scalar("training/reg_loss", loss.item(), self.global_step)
-                    self._writer.add_scalar("training/lamb", lamb, self.global_step)
-                    if not self.use_daily_transport:
-                        P_mean = P.mean(axis=0).detach()
-                        self._writer.add_scalar("training/P", P_mean.max() / P_mean.min(), self.global_step)
-                loss = loss - lamb * reg
-            else:
-                pred = all_preds.mean(dim=1)
-                loss = loss_fn(pred, label)
-
-            (loss / self.update_freq).backward()
-            if cur_step % self.update_freq == 0:
-                self.optimizer.step()
-                self.optimizer.zero_grad()
-
-            if self._writer is not None and not is_pretrain:
-                self._writer.add_scalar("training/total_loss", loss.item(), self.global_step)
-
-            total_loss += loss.item()
-            total_count += 1
-
-        if self.use_daily_transport and len(P_all) > 0:
-            P_all = pd.concat(P_all, axis=0)
-            prob_all = pd.concat(prob_all, axis=0)
-            choice_all = pd.concat(choice_all, axis=0)
-            P_all.index = data_set.restore_daily_index(P_all.index)
-            prob_all.index = P_all.index
-            choice_all.index = P_all.index
-            if not is_pretrain:
-                self._writer.add_image("P", plot(P_all), epoch, dataformats="HWC")
-                self._writer.add_image("prob", plot(prob_all), epoch, dataformats="HWC")
-                self._writer.add_image("choice", plot(choice_all), epoch, dataformats="HWC")
-
-        total_loss /= total_count
-
-        if self._writer is not None and not is_pretrain:
-            self._writer.add_scalar("training/loss", total_loss, epoch)
-
-        return total_loss
-
-    def test_epoch(self, epoch, data_set, return_pred=False, prefix="test", is_pretrain=False):
-
-        self.model.eval()
-        self.tra.eval()
-        data_set.eval()
-
-        preds = []
-        probs = []
-        P_all = []
-        metrics = []
-        for batch in tqdm(data_set):
-            data, state, label, count = batch["data"], batch["state"], batch["label"], batch["daily_count"]
-            index = batch["daily_index"] if self.use_daily_transport else batch["index"]
-
-            with torch.no_grad():
-                hidden = self.model(data)
-                all_preds, choice, prob = self.tra(hidden, state)
-
-            if is_pretrain or self.transport_method != "none":
-                loss, pred, L, P = self.transport_fn(
-                    all_preds,
-                    label,
-                    choice,
-                    prob,
-                    state.mean(dim=1),
-                    count,
-                    self.transport_method if not is_pretrain else "oracle",
-                    self.alpha,
-                    training=False,
-                )
-                data_set.assign_data(index, L)  # save loss to memory
-                if P is not None and return_pred:
-                    P_all.append(pd.DataFrame(P.cpu().numpy(), index=index))
-            else:
-                pred = all_preds.mean(dim=1)
-
-            X = np.c_[pred.cpu().numpy(), label.cpu().numpy(), all_preds.cpu().numpy()]
-            columns = ["score", "label"] + ["score_%d" % d for d in range(all_preds.shape[1])]
-            pred = pd.DataFrame(X, index=batch["index"], columns=columns)
-
-            metrics.append(evaluate(pred))
-
-            if return_pred:
-                preds.append(pred)
-                if prob is not None:
-                    columns = ["prob_%d" % d for d in range(all_preds.shape[1])]
-                    probs.append(pd.DataFrame(prob.cpu().numpy(), index=index, columns=columns))
-
-        metrics = pd.DataFrame(metrics)
-        metrics = {
-            "MSE": metrics.MSE.mean(),
-            "MAE": metrics.MAE.mean(),
-            "IC": metrics.IC.mean(),
-            "ICIR": metrics.IC.mean() / metrics.IC.std(),
-        }
-
-        if self._writer is not None and epoch >= 0 and not is_pretrain:
-            for key, value in metrics.items():
-                self._writer.add_scalar(prefix + "/" + key, value, epoch)
-
-        if return_pred:
-            preds = pd.concat(preds, axis=0)
-            preds.index = data_set.restore_index(preds.index)
-            preds.index = preds.index.swaplevel()
-            preds.sort_index(inplace=True)
-
-            if probs:
-                probs = pd.concat(probs, axis=0)
-                if self.use_daily_transport:
-                    probs.index = data_set.restore_daily_index(probs.index)
-                else:
-                    probs.index = data_set.restore_index(probs.index)
-                    probs.index = probs.index.swaplevel()
-                    probs.sort_index(inplace=True)
-
-            if len(P_all):
-                P_all = pd.concat(P_all, axis=0)
-                if self.use_daily_transport:
-                    P_all.index = data_set.restore_daily_index(P_all.index)
-                else:
-                    P_all.index = data_set.restore_index(P_all.index)
-                    P_all.index = P_all.index.swaplevel()
-                    P_all.sort_index(inplace=True)
-
-        return metrics, preds, probs, P_all
-
-    def _fit(self, train_set, valid_set, test_set, evals_result, is_pretrain=True):
-
-        best_score = -1
-        best_epoch = 0
-        stop_rounds = 0
-        best_params = {
-            "model": copy.deepcopy(self.model.state_dict()),
-            "tra": copy.deepcopy(self.tra.state_dict()),
-        }
-        # train
-        if not is_pretrain and self.transport_method != "none":
-            self.logger.info("init memory...")
-            self.test_epoch(-1, train_set)
-
-        for epoch in range(self.n_epochs):
-            self.logger.info("Epoch %d:", epoch)
-
-            self.logger.info("training...")
-            self.train_epoch(epoch, train_set, is_pretrain=is_pretrain)
-
-            self.logger.info("evaluating...")
-            # NOTE: during evaluating, the whole memory will be refreshed
-            if not is_pretrain and (self.transport_method == "router" or self.eval_train):
-                train_set.clear_memory()  # NOTE: clear the shared memory
-                train_metrics = self.test_epoch(epoch, train_set, is_pretrain=is_pretrain, prefix="train")[0]
-                evals_result["train"].append(train_metrics)
-                self.logger.info("train metrics: %s" % train_metrics)
-
-            valid_metrics = self.test_epoch(epoch, valid_set, is_pretrain=is_pretrain, prefix="valid")[0]
-            evals_result["valid"].append(valid_metrics)
-            self.logger.info("valid metrics: %s" % valid_metrics)
-
-            if self.eval_test:
-                test_metrics = self.test_epoch(epoch, test_set, is_pretrain=is_pretrain, prefix="test")[0]
-                evals_result["test"].append(test_metrics)
-                self.logger.info("test metrics: %s" % test_metrics)
-
-            if valid_metrics["IC"] > best_score:
-                best_score = valid_metrics["IC"]
-                stop_rounds = 0
-                best_epoch = epoch
-                best_params = {
-                    "model": copy.deepcopy(self.model.state_dict()),
-                    "tra": copy.deepcopy(self.tra.state_dict()),
-                }
-                if self.logdir is not None:
-                    torch.save(best_params, self.logdir + "/model.bin")
-            else:
-                stop_rounds += 1
-                if stop_rounds >= self.early_stop:
-                    self.logger.info("early stop @ %s" % epoch)
-                    break
-
-        self.logger.info("best score: %.6lf @ %d" % (best_score, best_epoch))
-        self.model.load_state_dict(best_params["model"])
-        self.tra.load_state_dict(best_params["tra"])
-
-        return best_score
-
-    def fit(self, dataset, evals_result=dict()):
-
-        assert isinstance(dataset, MTSDatasetH), "TRAModel only supports `qlib.contrib.data.dataset.MTSDatasetH`"
-
-        train_set, valid_set, test_set = dataset.prepare(["train", "valid", "test"])
-
-        self.fitted = True
-        self.global_step = -1
-
-        evals_result["train"] = []
-        evals_result["valid"] = []
-        evals_result["test"] = []
-
-        if self.pretrain:
-            self.logger.info("pretraining...")
-            self.optimizer = optim.Adam(
-                list(self.model.parameters()) + list(self.tra.predictors.parameters()), lr=self.lr
-            )
-            self._fit(train_set, valid_set, test_set, evals_result, is_pretrain=True)
-
-            # reset optimizer
-            self.optimizer = optim.Adam(list(self.model.parameters()) + list(self.tra.parameters()), lr=self.lr)
-
-        self.logger.info("training...")
-        best_score = self._fit(train_set, valid_set, test_set, evals_result, is_pretrain=False)
-
-        self.logger.info("inference")
-        train_metrics, train_preds, train_probs, train_P = self.test_epoch(-1, train_set, return_pred=True)
-        self.logger.info("train metrics: %s" % train_metrics)
-
-        valid_metrics, valid_preds, valid_probs, valid_P = self.test_epoch(-1, valid_set, return_pred=True)
-        self.logger.info("valid metrics: %s" % valid_metrics)
-
-        test_metrics, test_preds, test_probs, test_P = self.test_epoch(-1, test_set, return_pred=True)
-        self.logger.info("test metrics: %s" % test_metrics)
-
-        if self.logdir:
-            self.logger.info("save model & pred to local directory")
-
-            pd.concat({name: pd.DataFrame(evals_result[name]) for name in evals_result}, axis=1).to_csv(
-                self.logdir + "/logs.csv", index=False
-            )
-
-            torch.save({"model": self.model.state_dict(), "tra": self.tra.state_dict()}, self.logdir + "/model.bin")
-
-            train_preds.to_pickle(self.logdir + "/train_pred.pkl")
-            valid_preds.to_pickle(self.logdir + "/valid_pred.pkl")
-            test_preds.to_pickle(self.logdir + "/test_pred.pkl")
-
-            if len(train_probs):
-                train_probs.to_pickle(self.logdir + "/train_prob.pkl")
-                valid_probs.to_pickle(self.logdir + "/valid_prob.pkl")
-                test_probs.to_pickle(self.logdir + "/test_prob.pkl")
-
-            if len(train_P):
-                train_P.to_pickle(self.logdir + "/train_P.pkl")
-                valid_P.to_pickle(self.logdir + "/valid_P.pkl")
-                test_P.to_pickle(self.logdir + "/test_P.pkl")
-
-            info = {
-                "config": {
-                    "model_config": self.model_config,
-                    "tra_config": self.tra_config,
-                    "model_type": self.model_type,
-                    "lr": self.lr,
-                    "n_epochs": self.n_epochs,
-                    "early_stop": self.early_stop,
-                    "max_steps_per_epoch": self.max_steps_per_epoch,
-                    "lamb": self.lamb,
-                    "rho": self.rho,
-                    "alpha": self.alpha,
-                    "seed": self.seed,
-                    "logdir": self.logdir,
-                    "pretrain": self.pretrain,
-                    "init_state": self.init_state,
-                    "transport_method": self.transport_method,
-                    "use_daily_transport": self.use_daily_transport,
-                },
-                "best_eval_metric": -best_score,  # NOTE: -1 for minimize
-                "metrics": {"train": train_metrics, "valid": valid_metrics, "test": test_metrics},
-            }
-            with open(self.logdir + "/info.json", "w") as f:
-                json.dump(info, f)
-
-    def predict(self, dataset, segment="test"):
-
-        assert isinstance(dataset, MTSDatasetH), "TRAModel only supports `qlib.contrib.data.dataset.MTSDatasetH`"
-
-        if not self.fitted:
-            raise ValueError("model is not fitted yet!")
-
-        test_set = dataset.prepare(segment)
-
-        metrics, preds, _, _ = self.test_epoch(-1, test_set, return_pred=True)
-        self.logger.info("test metrics: %s" % metrics)
-
-        return preds
-
-
-class RNN(nn.Module):
-
-    """RNN Model
-
-    Args:
-        input_size (int): input size (# features)
-        hidden_size (int): hidden size
-        num_layers (int): number of hidden layers
-        rnn_arch (str): rnn architecture
-        use_attn (bool): whether use attention layer.
-            we use concat attention as https://github.com/fulifeng/Adv-ALSTM/
-        dropout (float): dropout rate
-    """
-
-    def __init__(
-        self,
-        input_size=16,
-        hidden_size=64,
-        num_layers=2,
-        rnn_arch="GRU",
-        use_attn=True,
-        dropout=0.0,
-        **kwargs,
-    ):
-        super().__init__()
-
-        self.input_size = input_size
-        self.hidden_size = hidden_size
-        self.num_layers = num_layers
-        self.rnn_arch = rnn_arch
-        self.use_attn = use_attn
-
-        if hidden_size < input_size:
-            # compression
-            self.input_proj = nn.Linear(input_size, hidden_size)
-        else:
-            self.input_proj = None
-
-        self.rnn = getattr(nn, rnn_arch)(
-            input_size=min(input_size, hidden_size),
-            hidden_size=hidden_size,
-            num_layers=num_layers,
-            batch_first=True,
-            dropout=dropout,
-        )
-
-        if self.use_attn:
-            self.W = nn.Linear(hidden_size, hidden_size)
-            self.u = nn.Linear(hidden_size, 1, bias=False)
-            self.output_size = hidden_size * 2
-        else:
-            self.output_size = hidden_size
-
-    def forward(self, x):
-
-        if self.input_proj is not None:
-            x = self.input_proj(x)
-
-        rnn_out, last_out = self.rnn(x)
-        if self.rnn_arch == "LSTM":
-            last_out = last_out[0]
-        last_out = last_out.mean(dim=0)
-
-        if self.use_attn:
-            laten = self.W(rnn_out).tanh()
-            scores = self.u(laten).softmax(dim=1)
-            att_out = (rnn_out * scores).sum(dim=1)
-            last_out = torch.cat([last_out, att_out], dim=1)
-
-        return last_out
-
-
-class PositionalEncoding(nn.Module):
-    # reference: https://pytorch.org/tutorials/beginner/transformer_tutorial.html
-    def __init__(self, d_model, dropout=0.1, max_len=5000):
-        super(PositionalEncoding, self).__init__()
-        self.dropout = nn.Dropout(p=dropout)
-
-        pe = torch.zeros(max_len, d_model)
-        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
-        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
-        pe[:, 0::2] = torch.sin(position * div_term)
-        pe[:, 1::2] = torch.cos(position * div_term)
-        pe = pe.unsqueeze(0).transpose(0, 1)
-        self.register_buffer("pe", pe)
-
-    def forward(self, x):
-        x = x + self.pe[: x.size(0), :]
-        return self.dropout(x)
-
-
-class Transformer(nn.Module):
-
-    """Transformer Model
-
-    Args:
-        input_size (int): input size (# features)
-        hidden_size (int): hidden size
-        num_layers (int): number of transformer layers
-        num_heads (int): number of heads in transformer
-        dropout (float): dropout rate
-    """
-
-    def __init__(
-        self,
-        input_size=16,
-        hidden_size=64,
-        num_layers=2,
-        num_heads=2,
-        dropout=0.0,
-        **kwargs,
-    ):
-        super().__init__()
-
-        self.input_size = input_size
-        self.hidden_size = hidden_size
-        self.num_layers = num_layers
-        self.num_heads = num_heads
-
-        self.input_proj = nn.Linear(input_size, hidden_size)
-
-        self.pe = PositionalEncoding(input_size, dropout)
-        layer = nn.TransformerEncoderLayer(
-            nhead=num_heads, dropout=dropout, d_model=hidden_size, dim_feedforward=hidden_size * 4
-        )
-        self.encoder = nn.TransformerEncoder(layer, num_layers=num_layers)
-
-        self.output_size = hidden_size
-
-    def forward(self, x):
-
-        x = x.permute(1, 0, 2).contiguous()  # the first dim need to be time
-        x = self.pe(x)
-
-        x = self.input_proj(x)
-        out = self.encoder(x)
-
-        return out[-1]
-
-
-class TRA(nn.Module):
-
-    """Temporal Routing Adaptor (TRA)
-
-    TRA takes historical prediction errors & latent representation as inputs,
-    then routes the input sample to a specific predictor for training & inference.
-
-    Args:
-        input_size (int): input size (RNN/Transformer's hidden size)
-        num_states (int): number of latent states (i.e., trading patterns)
-            If `num_states=1`, then TRA falls back to traditional methods
-        hidden_size (int): hidden size of the router
-        tau (float): gumbel softmax temperature
-        src_info (str): information for the router
-    """
-
-    def __init__(
-        self,
-        input_size,
-        num_states=1,
-        hidden_size=8,
-        rnn_arch="GRU",
-        num_layers=1,
-        dropout=0.0,
-        tau=1.0,
-        src_info="LR_TPE",
-    ):
-        super().__init__()
-
-        assert src_info in ["LR", "TPE", "LR_TPE"], "invalid `src_info`"
-
-        self.num_states = num_states
-        self.tau = tau
-        self.rnn_arch = rnn_arch
-        self.src_info = src_info
-
-        self.predictors = nn.Linear(input_size, num_states)
-
-        if self.num_states > 1:
-            if "TPE" in src_info:
-                self.router = getattr(nn, rnn_arch)(
-                    input_size=num_states,
-                    hidden_size=hidden_size,
-                    num_layers=num_layers,
-                    batch_first=True,
-                    dropout=dropout,
-                )
-                self.fc = nn.Linear(hidden_size + input_size if "LR" in src_info else hidden_size, num_states)
-            else:
-                self.fc = nn.Linear(input_size, num_states)
-
-    def reset_parameters(self):
-        for child in self.children():
-            child.reset_parameters()
-
-    def forward(self, hidden, hist_loss):
-
-        preds = self.predictors(hidden)
-
-        if self.num_states == 1:  # no need for router when having only one prediction
-            return preds, None, None
-
-        if "TPE" in self.src_info:
-            out = self.router(hist_loss)[1]  # TPE
-            if self.rnn_arch == "LSTM":
-                out = out[0]
-            out = out.mean(dim=0)
-            if "LR" in self.src_info:
-                out = torch.cat([hidden, out], dim=-1)  # LR_TPE
-        else:
-            out = hidden  # LR
-
-        out = self.fc(out)
-
-        choice = F.gumbel_softmax(out, dim=-1, tau=self.tau, hard=True)
-        prob = torch.softmax(out / self.tau, dim=-1)
-
-        return preds, choice, prob
-
-
-def evaluate(pred):
-    pred = pred.rank(pct=True)  # transform into percentiles
-    score = pred.score
-    label = pred.label
-    diff = score - label
-    MSE = (diff**2).mean()
-    MAE = (diff.abs()).mean()
-    IC = score.corr(label, method="spearman")
-    return {"MSE": MSE, "MAE": MAE, "IC": IC}
-
-
-def shoot_infs(inp_tensor):
-    """Replaces inf by maximum of tensor"""
-    mask_inf = torch.isinf(inp_tensor)
-    ind_inf = torch.nonzero(mask_inf, as_tuple=False)
-    if len(ind_inf) > 0:
-        for ind in ind_inf:
-            if len(ind) == 2:
-                inp_tensor[ind[0], ind[1]] = 0
-            elif len(ind) == 1:
-                inp_tensor[ind[0]] = 0
-        m = torch.max(inp_tensor)
-        for ind in ind_inf:
-            if len(ind) == 2:
-                inp_tensor[ind[0], ind[1]] = m
-            elif len(ind) == 1:
-                inp_tensor[ind[0]] = m
-    return inp_tensor
-
-
-def sinkhorn(Q, n_iters=3, epsilon=0.1):
-    # epsilon should be adjusted according to logits value's scale
-    with torch.no_grad():
-        Q = torch.exp(Q / epsilon)
-        Q = shoot_infs(Q)
-        for i in range(n_iters):
-            Q /= Q.sum(dim=0, keepdim=True)
-            Q /= Q.sum(dim=1, keepdim=True)
-    return Q
-
-
-def loss_fn(pred, label):
-    mask = ~torch.isnan(label)
-    if len(pred.shape) == 2:
-        label = label[:, None]
-    return (pred[mask] - label[mask]).pow(2).mean(dim=0)
-
-
-def minmax_norm(x):
-    xmin = x.min(dim=-1, keepdim=True).values
-    xmax = x.max(dim=-1, keepdim=True).values
-    mask = (xmin == xmax).squeeze()
-    x = (x - xmin) / (xmax - xmin + EPS)
-    x[mask] = 1
-    return x
-
-
-def transport_sample(all_preds, label, choice, prob, hist_loss, count, transport_method, alpha, training=False):
-    """
-    sample-wise transport
-
-    Args:
-        all_preds (torch.Tensor): predictions from all predictors, [sample x states]
-        label (torch.Tensor): label, [sample]
-        choice (torch.Tensor): gumbel softmax choice, [sample x states]
-        prob (torch.Tensor): router predicted probility, [sample x states]
-        hist_loss (torch.Tensor): history loss matrix, [sample x states]
-        count (list): sample counts for each day, empty list for sample-wise transport
-        transport_method (str): transportation method
-        alpha (float): fusion parameter for calculating transport loss matrix
-        training (bool): indicate training or inference
-    """
-    assert all_preds.shape == choice.shape
-    assert len(all_preds) == len(label)
-    assert transport_method in ["oracle", "router"]
-
-    all_loss = torch.zeros_like(all_preds)
-    mask = ~torch.isnan(label)
-    all_loss[mask] = (all_preds[mask] - label[mask, None]).pow(2)  # [sample x states]
-
-    L = minmax_norm(all_loss.detach())
-    Lh = L * alpha + minmax_norm(hist_loss) * (1 - alpha)  # add hist loss for transport
-    Lh = minmax_norm(Lh)
-    P = sinkhorn(-Lh)
-    del Lh
-
-    if transport_method == "router":
-        if training:
-            pred = (all_preds * choice).sum(dim=1)  # gumbel softmax
-        else:
-            pred = all_preds[range(len(all_preds)), prob.argmax(dim=-1)]  # argmax
-    else:
-        pred = (all_preds * P).sum(dim=1)
-
-    if transport_method == "router":
-        loss = loss_fn(pred, label)
-    else:
-        loss = (all_loss * P).sum(dim=1).mean()
-
-    return loss, pred, L, P
-
-
-def transport_daily(all_preds, label, choice, prob, hist_loss, count, transport_method, alpha, training=False):
-    """
-    daily transport
-
-    Args:
-        all_preds (torch.Tensor): predictions from all predictors, [sample x states]
-        label (torch.Tensor): label, [sample]
-        choice (torch.Tensor): gumbel softmax choice, [days x states]
-        prob (torch.Tensor): router predicted probility, [days x states]
-        hist_loss (torch.Tensor): history loss matrix, [days x states]
-        count (list): sample counts for each day, [days]
-        transport_method (str): transportation method
-        alpha (float): fusion parameter for calculating transport loss matrix
-        training (bool): indicate training or inference
-    """
-    assert len(prob) == len(count)
-    assert len(all_preds) == sum(count)
-    assert transport_method in ["oracle", "router"]
-
-    all_loss = []  # loss of all predictions
-    start = 0
-    for i, cnt in enumerate(count):
-        slc = slice(start, start + cnt)  # samples from the i-th day
-        start += cnt
-        tloss = loss_fn(all_preds[slc], label[slc])  # loss of the i-th day
-        all_loss.append(tloss)
-    all_loss = torch.stack(all_loss, dim=0)  # [days x states]
-
-    L = minmax_norm(all_loss.detach())
-    Lh = L * alpha + minmax_norm(hist_loss) * (1 - alpha)  # add hist loss for transport
-    Lh = minmax_norm(Lh)
-    P = sinkhorn(-Lh)
-    del Lh
-
-    pred = []
-    start = 0
-    for i, cnt in enumerate(count):
-        slc = slice(start, start + cnt)  # samples from the i-th day
-        start += cnt
-        if transport_method == "router":
-            if training:
-                tpred = all_preds[slc] @ choice[i]  # gumbel softmax
-            else:
-                tpred = all_preds[slc][:, prob[i].argmax(dim=-1)]  # argmax
-        else:
-            tpred = all_preds[slc] @ P[i]
-        pred.append(tpred)
-    pred = torch.cat(pred, dim=0)  # [samples]
-
-    if transport_method == "router":
-        loss = loss_fn(pred, label)
-    else:
-        loss = (all_loss * P).sum(dim=1).mean()
-
-    return loss, pred, L, P
-
-
-def load_state_dict_unsafe(model, state_dict):
-    """
-    Load state dict to provided model while ignore exceptions.
-    """
-
-    missing_keys = []
-    unexpected_keys = []
-    error_msgs = []
-
-    # copy state_dict so _load_from_state_dict can modify it
-    metadata = getattr(state_dict, "_metadata", None)
-    state_dict = state_dict.copy()
-    if metadata is not None:
-        state_dict._metadata = metadata
-
-    def load(module, prefix=""):
-        local_metadata = {} if metadata is None else metadata.get(prefix[:-1], {})
-        module._load_from_state_dict(
-            state_dict, prefix, local_metadata, True, missing_keys, unexpected_keys, error_msgs
-        )
-        for name, child in module._modules.items():
-            if child is not None:
-                load(child, prefix + name + ".")
-
-    load(model)
-    load = None  # break load->load reference cycle
-
-    return {"unexpected_keys": unexpected_keys, "missing_keys": missing_keys, "error_msgs": error_msgs}
-
-
-def plot(P):
-    assert isinstance(P, pd.DataFrame)
-
-    fig, axes = plt.subplots(1, 2, figsize=(10, 4))
-    P.plot.area(ax=axes[0], xlabel="")
-    P.idxmax(axis=1).value_counts().sort_index().plot.bar(ax=axes[1], xlabel="")
-    plt.tight_layout()
-
-    with io.BytesIO() as buf:
-        plt.savefig(buf, format="png")
-        buf.seek(0)
-        img = plt.imread(buf)
-        plt.close()
-
-    return np.uint8(img * 255)
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+import io
+import os
+import copy
+import math
+import json
+import numpy as np
+import pandas as pd
+import matplotlib.pyplot as plt
+
+import torch
+import torch.nn as nn
+import torch.optim as optim
+import torch.nn.functional as F
+
+try:
+    from torch.utils.tensorboard import SummaryWriter
+except ImportError:
+    SummaryWriter = None
+
+from tqdm import tqdm
+
+from qlib.constant import EPS
+from qlib.log import get_module_logger
+from qlib.model.base import Model
+from qlib.contrib.data.dataset import MTSDatasetH
+
+device = "cuda" if torch.cuda.is_available() else "cpu"
+
+
+class TRAModel(Model):
+    """
+    TRA Model
+
+    Args:
+        model_config (dict): model config (will be used by RNN or Transformer)
+        tra_config (dict): TRA config (will be used by TRA)
+        model_type (str): which backbone model to use (RNN/Transformer)
+        lr (float): learning rate
+        n_epochs (int): number of total epochs
+        early_stop (int): early stop when performance not improved at this step
+        update_freq (int): gradient update frequency
+        max_steps_per_epoch (int): maximum number of steps in one epoch
+        lamb (float): regularization parameter
+        rho (float): exponential decay rate for `lamb`
+        alpha (float): fusion parameter for calculating transport loss matrix
+        seed (int): random seed
+        logdir (str): local log directory
+        eval_train (bool): whether evaluate train set between epochs
+        eval_test (bool): whether evaluate test set between epochs
+        pretrain (bool): whether pretrain the backbone model before training TRA.
+            Note that only TRA will be optimized after pretraining
+        init_state (str): model init state path
+        freeze_model (bool): whether freeze backbone model parameters
+        freeze_predictors (bool): whether freeze predictors parameters
+        transport_method (str): transport method, can be none/router/oracle
+        memory_mode (str): memory mode, the same argument for MTSDatasetH
+    """
+
+    def __init__(
+        self,
+        model_config,
+        tra_config,
+        model_type="RNN",
+        lr=1e-3,
+        n_epochs=500,
+        early_stop=50,
+        update_freq=1,
+        max_steps_per_epoch=None,
+        lamb=0.0,
+        rho=0.99,
+        alpha=1.0,
+        seed=None,
+        logdir=None,
+        eval_train=False,
+        eval_test=False,
+        pretrain=False,
+        init_state=None,
+        reset_router=False,
+        freeze_model=False,
+        freeze_predictors=False,
+        transport_method="none",
+        memory_mode="sample",
+    ):
+        self.logger = get_module_logger("TRA")
+
+        assert memory_mode in ["sample", "daily"], "invalid memory mode"
+        assert transport_method in ["none", "router", "oracle"], f"invalid transport method {transport_method}"
+        assert transport_method == "none" or tra_config["num_states"] > 1, "optimal transport requires `num_states` > 1"
+        assert (
+            memory_mode != "daily" or tra_config["src_info"] == "TPE"
+        ), "daily transport can only support TPE as `src_info`"
+
+        if transport_method == "router" and not eval_train:
+            self.logger.warning("`eval_train` will be ignored when using TRA.router")
+
+        if seed is not None:
+            np.random.seed(seed)
+            torch.manual_seed(seed)
+
+        self.model_config = model_config
+        self.tra_config = tra_config
+        self.model_type = model_type
+        self.lr = lr
+        self.n_epochs = n_epochs
+        self.early_stop = early_stop
+        self.update_freq = update_freq
+        self.max_steps_per_epoch = max_steps_per_epoch
+        self.lamb = lamb
+        self.rho = rho
+        self.alpha = alpha
+        self.seed = seed
+        self.logdir = logdir
+        self.eval_train = eval_train
+        self.eval_test = eval_test
+        self.pretrain = pretrain
+        self.init_state = init_state
+        self.reset_router = reset_router
+        self.freeze_model = freeze_model
+        self.freeze_predictors = freeze_predictors
+        self.transport_method = transport_method
+        self.use_daily_transport = memory_mode == "daily"
+        self.transport_fn = transport_daily if self.use_daily_transport else transport_sample
+
+        self._writer = None
+        if self.logdir is not None:
+            if os.path.exists(self.logdir):
+                self.logger.warning(f"logdir {self.logdir} is not empty")
+            os.makedirs(self.logdir, exist_ok=True)
+            if SummaryWriter is not None:
+                self._writer = SummaryWriter(log_dir=self.logdir)
+
+        self._init_model()
+
+    def _init_model(self):
+        self.logger.info("init TRAModel...")
+
+        self.model = eval(self.model_type)(**self.model_config).to(device)
+        print(self.model)
+
+        self.tra = TRA(self.model.output_size, **self.tra_config).to(device)
+        print(self.tra)
+
+        if self.init_state:
+            self.logger.warning(f"load state dict from `init_state`")
+            state_dict = torch.load(self.init_state, map_location="cpu")
+            self.model.load_state_dict(state_dict["model"])
+            res = load_state_dict_unsafe(self.tra, state_dict["tra"])
+            self.logger.warning(str(res))
+
+        if self.reset_router:
+            self.logger.warning(f"reset TRA.router parameters")
+            self.tra.fc.reset_parameters()
+            self.tra.router.reset_parameters()
+
+        if self.freeze_model:
+            self.logger.warning(f"freeze model parameters")
+            for param in self.model.parameters():
+                param.requires_grad_(False)
+
+        if self.freeze_predictors:
+            self.logger.warning(f"freeze TRA.predictors parameters")
+            for param in self.tra.predictors.parameters():
+                param.requires_grad_(False)
+
+        self.logger.info("# model params: %d" % sum(p.numel() for p in self.model.parameters() if p.requires_grad))
+        self.logger.info("# tra params: %d" % sum(p.numel() for p in self.tra.parameters() if p.requires_grad))
+
+        self.optimizer = optim.Adam(list(self.model.parameters()) + list(self.tra.parameters()), lr=self.lr)
+
+        self.fitted = False
+        self.global_step = -1
+
+    def train_epoch(self, epoch, data_set, is_pretrain=False):
+        self.model.train()
+        self.tra.train()
+        data_set.train()
+        self.optimizer.zero_grad()
+
+        P_all = []
+        prob_all = []
+        choice_all = []
+        max_steps = len(data_set)
+        if self.max_steps_per_epoch is not None:
+            if epoch == 0 and self.max_steps_per_epoch < max_steps:
+                self.logger.info(f"max steps updated from {max_steps} to {self.max_steps_per_epoch}")
+            max_steps = min(self.max_steps_per_epoch, max_steps)
+
+        cur_step = 0
+        total_loss = 0
+        total_count = 0
+        for batch in tqdm(data_set, total=max_steps):
+            cur_step += 1
+            if cur_step > max_steps:
+                break
+
+            if not is_pretrain:
+                self.global_step += 1
+
+            data, state, label, count = batch["data"], batch["state"], batch["label"], batch["daily_count"]
+            index = batch["daily_index"] if self.use_daily_transport else batch["index"]
+
+            with torch.set_grad_enabled(not self.freeze_model):
+                hidden = self.model(data)
+
+            all_preds, choice, prob = self.tra(hidden, state)
+
+            if is_pretrain or self.transport_method != "none":
+                # NOTE: use oracle transport for pre-training
+                loss, pred, L, P = self.transport_fn(
+                    all_preds,
+                    label,
+                    choice,
+                    prob,
+                    state.mean(dim=1),
+                    count,
+                    self.transport_method if not is_pretrain else "oracle",
+                    self.alpha,
+                    training=True,
+                )
+                data_set.assign_data(index, L)  # save loss to memory
+                if self.use_daily_transport:  # only save for daily transport
+                    P_all.append(pd.DataFrame(P.detach().cpu().numpy(), index=index))
+                    prob_all.append(pd.DataFrame(prob.detach().cpu().numpy(), index=index))
+                    choice_all.append(pd.DataFrame(choice.detach().cpu().numpy(), index=index))
+                decay = self.rho ** (self.global_step // 100)  # decay every 100 steps
+                lamb = 0 if is_pretrain else self.lamb * decay
+                reg = prob.log().mul(P).sum(dim=1).mean()  # train router to predict TO assignment
+                if self._writer is not None and not is_pretrain:
+                    self._writer.add_scalar("training/router_loss", -reg.item(), self.global_step)
+                    self._writer.add_scalar("training/reg_loss", loss.item(), self.global_step)
+                    self._writer.add_scalar("training/lamb", lamb, self.global_step)
+                    if not self.use_daily_transport:
+                        P_mean = P.mean(axis=0).detach()
+                        self._writer.add_scalar("training/P", P_mean.max() / P_mean.min(), self.global_step)
+                loss = loss - lamb * reg
+            else:
+                pred = all_preds.mean(dim=1)
+                loss = loss_fn(pred, label)
+
+            (loss / self.update_freq).backward()
+            if cur_step % self.update_freq == 0:
+                self.optimizer.step()
+                self.optimizer.zero_grad()
+
+            if self._writer is not None and not is_pretrain:
+                self._writer.add_scalar("training/total_loss", loss.item(), self.global_step)
+
+            total_loss += loss.item()
+            total_count += 1
+
+        if self.use_daily_transport and len(P_all) > 0:
+            P_all = pd.concat(P_all, axis=0)
+            prob_all = pd.concat(prob_all, axis=0)
+            choice_all = pd.concat(choice_all, axis=0)
+            P_all.index = data_set.restore_daily_index(P_all.index)
+            prob_all.index = P_all.index
+            choice_all.index = P_all.index
+            if not is_pretrain:
+                self._writer.add_image("P", plot(P_all), epoch, dataformats="HWC")
+                self._writer.add_image("prob", plot(prob_all), epoch, dataformats="HWC")
+                self._writer.add_image("choice", plot(choice_all), epoch, dataformats="HWC")
+
+        total_loss /= total_count
+
+        if self._writer is not None and not is_pretrain:
+            self._writer.add_scalar("training/loss", total_loss, epoch)
+
+        return total_loss
+
+    def test_epoch(self, epoch, data_set, return_pred=False, prefix="test", is_pretrain=False):
+        self.model.eval()
+        self.tra.eval()
+        data_set.eval()
+
+        preds = []
+        probs = []
+        P_all = []
+        metrics = []
+        for batch in tqdm(data_set):
+            data, state, label, count = batch["data"], batch["state"], batch["label"], batch["daily_count"]
+            index = batch["daily_index"] if self.use_daily_transport else batch["index"]
+
+            with torch.no_grad():
+                hidden = self.model(data)
+                all_preds, choice, prob = self.tra(hidden, state)
+
+            if is_pretrain or self.transport_method != "none":
+                loss, pred, L, P = self.transport_fn(
+                    all_preds,
+                    label,
+                    choice,
+                    prob,
+                    state.mean(dim=1),
+                    count,
+                    self.transport_method if not is_pretrain else "oracle",
+                    self.alpha,
+                    training=False,
+                )
+                data_set.assign_data(index, L)  # save loss to memory
+                if P is not None and return_pred:
+                    P_all.append(pd.DataFrame(P.cpu().numpy(), index=index))
+            else:
+                pred = all_preds.mean(dim=1)
+
+            X = np.c_[pred.cpu().numpy(), label.cpu().numpy(), all_preds.cpu().numpy()]
+            columns = ["score", "label"] + ["score_%d" % d for d in range(all_preds.shape[1])]
+            pred = pd.DataFrame(X, index=batch["index"], columns=columns)
+
+            metrics.append(evaluate(pred))
+
+            if return_pred:
+                preds.append(pred)
+                if prob is not None:
+                    columns = ["prob_%d" % d for d in range(all_preds.shape[1])]
+                    probs.append(pd.DataFrame(prob.cpu().numpy(), index=index, columns=columns))
+
+        metrics = pd.DataFrame(metrics)
+        metrics = {
+            "MSE": metrics.MSE.mean(),
+            "MAE": metrics.MAE.mean(),
+            "IC": metrics.IC.mean(),
+            "ICIR": metrics.IC.mean() / metrics.IC.std(),
+        }
+
+        if self._writer is not None and epoch >= 0 and not is_pretrain:
+            for key, value in metrics.items():
+                self._writer.add_scalar(prefix + "/" + key, value, epoch)
+
+        if return_pred:
+            preds = pd.concat(preds, axis=0)
+            preds.index = data_set.restore_index(preds.index)
+            preds.index = preds.index.swaplevel()
+            preds.sort_index(inplace=True)
+
+            if probs:
+                probs = pd.concat(probs, axis=0)
+                if self.use_daily_transport:
+                    probs.index = data_set.restore_daily_index(probs.index)
+                else:
+                    probs.index = data_set.restore_index(probs.index)
+                    probs.index = probs.index.swaplevel()
+                    probs.sort_index(inplace=True)
+
+            if len(P_all):
+                P_all = pd.concat(P_all, axis=0)
+                if self.use_daily_transport:
+                    P_all.index = data_set.restore_daily_index(P_all.index)
+                else:
+                    P_all.index = data_set.restore_index(P_all.index)
+                    P_all.index = P_all.index.swaplevel()
+                    P_all.sort_index(inplace=True)
+
+        return metrics, preds, probs, P_all
+
+    def _fit(self, train_set, valid_set, test_set, evals_result, is_pretrain=True):
+        best_score = -1
+        best_epoch = 0
+        stop_rounds = 0
+        best_params = {
+            "model": copy.deepcopy(self.model.state_dict()),
+            "tra": copy.deepcopy(self.tra.state_dict()),
+        }
+        # train
+        if not is_pretrain and self.transport_method != "none":
+            self.logger.info("init memory...")
+            self.test_epoch(-1, train_set)
+
+        for epoch in range(self.n_epochs):
+            self.logger.info("Epoch %d:", epoch)
+
+            self.logger.info("training...")
+            self.train_epoch(epoch, train_set, is_pretrain=is_pretrain)
+
+            self.logger.info("evaluating...")
+            # NOTE: during evaluating, the whole memory will be refreshed
+            if not is_pretrain and (self.transport_method == "router" or self.eval_train):
+                train_set.clear_memory()  # NOTE: clear the shared memory
+                train_metrics = self.test_epoch(epoch, train_set, is_pretrain=is_pretrain, prefix="train")[0]
+                evals_result["train"].append(train_metrics)
+                self.logger.info("train metrics: %s" % train_metrics)
+
+            valid_metrics = self.test_epoch(epoch, valid_set, is_pretrain=is_pretrain, prefix="valid")[0]
+            evals_result["valid"].append(valid_metrics)
+            self.logger.info("valid metrics: %s" % valid_metrics)
+
+            if self.eval_test:
+                test_metrics = self.test_epoch(epoch, test_set, is_pretrain=is_pretrain, prefix="test")[0]
+                evals_result["test"].append(test_metrics)
+                self.logger.info("test metrics: %s" % test_metrics)
+
+            if valid_metrics["IC"] > best_score:
+                best_score = valid_metrics["IC"]
+                stop_rounds = 0
+                best_epoch = epoch
+                best_params = {
+                    "model": copy.deepcopy(self.model.state_dict()),
+                    "tra": copy.deepcopy(self.tra.state_dict()),
+                }
+                if self.logdir is not None:
+                    torch.save(best_params, self.logdir + "/model.bin")
+            else:
+                stop_rounds += 1
+                if stop_rounds >= self.early_stop:
+                    self.logger.info("early stop @ %s" % epoch)
+                    break
+
+        self.logger.info("best score: %.6lf @ %d" % (best_score, best_epoch))
+        self.model.load_state_dict(best_params["model"])
+        self.tra.load_state_dict(best_params["tra"])
+
+        return best_score
+
+    def fit(self, dataset, evals_result=dict()):
+        assert isinstance(dataset, MTSDatasetH), "TRAModel only supports `qlib.contrib.data.dataset.MTSDatasetH`"
+
+        train_set, valid_set, test_set = dataset.prepare(["train", "valid", "test"])
+
+        self.fitted = True
+        self.global_step = -1
+
+        evals_result["train"] = []
+        evals_result["valid"] = []
+        evals_result["test"] = []
+
+        if self.pretrain:
+            self.logger.info("pretraining...")
+            self.optimizer = optim.Adam(
+                list(self.model.parameters()) + list(self.tra.predictors.parameters()), lr=self.lr
+            )
+            self._fit(train_set, valid_set, test_set, evals_result, is_pretrain=True)
+
+            # reset optimizer
+            self.optimizer = optim.Adam(list(self.model.parameters()) + list(self.tra.parameters()), lr=self.lr)
+
+        self.logger.info("training...")
+        best_score = self._fit(train_set, valid_set, test_set, evals_result, is_pretrain=False)
+
+        self.logger.info("inference")
+        train_metrics, train_preds, train_probs, train_P = self.test_epoch(-1, train_set, return_pred=True)
+        self.logger.info("train metrics: %s" % train_metrics)
+
+        valid_metrics, valid_preds, valid_probs, valid_P = self.test_epoch(-1, valid_set, return_pred=True)
+        self.logger.info("valid metrics: %s" % valid_metrics)
+
+        test_metrics, test_preds, test_probs, test_P = self.test_epoch(-1, test_set, return_pred=True)
+        self.logger.info("test metrics: %s" % test_metrics)
+
+        if self.logdir:
+            self.logger.info("save model & pred to local directory")
+
+            pd.concat({name: pd.DataFrame(evals_result[name]) for name in evals_result}, axis=1).to_csv(
+                self.logdir + "/logs.csv", index=False
+            )
+
+            torch.save({"model": self.model.state_dict(), "tra": self.tra.state_dict()}, self.logdir + "/model.bin")
+
+            train_preds.to_pickle(self.logdir + "/train_pred.pkl")
+            valid_preds.to_pickle(self.logdir + "/valid_pred.pkl")
+            test_preds.to_pickle(self.logdir + "/test_pred.pkl")
+
+            if len(train_probs):
+                train_probs.to_pickle(self.logdir + "/train_prob.pkl")
+                valid_probs.to_pickle(self.logdir + "/valid_prob.pkl")
+                test_probs.to_pickle(self.logdir + "/test_prob.pkl")
+
+            if len(train_P):
+                train_P.to_pickle(self.logdir + "/train_P.pkl")
+                valid_P.to_pickle(self.logdir + "/valid_P.pkl")
+                test_P.to_pickle(self.logdir + "/test_P.pkl")
+
+            info = {
+                "config": {
+                    "model_config": self.model_config,
+                    "tra_config": self.tra_config,
+                    "model_type": self.model_type,
+                    "lr": self.lr,
+                    "n_epochs": self.n_epochs,
+                    "early_stop": self.early_stop,
+                    "max_steps_per_epoch": self.max_steps_per_epoch,
+                    "lamb": self.lamb,
+                    "rho": self.rho,
+                    "alpha": self.alpha,
+                    "seed": self.seed,
+                    "logdir": self.logdir,
+                    "pretrain": self.pretrain,
+                    "init_state": self.init_state,
+                    "transport_method": self.transport_method,
+                    "use_daily_transport": self.use_daily_transport,
+                },
+                "best_eval_metric": -best_score,  # NOTE: -1 for minimize
+                "metrics": {"train": train_metrics, "valid": valid_metrics, "test": test_metrics},
+            }
+            with open(self.logdir + "/info.json", "w") as f:
+                json.dump(info, f)
+
+    def predict(self, dataset, segment="test"):
+        assert isinstance(dataset, MTSDatasetH), "TRAModel only supports `qlib.contrib.data.dataset.MTSDatasetH`"
+
+        if not self.fitted:
+            raise ValueError("model is not fitted yet!")
+
+        test_set = dataset.prepare(segment)
+
+        metrics, preds, _, _ = self.test_epoch(-1, test_set, return_pred=True)
+        self.logger.info("test metrics: %s" % metrics)
+
+        return preds
+
+
+class RNN(nn.Module):
+
+    """RNN Model
+
+    Args:
+        input_size (int): input size (# features)
+        hidden_size (int): hidden size
+        num_layers (int): number of hidden layers
+        rnn_arch (str): rnn architecture
+        use_attn (bool): whether use attention layer.
+            we use concat attention as https://github.com/fulifeng/Adv-ALSTM/
+        dropout (float): dropout rate
+    """
+
+    def __init__(
+        self,
+        input_size=16,
+        hidden_size=64,
+        num_layers=2,
+        rnn_arch="GRU",
+        use_attn=True,
+        dropout=0.0,
+        **kwargs,
+    ):
+        super().__init__()
+
+        self.input_size = input_size
+        self.hidden_size = hidden_size
+        self.num_layers = num_layers
+        self.rnn_arch = rnn_arch
+        self.use_attn = use_attn
+
+        if hidden_size < input_size:
+            # compression
+            self.input_proj = nn.Linear(input_size, hidden_size)
+        else:
+            self.input_proj = None
+
+        self.rnn = getattr(nn, rnn_arch)(
+            input_size=min(input_size, hidden_size),
+            hidden_size=hidden_size,
+            num_layers=num_layers,
+            batch_first=True,
+            dropout=dropout,
+        )
+
+        if self.use_attn:
+            self.W = nn.Linear(hidden_size, hidden_size)
+            self.u = nn.Linear(hidden_size, 1, bias=False)
+            self.output_size = hidden_size * 2
+        else:
+            self.output_size = hidden_size
+
+    def forward(self, x):
+        if self.input_proj is not None:
+            x = self.input_proj(x)
+
+        rnn_out, last_out = self.rnn(x)
+        if self.rnn_arch == "LSTM":
+            last_out = last_out[0]
+        last_out = last_out.mean(dim=0)
+
+        if self.use_attn:
+            laten = self.W(rnn_out).tanh()
+            scores = self.u(laten).softmax(dim=1)
+            att_out = (rnn_out * scores).sum(dim=1)
+            last_out = torch.cat([last_out, att_out], dim=1)
+
+        return last_out
+
+
+class PositionalEncoding(nn.Module):
+    # reference: https://pytorch.org/tutorials/beginner/transformer_tutorial.html
+    def __init__(self, d_model, dropout=0.1, max_len=5000):
+        super(PositionalEncoding, self).__init__()
+        self.dropout = nn.Dropout(p=dropout)
+
+        pe = torch.zeros(max_len, d_model)
+        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
+        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
+        pe[:, 0::2] = torch.sin(position * div_term)
+        pe[:, 1::2] = torch.cos(position * div_term)
+        pe = pe.unsqueeze(0).transpose(0, 1)
+        self.register_buffer("pe", pe)
+
+    def forward(self, x):
+        x = x + self.pe[: x.size(0), :]
+        return self.dropout(x)
+
+
+class Transformer(nn.Module):
+
+    """Transformer Model
+
+    Args:
+        input_size (int): input size (# features)
+        hidden_size (int): hidden size
+        num_layers (int): number of transformer layers
+        num_heads (int): number of heads in transformer
+        dropout (float): dropout rate
+    """
+
+    def __init__(
+        self,
+        input_size=16,
+        hidden_size=64,
+        num_layers=2,
+        num_heads=2,
+        dropout=0.0,
+        **kwargs,
+    ):
+        super().__init__()
+
+        self.input_size = input_size
+        self.hidden_size = hidden_size
+        self.num_layers = num_layers
+        self.num_heads = num_heads
+
+        self.input_proj = nn.Linear(input_size, hidden_size)
+
+        self.pe = PositionalEncoding(input_size, dropout)
+        layer = nn.TransformerEncoderLayer(
+            nhead=num_heads, dropout=dropout, d_model=hidden_size, dim_feedforward=hidden_size * 4
+        )
+        self.encoder = nn.TransformerEncoder(layer, num_layers=num_layers)
+
+        self.output_size = hidden_size
+
+    def forward(self, x):
+        x = x.permute(1, 0, 2).contiguous()  # the first dim need to be time
+        x = self.pe(x)
+
+        x = self.input_proj(x)
+        out = self.encoder(x)
+
+        return out[-1]
+
+
+class TRA(nn.Module):
+
+    """Temporal Routing Adaptor (TRA)
+
+    TRA takes historical prediction errors & latent representation as inputs,
+    then routes the input sample to a specific predictor for training & inference.
+
+    Args:
+        input_size (int): input size (RNN/Transformer's hidden size)
+        num_states (int): number of latent states (i.e., trading patterns)
+            If `num_states=1`, then TRA falls back to traditional methods
+        hidden_size (int): hidden size of the router
+        tau (float): gumbel softmax temperature
+        src_info (str): information for the router
+    """
+
+    def __init__(
+        self,
+        input_size,
+        num_states=1,
+        hidden_size=8,
+        rnn_arch="GRU",
+        num_layers=1,
+        dropout=0.0,
+        tau=1.0,
+        src_info="LR_TPE",
+    ):
+        super().__init__()
+
+        assert src_info in ["LR", "TPE", "LR_TPE"], "invalid `src_info`"
+
+        self.num_states = num_states
+        self.tau = tau
+        self.rnn_arch = rnn_arch
+        self.src_info = src_info
+
+        self.predictors = nn.Linear(input_size, num_states)
+
+        if self.num_states > 1:
+            if "TPE" in src_info:
+                self.router = getattr(nn, rnn_arch)(
+                    input_size=num_states,
+                    hidden_size=hidden_size,
+                    num_layers=num_layers,
+                    batch_first=True,
+                    dropout=dropout,
+                )
+                self.fc = nn.Linear(hidden_size + input_size if "LR" in src_info else hidden_size, num_states)
+            else:
+                self.fc = nn.Linear(input_size, num_states)
+
+    def reset_parameters(self):
+        for child in self.children():
+            child.reset_parameters()
+
+    def forward(self, hidden, hist_loss):
+        preds = self.predictors(hidden)
+
+        if self.num_states == 1:  # no need for router when having only one prediction
+            return preds, None, None
+
+        if "TPE" in self.src_info:
+            out = self.router(hist_loss)[1]  # TPE
+            if self.rnn_arch == "LSTM":
+                out = out[0]
+            out = out.mean(dim=0)
+            if "LR" in self.src_info:
+                out = torch.cat([hidden, out], dim=-1)  # LR_TPE
+        else:
+            out = hidden  # LR
+
+        out = self.fc(out)
+
+        choice = F.gumbel_softmax(out, dim=-1, tau=self.tau, hard=True)
+        prob = torch.softmax(out / self.tau, dim=-1)
+
+        return preds, choice, prob
+
+
+def evaluate(pred):
+    pred = pred.rank(pct=True)  # transform into percentiles
+    score = pred.score
+    label = pred.label
+    diff = score - label
+    MSE = (diff**2).mean()
+    MAE = (diff.abs()).mean()
+    IC = score.corr(label, method="spearman")
+    return {"MSE": MSE, "MAE": MAE, "IC": IC}
+
+
+def shoot_infs(inp_tensor):
+    """Replaces inf by maximum of tensor"""
+    mask_inf = torch.isinf(inp_tensor)
+    ind_inf = torch.nonzero(mask_inf, as_tuple=False)
+    if len(ind_inf) > 0:
+        for ind in ind_inf:
+            if len(ind) == 2:
+                inp_tensor[ind[0], ind[1]] = 0
+            elif len(ind) == 1:
+                inp_tensor[ind[0]] = 0
+        m = torch.max(inp_tensor)
+        for ind in ind_inf:
+            if len(ind) == 2:
+                inp_tensor[ind[0], ind[1]] = m
+            elif len(ind) == 1:
+                inp_tensor[ind[0]] = m
+    return inp_tensor
+
+
+def sinkhorn(Q, n_iters=3, epsilon=0.1):
+    # epsilon should be adjusted according to logits value's scale
+    with torch.no_grad():
+        Q = torch.exp(Q / epsilon)
+        Q = shoot_infs(Q)
+        for i in range(n_iters):
+            Q /= Q.sum(dim=0, keepdim=True)
+            Q /= Q.sum(dim=1, keepdim=True)
+    return Q
+
+
+def loss_fn(pred, label):
+    mask = ~torch.isnan(label)
+    if len(pred.shape) == 2:
+        label = label[:, None]
+    return (pred[mask] - label[mask]).pow(2).mean(dim=0)
+
+
+def minmax_norm(x):
+    xmin = x.min(dim=-1, keepdim=True).values
+    xmax = x.max(dim=-1, keepdim=True).values
+    mask = (xmin == xmax).squeeze()
+    x = (x - xmin) / (xmax - xmin + EPS)
+    x[mask] = 1
+    return x
+
+
+def transport_sample(all_preds, label, choice, prob, hist_loss, count, transport_method, alpha, training=False):
+    """
+    sample-wise transport
+
+    Args:
+        all_preds (torch.Tensor): predictions from all predictors, [sample x states]
+        label (torch.Tensor): label, [sample]
+        choice (torch.Tensor): gumbel softmax choice, [sample x states]
+        prob (torch.Tensor): router predicted probility, [sample x states]
+        hist_loss (torch.Tensor): history loss matrix, [sample x states]
+        count (list): sample counts for each day, empty list for sample-wise transport
+        transport_method (str): transportation method
+        alpha (float): fusion parameter for calculating transport loss matrix
+        training (bool): indicate training or inference
+    """
+    assert all_preds.shape == choice.shape
+    assert len(all_preds) == len(label)
+    assert transport_method in ["oracle", "router"]
+
+    all_loss = torch.zeros_like(all_preds)
+    mask = ~torch.isnan(label)
+    all_loss[mask] = (all_preds[mask] - label[mask, None]).pow(2)  # [sample x states]
+
+    L = minmax_norm(all_loss.detach())
+    Lh = L * alpha + minmax_norm(hist_loss) * (1 - alpha)  # add hist loss for transport
+    Lh = minmax_norm(Lh)
+    P = sinkhorn(-Lh)
+    del Lh
+
+    if transport_method == "router":
+        if training:
+            pred = (all_preds * choice).sum(dim=1)  # gumbel softmax
+        else:
+            pred = all_preds[range(len(all_preds)), prob.argmax(dim=-1)]  # argmax
+    else:
+        pred = (all_preds * P).sum(dim=1)
+
+    if transport_method == "router":
+        loss = loss_fn(pred, label)
+    else:
+        loss = (all_loss * P).sum(dim=1).mean()
+
+    return loss, pred, L, P
+
+
+def transport_daily(all_preds, label, choice, prob, hist_loss, count, transport_method, alpha, training=False):
+    """
+    daily transport
+
+    Args:
+        all_preds (torch.Tensor): predictions from all predictors, [sample x states]
+        label (torch.Tensor): label, [sample]
+        choice (torch.Tensor): gumbel softmax choice, [days x states]
+        prob (torch.Tensor): router predicted probility, [days x states]
+        hist_loss (torch.Tensor): history loss matrix, [days x states]
+        count (list): sample counts for each day, [days]
+        transport_method (str): transportation method
+        alpha (float): fusion parameter for calculating transport loss matrix
+        training (bool): indicate training or inference
+    """
+    assert len(prob) == len(count)
+    assert len(all_preds) == sum(count)
+    assert transport_method in ["oracle", "router"]
+
+    all_loss = []  # loss of all predictions
+    start = 0
+    for i, cnt in enumerate(count):
+        slc = slice(start, start + cnt)  # samples from the i-th day
+        start += cnt
+        tloss = loss_fn(all_preds[slc], label[slc])  # loss of the i-th day
+        all_loss.append(tloss)
+    all_loss = torch.stack(all_loss, dim=0)  # [days x states]
+
+    L = minmax_norm(all_loss.detach())
+    Lh = L * alpha + minmax_norm(hist_loss) * (1 - alpha)  # add hist loss for transport
+    Lh = minmax_norm(Lh)
+    P = sinkhorn(-Lh)
+    del Lh
+
+    pred = []
+    start = 0
+    for i, cnt in enumerate(count):
+        slc = slice(start, start + cnt)  # samples from the i-th day
+        start += cnt
+        if transport_method == "router":
+            if training:
+                tpred = all_preds[slc] @ choice[i]  # gumbel softmax
+            else:
+                tpred = all_preds[slc][:, prob[i].argmax(dim=-1)]  # argmax
+        else:
+            tpred = all_preds[slc] @ P[i]
+        pred.append(tpred)
+    pred = torch.cat(pred, dim=0)  # [samples]
+
+    if transport_method == "router":
+        loss = loss_fn(pred, label)
+    else:
+        loss = (all_loss * P).sum(dim=1).mean()
+
+    return loss, pred, L, P
+
+
+def load_state_dict_unsafe(model, state_dict):
+    """
+    Load state dict to provided model while ignore exceptions.
+    """
+
+    missing_keys = []
+    unexpected_keys = []
+    error_msgs = []
+
+    # copy state_dict so _load_from_state_dict can modify it
+    metadata = getattr(state_dict, "_metadata", None)
+    state_dict = state_dict.copy()
+    if metadata is not None:
+        state_dict._metadata = metadata
+
+    def load(module, prefix=""):
+        local_metadata = {} if metadata is None else metadata.get(prefix[:-1], {})
+        module._load_from_state_dict(
+            state_dict, prefix, local_metadata, True, missing_keys, unexpected_keys, error_msgs
+        )
+        for name, child in module._modules.items():
+            if child is not None:
+                load(child, prefix + name + ".")
+
+    load(model)
+    load = None  # break load->load reference cycle
+
+    return {"unexpected_keys": unexpected_keys, "missing_keys": missing_keys, "error_msgs": error_msgs}
+
+
+def plot(P):
+    assert isinstance(P, pd.DataFrame)
+
+    fig, axes = plt.subplots(1, 2, figsize=(10, 4))
+    P.plot.area(ax=axes[0], xlabel="")
+    P.idxmax(axis=1).value_counts().sort_index().plot.bar(ax=axes[1], xlabel="")
+    plt.tight_layout()
+
+    with io.BytesIO() as buf:
+        plt.savefig(buf, format="png")
+        buf.seek(0)
+        img = plt.imread(buf)
+        plt.close()
+
+    return np.uint8(img * 255)
```

## qlib/contrib/model/pytorch_transformer.py

```diff
@@ -41,15 +41,14 @@
         optimizer="adam",
         reg=1e-3,
         n_jobs=10,
         GPU=0,
         seed=None,
         **kwargs
     ):
-
         # set hyper-parameters.
         self.d_model = d_model
         self.dropout = dropout
         self.n_epochs = n_epochs
         self.lr = lr
         self.reg = reg
         self.metric = metric
@@ -91,34 +90,31 @@
 
         if self.loss == "mse":
             return self.mse(pred[mask], label[mask])
 
         raise ValueError("unknown loss `%s`" % self.loss)
 
     def metric_fn(self, pred, label):
-
         mask = torch.isfinite(label)
 
         if self.metric in ("", "loss"):
             return -self.loss_fn(pred[mask], label[mask])
 
         raise ValueError("unknown metric `%s`" % self.metric)
 
     def train_epoch(self, x_train, y_train):
-
         x_train_values = x_train.values
         y_train_values = np.squeeze(y_train.values)
 
         self.model.train()
 
         indices = np.arange(len(x_train_values))
         np.random.shuffle(indices)
 
         for i in range(len(indices))[:: self.batch_size]:
-
             if len(indices) - i < self.batch_size:
                 break
 
             feature = torch.from_numpy(x_train_values[indices[i : i + self.batch_size]]).float().to(self.device)
             label = torch.from_numpy(y_train_values[indices[i : i + self.batch_size]]).float().to(self.device)
 
             pred = self.model(feature)
@@ -126,28 +122,26 @@
 
             self.train_optimizer.zero_grad()
             loss.backward()
             torch.nn.utils.clip_grad_value_(self.model.parameters(), 3.0)
             self.train_optimizer.step()
 
     def test_epoch(self, data_x, data_y):
-
         # prepare training data
         x_values = data_x.values
         y_values = np.squeeze(data_y.values)
 
         self.model.eval()
 
         scores = []
         losses = []
 
         indices = np.arange(len(x_values))
 
         for i in range(len(indices))[:: self.batch_size]:
-
             if len(indices) - i < self.batch_size:
                 break
 
             feature = torch.from_numpy(x_values[indices[i : i + self.batch_size]]).float().to(self.device)
             label = torch.from_numpy(y_values[indices[i : i + self.batch_size]]).float().to(self.device)
 
             with torch.no_grad():
@@ -162,15 +156,14 @@
 
     def fit(
         self,
         dataset: DatasetH,
         evals_result=dict(),
         save_path=None,
     ):
-
         df_train, df_valid, df_test = dataset.prepare(
             ["train", "valid", "test"],
             col_set=["feature", "label"],
             data_key=DataHandlerLP.DK_L,
         )
         if df_train.empty or df_valid.empty:
             raise ValueError("Empty data from dataset, please check your dataset config.")
@@ -227,15 +220,14 @@
         index = x_test.index
         self.model.eval()
         x_values = x_test.values
         sample_num = x_values.shape[0]
         preds = []
 
         for begin in range(sample_num)[:: self.batch_size]:
-
             if sample_num - begin < self.batch_size:
                 end = sample_num
             else:
                 end = begin + self.batch_size
 
             x_batch = torch.from_numpy(x_values[begin:end]).float().to(self.device)
```

## qlib/contrib/model/pytorch_transformer_ts.py

```diff
@@ -39,15 +39,14 @@
         optimizer="adam",
         reg=1e-3,
         n_jobs=10,
         GPU=0,
         seed=None,
         **kwargs
     ):
-
         # set hyper-parameters.
         self.d_model = d_model
         self.dropout = dropout
         self.n_epochs = n_epochs
         self.lr = lr
         self.reg = reg
         self.metric = metric
@@ -89,24 +88,22 @@
 
         if self.loss == "mse":
             return self.mse(pred[mask], label[mask])
 
         raise ValueError("unknown loss `%s`" % self.loss)
 
     def metric_fn(self, pred, label):
-
         mask = torch.isfinite(label)
 
         if self.metric in ("", "loss"):
             return -self.loss_fn(pred[mask], label[mask])
 
         raise ValueError("unknown metric `%s`" % self.metric)
 
     def train_epoch(self, data_loader):
-
         self.model.train()
 
         for data in data_loader:
             feature = data[:, :, 0:-1].to(self.device)
             label = data[:, -1, -1].to(self.device)
 
             pred = self.model(feature.float())  # .float()
@@ -114,22 +111,20 @@
 
             self.train_optimizer.zero_grad()
             loss.backward()
             torch.nn.utils.clip_grad_value_(self.model.parameters(), 3.0)
             self.train_optimizer.step()
 
     def test_epoch(self, data_loader):
-
         self.model.eval()
 
         scores = []
         losses = []
 
         for data in data_loader:
-
             feature = data[:, :, 0:-1].to(self.device)
             label = data[:, -1, -1].to(self.device)
 
             with torch.no_grad():
                 pred = self.model(feature.float())  # .float()
                 loss = self.loss_fn(pred, label)
                 losses.append(loss.item())
@@ -141,15 +136,14 @@
 
     def fit(
         self,
         dataset: DatasetH,
         evals_result=dict(),
         save_path=None,
     ):
-
         dl_train = dataset.prepare("train", col_set=["feature", "label"], data_key=DataHandlerLP.DK_L)
         dl_valid = dataset.prepare("valid", col_set=["feature", "label"], data_key=DataHandlerLP.DK_L)
 
         if dl_train.empty or dl_valid.empty:
             raise ValueError("Empty data from dataset, please check your dataset config.")
 
         dl_train.config(fillna_type="ffill+bfill")  # process nan brought by dataloader
```

## qlib/contrib/model/pytorch_utils.py

 * *Ordering differences only*

```diff
@@ -1,37 +1,37 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-import torch.nn as nn
-
-
-def count_parameters(models_or_parameters, unit="m"):
-    """
-    This function is to obtain the storage size unit of a (or multiple) models.
-
-    Parameters
-    ----------
-    models_or_parameters : PyTorch model(s) or a list of parameters.
-    unit : the storage size unit.
-
-    Returns
-    -------
-    The number of parameters of the given model(s) or parameters.
-    """
-    if isinstance(models_or_parameters, nn.Module):
-        counts = sum(v.numel() for v in models_or_parameters.parameters())
-    elif isinstance(models_or_parameters, nn.Parameter):
-        counts = models_or_parameters.numel()
-    elif isinstance(models_or_parameters, (list, tuple)):
-        return sum(count_parameters(x, unit) for x in models_or_parameters)
-    else:
-        counts = sum(v.numel() for v in models_or_parameters)
-    unit = unit.lower()
-    if unit in ("kb", "k"):
-        counts /= 2**10
-    elif unit in ("mb", "m"):
-        counts /= 2**20
-    elif unit in ("gb", "g"):
-        counts /= 2**30
-    elif unit is not None:
-        raise ValueError("Unknown unit: {:}".format(unit))
-    return counts
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+import torch.nn as nn
+
+
+def count_parameters(models_or_parameters, unit="m"):
+    """
+    This function is to obtain the storage size unit of a (or multiple) models.
+
+    Parameters
+    ----------
+    models_or_parameters : PyTorch model(s) or a list of parameters.
+    unit : the storage size unit.
+
+    Returns
+    -------
+    The number of parameters of the given model(s) or parameters.
+    """
+    if isinstance(models_or_parameters, nn.Module):
+        counts = sum(v.numel() for v in models_or_parameters.parameters())
+    elif isinstance(models_or_parameters, nn.Parameter):
+        counts = models_or_parameters.numel()
+    elif isinstance(models_or_parameters, (list, tuple)):
+        return sum(count_parameters(x, unit) for x in models_or_parameters)
+    else:
+        counts = sum(v.numel() for v in models_or_parameters)
+    unit = unit.lower()
+    if unit in ("kb", "k"):
+        counts /= 2**10
+    elif unit in ("mb", "m"):
+        counts /= 2**20
+    elif unit in ("gb", "g"):
+        counts /= 2**30
+    elif unit is not None:
+        raise ValueError("Unknown unit: {:}".format(unit))
+    return counts
```

## qlib/contrib/model/tcn.py

 * *Ordering differences only*

```diff
@@ -1,76 +1,76 @@
-# MIT License
-# Copyright (c) 2018 CMU Locus Lab
-import torch.nn as nn
-from torch.nn.utils import weight_norm
-
-
-class Chomp1d(nn.Module):
-    def __init__(self, chomp_size):
-        super(Chomp1d, self).__init__()
-        self.chomp_size = chomp_size
-
-    def forward(self, x):
-        return x[:, :, : -self.chomp_size].contiguous()
-
-
-class TemporalBlock(nn.Module):
-    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):
-        super(TemporalBlock, self).__init__()
-        self.conv1 = weight_norm(
-            nn.Conv1d(n_inputs, n_outputs, kernel_size, stride=stride, padding=padding, dilation=dilation)
-        )
-        self.chomp1 = Chomp1d(padding)
-        self.relu1 = nn.ReLU()
-        self.dropout1 = nn.Dropout(dropout)
-
-        self.conv2 = weight_norm(
-            nn.Conv1d(n_outputs, n_outputs, kernel_size, stride=stride, padding=padding, dilation=dilation)
-        )
-        self.chomp2 = Chomp1d(padding)
-        self.relu2 = nn.ReLU()
-        self.dropout2 = nn.Dropout(dropout)
-
-        self.net = nn.Sequential(
-            self.conv1, self.chomp1, self.relu1, self.dropout1, self.conv2, self.chomp2, self.relu2, self.dropout2
-        )
-        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None
-        self.relu = nn.ReLU()
-        self.init_weights()
-
-    def init_weights(self):
-        self.conv1.weight.data.normal_(0, 0.01)
-        self.conv2.weight.data.normal_(0, 0.01)
-        if self.downsample is not None:
-            self.downsample.weight.data.normal_(0, 0.01)
-
-    def forward(self, x):
-        out = self.net(x)
-        res = x if self.downsample is None else self.downsample(x)
-        return self.relu(out + res)
-
-
-class TemporalConvNet(nn.Module):
-    def __init__(self, num_inputs, num_channels, kernel_size=2, dropout=0.2):
-        super(TemporalConvNet, self).__init__()
-        layers = []
-        num_levels = len(num_channels)
-        for i in range(num_levels):
-            dilation_size = 2**i
-            in_channels = num_inputs if i == 0 else num_channels[i - 1]
-            out_channels = num_channels[i]
-            layers += [
-                TemporalBlock(
-                    in_channels,
-                    out_channels,
-                    kernel_size,
-                    stride=1,
-                    dilation=dilation_size,
-                    padding=(kernel_size - 1) * dilation_size,
-                    dropout=dropout,
-                )
-            ]
-
-        self.network = nn.Sequential(*layers)
-
-    def forward(self, x):
-        return self.network(x)
+# MIT License
+# Copyright (c) 2018 CMU Locus Lab
+import torch.nn as nn
+from torch.nn.utils import weight_norm
+
+
+class Chomp1d(nn.Module):
+    def __init__(self, chomp_size):
+        super(Chomp1d, self).__init__()
+        self.chomp_size = chomp_size
+
+    def forward(self, x):
+        return x[:, :, : -self.chomp_size].contiguous()
+
+
+class TemporalBlock(nn.Module):
+    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):
+        super(TemporalBlock, self).__init__()
+        self.conv1 = weight_norm(
+            nn.Conv1d(n_inputs, n_outputs, kernel_size, stride=stride, padding=padding, dilation=dilation)
+        )
+        self.chomp1 = Chomp1d(padding)
+        self.relu1 = nn.ReLU()
+        self.dropout1 = nn.Dropout(dropout)
+
+        self.conv2 = weight_norm(
+            nn.Conv1d(n_outputs, n_outputs, kernel_size, stride=stride, padding=padding, dilation=dilation)
+        )
+        self.chomp2 = Chomp1d(padding)
+        self.relu2 = nn.ReLU()
+        self.dropout2 = nn.Dropout(dropout)
+
+        self.net = nn.Sequential(
+            self.conv1, self.chomp1, self.relu1, self.dropout1, self.conv2, self.chomp2, self.relu2, self.dropout2
+        )
+        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None
+        self.relu = nn.ReLU()
+        self.init_weights()
+
+    def init_weights(self):
+        self.conv1.weight.data.normal_(0, 0.01)
+        self.conv2.weight.data.normal_(0, 0.01)
+        if self.downsample is not None:
+            self.downsample.weight.data.normal_(0, 0.01)
+
+    def forward(self, x):
+        out = self.net(x)
+        res = x if self.downsample is None else self.downsample(x)
+        return self.relu(out + res)
+
+
+class TemporalConvNet(nn.Module):
+    def __init__(self, num_inputs, num_channels, kernel_size=2, dropout=0.2):
+        super(TemporalConvNet, self).__init__()
+        layers = []
+        num_levels = len(num_channels)
+        for i in range(num_levels):
+            dilation_size = 2**i
+            in_channels = num_inputs if i == 0 else num_channels[i - 1]
+            out_channels = num_channels[i]
+            layers += [
+                TemporalBlock(
+                    in_channels,
+                    out_channels,
+                    kernel_size,
+                    stride=1,
+                    dilation=dilation_size,
+                    padding=(kernel_size - 1) * dilation_size,
+                    dropout=dropout,
+                )
+            ]
+
+        self.network = nn.Sequential(*layers)
+
+    def forward(self, x):
+        return self.network(x)
```

## qlib/contrib/model/xgboost.py

```diff
@@ -1,86 +1,85 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-import numpy as np
-import pandas as pd
-import xgboost as xgb
-from typing import Text, Union
-from ...model.base import Model
-from ...data.dataset import DatasetH
-from ...data.dataset.handler import DataHandlerLP
-from ...model.interpret.base import FeatureInt
-from ...data.dataset.weight import Reweighter
-
-
-class XGBModel(Model, FeatureInt):
-    """XGBModel Model"""
-
-    def __init__(self, **kwargs):
-        self._params = {}
-        self._params.update(kwargs)
-        self.model = None
-
-    def fit(
-        self,
-        dataset: DatasetH,
-        num_boost_round=1000,
-        early_stopping_rounds=50,
-        verbose_eval=20,
-        evals_result=dict(),
-        reweighter=None,
-        **kwargs
-    ):
-
-        df_train, df_valid = dataset.prepare(
-            ["train", "valid"],
-            col_set=["feature", "label"],
-            data_key=DataHandlerLP.DK_L,
-        )
-        x_train, y_train = df_train["feature"], df_train["label"]
-        x_valid, y_valid = df_valid["feature"], df_valid["label"]
-
-        # Lightgbm need 1D array as its label
-        if y_train.values.ndim == 2 and y_train.values.shape[1] == 1:
-            y_train_1d, y_valid_1d = np.squeeze(y_train.values), np.squeeze(y_valid.values)
-        else:
-            raise ValueError("XGBoost doesn't support multi-label training")
-
-        if reweighter is None:
-            w_train = None
-            w_valid = None
-        elif isinstance(reweighter, Reweighter):
-            w_train = reweighter.reweight(df_train)
-            w_valid = reweighter.reweight(df_valid)
-        else:
-            raise ValueError("Unsupported reweighter type.")
-
-        dtrain = xgb.DMatrix(x_train.values, label=y_train_1d, weight=w_train)
-        dvalid = xgb.DMatrix(x_valid.values, label=y_valid_1d, weight=w_valid)
-        self.model = xgb.train(
-            self._params,
-            dtrain=dtrain,
-            num_boost_round=num_boost_round,
-            evals=[(dtrain, "train"), (dvalid, "valid")],
-            early_stopping_rounds=early_stopping_rounds,
-            verbose_eval=verbose_eval,
-            evals_result=evals_result,
-            **kwargs
-        )
-        evals_result["train"] = list(evals_result["train"].values())[0]
-        evals_result["valid"] = list(evals_result["valid"].values())[0]
-
-    def predict(self, dataset: DatasetH, segment: Union[Text, slice] = "test"):
-        if self.model is None:
-            raise ValueError("model is not fitted yet!")
-        x_test = dataset.prepare(segment, col_set="feature", data_key=DataHandlerLP.DK_I)
-        return pd.Series(self.model.predict(xgb.DMatrix(x_test)), index=x_test.index)
-
-    def get_feature_importance(self, *args, **kwargs) -> pd.Series:
-        """get feature importance
-
-        Notes
-        -------
-            parameters reference:
-                https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.Booster.get_score
-        """
-        return pd.Series(self.model.get_score(*args, **kwargs)).sort_values(ascending=False)
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+import numpy as np
+import pandas as pd
+import xgboost as xgb
+from typing import Text, Union
+from ...model.base import Model
+from ...data.dataset import DatasetH
+from ...data.dataset.handler import DataHandlerLP
+from ...model.interpret.base import FeatureInt
+from ...data.dataset.weight import Reweighter
+
+
+class XGBModel(Model, FeatureInt):
+    """XGBModel Model"""
+
+    def __init__(self, **kwargs):
+        self._params = {}
+        self._params.update(kwargs)
+        self.model = None
+
+    def fit(
+        self,
+        dataset: DatasetH,
+        num_boost_round=1000,
+        early_stopping_rounds=50,
+        verbose_eval=20,
+        evals_result=dict(),
+        reweighter=None,
+        **kwargs
+    ):
+        df_train, df_valid = dataset.prepare(
+            ["train", "valid"],
+            col_set=["feature", "label"],
+            data_key=DataHandlerLP.DK_L,
+        )
+        x_train, y_train = df_train["feature"], df_train["label"]
+        x_valid, y_valid = df_valid["feature"], df_valid["label"]
+
+        # Lightgbm need 1D array as its label
+        if y_train.values.ndim == 2 and y_train.values.shape[1] == 1:
+            y_train_1d, y_valid_1d = np.squeeze(y_train.values), np.squeeze(y_valid.values)
+        else:
+            raise ValueError("XGBoost doesn't support multi-label training")
+
+        if reweighter is None:
+            w_train = None
+            w_valid = None
+        elif isinstance(reweighter, Reweighter):
+            w_train = reweighter.reweight(df_train)
+            w_valid = reweighter.reweight(df_valid)
+        else:
+            raise ValueError("Unsupported reweighter type.")
+
+        dtrain = xgb.DMatrix(x_train.values, label=y_train_1d, weight=w_train)
+        dvalid = xgb.DMatrix(x_valid.values, label=y_valid_1d, weight=w_valid)
+        self.model = xgb.train(
+            self._params,
+            dtrain=dtrain,
+            num_boost_round=num_boost_round,
+            evals=[(dtrain, "train"), (dvalid, "valid")],
+            early_stopping_rounds=early_stopping_rounds,
+            verbose_eval=verbose_eval,
+            evals_result=evals_result,
+            **kwargs
+        )
+        evals_result["train"] = list(evals_result["train"].values())[0]
+        evals_result["valid"] = list(evals_result["valid"].values())[0]
+
+    def predict(self, dataset: DatasetH, segment: Union[Text, slice] = "test"):
+        if self.model is None:
+            raise ValueError("model is not fitted yet!")
+        x_test = dataset.prepare(segment, col_set="feature", data_key=DataHandlerLP.DK_I)
+        return pd.Series(self.model.predict(xgb.DMatrix(x_test)), index=x_test.index)
+
+    def get_feature_importance(self, *args, **kwargs) -> pd.Series:
+        """get feature importance
+
+        Notes
+        -------
+            parameters reference:
+                https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.Booster.get_score
+        """
+        return pd.Series(self.model.get_score(*args, **kwargs)).sort_values(ascending=False)
```

## qlib/contrib/online/__init__.py

 * *Ordering differences only*

```diff
@@ -1,21 +1,21 @@
-# pylint: skip-file
-# flake8: noqa
-
-'''
-TODO:
-
-- Online needs that the model have such method
-    def get_data_with_date(self, date, **kwargs):
-        """
-        Will be called in online module
-        need to return the data that used to predict the label (score) of stocks at date.
-
-        :param
-            date: pd.Timestamp
-                predict date
-        :return:
-            data: the input data that used to predict the label (score) of stocks at predict date.
-        """
-        raise NotImplementedError("get_data_with_date for this model is not implemented.")
-
-'''
+# pylint: skip-file
+# flake8: noqa
+
+'''
+TODO:
+
+- Online needs that the model have such method
+    def get_data_with_date(self, date, **kwargs):
+        """
+        Will be called in online module
+        need to return the data that used to predict the label (score) of stocks at date.
+
+        :param
+            date: pd.Timestamp
+                predict date
+        :return:
+            data: the input data that used to predict the label (score) of stocks at predict date.
+        """
+        raise NotImplementedError("get_data_with_date for this model is not implemented.")
+
+'''
```

## qlib/contrib/online/manager.py

 * *Ordering differences only*

```diff
@@ -1,147 +1,147 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-# pylint: skip-file
-# flake8: noqa
-
-import yaml
-import pathlib
-import pandas as pd
-import shutil
-from ...backtest.account import Account
-from .user import User
-from .utils import load_instance, save_instance
-from ...utils import init_instance_by_config
-
-
-class UserManager:
-    def __init__(self, user_data_path, save_report=True):
-        """
-        This module is designed to manager the users in online system
-        all users' data were assumed to be saved in user_data_path
-            Parameter
-                user_data_path : string
-                    data path that all users' data were saved in
-
-        variables:
-            data_path : string
-                data path that all users' data were saved in
-            users_file : string
-                A path of the file record the add_date of users
-            save_report : bool
-                whether to save report after each trading process
-            users : dict{}
-                [user_id]->User()
-                the python dict save instances of User() for each user_id
-            user_record : pd.Dataframe
-                user_id(string), add_date(string)
-                indicate the add_date for each users
-        """
-        self.data_path = pathlib.Path(user_data_path)
-        self.users_file = self.data_path / "users.csv"
-        self.save_report = save_report
-        self.users = {}
-        self.user_record = None
-
-    def load_users(self):
-        """
-        load all users' data into manager
-        """
-        self.users = {}
-        self.user_record = pd.read_csv(self.users_file, index_col=0)
-        for user_id in self.user_record.index:
-            self.users[user_id] = self.load_user(user_id)
-
-    def load_user(self, user_id):
-        """
-        return a instance of User() represents a user to be processed
-            Parameter
-                user_id : string
-            :return
-                user : User()
-        """
-        account_path = self.data_path / user_id
-        strategy_file = self.data_path / user_id / "strategy_{}.pickle".format(user_id)
-        model_file = self.data_path / user_id / "model_{}.pickle".format(user_id)
-        cur_user_list = list(self.users)
-        if user_id in cur_user_list:
-            raise ValueError("User {} has been loaded".format(user_id))
-        else:
-            trade_account = Account(0)
-            trade_account.load_account(account_path)
-            strategy = load_instance(strategy_file)
-            model = load_instance(model_file)
-            user = User(account=trade_account, strategy=strategy, model=model)
-            return user
-
-    def save_user_data(self, user_id):
-        """
-        save a instance of User() to user data path
-            Parameter
-                user_id : string
-        """
-        if not user_id in self.users:
-            raise ValueError("Cannot find user {}".format(user_id))
-        self.users[user_id].account.save_account(self.data_path / user_id)
-        save_instance(
-            self.users[user_id].strategy,
-            self.data_path / user_id / "strategy_{}.pickle".format(user_id),
-        )
-        save_instance(
-            self.users[user_id].model,
-            self.data_path / user_id / "model_{}.pickle".format(user_id),
-        )
-
-    def add_user(self, user_id, config_file, add_date):
-        """
-        add the new user {user_id} into user data
-        will create a new folder named "{user_id}" in user data path
-            Parameter
-                user_id : string
-                init_cash : int
-                config_file : str/pathlib.Path()
-                   path of config file
-        """
-        config_file = pathlib.Path(config_file)
-        if not config_file.exists():
-            raise ValueError("Cannot find config file {}".format(config_file))
-        user_path = self.data_path / user_id
-        if user_path.exists():
-            raise ValueError("User data for {} already exists".format(user_id))
-
-        with config_file.open("r") as fp:
-            config = yaml.safe_load(fp)
-        # load model
-        model = init_instance_by_config(config["model"])
-
-        # load strategy
-        strategy = init_instance_by_config(config["strategy"])
-        init_args = strategy.get_init_args_from_model(model, add_date)
-        strategy.init(**init_args)
-
-        # init Account
-        trade_account = Account(init_cash=config["init_cash"])
-
-        # save user
-        user_path.mkdir()
-        save_instance(model, self.data_path / user_id / "model_{}.pickle".format(user_id))
-        save_instance(strategy, self.data_path / user_id / "strategy_{}.pickle".format(user_id))
-        trade_account.save_account(self.data_path / user_id)
-        user_record = pd.read_csv(self.users_file, index_col=0)
-        user_record.loc[user_id] = [add_date]
-        user_record.to_csv(self.users_file)
-
-    def remove_user(self, user_id):
-        """
-        remove user {user_id} in current user dataset
-        will delete the folder "{user_id}" in user data path
-            :param
-                user_id : string
-        """
-        user_path = self.data_path / user_id
-        if not user_path.exists():
-            raise ValueError("Cannot find user data {}".format(user_id))
-        shutil.rmtree(user_path)
-        user_record = pd.read_csv(self.users_file, index_col=0)
-        user_record.drop([user_id], inplace=True)
-        user_record.to_csv(self.users_file)
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+# pylint: skip-file
+# flake8: noqa
+
+import yaml
+import pathlib
+import pandas as pd
+import shutil
+from ...backtest.account import Account
+from .user import User
+from .utils import load_instance, save_instance
+from ...utils import init_instance_by_config
+
+
+class UserManager:
+    def __init__(self, user_data_path, save_report=True):
+        """
+        This module is designed to manager the users in online system
+        all users' data were assumed to be saved in user_data_path
+            Parameter
+                user_data_path : string
+                    data path that all users' data were saved in
+
+        variables:
+            data_path : string
+                data path that all users' data were saved in
+            users_file : string
+                A path of the file record the add_date of users
+            save_report : bool
+                whether to save report after each trading process
+            users : dict{}
+                [user_id]->User()
+                the python dict save instances of User() for each user_id
+            user_record : pd.Dataframe
+                user_id(string), add_date(string)
+                indicate the add_date for each users
+        """
+        self.data_path = pathlib.Path(user_data_path)
+        self.users_file = self.data_path / "users.csv"
+        self.save_report = save_report
+        self.users = {}
+        self.user_record = None
+
+    def load_users(self):
+        """
+        load all users' data into manager
+        """
+        self.users = {}
+        self.user_record = pd.read_csv(self.users_file, index_col=0)
+        for user_id in self.user_record.index:
+            self.users[user_id] = self.load_user(user_id)
+
+    def load_user(self, user_id):
+        """
+        return a instance of User() represents a user to be processed
+            Parameter
+                user_id : string
+            :return
+                user : User()
+        """
+        account_path = self.data_path / user_id
+        strategy_file = self.data_path / user_id / "strategy_{}.pickle".format(user_id)
+        model_file = self.data_path / user_id / "model_{}.pickle".format(user_id)
+        cur_user_list = list(self.users)
+        if user_id in cur_user_list:
+            raise ValueError("User {} has been loaded".format(user_id))
+        else:
+            trade_account = Account(0)
+            trade_account.load_account(account_path)
+            strategy = load_instance(strategy_file)
+            model = load_instance(model_file)
+            user = User(account=trade_account, strategy=strategy, model=model)
+            return user
+
+    def save_user_data(self, user_id):
+        """
+        save a instance of User() to user data path
+            Parameter
+                user_id : string
+        """
+        if not user_id in self.users:
+            raise ValueError("Cannot find user {}".format(user_id))
+        self.users[user_id].account.save_account(self.data_path / user_id)
+        save_instance(
+            self.users[user_id].strategy,
+            self.data_path / user_id / "strategy_{}.pickle".format(user_id),
+        )
+        save_instance(
+            self.users[user_id].model,
+            self.data_path / user_id / "model_{}.pickle".format(user_id),
+        )
+
+    def add_user(self, user_id, config_file, add_date):
+        """
+        add the new user {user_id} into user data
+        will create a new folder named "{user_id}" in user data path
+            Parameter
+                user_id : string
+                init_cash : int
+                config_file : str/pathlib.Path()
+                   path of config file
+        """
+        config_file = pathlib.Path(config_file)
+        if not config_file.exists():
+            raise ValueError("Cannot find config file {}".format(config_file))
+        user_path = self.data_path / user_id
+        if user_path.exists():
+            raise ValueError("User data for {} already exists".format(user_id))
+
+        with config_file.open("r") as fp:
+            config = yaml.safe_load(fp)
+        # load model
+        model = init_instance_by_config(config["model"])
+
+        # load strategy
+        strategy = init_instance_by_config(config["strategy"])
+        init_args = strategy.get_init_args_from_model(model, add_date)
+        strategy.init(**init_args)
+
+        # init Account
+        trade_account = Account(init_cash=config["init_cash"])
+
+        # save user
+        user_path.mkdir()
+        save_instance(model, self.data_path / user_id / "model_{}.pickle".format(user_id))
+        save_instance(strategy, self.data_path / user_id / "strategy_{}.pickle".format(user_id))
+        trade_account.save_account(self.data_path / user_id)
+        user_record = pd.read_csv(self.users_file, index_col=0)
+        user_record.loc[user_id] = [add_date]
+        user_record.to_csv(self.users_file)
+
+    def remove_user(self, user_id):
+        """
+        remove user {user_id} in current user dataset
+        will delete the folder "{user_id}" in user data path
+            :param
+                user_id : string
+        """
+        user_path = self.data_path / user_id
+        if not user_path.exists():
+            raise ValueError("Cannot find user data {}".format(user_id))
+        shutil.rmtree(user_path)
+        user_record = pd.read_csv(self.users_file, index_col=0)
+        user_record.drop([user_id], inplace=True)
+        user_record.to_csv(self.users_file)
```

## qlib/contrib/online/online_model.py

 * *Ordering differences only*

```diff
@@ -1,39 +1,39 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-# pylint: skip-file
-# flake8: noqa
-
-import random
-import pandas as pd
-from ...data import D
-from ..model.base import Model
-
-
-class ScoreFileModel(Model):
-    """
-    This model will load a score file, and return score at date exists in score file.
-    """
-
-    def __init__(self, score_path):
-        pred_test = pd.read_csv(score_path, index_col=[0, 1], parse_dates=True, infer_datetime_format=True)
-        self.pred = pred_test
-
-    def get_data_with_date(self, date, **kwargs):
-        score = self.pred.loc(axis=0)[:, date]  # (stock_id, trade_date) multi_index, score in pdate
-        score_series = score.reset_index(level="datetime", drop=True)[
-            "score"
-        ]  # pd.Series ; index:stock_id, data: score
-        return score_series
-
-    def predict(self, x_test, **kwargs):
-        return x_test
-
-    def score(self, x_test, **kwargs):
-        return
-
-    def fit(self, x_train, y_train, x_valid, y_valid, w_train=None, w_valid=None, **kwargs):
-        return
-
-    def save(self, fname, **kwargs):
-        return
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+# pylint: skip-file
+# flake8: noqa
+
+import random
+import pandas as pd
+from ...data import D
+from ..model.base import Model
+
+
+class ScoreFileModel(Model):
+    """
+    This model will load a score file, and return score at date exists in score file.
+    """
+
+    def __init__(self, score_path):
+        pred_test = pd.read_csv(score_path, index_col=[0, 1], parse_dates=True, infer_datetime_format=True)
+        self.pred = pred_test
+
+    def get_data_with_date(self, date, **kwargs):
+        score = self.pred.loc(axis=0)[:, date]  # (stock_id, trade_date) multi_index, score in pdate
+        score_series = score.reset_index(level="datetime", drop=True)[
+            "score"
+        ]  # pd.Series ; index:stock_id, data: score
+        return score_series
+
+    def predict(self, x_test, **kwargs):
+        return x_test
+
+    def score(self, x_test, **kwargs):
+        return
+
+    def fit(self, x_train, y_train, x_valid, y_valid, w_train=None, w_valid=None, **kwargs):
+        return
+
+    def save(self, fname, **kwargs):
+        return
```

## qlib/contrib/online/user.py

 * *Ordering differences only*

```diff
@@ -1,77 +1,77 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-# pylint: skip-file
-# flake8: noqa
-
-import logging
-
-from ...log import get_module_logger
-from ..evaluate import risk_analysis
-from ...data import D
-
-
-class User:
-    def __init__(self, account, strategy, model, verbose=False):
-        """
-        A user in online system, which contains account, strategy and model three module.
-            Parameter
-                account : Account()
-                strategy :
-                    a strategy instance
-                model :
-                    a model instance
-                report_save_path : string
-                    the path to save report. Will not save report if None
-                verbose : bool
-                    Whether to print the info during the process
-        """
-        self.logger = get_module_logger("User", level=logging.INFO)
-        self.account = account
-        self.strategy = strategy
-        self.model = model
-        self.verbose = verbose
-
-    def init_state(self, date):
-        """
-        init state when each trading date begin
-            Parameter
-                date : pd.Timestamp
-        """
-        self.account.init_state(today=date)
-        self.strategy.init_state(trade_date=date, model=self.model, account=self.account)
-        return
-
-    def get_latest_trading_date(self):
-        """
-        return the latest trading date for user {user_id}
-            Parameter
-                user_id : string
-            :return
-                date : string (e.g '2018-10-08')
-        """
-        if not self.account.last_trade_date:
-            return None
-        return str(self.account.last_trade_date.date())
-
-    def showReport(self, benchmark="SH000905"):
-        """
-        show the newly report (mean, std, information_ratio, annualized_return)
-            Parameter
-                benchmark : string
-                    bench that to be compared, 'SH000905' for csi500
-        """
-        bench = D.features([benchmark], ["$change"], disk_cache=True).loc[benchmark, "$change"]
-        portfolio_metrics = self.account.portfolio_metrics.generate_portfolio_metrics_dataframe()
-        portfolio_metrics["bench"] = bench
-        analysis_result = {"pred": {}, "excess_return_without_cost": {}, "excess_return_with_cost": {}}
-        r = (portfolio_metrics["return"] - portfolio_metrics["bench"]).dropna()
-        analysis_result["excess_return_without_cost"][0] = risk_analysis(r)
-        r = (portfolio_metrics["return"] - portfolio_metrics["bench"] - portfolio_metrics["cost"]).dropna()
-        analysis_result["excess_return_with_cost"][0] = risk_analysis(r)
-        self.logger.info("Result of porfolio:")
-        self.logger.info("excess_return_without_cost:")
-        self.logger.info(analysis_result["excess_return_without_cost"][0])
-        self.logger.info("excess_return_with_cost:")
-        self.logger.info(analysis_result["excess_return_with_cost"][0])
-        return portfolio_metrics
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+# pylint: skip-file
+# flake8: noqa
+
+import logging
+
+from ...log import get_module_logger
+from ..evaluate import risk_analysis
+from ...data import D
+
+
+class User:
+    def __init__(self, account, strategy, model, verbose=False):
+        """
+        A user in online system, which contains account, strategy and model three module.
+            Parameter
+                account : Account()
+                strategy :
+                    a strategy instance
+                model :
+                    a model instance
+                report_save_path : string
+                    the path to save report. Will not save report if None
+                verbose : bool
+                    Whether to print the info during the process
+        """
+        self.logger = get_module_logger("User", level=logging.INFO)
+        self.account = account
+        self.strategy = strategy
+        self.model = model
+        self.verbose = verbose
+
+    def init_state(self, date):
+        """
+        init state when each trading date begin
+            Parameter
+                date : pd.Timestamp
+        """
+        self.account.init_state(today=date)
+        self.strategy.init_state(trade_date=date, model=self.model, account=self.account)
+        return
+
+    def get_latest_trading_date(self):
+        """
+        return the latest trading date for user {user_id}
+            Parameter
+                user_id : string
+            :return
+                date : string (e.g '2018-10-08')
+        """
+        if not self.account.last_trade_date:
+            return None
+        return str(self.account.last_trade_date.date())
+
+    def showReport(self, benchmark="SH000905"):
+        """
+        show the newly report (mean, std, information_ratio, annualized_return)
+            Parameter
+                benchmark : string
+                    bench that to be compared, 'SH000905' for csi500
+        """
+        bench = D.features([benchmark], ["$change"], disk_cache=True).loc[benchmark, "$change"]
+        portfolio_metrics = self.account.portfolio_metrics.generate_portfolio_metrics_dataframe()
+        portfolio_metrics["bench"] = bench
+        analysis_result = {"pred": {}, "excess_return_without_cost": {}, "excess_return_with_cost": {}}
+        r = (portfolio_metrics["return"] - portfolio_metrics["bench"]).dropna()
+        analysis_result["excess_return_without_cost"][0] = risk_analysis(r)
+        r = (portfolio_metrics["return"] - portfolio_metrics["bench"] - portfolio_metrics["cost"]).dropna()
+        analysis_result["excess_return_with_cost"][0] = risk_analysis(r)
+        self.logger.info("Result of porfolio:")
+        self.logger.info("excess_return_without_cost:")
+        self.logger.info(analysis_result["excess_return_without_cost"][0])
+        self.logger.info("excess_return_with_cost:")
+        self.logger.info(analysis_result["excess_return_with_cost"][0])
+        return portfolio_metrics
```

## qlib/contrib/online/utils.py

 * *Ordering differences only*

```diff
@@ -1,98 +1,98 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-# pylint: skip-file
-# flake8: noqa
-
-import pathlib
-import pickle
-import yaml
-import pandas as pd
-from ...data import D
-from ...config import C
-from ...log import get_module_logger
-from ...utils import get_next_trading_date
-from ...backtest.exchange import Exchange
-
-log = get_module_logger("utils")
-
-
-def load_instance(file_path):
-    """
-    load a pickle file
-        Parameter
-           file_path : string / pathlib.Path()
-                path of file to be loaded
-        :return
-            An instance loaded from file
-    """
-    file_path = pathlib.Path(file_path)
-    if not file_path.exists():
-        raise ValueError("Cannot find file {}".format(file_path))
-    with file_path.open("rb") as fr:
-        instance = pickle.load(fr)
-    return instance
-
-
-def save_instance(instance, file_path):
-    """
-    save(dump) an instance to a pickle file
-        Parameter
-            instance :
-                data to be dumped
-            file_path : string / pathlib.Path()
-                path of file to be dumped
-    """
-    file_path = pathlib.Path(file_path)
-    with file_path.open("wb") as fr:
-        pickle.dump(instance, fr, C.dump_protocol_version)
-
-
-def create_user_folder(path):
-    path = pathlib.Path(path)
-    if path.exists():
-        return
-    path.mkdir(parents=True)
-    head = pd.DataFrame(columns=("user_id", "add_date"))
-    head.to_csv(path / "users.csv", index=None)
-
-
-def prepare(um, today, user_id, exchange_config=None):
-    """
-    1. Get the dates that need to do trading till today for user {user_id}
-        dates[0] indicate the latest trading date of User{user_id},
-        if User{user_id} haven't do trading before, than dates[0] presents the init date of User{user_id}.
-    2. Set the exchange with exchange_config file
-
-        Parameter
-            um : UserManager()
-            today : pd.Timestamp()
-            user_id : str
-        :return
-            dates : list of pd.Timestamp
-            trade_exchange : Exchange()
-    """
-    # get latest trading date for {user_id}
-    # if is None, indicate it haven't traded, then last trading date is init date of {user_id}
-    latest_trading_date = um.users[user_id].get_latest_trading_date()
-    if not latest_trading_date:
-        latest_trading_date = um.user_record.loc[user_id][0]
-
-    if str(today.date()) < latest_trading_date:
-        log.warning("user_id:{}, last trading date {} after today {}".format(user_id, latest_trading_date, today))
-        return [pd.Timestamp(latest_trading_date)], None
-
-    dates = D.calendar(
-        start_time=pd.Timestamp(latest_trading_date),
-        end_time=pd.Timestamp(today),
-        future=True,
-    )
-    dates = list(dates)
-    dates.append(get_next_trading_date(dates[-1], future=True))
-    if exchange_config:
-        with pathlib.Path(exchange_config).open("r") as fp:
-            exchange_paras = yaml.safe_load(fp)
-    else:
-        exchange_paras = {}
-    trade_exchange = Exchange(trade_dates=dates, **exchange_paras)
-    return dates, trade_exchange
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+# pylint: skip-file
+# flake8: noqa
+
+import pathlib
+import pickle
+import yaml
+import pandas as pd
+from ...data import D
+from ...config import C
+from ...log import get_module_logger
+from ...utils import get_next_trading_date
+from ...backtest.exchange import Exchange
+
+log = get_module_logger("utils")
+
+
+def load_instance(file_path):
+    """
+    load a pickle file
+        Parameter
+           file_path : string / pathlib.Path()
+                path of file to be loaded
+        :return
+            An instance loaded from file
+    """
+    file_path = pathlib.Path(file_path)
+    if not file_path.exists():
+        raise ValueError("Cannot find file {}".format(file_path))
+    with file_path.open("rb") as fr:
+        instance = pickle.load(fr)
+    return instance
+
+
+def save_instance(instance, file_path):
+    """
+    save(dump) an instance to a pickle file
+        Parameter
+            instance :
+                data to be dumped
+            file_path : string / pathlib.Path()
+                path of file to be dumped
+    """
+    file_path = pathlib.Path(file_path)
+    with file_path.open("wb") as fr:
+        pickle.dump(instance, fr, C.dump_protocol_version)
+
+
+def create_user_folder(path):
+    path = pathlib.Path(path)
+    if path.exists():
+        return
+    path.mkdir(parents=True)
+    head = pd.DataFrame(columns=("user_id", "add_date"))
+    head.to_csv(path / "users.csv", index=None)
+
+
+def prepare(um, today, user_id, exchange_config=None):
+    """
+    1. Get the dates that need to do trading till today for user {user_id}
+        dates[0] indicate the latest trading date of User{user_id},
+        if User{user_id} haven't do trading before, than dates[0] presents the init date of User{user_id}.
+    2. Set the exchange with exchange_config file
+
+        Parameter
+            um : UserManager()
+            today : pd.Timestamp()
+            user_id : str
+        :return
+            dates : list of pd.Timestamp
+            trade_exchange : Exchange()
+    """
+    # get latest trading date for {user_id}
+    # if is None, indicate it haven't traded, then last trading date is init date of {user_id}
+    latest_trading_date = um.users[user_id].get_latest_trading_date()
+    if not latest_trading_date:
+        latest_trading_date = um.user_record.loc[user_id][0]
+
+    if str(today.date()) < latest_trading_date:
+        log.warning("user_id:{}, last trading date {} after today {}".format(user_id, latest_trading_date, today))
+        return [pd.Timestamp(latest_trading_date)], None
+
+    dates = D.calendar(
+        start_time=pd.Timestamp(latest_trading_date),
+        end_time=pd.Timestamp(today),
+        future=True,
+    )
+    dates = list(dates)
+    dates.append(get_next_trading_date(dates[-1], future=True))
+    if exchange_config:
+        with pathlib.Path(exchange_config).open("r") as fp:
+            exchange_paras = yaml.safe_load(fp)
+    else:
+        exchange_paras = {}
+    trade_exchange = Exchange(trade_dates=dates, **exchange_paras)
+    return dates, trade_exchange
```

## qlib/contrib/ops/high_freq.py

 * *Ordering differences only*

```diff
@@ -1,277 +1,277 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-import numpy as np
-import pandas as pd
-from datetime import datetime
-
-from qlib.data.cache import H
-from qlib.data.data import Cal
-from qlib.data.ops import ElemOperator, PairOperator
-from qlib.utils.time import time_to_day_index
-
-
-def get_calendar_day(freq="1min", future=False):
-    """
-    Load High-Freq Calendar Date Using Memcache.
-    !!!NOTE: Loading the calendar is quite slow. So loading calendar before start multiprocessing will make it faster.
-
-    Parameters
-    ----------
-    freq : str
-        frequency of read calendar file.
-    future : bool
-        whether including future trading day.
-
-    Returns
-    -------
-    _calendar:
-        array of date.
-    """
-    flag = f"{freq}_future_{future}_day"
-    if flag in H["c"]:
-        _calendar = H["c"][flag]
-    else:
-        _calendar = np.array(list(map(lambda x: x.date(), Cal.load_calendar(freq, future))))
-        H["c"][flag] = _calendar
-    return _calendar
-
-
-def get_calendar_minute(freq="day", future=False):
-    """Load High-Freq Calendar Minute Using Memcache"""
-    flag = f"{freq}_future_{future}_day"
-    if flag in H["c"]:
-        _calendar = H["c"][flag]
-    else:
-        _calendar = np.array(list(map(lambda x: x.minute // 30, Cal.load_calendar(freq, future))))
-        H["c"][flag] = _calendar
-    return _calendar
-
-
-class DayCumsum(ElemOperator):
-    """DayCumsum Operator during start time and end time.
-
-    Parameters
-    ----------
-    feature : Expression
-        feature instance
-    start : str
-        the start time of backtest in one day.
-        !!!NOTE: "9:30" means the time period of (9:30, 9:31) is in transaction.
-    end : str
-        the end time of backtest in one day.
-        !!!NOTE: "14:59" means the time period of (14:59, 15:00) is in transaction,
-                but (15:00, 15:01) is not.
-        So start="9:30" and end="14:59" means trading all day.
-
-    Returns
-    ----------
-    feature:
-        a series of that each value equals the cumsum value during start time and end time.
-        Otherwise, the value is zero.
-    """
-
-    def __init__(self, feature, start: str = "9:30", end: str = "14:59", data_granularity: int = 1):
-        self.feature = feature
-        self.start = datetime.strptime(start, "%H:%M")
-        self.end = datetime.strptime(end, "%H:%M")
-
-        self.morning_open = datetime.strptime("9:30", "%H:%M")
-        self.morning_close = datetime.strptime("11:30", "%H:%M")
-        self.noon_open = datetime.strptime("13:00", "%H:%M")
-        self.noon_close = datetime.strptime("15:00", "%H:%M")
-
-        self.data_granularity = data_granularity
-        self.start_id = time_to_day_index(self.start) // self.data_granularity
-        self.end_id = time_to_day_index(self.end) // self.data_granularity
-        assert 240 % self.data_granularity == 0
-
-    def period_cusum(self, df):
-        df = df.copy()
-        assert len(df) == 240 // self.data_granularity
-        df.iloc[0 : self.start_id] = 0
-        df = df.cumsum()
-        df.iloc[self.end_id + 1 : 240 // self.data_granularity] = 0
-        return df
-
-    def _load_internal(self, instrument, start_index, end_index, freq):
-        _calendar = get_calendar_day(freq=freq)
-        series = self.feature.load(instrument, start_index, end_index, freq)
-        return series.groupby(_calendar[series.index]).transform(self.period_cusum)
-
-
-class DayLast(ElemOperator):
-    """DayLast Operator
-
-    Parameters
-    ----------
-    feature : Expression
-        feature instance
-
-    Returns
-    ----------
-    feature:
-        a series of that each value equals the last value of its day
-    """
-
-    def _load_internal(self, instrument, start_index, end_index, freq):
-        _calendar = get_calendar_day(freq=freq)
-        series = self.feature.load(instrument, start_index, end_index, freq)
-        return series.groupby(_calendar[series.index]).transform("last")
-
-
-class FFillNan(ElemOperator):
-    """FFillNan Operator
-
-    Parameters
-    ----------
-    feature : Expression
-        feature instance
-
-    Returns
-    ----------
-    feature:
-        a forward fill nan feature
-    """
-
-    def _load_internal(self, instrument, start_index, end_index, freq):
-        series = self.feature.load(instrument, start_index, end_index, freq)
-        return series.fillna(method="ffill")
-
-
-class BFillNan(ElemOperator):
-    """BFillNan Operator
-
-    Parameters
-    ----------
-    feature : Expression
-        feature instance
-
-    Returns
-    ----------
-    feature:
-        a backfoward fill nan feature
-    """
-
-    def _load_internal(self, instrument, start_index, end_index, freq):
-        series = self.feature.load(instrument, start_index, end_index, freq)
-        return series.fillna(method="bfill")
-
-
-class Date(ElemOperator):
-    """Date Operator
-
-    Parameters
-    ----------
-    feature : Expression
-        feature instance
-
-    Returns
-    ----------
-    feature:
-        a series of that each value is the date corresponding to feature.index
-    """
-
-    def _load_internal(self, instrument, start_index, end_index, freq):
-        _calendar = get_calendar_day(freq=freq)
-        series = self.feature.load(instrument, start_index, end_index, freq)
-        return pd.Series(_calendar[series.index], index=series.index)
-
-
-class Select(PairOperator):
-    """Select Operator
-
-    Parameters
-    ----------
-    feature_left : Expression
-        feature instance, select condition
-    feature_right : Expression
-        feature instance, select value
-
-    Returns
-    ----------
-    feature:
-        value(feature_right) that meets the condition(feature_left)
-
-    """
-
-    def _load_internal(self, instrument, start_index, end_index, freq):
-        series_condition = self.feature_left.load(instrument, start_index, end_index, freq)
-        series_feature = self.feature_right.load(instrument, start_index, end_index, freq)
-        return series_feature.loc[series_condition]
-
-
-class IsNull(ElemOperator):
-    """IsNull Operator
-
-    Parameters
-    ----------
-    feature : Expression
-        feature instance
-
-    Returns
-    ----------
-    feature:
-        A series indicating whether the feature is nan
-    """
-
-    def _load_internal(self, instrument, start_index, end_index, freq):
-        series = self.feature.load(instrument, start_index, end_index, freq)
-        return series.isnull()
-
-
-class IsInf(ElemOperator):
-    """IsInf Operator
-
-    Parameters
-    ----------
-    feature : Expression
-        feature instance
-
-    Returns
-    ----------
-    feature:
-        A series indicating whether the feature is inf
-    """
-
-    def _load_internal(self, instrument, start_index, end_index, freq):
-        series = self.feature.load(instrument, start_index, end_index, freq)
-        return np.isinf(series)
-
-
-class Cut(ElemOperator):
-    """Cut Operator
-
-    Parameters
-    ----------
-    feature : Expression
-        feature instance
-    l : int
-        l > 0, delete the first l elements of feature (default is None, which means 0)
-    r : int
-        r < 0, delete the last -r elements of feature (default is None, which means 0)
-    Returns
-    ----------
-    feature:
-        A series with the first l and last -r elements deleted from the feature.
-        Note: It is deleted from the raw data, not the sliced data
-    """
-
-    def __init__(self, feature, left=None, right=None):
-        self.left = left
-        self.right = right
-        if (self.left is not None and self.left <= 0) or (self.right is not None and self.right >= 0):
-            raise ValueError("Cut operator l shoud > 0 and r should < 0")
-
-        super(Cut, self).__init__(feature)
-
-    def _load_internal(self, instrument, start_index, end_index, freq):
-        series = self.feature.load(instrument, start_index, end_index, freq)
-        return series.iloc[self.left : self.right]
-
-    def get_extended_window_size(self):
-        ll = 0 if self.left is None else self.left
-        rr = 0 if self.right is None else abs(self.right)
-        lft_etd, rght_etd = self.feature.get_extended_window_size()
-        lft_etd = lft_etd + ll
-        rght_etd = rght_etd + rr
-        return lft_etd, rght_etd
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+import numpy as np
+import pandas as pd
+from datetime import datetime
+
+from qlib.data.cache import H
+from qlib.data.data import Cal
+from qlib.data.ops import ElemOperator, PairOperator
+from qlib.utils.time import time_to_day_index
+
+
+def get_calendar_day(freq="1min", future=False):
+    """
+    Load High-Freq Calendar Date Using Memcache.
+    !!!NOTE: Loading the calendar is quite slow. So loading calendar before start multiprocessing will make it faster.
+
+    Parameters
+    ----------
+    freq : str
+        frequency of read calendar file.
+    future : bool
+        whether including future trading day.
+
+    Returns
+    -------
+    _calendar:
+        array of date.
+    """
+    flag = f"{freq}_future_{future}_day"
+    if flag in H["c"]:
+        _calendar = H["c"][flag]
+    else:
+        _calendar = np.array(list(map(lambda x: x.date(), Cal.load_calendar(freq, future))))
+        H["c"][flag] = _calendar
+    return _calendar
+
+
+def get_calendar_minute(freq="day", future=False):
+    """Load High-Freq Calendar Minute Using Memcache"""
+    flag = f"{freq}_future_{future}_day"
+    if flag in H["c"]:
+        _calendar = H["c"][flag]
+    else:
+        _calendar = np.array(list(map(lambda x: x.minute // 30, Cal.load_calendar(freq, future))))
+        H["c"][flag] = _calendar
+    return _calendar
+
+
+class DayCumsum(ElemOperator):
+    """DayCumsum Operator during start time and end time.
+
+    Parameters
+    ----------
+    feature : Expression
+        feature instance
+    start : str
+        the start time of backtest in one day.
+        !!!NOTE: "9:30" means the time period of (9:30, 9:31) is in transaction.
+    end : str
+        the end time of backtest in one day.
+        !!!NOTE: "14:59" means the time period of (14:59, 15:00) is in transaction,
+                but (15:00, 15:01) is not.
+        So start="9:30" and end="14:59" means trading all day.
+
+    Returns
+    ----------
+    feature:
+        a series of that each value equals the cumsum value during start time and end time.
+        Otherwise, the value is zero.
+    """
+
+    def __init__(self, feature, start: str = "9:30", end: str = "14:59", data_granularity: int = 1):
+        self.feature = feature
+        self.start = datetime.strptime(start, "%H:%M")
+        self.end = datetime.strptime(end, "%H:%M")
+
+        self.morning_open = datetime.strptime("9:30", "%H:%M")
+        self.morning_close = datetime.strptime("11:30", "%H:%M")
+        self.noon_open = datetime.strptime("13:00", "%H:%M")
+        self.noon_close = datetime.strptime("15:00", "%H:%M")
+
+        self.data_granularity = data_granularity
+        self.start_id = time_to_day_index(self.start) // self.data_granularity
+        self.end_id = time_to_day_index(self.end) // self.data_granularity
+        assert 240 % self.data_granularity == 0
+
+    def period_cusum(self, df):
+        df = df.copy()
+        assert len(df) == 240 // self.data_granularity
+        df.iloc[0 : self.start_id] = 0
+        df = df.cumsum()
+        df.iloc[self.end_id + 1 : 240 // self.data_granularity] = 0
+        return df
+
+    def _load_internal(self, instrument, start_index, end_index, freq):
+        _calendar = get_calendar_day(freq=freq)
+        series = self.feature.load(instrument, start_index, end_index, freq)
+        return series.groupby(_calendar[series.index]).transform(self.period_cusum)
+
+
+class DayLast(ElemOperator):
+    """DayLast Operator
+
+    Parameters
+    ----------
+    feature : Expression
+        feature instance
+
+    Returns
+    ----------
+    feature:
+        a series of that each value equals the last value of its day
+    """
+
+    def _load_internal(self, instrument, start_index, end_index, freq):
+        _calendar = get_calendar_day(freq=freq)
+        series = self.feature.load(instrument, start_index, end_index, freq)
+        return series.groupby(_calendar[series.index]).transform("last")
+
+
+class FFillNan(ElemOperator):
+    """FFillNan Operator
+
+    Parameters
+    ----------
+    feature : Expression
+        feature instance
+
+    Returns
+    ----------
+    feature:
+        a forward fill nan feature
+    """
+
+    def _load_internal(self, instrument, start_index, end_index, freq):
+        series = self.feature.load(instrument, start_index, end_index, freq)
+        return series.fillna(method="ffill")
+
+
+class BFillNan(ElemOperator):
+    """BFillNan Operator
+
+    Parameters
+    ----------
+    feature : Expression
+        feature instance
+
+    Returns
+    ----------
+    feature:
+        a backfoward fill nan feature
+    """
+
+    def _load_internal(self, instrument, start_index, end_index, freq):
+        series = self.feature.load(instrument, start_index, end_index, freq)
+        return series.fillna(method="bfill")
+
+
+class Date(ElemOperator):
+    """Date Operator
+
+    Parameters
+    ----------
+    feature : Expression
+        feature instance
+
+    Returns
+    ----------
+    feature:
+        a series of that each value is the date corresponding to feature.index
+    """
+
+    def _load_internal(self, instrument, start_index, end_index, freq):
+        _calendar = get_calendar_day(freq=freq)
+        series = self.feature.load(instrument, start_index, end_index, freq)
+        return pd.Series(_calendar[series.index], index=series.index)
+
+
+class Select(PairOperator):
+    """Select Operator
+
+    Parameters
+    ----------
+    feature_left : Expression
+        feature instance, select condition
+    feature_right : Expression
+        feature instance, select value
+
+    Returns
+    ----------
+    feature:
+        value(feature_right) that meets the condition(feature_left)
+
+    """
+
+    def _load_internal(self, instrument, start_index, end_index, freq):
+        series_condition = self.feature_left.load(instrument, start_index, end_index, freq)
+        series_feature = self.feature_right.load(instrument, start_index, end_index, freq)
+        return series_feature.loc[series_condition]
+
+
+class IsNull(ElemOperator):
+    """IsNull Operator
+
+    Parameters
+    ----------
+    feature : Expression
+        feature instance
+
+    Returns
+    ----------
+    feature:
+        A series indicating whether the feature is nan
+    """
+
+    def _load_internal(self, instrument, start_index, end_index, freq):
+        series = self.feature.load(instrument, start_index, end_index, freq)
+        return series.isnull()
+
+
+class IsInf(ElemOperator):
+    """IsInf Operator
+
+    Parameters
+    ----------
+    feature : Expression
+        feature instance
+
+    Returns
+    ----------
+    feature:
+        A series indicating whether the feature is inf
+    """
+
+    def _load_internal(self, instrument, start_index, end_index, freq):
+        series = self.feature.load(instrument, start_index, end_index, freq)
+        return np.isinf(series)
+
+
+class Cut(ElemOperator):
+    """Cut Operator
+
+    Parameters
+    ----------
+    feature : Expression
+        feature instance
+    l : int
+        l > 0, delete the first l elements of feature (default is None, which means 0)
+    r : int
+        r < 0, delete the last -r elements of feature (default is None, which means 0)
+    Returns
+    ----------
+    feature:
+        A series with the first l and last -r elements deleted from the feature.
+        Note: It is deleted from the raw data, not the sliced data
+    """
+
+    def __init__(self, feature, left=None, right=None):
+        self.left = left
+        self.right = right
+        if (self.left is not None and self.left <= 0) or (self.right is not None and self.right >= 0):
+            raise ValueError("Cut operator l shoud > 0 and r should < 0")
+
+        super(Cut, self).__init__(feature)
+
+    def _load_internal(self, instrument, start_index, end_index, freq):
+        series = self.feature.load(instrument, start_index, end_index, freq)
+        return series.iloc[self.left : self.right]
+
+    def get_extended_window_size(self):
+        ll = 0 if self.left is None else self.left
+        rr = 0 if self.right is None else abs(self.right)
+        lft_etd, rght_etd = self.feature.get_extended_window_size()
+        lft_etd = lft_etd + ll
+        rght_etd = rght_etd + rr
+        return lft_etd, rght_etd
```

## qlib/contrib/report/__init__.py

 * *Ordering differences only*

```diff
@@ -1,11 +1,11 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-GRAPH_NAME_LIST = [
-    "analysis_position.report_graph",
-    "analysis_position.score_ic_graph",
-    "analysis_position.cumulative_return_graph",
-    "analysis_position.risk_analysis_graph",
-    "analysis_position.rank_label_graph",
-    "analysis_model.model_performance_graph",
-]
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+GRAPH_NAME_LIST = [
+    "analysis_position.report_graph",
+    "analysis_position.score_ic_graph",
+    "analysis_position.cumulative_return_graph",
+    "analysis_position.risk_analysis_graph",
+    "analysis_position.rank_label_graph",
+    "analysis_model.model_performance_graph",
+]
```

## qlib/contrib/report/graph.py

```diff
@@ -1,385 +1,384 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-import math
-import importlib
-from typing import Iterable
-
-import pandas as pd
-
-import plotly.offline as py
-import plotly.graph_objs as go
-
-from plotly.subplots import make_subplots
-from plotly.figure_factory import create_distplot
-
-
-class BaseGraph:
-
-    _name = None
-
-    def __init__(
-        self, df: pd.DataFrame = None, layout: dict = None, graph_kwargs: dict = None, name_dict: dict = None, **kwargs
-    ):
-        """
-
-        :param df:
-        :param layout:
-        :param graph_kwargs:
-        :param name_dict:
-        :param kwargs:
-            layout: dict
-                go.Layout parameters
-            graph_kwargs: dict
-                Graph parameters, eg: go.Bar(**graph_kwargs)
-        """
-        self._df = df
-
-        self._layout = dict() if layout is None else layout
-        self._graph_kwargs = dict() if graph_kwargs is None else graph_kwargs
-        self._name_dict = name_dict
-
-        self.data = None
-
-        self._init_parameters(**kwargs)
-        self._init_data()
-
-    def _init_data(self):
-        """
-
-        :return:
-        """
-        if self._df.empty:
-            raise ValueError("df is empty.")
-
-        self.data = self._get_data()
-
-    def _init_parameters(self, **kwargs):
-        """
-
-        :param kwargs
-        """
-
-        # Instantiate graphics parameters
-        self._graph_type = self._name.lower().capitalize()
-
-        # Displayed column name
-        if self._name_dict is None:
-            self._name_dict = {_item: _item for _item in self._df.columns}
-
-    @staticmethod
-    def get_instance_with_graph_parameters(graph_type: str = None, **kwargs):
-        """
-
-        :param graph_type:
-        :param kwargs:
-        :return:
-        """
-        try:
-            _graph_module = importlib.import_module("plotly.graph_objs")
-            _graph_class = getattr(_graph_module, graph_type)
-        except AttributeError:
-            _graph_module = importlib.import_module("qlib.contrib.report.graph")
-            _graph_class = getattr(_graph_module, graph_type)
-        return _graph_class(**kwargs)
-
-    @staticmethod
-    def show_graph_in_notebook(figure_list: Iterable[go.Figure] = None):
-        """
-
-        :param figure_list:
-        :return:
-        """
-        py.init_notebook_mode()
-        for _fig in figure_list:
-            # NOTE: displays figures: https://plotly.com/python/renderers/
-            # default: plotly_mimetype+notebook
-            # support renderers: import plotly.io as pio; print(pio.renderers)
-            renderer = None
-            try:
-                # in notebook
-                _ipykernel = str(type(get_ipython()))
-                if "google.colab" in _ipykernel:
-                    renderer = "colab"
-            except NameError:
-                pass
-
-            _fig.show(renderer=renderer)
-
-    def _get_layout(self) -> go.Layout:
-        """
-
-        :return:
-        """
-        return go.Layout(**self._layout)
-
-    def _get_data(self) -> list:
-        """
-
-        :return:
-        """
-
-        _data = [
-            self.get_instance_with_graph_parameters(
-                graph_type=self._graph_type, x=self._df.index, y=self._df[_col], name=_name, **self._graph_kwargs
-            )
-            for _col, _name in self._name_dict.items()
-        ]
-        return _data
-
-    @property
-    def figure(self) -> go.Figure:
-        """
-
-        :return:
-        """
-        _figure = go.Figure(data=self.data, layout=self._get_layout())
-        # NOTE: Use the default theme from plotly version 3.x, template=None
-        _figure["layout"].update(template=None)
-        return _figure
-
-
-class ScatterGraph(BaseGraph):
-    _name = "scatter"
-
-
-class BarGraph(BaseGraph):
-    _name = "bar"
-
-
-class DistplotGraph(BaseGraph):
-    _name = "distplot"
-
-    def _get_data(self):
-        """
-
-        :return:
-        """
-        _t_df = self._df.dropna()
-        _data_list = [_t_df[_col] for _col in self._name_dict]
-        _label_list = list(self._name_dict.values())
-        _fig = create_distplot(_data_list, _label_list, show_rug=False, **self._graph_kwargs)
-
-        return _fig["data"]
-
-
-class HeatmapGraph(BaseGraph):
-    _name = "heatmap"
-
-    def _get_data(self):
-        """
-
-        :return:
-        """
-        _data = [
-            self.get_instance_with_graph_parameters(
-                graph_type=self._graph_type,
-                x=self._df.columns,
-                y=self._df.index,
-                z=self._df.values.tolist(),
-                **self._graph_kwargs
-            )
-        ]
-        return _data
-
-
-class HistogramGraph(BaseGraph):
-    _name = "histogram"
-
-    def _get_data(self):
-        """
-
-        :return:
-        """
-        _data = [
-            self.get_instance_with_graph_parameters(
-                graph_type=self._graph_type, x=self._df[_col], name=_name, **self._graph_kwargs
-            )
-            for _col, _name in self._name_dict.items()
-        ]
-        return _data
-
-
-class SubplotsGraph:
-    """Create subplots same as df.plot(subplots=True)
-
-    Simple package for `plotly.tools.subplots`
-    """
-
-    def __init__(
-        self,
-        df: pd.DataFrame = None,
-        kind_map: dict = None,
-        layout: dict = None,
-        sub_graph_layout: dict = None,
-        sub_graph_data: list = None,
-        subplots_kwargs: dict = None,
-        **kwargs
-    ):
-        """
-
-        :param df: pd.DataFrame
-
-        :param kind_map: dict, subplots graph kind and kwargs
-            eg: dict(kind='ScatterGraph', kwargs=dict())
-
-        :param layout: `go.Layout` parameters
-
-        :param sub_graph_layout: Layout of each graphic, similar to 'layout'
-
-        :param sub_graph_data: Instantiation parameters for each sub-graphic
-            eg: [(column_name, instance_parameters), ]
-
-            column_name: str or go.Figure
-
-            Instance_parameters:
-
-                - row: int, the row where the graph is located
-
-                - col: int, the col where the graph is located
-
-                - name: str, show name, default column_name in 'df'
-
-                - kind: str, graph kind, default `kind` param, eg: bar, scatter, ...
-
-                - graph_kwargs: dict, graph kwargs, default {}, used in `go.Bar(**graph_kwargs)`
-
-        :param subplots_kwargs: `plotly.tools.make_subplots` original parameters
-
-                - shared_xaxes: bool, default False
-
-                - shared_yaxes: bool, default False
-
-                - vertical_spacing: float, default 0.3 / rows
-
-                - subplot_titles: list, default []
-                    If `sub_graph_data` is None, will generate 'subplot_titles' according to `df.columns`,
-                    this field will be discarded
-
-
-                - specs: list, see `make_subplots` docs
-
-                - rows: int, Number of rows in the subplot grid, default 1
-                    If `sub_graph_data` is None, will generate 'rows' according to `df`, this field will be discarded
-
-                - cols: int, Number of cols in the subplot grid, default 1
-                    If `sub_graph_data` is None, will generate 'cols' according to `df`, this field will be discarded
-
-
-        :param kwargs:
-
-        """
-
-        self._df = df
-        self._layout = layout
-        self._sub_graph_layout = sub_graph_layout
-
-        self._kind_map = kind_map
-        if self._kind_map is None:
-            self._kind_map = dict(kind="ScatterGraph", kwargs=dict())
-
-        self._subplots_kwargs = subplots_kwargs
-        if self._subplots_kwargs is None:
-            self._init_subplots_kwargs()
-
-        self.__cols = self._subplots_kwargs.get("cols", 2)  # pylint: disable=W0238
-        self.__rows = self._subplots_kwargs.get(  # pylint: disable=W0238
-            "rows", math.ceil(len(self._df.columns) / self.__cols)
-        )
-
-        self._sub_graph_data = sub_graph_data
-        if self._sub_graph_data is None:
-            self._init_sub_graph_data()
-
-        self._init_figure()
-
-    def _init_sub_graph_data(self):
-        """
-
-        :return:
-        """
-        self._sub_graph_data = []
-        self._subplot_titles = []
-
-        for i, column_name in enumerate(self._df.columns):
-            row = math.ceil((i + 1) / self.__cols)
-            _temp = (i + 1) % self.__cols
-            col = _temp if _temp else self.__cols
-            res_name = column_name.replace("_", " ")
-            _temp_row_data = (
-                column_name,
-                dict(
-                    row=row,
-                    col=col,
-                    name=res_name,
-                    kind=self._kind_map["kind"],
-                    graph_kwargs=self._kind_map["kwargs"],
-                ),
-            )
-            self._sub_graph_data.append(_temp_row_data)
-            self._subplot_titles.append(res_name)
-
-    def _init_subplots_kwargs(self):
-        """
-
-        :return:
-        """
-        # Default cols, rows
-        _cols = 2
-        _rows = math.ceil(len(self._df.columns) / 2)
-        self._subplots_kwargs = dict()
-        self._subplots_kwargs["rows"] = _rows
-        self._subplots_kwargs["cols"] = _cols
-        self._subplots_kwargs["shared_xaxes"] = False
-        self._subplots_kwargs["shared_yaxes"] = False
-        self._subplots_kwargs["vertical_spacing"] = 0.3 / _rows
-        self._subplots_kwargs["print_grid"] = False
-        self._subplots_kwargs["subplot_titles"] = self._df.columns.tolist()
-
-    def _init_figure(self):
-        """
-
-        :return:
-        """
-        self._figure = make_subplots(**self._subplots_kwargs)
-
-        for column_name, column_map in self._sub_graph_data:
-            if isinstance(column_name, go.Figure):
-                _graph_obj = column_name
-            elif isinstance(column_name, str):
-                temp_name = column_map.get("name", column_name.replace("_", " "))
-                kind = column_map.get("kind", self._kind_map.get("kind", "ScatterGraph"))
-                _graph_kwargs = column_map.get("graph_kwargs", self._kind_map.get("kwargs", {}))
-                _graph_obj = BaseGraph.get_instance_with_graph_parameters(
-                    kind,
-                    **dict(
-                        df=self._df.loc[:, [column_name]],
-                        name_dict={column_name: temp_name},
-                        graph_kwargs=_graph_kwargs,
-                    )
-                )
-            else:
-                raise TypeError()
-
-            row = column_map["row"]
-            col = column_map["col"]
-
-            _graph_data = getattr(_graph_obj, "data")
-            # for _item in _graph_data:
-            #     _item.pop('xaxis', None)
-            #     _item.pop('yaxis', None)
-
-            for _g_obj in _graph_data:
-                self._figure.add_trace(_g_obj, row=row, col=col)
-
-        if self._sub_graph_layout is not None:
-            for k, v in self._sub_graph_layout.items():
-                self._figure["layout"][k].update(v)
-
-        # NOTE: Use the default theme from plotly version 3.x: template=None
-        self._figure["layout"].update(template=None)
-        self._figure["layout"].update(self._layout)
-
-    @property
-    def figure(self):
-        return self._figure
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+import math
+import importlib
+from typing import Iterable
+
+import pandas as pd
+
+import plotly.offline as py
+import plotly.graph_objs as go
+
+from plotly.subplots import make_subplots
+from plotly.figure_factory import create_distplot
+
+
+class BaseGraph:
+    _name = None
+
+    def __init__(
+        self, df: pd.DataFrame = None, layout: dict = None, graph_kwargs: dict = None, name_dict: dict = None, **kwargs
+    ):
+        """
+
+        :param df:
+        :param layout:
+        :param graph_kwargs:
+        :param name_dict:
+        :param kwargs:
+            layout: dict
+                go.Layout parameters
+            graph_kwargs: dict
+                Graph parameters, eg: go.Bar(**graph_kwargs)
+        """
+        self._df = df
+
+        self._layout = dict() if layout is None else layout
+        self._graph_kwargs = dict() if graph_kwargs is None else graph_kwargs
+        self._name_dict = name_dict
+
+        self.data = None
+
+        self._init_parameters(**kwargs)
+        self._init_data()
+
+    def _init_data(self):
+        """
+
+        :return:
+        """
+        if self._df.empty:
+            raise ValueError("df is empty.")
+
+        self.data = self._get_data()
+
+    def _init_parameters(self, **kwargs):
+        """
+
+        :param kwargs
+        """
+
+        # Instantiate graphics parameters
+        self._graph_type = self._name.lower().capitalize()
+
+        # Displayed column name
+        if self._name_dict is None:
+            self._name_dict = {_item: _item for _item in self._df.columns}
+
+    @staticmethod
+    def get_instance_with_graph_parameters(graph_type: str = None, **kwargs):
+        """
+
+        :param graph_type:
+        :param kwargs:
+        :return:
+        """
+        try:
+            _graph_module = importlib.import_module("plotly.graph_objs")
+            _graph_class = getattr(_graph_module, graph_type)
+        except AttributeError:
+            _graph_module = importlib.import_module("qlib.contrib.report.graph")
+            _graph_class = getattr(_graph_module, graph_type)
+        return _graph_class(**kwargs)
+
+    @staticmethod
+    def show_graph_in_notebook(figure_list: Iterable[go.Figure] = None):
+        """
+
+        :param figure_list:
+        :return:
+        """
+        py.init_notebook_mode()
+        for _fig in figure_list:
+            # NOTE: displays figures: https://plotly.com/python/renderers/
+            # default: plotly_mimetype+notebook
+            # support renderers: import plotly.io as pio; print(pio.renderers)
+            renderer = None
+            try:
+                # in notebook
+                _ipykernel = str(type(get_ipython()))
+                if "google.colab" in _ipykernel:
+                    renderer = "colab"
+            except NameError:
+                pass
+
+            _fig.show(renderer=renderer)
+
+    def _get_layout(self) -> go.Layout:
+        """
+
+        :return:
+        """
+        return go.Layout(**self._layout)
+
+    def _get_data(self) -> list:
+        """
+
+        :return:
+        """
+
+        _data = [
+            self.get_instance_with_graph_parameters(
+                graph_type=self._graph_type, x=self._df.index, y=self._df[_col], name=_name, **self._graph_kwargs
+            )
+            for _col, _name in self._name_dict.items()
+        ]
+        return _data
+
+    @property
+    def figure(self) -> go.Figure:
+        """
+
+        :return:
+        """
+        _figure = go.Figure(data=self.data, layout=self._get_layout())
+        # NOTE: Use the default theme from plotly version 3.x, template=None
+        _figure["layout"].update(template=None)
+        return _figure
+
+
+class ScatterGraph(BaseGraph):
+    _name = "scatter"
+
+
+class BarGraph(BaseGraph):
+    _name = "bar"
+
+
+class DistplotGraph(BaseGraph):
+    _name = "distplot"
+
+    def _get_data(self):
+        """
+
+        :return:
+        """
+        _t_df = self._df.dropna()
+        _data_list = [_t_df[_col] for _col in self._name_dict]
+        _label_list = list(self._name_dict.values())
+        _fig = create_distplot(_data_list, _label_list, show_rug=False, **self._graph_kwargs)
+
+        return _fig["data"]
+
+
+class HeatmapGraph(BaseGraph):
+    _name = "heatmap"
+
+    def _get_data(self):
+        """
+
+        :return:
+        """
+        _data = [
+            self.get_instance_with_graph_parameters(
+                graph_type=self._graph_type,
+                x=self._df.columns,
+                y=self._df.index,
+                z=self._df.values.tolist(),
+                **self._graph_kwargs
+            )
+        ]
+        return _data
+
+
+class HistogramGraph(BaseGraph):
+    _name = "histogram"
+
+    def _get_data(self):
+        """
+
+        :return:
+        """
+        _data = [
+            self.get_instance_with_graph_parameters(
+                graph_type=self._graph_type, x=self._df[_col], name=_name, **self._graph_kwargs
+            )
+            for _col, _name in self._name_dict.items()
+        ]
+        return _data
+
+
+class SubplotsGraph:
+    """Create subplots same as df.plot(subplots=True)
+
+    Simple package for `plotly.tools.subplots`
+    """
+
+    def __init__(
+        self,
+        df: pd.DataFrame = None,
+        kind_map: dict = None,
+        layout: dict = None,
+        sub_graph_layout: dict = None,
+        sub_graph_data: list = None,
+        subplots_kwargs: dict = None,
+        **kwargs
+    ):
+        """
+
+        :param df: pd.DataFrame
+
+        :param kind_map: dict, subplots graph kind and kwargs
+            eg: dict(kind='ScatterGraph', kwargs=dict())
+
+        :param layout: `go.Layout` parameters
+
+        :param sub_graph_layout: Layout of each graphic, similar to 'layout'
+
+        :param sub_graph_data: Instantiation parameters for each sub-graphic
+            eg: [(column_name, instance_parameters), ]
+
+            column_name: str or go.Figure
+
+            Instance_parameters:
+
+                - row: int, the row where the graph is located
+
+                - col: int, the col where the graph is located
+
+                - name: str, show name, default column_name in 'df'
+
+                - kind: str, graph kind, default `kind` param, eg: bar, scatter, ...
+
+                - graph_kwargs: dict, graph kwargs, default {}, used in `go.Bar(**graph_kwargs)`
+
+        :param subplots_kwargs: `plotly.tools.make_subplots` original parameters
+
+                - shared_xaxes: bool, default False
+
+                - shared_yaxes: bool, default False
+
+                - vertical_spacing: float, default 0.3 / rows
+
+                - subplot_titles: list, default []
+                    If `sub_graph_data` is None, will generate 'subplot_titles' according to `df.columns`,
+                    this field will be discarded
+
+
+                - specs: list, see `make_subplots` docs
+
+                - rows: int, Number of rows in the subplot grid, default 1
+                    If `sub_graph_data` is None, will generate 'rows' according to `df`, this field will be discarded
+
+                - cols: int, Number of cols in the subplot grid, default 1
+                    If `sub_graph_data` is None, will generate 'cols' according to `df`, this field will be discarded
+
+
+        :param kwargs:
+
+        """
+
+        self._df = df
+        self._layout = layout
+        self._sub_graph_layout = sub_graph_layout
+
+        self._kind_map = kind_map
+        if self._kind_map is None:
+            self._kind_map = dict(kind="ScatterGraph", kwargs=dict())
+
+        self._subplots_kwargs = subplots_kwargs
+        if self._subplots_kwargs is None:
+            self._init_subplots_kwargs()
+
+        self.__cols = self._subplots_kwargs.get("cols", 2)  # pylint: disable=W0238
+        self.__rows = self._subplots_kwargs.get(  # pylint: disable=W0238
+            "rows", math.ceil(len(self._df.columns) / self.__cols)
+        )
+
+        self._sub_graph_data = sub_graph_data
+        if self._sub_graph_data is None:
+            self._init_sub_graph_data()
+
+        self._init_figure()
+
+    def _init_sub_graph_data(self):
+        """
+
+        :return:
+        """
+        self._sub_graph_data = []
+        self._subplot_titles = []
+
+        for i, column_name in enumerate(self._df.columns):
+            row = math.ceil((i + 1) / self.__cols)
+            _temp = (i + 1) % self.__cols
+            col = _temp if _temp else self.__cols
+            res_name = column_name.replace("_", " ")
+            _temp_row_data = (
+                column_name,
+                dict(
+                    row=row,
+                    col=col,
+                    name=res_name,
+                    kind=self._kind_map["kind"],
+                    graph_kwargs=self._kind_map["kwargs"],
+                ),
+            )
+            self._sub_graph_data.append(_temp_row_data)
+            self._subplot_titles.append(res_name)
+
+    def _init_subplots_kwargs(self):
+        """
+
+        :return:
+        """
+        # Default cols, rows
+        _cols = 2
+        _rows = math.ceil(len(self._df.columns) / 2)
+        self._subplots_kwargs = dict()
+        self._subplots_kwargs["rows"] = _rows
+        self._subplots_kwargs["cols"] = _cols
+        self._subplots_kwargs["shared_xaxes"] = False
+        self._subplots_kwargs["shared_yaxes"] = False
+        self._subplots_kwargs["vertical_spacing"] = 0.3 / _rows
+        self._subplots_kwargs["print_grid"] = False
+        self._subplots_kwargs["subplot_titles"] = self._df.columns.tolist()
+
+    def _init_figure(self):
+        """
+
+        :return:
+        """
+        self._figure = make_subplots(**self._subplots_kwargs)
+
+        for column_name, column_map in self._sub_graph_data:
+            if isinstance(column_name, go.Figure):
+                _graph_obj = column_name
+            elif isinstance(column_name, str):
+                temp_name = column_map.get("name", column_name.replace("_", " "))
+                kind = column_map.get("kind", self._kind_map.get("kind", "ScatterGraph"))
+                _graph_kwargs = column_map.get("graph_kwargs", self._kind_map.get("kwargs", {}))
+                _graph_obj = BaseGraph.get_instance_with_graph_parameters(
+                    kind,
+                    **dict(
+                        df=self._df.loc[:, [column_name]],
+                        name_dict={column_name: temp_name},
+                        graph_kwargs=_graph_kwargs,
+                    )
+                )
+            else:
+                raise TypeError()
+
+            row = column_map["row"]
+            col = column_map["col"]
+
+            _graph_data = getattr(_graph_obj, "data")
+            # for _item in _graph_data:
+            #     _item.pop('xaxis', None)
+            #     _item.pop('yaxis', None)
+
+            for _g_obj in _graph_data:
+                self._figure.add_trace(_g_obj, row=row, col=col)
+
+        if self._sub_graph_layout is not None:
+            for k, v in self._sub_graph_layout.items():
+                self._figure["layout"][k].update(v)
+
+        # NOTE: Use the default theme from plotly version 3.x: template=None
+        self._figure["layout"].update(template=None)
+        self._figure["layout"].update(self._layout)
+
+    @property
+    def figure(self):
+        return self._figure
```

## qlib/contrib/report/utils.py

 * *Ordering differences only*

```diff
@@ -1,74 +1,74 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-import matplotlib.pyplot as plt
-import pandas as pd
-
-
-def sub_fig_generator(sub_fs=(3, 3), col_n=10, row_n=1, wspace=None, hspace=None, sharex=False, sharey=False):
-    """sub_fig_generator.
-    it will return a generator, each row contains <col_n> sub graph
-
-    FIXME: Known limitation:
-    - The last row will not be plotted automatically, please plot it outside the function
-
-    Parameters
-    ----------
-    sub_fs :
-        the figure size of each subgraph in <col_n> * <row_n> subgraphs
-    col_n :
-        the number of subgraph in each row;  It will generating a new graph after generating <col_n> of subgraphs.
-    row_n :
-        the number of subgraph in each column
-    wspace :
-        the width of the space for subgraphs in each row
-    hspace :
-        the height of blank space for subgraphs in each column
-        You can try 0.3 if you feel it is too crowded
-
-    Returns
-    -------
-    It will return graphs with the shape of <col_n> each iter (it is squeezed).
-    """
-    assert col_n > 1
-
-    while True:
-        fig, axes = plt.subplots(
-            row_n, col_n, figsize=(sub_fs[0] * col_n, sub_fs[1] * row_n), sharex=sharex, sharey=sharey
-        )
-        plt.subplots_adjust(wspace=wspace, hspace=hspace)
-        axes = axes.reshape(row_n, col_n)
-
-        for col in range(col_n):
-            res = axes[:, col].squeeze()
-            if res.size == 1:
-                res = res.item()
-            yield res
-        plt.show()
-
-
-def guess_plotly_rangebreaks(dt_index: pd.DatetimeIndex):
-    """
-    This function `guesses` the rangebreaks required to remove gaps in datetime index.
-    It basically calculates the difference between a `continuous` datetime index and index given.
-
-    For more details on `rangebreaks` params in plotly, see
-    https://plotly.com/python/reference/layout/xaxis/#layout-xaxis-rangebreaks
-
-    Parameters
-    ----------
-    dt_index: pd.DatetimeIndex
-    The datetimes of the data.
-
-    Returns
-    -------
-    the `rangebreaks` to be passed into plotly axis.
-
-    """
-    dt_idx = dt_index.sort_values()
-    gaps = dt_idx[1:] - dt_idx[:-1]
-    min_gap = gaps.min()
-    gaps_to_break = {}
-    for gap, d in zip(gaps, dt_idx[:-1]):
-        if gap > min_gap:
-            gaps_to_break.setdefault(gap - min_gap, []).append(d + min_gap)
-    return [dict(values=v, dvalue=int(k.total_seconds() * 1000)) for k, v in gaps_to_break.items()]
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+import matplotlib.pyplot as plt
+import pandas as pd
+
+
+def sub_fig_generator(sub_fs=(3, 3), col_n=10, row_n=1, wspace=None, hspace=None, sharex=False, sharey=False):
+    """sub_fig_generator.
+    it will return a generator, each row contains <col_n> sub graph
+
+    FIXME: Known limitation:
+    - The last row will not be plotted automatically, please plot it outside the function
+
+    Parameters
+    ----------
+    sub_fs :
+        the figure size of each subgraph in <col_n> * <row_n> subgraphs
+    col_n :
+        the number of subgraph in each row;  It will generating a new graph after generating <col_n> of subgraphs.
+    row_n :
+        the number of subgraph in each column
+    wspace :
+        the width of the space for subgraphs in each row
+    hspace :
+        the height of blank space for subgraphs in each column
+        You can try 0.3 if you feel it is too crowded
+
+    Returns
+    -------
+    It will return graphs with the shape of <col_n> each iter (it is squeezed).
+    """
+    assert col_n > 1
+
+    while True:
+        fig, axes = plt.subplots(
+            row_n, col_n, figsize=(sub_fs[0] * col_n, sub_fs[1] * row_n), sharex=sharex, sharey=sharey
+        )
+        plt.subplots_adjust(wspace=wspace, hspace=hspace)
+        axes = axes.reshape(row_n, col_n)
+
+        for col in range(col_n):
+            res = axes[:, col].squeeze()
+            if res.size == 1:
+                res = res.item()
+            yield res
+        plt.show()
+
+
+def guess_plotly_rangebreaks(dt_index: pd.DatetimeIndex):
+    """
+    This function `guesses` the rangebreaks required to remove gaps in datetime index.
+    It basically calculates the difference between a `continuous` datetime index and index given.
+
+    For more details on `rangebreaks` params in plotly, see
+    https://plotly.com/python/reference/layout/xaxis/#layout-xaxis-rangebreaks
+
+    Parameters
+    ----------
+    dt_index: pd.DatetimeIndex
+    The datetimes of the data.
+
+    Returns
+    -------
+    the `rangebreaks` to be passed into plotly axis.
+
+    """
+    dt_idx = dt_index.sort_values()
+    gaps = dt_idx[1:] - dt_idx[:-1]
+    min_gap = gaps.min()
+    gaps_to_break = {}
+    for gap, d in zip(gaps, dt_idx[:-1]):
+        if gap > min_gap:
+            gaps_to_break.setdefault(gap - min_gap, []).append(d + min_gap)
+    return [dict(values=v, dvalue=int(k.total_seconds() * 1000)) for k, v in gaps_to_break.items()]
```

## qlib/contrib/report/analysis_model/__init__.py

 * *Ordering differences only*

```diff
@@ -1,7 +1,7 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-from .analysis_model_performance import model_performance_graph
-
-
-__all__ = ["model_performance_graph"]
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+from .analysis_model_performance import model_performance_graph
+
+
+__all__ = ["model_performance_graph"]
```

## qlib/contrib/report/analysis_model/analysis_model_performance.py

 * *Ordering differences only*

```diff
@@ -1,337 +1,337 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-from functools import partial
-
-import pandas as pd
-
-import plotly.graph_objs as go
-
-import statsmodels.api as sm
-import matplotlib.pyplot as plt
-
-from scipy import stats
-
-from typing import Sequence
-from qlib.typehint import Literal
-
-from ..graph import ScatterGraph, SubplotsGraph, BarGraph, HeatmapGraph
-from ..utils import guess_plotly_rangebreaks
-
-
-def _group_return(pred_label: pd.DataFrame = None, reverse: bool = False, N: int = 5, **kwargs) -> tuple:
-    """
-
-    :param pred_label:
-    :param reverse:
-    :param N:
-    :return:
-    """
-    if reverse:
-        pred_label["score"] *= -1
-
-    pred_label = pred_label.sort_values("score", ascending=False)
-
-    # Group1 ~ Group5 only consider the dropna values
-    pred_label_drop = pred_label.dropna(subset=["score"])
-
-    # Group
-    t_df = pd.DataFrame(
-        {
-            "Group%d"
-            % (i + 1): pred_label_drop.groupby(level="datetime")["label"].apply(
-                lambda x: x[len(x) // N * i : len(x) // N * (i + 1)].mean()  # pylint: disable=W0640
-            )
-            for i in range(N)
-        }
-    )
-    t_df.index = pd.to_datetime(t_df.index)
-
-    # Long-Short
-    t_df["long-short"] = t_df["Group1"] - t_df["Group%d" % N]
-
-    # Long-Average
-    t_df["long-average"] = t_df["Group1"] - pred_label.groupby(level="datetime")["label"].mean()
-
-    t_df = t_df.dropna(how="all")  # for days which does not contain label
-    # Cumulative Return By Group
-    group_scatter_figure = ScatterGraph(
-        t_df.cumsum(),
-        layout=dict(
-            title="Cumulative Return",
-            xaxis=dict(tickangle=45, rangebreaks=kwargs.get("rangebreaks", guess_plotly_rangebreaks(t_df.index))),
-        ),
-    ).figure
-
-    t_df = t_df.loc[:, ["long-short", "long-average"]]
-    _bin_size = float(((t_df.max() - t_df.min()) / 20).min())
-    group_hist_figure = SubplotsGraph(
-        t_df,
-        kind_map=dict(kind="DistplotGraph", kwargs=dict(bin_size=_bin_size)),
-        subplots_kwargs=dict(
-            rows=1,
-            cols=2,
-            print_grid=False,
-            subplot_titles=["long-short", "long-average"],
-        ),
-    ).figure
-
-    return group_scatter_figure, group_hist_figure
-
-
-def _plot_qq(data: pd.Series = None, dist=stats.norm) -> go.Figure:
-    """
-
-    :param data:
-    :param dist:
-    :return:
-    """
-    # NOTE: plotly.tools.mpl_to_plotly not actively maintained, resulting in errors in the new version of matplotlib,
-    # ref: https://github.com/plotly/plotly.py/issues/2913#issuecomment-730071567
-    # removing plotly.tools.mpl_to_plotly for greater compatibility with matplotlib versions
-    _plt_fig = sm.qqplot(data.dropna(), dist=dist, fit=True, line="45")
-    plt.close(_plt_fig)
-    qqplot_data = _plt_fig.gca().lines
-    fig = go.Figure()
-
-    fig.add_trace(
-        {
-            "type": "scatter",
-            "x": qqplot_data[0].get_xdata(),
-            "y": qqplot_data[0].get_ydata(),
-            "mode": "markers",
-            "marker": {"color": "#19d3f3"},
-        }
-    )
-
-    fig.add_trace(
-        {
-            "type": "scatter",
-            "x": qqplot_data[1].get_xdata(),
-            "y": qqplot_data[1].get_ydata(),
-            "mode": "lines",
-            "line": {"color": "#636efa"},
-        }
-    )
-    del qqplot_data
-    return fig
-
-
-def _pred_ic(
-    pred_label: pd.DataFrame = None, methods: Sequence[Literal["IC", "Rank IC"]] = ("IC", "Rank IC"), **kwargs
-) -> tuple:
-    """
-
-    :param pred_label: pd.DataFrame
-    must contain one column of realized return with name `label` and one column of predicted score names `score`.
-    :param methods: Sequence[Literal["IC", "Rank IC"]]
-    IC series to plot.
-    IC is sectional pearson correlation between label and score
-    Rank IC is the spearman correlation between label and score
-    For the Monthly IC, IC histogram, IC Q-Q plot.  Only the first type of IC will be plotted.
-    :return:
-    """
-    _methods_mapping = {"IC": "pearson", "Rank IC": "spearman"}
-
-    def _corr_series(x, method):
-        return x["label"].corr(x["score"], method=method)
-
-    ic_df = pd.concat(
-        [
-            pred_label.groupby(level="datetime").apply(partial(_corr_series, method=_methods_mapping[m])).rename(m)
-            for m in methods
-        ],
-        axis=1,
-    )
-    _ic = ic_df.iloc(axis=1)[0]
-
-    _index = _ic.index.get_level_values(0).astype("str").str.replace("-", "").str.slice(0, 6)
-    _monthly_ic = _ic.groupby(_index).mean()
-    _monthly_ic.index = pd.MultiIndex.from_arrays(
-        [_monthly_ic.index.str.slice(0, 4), _monthly_ic.index.str.slice(4, 6)],
-        names=["year", "month"],
-    )
-
-    # fill month
-    _month_list = pd.date_range(
-        start=pd.Timestamp(f"{_index.min()[:4]}0101"),
-        end=pd.Timestamp(f"{_index.max()[:4]}1231"),
-        freq="1M",
-    )
-    _years = []
-    _month = []
-    for _date in _month_list:
-        _date = _date.strftime("%Y%m%d")
-        _years.append(_date[:4])
-        _month.append(_date[4:6])
-
-    fill_index = pd.MultiIndex.from_arrays([_years, _month], names=["year", "month"])
-
-    _monthly_ic = _monthly_ic.reindex(fill_index)
-
-    ic_bar_figure = ic_figure(ic_df, kwargs.get("show_nature_day", False))
-
-    ic_heatmap_figure = HeatmapGraph(
-        _monthly_ic.unstack(),
-        layout=dict(title="Monthly IC", xaxis=dict(dtick=1), yaxis=dict(tickformat="04d", dtick=1)),
-        graph_kwargs=dict(xtype="array", ytype="array"),
-    ).figure
-
-    dist = stats.norm
-    _qqplot_fig = _plot_qq(_ic, dist)
-
-    if isinstance(dist, stats.norm.__class__):
-        dist_name = "Normal"
-    else:
-        dist_name = "Unknown"
-
-    _ic_df = _ic.to_frame("IC")
-    _bin_size = ((_ic_df.max() - _ic_df.min()) / 20).min()
-    _sub_graph_data = [
-        (
-            "IC",
-            dict(
-                row=1,
-                col=1,
-                name="",
-                kind="DistplotGraph",
-                graph_kwargs=dict(bin_size=_bin_size),
-            ),
-        ),
-        (_qqplot_fig, dict(row=1, col=2)),
-    ]
-    ic_hist_figure = SubplotsGraph(
-        _ic_df.dropna(),
-        kind_map=dict(kind="HistogramGraph", kwargs=dict()),
-        subplots_kwargs=dict(
-            rows=1,
-            cols=2,
-            print_grid=False,
-            subplot_titles=["IC", "IC %s Dist. Q-Q" % dist_name],
-        ),
-        sub_graph_data=_sub_graph_data,
-        layout=dict(
-            yaxis2=dict(title="Observed Quantile"),
-            xaxis2=dict(title=f"{dist_name} Distribution Quantile"),
-        ),
-    ).figure
-
-    return ic_bar_figure, ic_heatmap_figure, ic_hist_figure
-
-
-def _pred_autocorr(pred_label: pd.DataFrame, lag=1, **kwargs) -> tuple:
-    pred = pred_label.copy()
-    pred["score_last"] = pred.groupby(level="instrument")["score"].shift(lag)
-    ac = pred.groupby(level="datetime").apply(lambda x: x["score"].rank(pct=True).corr(x["score_last"].rank(pct=True)))
-    _df = ac.to_frame("value")
-    ac_figure = ScatterGraph(
-        _df,
-        layout=dict(
-            title="Auto Correlation",
-            xaxis=dict(tickangle=45, rangebreaks=kwargs.get("rangebreaks", guess_plotly_rangebreaks(_df.index))),
-        ),
-    ).figure
-    return (ac_figure,)
-
-
-def _pred_turnover(pred_label: pd.DataFrame, N=5, lag=1, **kwargs) -> tuple:
-    pred = pred_label.copy()
-    pred["score_last"] = pred.groupby(level="instrument")["score"].shift(lag)
-    top = pred.groupby(level="datetime").apply(
-        lambda x: 1
-        - x.nlargest(len(x) // N, columns="score").index.isin(x.nlargest(len(x) // N, columns="score_last").index).sum()
-        / (len(x) // N)
-    )
-    bottom = pred.groupby(level="datetime").apply(
-        lambda x: 1
-        - x.nsmallest(len(x) // N, columns="score")
-        .index.isin(x.nsmallest(len(x) // N, columns="score_last").index)
-        .sum()
-        / (len(x) // N)
-    )
-    r_df = pd.DataFrame(
-        {
-            "Top": top,
-            "Bottom": bottom,
-        }
-    )
-    turnover_figure = ScatterGraph(
-        r_df,
-        layout=dict(
-            title="Top-Bottom Turnover",
-            xaxis=dict(tickangle=45, rangebreaks=kwargs.get("rangebreaks", guess_plotly_rangebreaks(r_df.index))),
-        ),
-    ).figure
-    return (turnover_figure,)
-
-
-def ic_figure(ic_df: pd.DataFrame, show_nature_day=True, **kwargs) -> go.Figure:
-    r"""IC figure
-
-    :param ic_df: ic DataFrame
-    :param show_nature_day: whether to display the abscissa of non-trading day
-    :param \*\*kwargs: contains some parameters to control plot style in plotly. Currently, supports
-       - `rangebreaks`: https://plotly.com/python/time-series/#Hiding-Weekends-and-Holidays
-    :return: plotly.graph_objs.Figure
-    """
-    if show_nature_day:
-        date_index = pd.date_range(ic_df.index.min(), ic_df.index.max())
-        ic_df = ic_df.reindex(date_index)
-    ic_bar_figure = BarGraph(
-        ic_df,
-        layout=dict(
-            title="Information Coefficient (IC)",
-            xaxis=dict(tickangle=45, rangebreaks=kwargs.get("rangebreaks", guess_plotly_rangebreaks(ic_df.index))),
-        ),
-    ).figure
-    return ic_bar_figure
-
-
-def model_performance_graph(
-    pred_label: pd.DataFrame,
-    lag: int = 1,
-    N: int = 5,
-    reverse=False,
-    rank=False,
-    graph_names: list = ["group_return", "pred_ic", "pred_autocorr"],
-    show_notebook: bool = True,
-    show_nature_day: bool = False,
-    **kwargs,
-) -> [list, tuple]:
-    r"""Model performance
-
-    :param pred_label: index is **pd.MultiIndex**, index name is **[instrument, datetime]**; columns names is **[score, label]**.
-           It is usually same as the label of model training(e.g. "Ref($close, -2)/Ref($close, -1) - 1").
-
-
-            .. code-block:: python
-
-                instrument  datetime        score       label
-                SH600004    2017-12-11  -0.013502       -0.013502
-                                2017-12-12  -0.072367       -0.072367
-                                2017-12-13  -0.068605       -0.068605
-                                2017-12-14  0.012440        0.012440
-                                2017-12-15  -0.102778       -0.102778
-
-
-    :param lag: `pred.groupby(level='instrument')['score'].shift(lag)`. It will be only used in the auto-correlation computing.
-    :param N: group number, default 5.
-    :param reverse: if `True`, `pred['score'] *= -1`.
-    :param rank: if **True**, calculate rank ic.
-    :param graph_names: graph names; default ['cumulative_return', 'pred_ic', 'pred_autocorr', 'pred_turnover'].
-    :param show_notebook: whether to display graphics in notebook, the default is `True`.
-    :param show_nature_day: whether to display the abscissa of non-trading day.
-    :param \*\*kwargs: contains some parameters to control plot style in plotly. Currently, supports
-       - `rangebreaks`: https://plotly.com/python/time-series/#Hiding-Weekends-and-Holidays
-    :return: if show_notebook is True, display in notebook; else return `plotly.graph_objs.Figure` list.
-    """
-    figure_list = []
-    for graph_name in graph_names:
-        fun_res = eval(f"_{graph_name}")(
-            pred_label=pred_label, lag=lag, N=N, reverse=reverse, rank=rank, show_nature_day=show_nature_day, **kwargs
-        )
-        figure_list += fun_res
-
-    if show_notebook:
-        BarGraph.show_graph_in_notebook(figure_list)
-    else:
-        return figure_list
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+from functools import partial
+
+import pandas as pd
+
+import plotly.graph_objs as go
+
+import statsmodels.api as sm
+import matplotlib.pyplot as plt
+
+from scipy import stats
+
+from typing import Sequence
+from qlib.typehint import Literal
+
+from ..graph import ScatterGraph, SubplotsGraph, BarGraph, HeatmapGraph
+from ..utils import guess_plotly_rangebreaks
+
+
+def _group_return(pred_label: pd.DataFrame = None, reverse: bool = False, N: int = 5, **kwargs) -> tuple:
+    """
+
+    :param pred_label:
+    :param reverse:
+    :param N:
+    :return:
+    """
+    if reverse:
+        pred_label["score"] *= -1
+
+    pred_label = pred_label.sort_values("score", ascending=False)
+
+    # Group1 ~ Group5 only consider the dropna values
+    pred_label_drop = pred_label.dropna(subset=["score"])
+
+    # Group
+    t_df = pd.DataFrame(
+        {
+            "Group%d"
+            % (i + 1): pred_label_drop.groupby(level="datetime")["label"].apply(
+                lambda x: x[len(x) // N * i : len(x) // N * (i + 1)].mean()  # pylint: disable=W0640
+            )
+            for i in range(N)
+        }
+    )
+    t_df.index = pd.to_datetime(t_df.index)
+
+    # Long-Short
+    t_df["long-short"] = t_df["Group1"] - t_df["Group%d" % N]
+
+    # Long-Average
+    t_df["long-average"] = t_df["Group1"] - pred_label.groupby(level="datetime")["label"].mean()
+
+    t_df = t_df.dropna(how="all")  # for days which does not contain label
+    # Cumulative Return By Group
+    group_scatter_figure = ScatterGraph(
+        t_df.cumsum(),
+        layout=dict(
+            title="Cumulative Return",
+            xaxis=dict(tickangle=45, rangebreaks=kwargs.get("rangebreaks", guess_plotly_rangebreaks(t_df.index))),
+        ),
+    ).figure
+
+    t_df = t_df.loc[:, ["long-short", "long-average"]]
+    _bin_size = float(((t_df.max() - t_df.min()) / 20).min())
+    group_hist_figure = SubplotsGraph(
+        t_df,
+        kind_map=dict(kind="DistplotGraph", kwargs=dict(bin_size=_bin_size)),
+        subplots_kwargs=dict(
+            rows=1,
+            cols=2,
+            print_grid=False,
+            subplot_titles=["long-short", "long-average"],
+        ),
+    ).figure
+
+    return group_scatter_figure, group_hist_figure
+
+
+def _plot_qq(data: pd.Series = None, dist=stats.norm) -> go.Figure:
+    """
+
+    :param data:
+    :param dist:
+    :return:
+    """
+    # NOTE: plotly.tools.mpl_to_plotly not actively maintained, resulting in errors in the new version of matplotlib,
+    # ref: https://github.com/plotly/plotly.py/issues/2913#issuecomment-730071567
+    # removing plotly.tools.mpl_to_plotly for greater compatibility with matplotlib versions
+    _plt_fig = sm.qqplot(data.dropna(), dist=dist, fit=True, line="45")
+    plt.close(_plt_fig)
+    qqplot_data = _plt_fig.gca().lines
+    fig = go.Figure()
+
+    fig.add_trace(
+        {
+            "type": "scatter",
+            "x": qqplot_data[0].get_xdata(),
+            "y": qqplot_data[0].get_ydata(),
+            "mode": "markers",
+            "marker": {"color": "#19d3f3"},
+        }
+    )
+
+    fig.add_trace(
+        {
+            "type": "scatter",
+            "x": qqplot_data[1].get_xdata(),
+            "y": qqplot_data[1].get_ydata(),
+            "mode": "lines",
+            "line": {"color": "#636efa"},
+        }
+    )
+    del qqplot_data
+    return fig
+
+
+def _pred_ic(
+    pred_label: pd.DataFrame = None, methods: Sequence[Literal["IC", "Rank IC"]] = ("IC", "Rank IC"), **kwargs
+) -> tuple:
+    """
+
+    :param pred_label: pd.DataFrame
+    must contain one column of realized return with name `label` and one column of predicted score names `score`.
+    :param methods: Sequence[Literal["IC", "Rank IC"]]
+    IC series to plot.
+    IC is sectional pearson correlation between label and score
+    Rank IC is the spearman correlation between label and score
+    For the Monthly IC, IC histogram, IC Q-Q plot.  Only the first type of IC will be plotted.
+    :return:
+    """
+    _methods_mapping = {"IC": "pearson", "Rank IC": "spearman"}
+
+    def _corr_series(x, method):
+        return x["label"].corr(x["score"], method=method)
+
+    ic_df = pd.concat(
+        [
+            pred_label.groupby(level="datetime").apply(partial(_corr_series, method=_methods_mapping[m])).rename(m)
+            for m in methods
+        ],
+        axis=1,
+    )
+    _ic = ic_df.iloc(axis=1)[0]
+
+    _index = _ic.index.get_level_values(0).astype("str").str.replace("-", "").str.slice(0, 6)
+    _monthly_ic = _ic.groupby(_index).mean()
+    _monthly_ic.index = pd.MultiIndex.from_arrays(
+        [_monthly_ic.index.str.slice(0, 4), _monthly_ic.index.str.slice(4, 6)],
+        names=["year", "month"],
+    )
+
+    # fill month
+    _month_list = pd.date_range(
+        start=pd.Timestamp(f"{_index.min()[:4]}0101"),
+        end=pd.Timestamp(f"{_index.max()[:4]}1231"),
+        freq="1M",
+    )
+    _years = []
+    _month = []
+    for _date in _month_list:
+        _date = _date.strftime("%Y%m%d")
+        _years.append(_date[:4])
+        _month.append(_date[4:6])
+
+    fill_index = pd.MultiIndex.from_arrays([_years, _month], names=["year", "month"])
+
+    _monthly_ic = _monthly_ic.reindex(fill_index)
+
+    ic_bar_figure = ic_figure(ic_df, kwargs.get("show_nature_day", False))
+
+    ic_heatmap_figure = HeatmapGraph(
+        _monthly_ic.unstack(),
+        layout=dict(title="Monthly IC", xaxis=dict(dtick=1), yaxis=dict(tickformat="04d", dtick=1)),
+        graph_kwargs=dict(xtype="array", ytype="array"),
+    ).figure
+
+    dist = stats.norm
+    _qqplot_fig = _plot_qq(_ic, dist)
+
+    if isinstance(dist, stats.norm.__class__):
+        dist_name = "Normal"
+    else:
+        dist_name = "Unknown"
+
+    _ic_df = _ic.to_frame("IC")
+    _bin_size = ((_ic_df.max() - _ic_df.min()) / 20).min()
+    _sub_graph_data = [
+        (
+            "IC",
+            dict(
+                row=1,
+                col=1,
+                name="",
+                kind="DistplotGraph",
+                graph_kwargs=dict(bin_size=_bin_size),
+            ),
+        ),
+        (_qqplot_fig, dict(row=1, col=2)),
+    ]
+    ic_hist_figure = SubplotsGraph(
+        _ic_df.dropna(),
+        kind_map=dict(kind="HistogramGraph", kwargs=dict()),
+        subplots_kwargs=dict(
+            rows=1,
+            cols=2,
+            print_grid=False,
+            subplot_titles=["IC", "IC %s Dist. Q-Q" % dist_name],
+        ),
+        sub_graph_data=_sub_graph_data,
+        layout=dict(
+            yaxis2=dict(title="Observed Quantile"),
+            xaxis2=dict(title=f"{dist_name} Distribution Quantile"),
+        ),
+    ).figure
+
+    return ic_bar_figure, ic_heatmap_figure, ic_hist_figure
+
+
+def _pred_autocorr(pred_label: pd.DataFrame, lag=1, **kwargs) -> tuple:
+    pred = pred_label.copy()
+    pred["score_last"] = pred.groupby(level="instrument")["score"].shift(lag)
+    ac = pred.groupby(level="datetime").apply(lambda x: x["score"].rank(pct=True).corr(x["score_last"].rank(pct=True)))
+    _df = ac.to_frame("value")
+    ac_figure = ScatterGraph(
+        _df,
+        layout=dict(
+            title="Auto Correlation",
+            xaxis=dict(tickangle=45, rangebreaks=kwargs.get("rangebreaks", guess_plotly_rangebreaks(_df.index))),
+        ),
+    ).figure
+    return (ac_figure,)
+
+
+def _pred_turnover(pred_label: pd.DataFrame, N=5, lag=1, **kwargs) -> tuple:
+    pred = pred_label.copy()
+    pred["score_last"] = pred.groupby(level="instrument")["score"].shift(lag)
+    top = pred.groupby(level="datetime").apply(
+        lambda x: 1
+        - x.nlargest(len(x) // N, columns="score").index.isin(x.nlargest(len(x) // N, columns="score_last").index).sum()
+        / (len(x) // N)
+    )
+    bottom = pred.groupby(level="datetime").apply(
+        lambda x: 1
+        - x.nsmallest(len(x) // N, columns="score")
+        .index.isin(x.nsmallest(len(x) // N, columns="score_last").index)
+        .sum()
+        / (len(x) // N)
+    )
+    r_df = pd.DataFrame(
+        {
+            "Top": top,
+            "Bottom": bottom,
+        }
+    )
+    turnover_figure = ScatterGraph(
+        r_df,
+        layout=dict(
+            title="Top-Bottom Turnover",
+            xaxis=dict(tickangle=45, rangebreaks=kwargs.get("rangebreaks", guess_plotly_rangebreaks(r_df.index))),
+        ),
+    ).figure
+    return (turnover_figure,)
+
+
+def ic_figure(ic_df: pd.DataFrame, show_nature_day=True, **kwargs) -> go.Figure:
+    r"""IC figure
+
+    :param ic_df: ic DataFrame
+    :param show_nature_day: whether to display the abscissa of non-trading day
+    :param \*\*kwargs: contains some parameters to control plot style in plotly. Currently, supports
+       - `rangebreaks`: https://plotly.com/python/time-series/#Hiding-Weekends-and-Holidays
+    :return: plotly.graph_objs.Figure
+    """
+    if show_nature_day:
+        date_index = pd.date_range(ic_df.index.min(), ic_df.index.max())
+        ic_df = ic_df.reindex(date_index)
+    ic_bar_figure = BarGraph(
+        ic_df,
+        layout=dict(
+            title="Information Coefficient (IC)",
+            xaxis=dict(tickangle=45, rangebreaks=kwargs.get("rangebreaks", guess_plotly_rangebreaks(ic_df.index))),
+        ),
+    ).figure
+    return ic_bar_figure
+
+
+def model_performance_graph(
+    pred_label: pd.DataFrame,
+    lag: int = 1,
+    N: int = 5,
+    reverse=False,
+    rank=False,
+    graph_names: list = ["group_return", "pred_ic", "pred_autocorr"],
+    show_notebook: bool = True,
+    show_nature_day: bool = False,
+    **kwargs,
+) -> [list, tuple]:
+    r"""Model performance
+
+    :param pred_label: index is **pd.MultiIndex**, index name is **[instrument, datetime]**; columns names is **[score, label]**.
+           It is usually same as the label of model training(e.g. "Ref($close, -2)/Ref($close, -1) - 1").
+
+
+            .. code-block:: python
+
+                instrument  datetime        score       label
+                SH600004    2017-12-11  -0.013502       -0.013502
+                                2017-12-12  -0.072367       -0.072367
+                                2017-12-13  -0.068605       -0.068605
+                                2017-12-14  0.012440        0.012440
+                                2017-12-15  -0.102778       -0.102778
+
+
+    :param lag: `pred.groupby(level='instrument')['score'].shift(lag)`. It will be only used in the auto-correlation computing.
+    :param N: group number, default 5.
+    :param reverse: if `True`, `pred['score'] *= -1`.
+    :param rank: if **True**, calculate rank ic.
+    :param graph_names: graph names; default ['cumulative_return', 'pred_ic', 'pred_autocorr', 'pred_turnover'].
+    :param show_notebook: whether to display graphics in notebook, the default is `True`.
+    :param show_nature_day: whether to display the abscissa of non-trading day.
+    :param \*\*kwargs: contains some parameters to control plot style in plotly. Currently, supports
+       - `rangebreaks`: https://plotly.com/python/time-series/#Hiding-Weekends-and-Holidays
+    :return: if show_notebook is True, display in notebook; else return `plotly.graph_objs.Figure` list.
+    """
+    figure_list = []
+    for graph_name in graph_names:
+        fun_res = eval(f"_{graph_name}")(
+            pred_label=pred_label, lag=lag, N=N, reverse=reverse, rank=rank, show_nature_day=show_nature_day, **kwargs
+        )
+        figure_list += fun_res
+
+    if show_notebook:
+        BarGraph.show_graph_in_notebook(figure_list)
+    else:
+        return figure_list
```

## qlib/contrib/report/analysis_position/__init__.py

 * *Ordering differences only*

```diff
@@ -1,11 +1,11 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-from .cumulative_return import cumulative_return_graph
-from .score_ic import score_ic_graph
-from .report import report_graph
-from .rank_label import rank_label_graph
-from .risk_analysis import risk_analysis_graph
-
-
-__all__ = ["cumulative_return_graph", "score_ic_graph", "report_graph", "rank_label_graph", "risk_analysis_graph"]
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+from .cumulative_return import cumulative_return_graph
+from .score_ic import score_ic_graph
+from .report import report_graph
+from .rank_label import rank_label_graph
+from .risk_analysis import risk_analysis_graph
+
+
+__all__ = ["cumulative_return_graph", "score_ic_graph", "report_graph", "rank_label_graph", "risk_analysis_graph"]
```

## qlib/contrib/report/analysis_position/cumulative_return.py

 * *Ordering differences only*

```diff
@@ -1,273 +1,273 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-import copy
-from typing import Iterable
-
-import pandas as pd
-import plotly.graph_objs as go
-
-from ..graph import BaseGraph, SubplotsGraph
-
-from ..analysis_position.parse_position import get_position_data
-
-
-def _get_cum_return_data_with_position(
-    position: dict,
-    report_normal: pd.DataFrame,
-    label_data: pd.DataFrame,
-    start_date=None,
-    end_date=None,
-):
-    """
-
-    :param position:
-    :param report_normal:
-    :param label_data:
-    :param start_date:
-    :param end_date:
-    :return:
-    """
-    _cumulative_return_df = get_position_data(
-        position=position,
-        report_normal=report_normal,
-        label_data=label_data,
-        start_date=start_date,
-        end_date=end_date,
-    ).copy()
-
-    _cumulative_return_df["label"] = _cumulative_return_df["label"] - _cumulative_return_df["bench"]
-    _cumulative_return_df = _cumulative_return_df.dropna()
-    df_gp = _cumulative_return_df.groupby(level="datetime")
-    result_list = []
-    for gp in df_gp:
-        date = gp[0]
-        day_df = gp[1]
-
-        _hold_df = day_df[day_df["status"] == 0]
-        _buy_df = day_df[day_df["status"] == 1]
-        _sell_df = day_df[day_df["status"] == -1]
-
-        hold_value = (_hold_df["label"] * _hold_df["weight"]).sum()
-        hold_weight = _hold_df["weight"].sum()
-        hold_mean = (hold_value / hold_weight) if hold_weight else 0
-
-        sell_value = (_sell_df["label"] * _sell_df["weight"]).sum()
-        sell_weight = _sell_df["weight"].sum()
-        sell_mean = (sell_value / sell_weight) if sell_weight else 0
-
-        buy_value = (_buy_df["label"] * _buy_df["weight"]).sum()
-        buy_weight = _buy_df["weight"].sum()
-        buy_mean = (buy_value / buy_weight) if buy_weight else 0
-
-        result_list.append(
-            dict(
-                hold_value=hold_value,
-                hold_mean=hold_mean,
-                hold_weight=hold_weight,
-                buy_value=buy_value,
-                buy_mean=buy_mean,
-                buy_weight=buy_weight,
-                sell_value=sell_value,
-                sell_mean=sell_mean,
-                sell_weight=sell_weight,
-                buy_minus_sell_value=buy_value - sell_value,
-                buy_minus_sell_mean=buy_mean - sell_mean,
-                buy_plus_sell_weight=buy_weight + sell_weight,
-                date=date,
-            )
-        )
-
-    r_df = pd.DataFrame(data=result_list)
-    r_df["cum_hold"] = r_df["hold_mean"].cumsum()
-    r_df["cum_buy"] = r_df["buy_mean"].cumsum()
-    r_df["cum_sell"] = r_df["sell_mean"].cumsum()
-    r_df["cum_buy_minus_sell"] = r_df["buy_minus_sell_mean"].cumsum()
-    return r_df
-
-
-def _get_figure_with_position(
-    position: dict,
-    report_normal: pd.DataFrame,
-    label_data: pd.DataFrame,
-    start_date=None,
-    end_date=None,
-) -> Iterable[go.Figure]:
-    """Get average analysis figures
-
-    :param position: position
-    :param report_normal:
-    :param label_data:
-    :param start_date:
-    :param end_date:
-    :return:
-    """
-
-    cum_return_df = _get_cum_return_data_with_position(position, report_normal, label_data, start_date, end_date)
-    cum_return_df = cum_return_df.set_index("date")
-    # FIXME: support HIGH-FREQ
-    cum_return_df.index = cum_return_df.index.strftime("%Y-%m-%d")
-
-    # Create figures
-    for _t_name in ["buy", "sell", "buy_minus_sell", "hold"]:
-        sub_graph_data = [
-            (
-                "cum_{}".format(_t_name),
-                dict(row=1, col=1, graph_kwargs={"mode": "lines+markers", "xaxis": "x3"}),
-            ),
-            (
-                "{}_weight".format(_t_name.replace("minus", "plus") if "minus" in _t_name else _t_name),
-                dict(row=2, col=1),
-            ),
-            (
-                "{}_value".format(_t_name),
-                dict(row=1, col=2, kind="HistogramGraph", graph_kwargs={}),
-            ),
-        ]
-
-        _default_xaxis = dict(showline=False, zeroline=True, tickangle=45)
-        _default_yaxis = dict(zeroline=True, showline=True, showticklabels=True)
-        sub_graph_layout = dict(
-            xaxis1=dict(**_default_xaxis, type="category", showticklabels=False),
-            xaxis3=dict(**_default_xaxis, type="category"),
-            xaxis2=_default_xaxis,
-            yaxis1=dict(**_default_yaxis, title=_t_name),
-            yaxis2=_default_yaxis,
-            yaxis3=_default_yaxis,
-        )
-
-        mean_value = cum_return_df["{}_value".format(_t_name)].mean()
-        layout = dict(
-            height=500,
-            title=f"{_t_name}(the red line in the histogram on the right represents the average)",
-            shapes=[
-                {
-                    "type": "line",
-                    "xref": "x2",
-                    "yref": "paper",
-                    "x0": mean_value,
-                    "y0": 0,
-                    "x1": mean_value,
-                    "y1": 1,
-                    # NOTE: 'fillcolor': '#d3d3d3', 'opacity': 0.3,
-                    "line": {"color": "red", "width": 1},
-                },
-            ],
-        )
-
-        kind_map = dict(kind="ScatterGraph", kwargs=dict(mode="lines+markers"))
-        specs = [
-            [{"rowspan": 1}, {"rowspan": 2}],
-            [{"rowspan": 1}, None],
-        ]
-        subplots_kwargs = dict(
-            vertical_spacing=0.01,
-            rows=2,
-            cols=2,
-            row_width=[1, 2],
-            column_width=[3, 1],
-            print_grid=False,
-            specs=specs,
-        )
-        yield SubplotsGraph(
-            cum_return_df,
-            layout=layout,
-            kind_map=kind_map,
-            sub_graph_layout=sub_graph_layout,
-            sub_graph_data=sub_graph_data,
-            subplots_kwargs=subplots_kwargs,
-        ).figure
-
-
-def cumulative_return_graph(
-    position: dict,
-    report_normal: pd.DataFrame,
-    label_data: pd.DataFrame,
-    show_notebook=True,
-    start_date=None,
-    end_date=None,
-) -> Iterable[go.Figure]:
-    """Backtest buy, sell, and holding cumulative return graph
-
-        Example:
-
-
-            .. code-block:: python
-
-                from qlib.data import D
-                from qlib.contrib.evaluate import risk_analysis, backtest, long_short_backtest
-                from qlib.contrib.strategy import TopkDropoutStrategy
-
-                # backtest parameters
-                bparas = {}
-                bparas['limit_threshold'] = 0.095
-                bparas['account'] = 1000000000
-
-                sparas = {}
-                sparas['topk'] = 50
-                sparas['n_drop'] = 5
-                strategy = TopkDropoutStrategy(**sparas)
-
-                report_normal_df, positions = backtest(pred_df, strategy, **bparas)
-
-                pred_df_dates = pred_df.index.get_level_values(level='datetime')
-                features_df = D.features(D.instruments('csi500'), ['Ref($close, -1)/$close - 1'], pred_df_dates.min(), pred_df_dates.max())
-                features_df.columns = ['label']
-
-                qcr.analysis_position.cumulative_return_graph(positions, report_normal_df, features_df)
-
-
-        Graph desc:
-
-            - Axis X: Trading day.
-            - Axis Y:
-            - Above axis Y: `(((Ref($close, -1)/$close - 1) * weight).sum() / weight.sum()).cumsum()`.
-            - Below axis Y: Daily weight sum.
-            - In the **sell** graph, `y < 0` stands for profit; in other cases, `y > 0` stands for profit.
-            - In the **buy_minus_sell** graph, the **y** value of the **weight** graph at the bottom is `buy_weight + sell_weight`.
-            - In each graph, the **red line** in the histogram on the right represents the average.
-
-    :param position: position data
-    :param report_normal:
-
-
-            .. code-block:: python
-
-                                return      cost        bench       turnover
-                date
-                2017-01-04  0.003421    0.000864    0.011693    0.576325
-                2017-01-05  0.000508    0.000447    0.000721    0.227882
-                2017-01-06  -0.003321   0.000212    -0.004322   0.102765
-                2017-01-09  0.006753    0.000212    0.006874    0.105864
-                2017-01-10  -0.000416   0.000440    -0.003350   0.208396
-
-
-    :param label_data: `D.features` result; index is `pd.MultiIndex`, index name is [`instrument`, `datetime`]; columns names is [`label`].
-
-        **The label T is the change from T to T+1**, it is recommended to use ``close``, example: `D.features(D.instruments('csi500'), ['Ref($close, -1)/$close-1'])`
-
-
-            .. code-block:: python
-
-                                                label
-                instrument  datetime
-                SH600004        2017-12-11  -0.013502
-                                2017-12-12  -0.072367
-                                2017-12-13  -0.068605
-                                2017-12-14  0.012440
-                                2017-12-15  -0.102778
-
-
-    :param show_notebook: True or False. If True, show graph in notebook, else return figures
-    :param start_date: start date
-    :param end_date: end date
-    :return:
-    """
-    position = copy.deepcopy(position)
-    report_normal = report_normal.copy()
-    label_data.columns = ["label"]
-    _figures = _get_figure_with_position(position, report_normal, label_data, start_date, end_date)
-    if show_notebook:
-        BaseGraph.show_graph_in_notebook(_figures)
-    else:
-        return _figures
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+import copy
+from typing import Iterable
+
+import pandas as pd
+import plotly.graph_objs as go
+
+from ..graph import BaseGraph, SubplotsGraph
+
+from ..analysis_position.parse_position import get_position_data
+
+
+def _get_cum_return_data_with_position(
+    position: dict,
+    report_normal: pd.DataFrame,
+    label_data: pd.DataFrame,
+    start_date=None,
+    end_date=None,
+):
+    """
+
+    :param position:
+    :param report_normal:
+    :param label_data:
+    :param start_date:
+    :param end_date:
+    :return:
+    """
+    _cumulative_return_df = get_position_data(
+        position=position,
+        report_normal=report_normal,
+        label_data=label_data,
+        start_date=start_date,
+        end_date=end_date,
+    ).copy()
+
+    _cumulative_return_df["label"] = _cumulative_return_df["label"] - _cumulative_return_df["bench"]
+    _cumulative_return_df = _cumulative_return_df.dropna()
+    df_gp = _cumulative_return_df.groupby(level="datetime")
+    result_list = []
+    for gp in df_gp:
+        date = gp[0]
+        day_df = gp[1]
+
+        _hold_df = day_df[day_df["status"] == 0]
+        _buy_df = day_df[day_df["status"] == 1]
+        _sell_df = day_df[day_df["status"] == -1]
+
+        hold_value = (_hold_df["label"] * _hold_df["weight"]).sum()
+        hold_weight = _hold_df["weight"].sum()
+        hold_mean = (hold_value / hold_weight) if hold_weight else 0
+
+        sell_value = (_sell_df["label"] * _sell_df["weight"]).sum()
+        sell_weight = _sell_df["weight"].sum()
+        sell_mean = (sell_value / sell_weight) if sell_weight else 0
+
+        buy_value = (_buy_df["label"] * _buy_df["weight"]).sum()
+        buy_weight = _buy_df["weight"].sum()
+        buy_mean = (buy_value / buy_weight) if buy_weight else 0
+
+        result_list.append(
+            dict(
+                hold_value=hold_value,
+                hold_mean=hold_mean,
+                hold_weight=hold_weight,
+                buy_value=buy_value,
+                buy_mean=buy_mean,
+                buy_weight=buy_weight,
+                sell_value=sell_value,
+                sell_mean=sell_mean,
+                sell_weight=sell_weight,
+                buy_minus_sell_value=buy_value - sell_value,
+                buy_minus_sell_mean=buy_mean - sell_mean,
+                buy_plus_sell_weight=buy_weight + sell_weight,
+                date=date,
+            )
+        )
+
+    r_df = pd.DataFrame(data=result_list)
+    r_df["cum_hold"] = r_df["hold_mean"].cumsum()
+    r_df["cum_buy"] = r_df["buy_mean"].cumsum()
+    r_df["cum_sell"] = r_df["sell_mean"].cumsum()
+    r_df["cum_buy_minus_sell"] = r_df["buy_minus_sell_mean"].cumsum()
+    return r_df
+
+
+def _get_figure_with_position(
+    position: dict,
+    report_normal: pd.DataFrame,
+    label_data: pd.DataFrame,
+    start_date=None,
+    end_date=None,
+) -> Iterable[go.Figure]:
+    """Get average analysis figures
+
+    :param position: position
+    :param report_normal:
+    :param label_data:
+    :param start_date:
+    :param end_date:
+    :return:
+    """
+
+    cum_return_df = _get_cum_return_data_with_position(position, report_normal, label_data, start_date, end_date)
+    cum_return_df = cum_return_df.set_index("date")
+    # FIXME: support HIGH-FREQ
+    cum_return_df.index = cum_return_df.index.strftime("%Y-%m-%d")
+
+    # Create figures
+    for _t_name in ["buy", "sell", "buy_minus_sell", "hold"]:
+        sub_graph_data = [
+            (
+                "cum_{}".format(_t_name),
+                dict(row=1, col=1, graph_kwargs={"mode": "lines+markers", "xaxis": "x3"}),
+            ),
+            (
+                "{}_weight".format(_t_name.replace("minus", "plus") if "minus" in _t_name else _t_name),
+                dict(row=2, col=1),
+            ),
+            (
+                "{}_value".format(_t_name),
+                dict(row=1, col=2, kind="HistogramGraph", graph_kwargs={}),
+            ),
+        ]
+
+        _default_xaxis = dict(showline=False, zeroline=True, tickangle=45)
+        _default_yaxis = dict(zeroline=True, showline=True, showticklabels=True)
+        sub_graph_layout = dict(
+            xaxis1=dict(**_default_xaxis, type="category", showticklabels=False),
+            xaxis3=dict(**_default_xaxis, type="category"),
+            xaxis2=_default_xaxis,
+            yaxis1=dict(**_default_yaxis, title=_t_name),
+            yaxis2=_default_yaxis,
+            yaxis3=_default_yaxis,
+        )
+
+        mean_value = cum_return_df["{}_value".format(_t_name)].mean()
+        layout = dict(
+            height=500,
+            title=f"{_t_name}(the red line in the histogram on the right represents the average)",
+            shapes=[
+                {
+                    "type": "line",
+                    "xref": "x2",
+                    "yref": "paper",
+                    "x0": mean_value,
+                    "y0": 0,
+                    "x1": mean_value,
+                    "y1": 1,
+                    # NOTE: 'fillcolor': '#d3d3d3', 'opacity': 0.3,
+                    "line": {"color": "red", "width": 1},
+                },
+            ],
+        )
+
+        kind_map = dict(kind="ScatterGraph", kwargs=dict(mode="lines+markers"))
+        specs = [
+            [{"rowspan": 1}, {"rowspan": 2}],
+            [{"rowspan": 1}, None],
+        ]
+        subplots_kwargs = dict(
+            vertical_spacing=0.01,
+            rows=2,
+            cols=2,
+            row_width=[1, 2],
+            column_width=[3, 1],
+            print_grid=False,
+            specs=specs,
+        )
+        yield SubplotsGraph(
+            cum_return_df,
+            layout=layout,
+            kind_map=kind_map,
+            sub_graph_layout=sub_graph_layout,
+            sub_graph_data=sub_graph_data,
+            subplots_kwargs=subplots_kwargs,
+        ).figure
+
+
+def cumulative_return_graph(
+    position: dict,
+    report_normal: pd.DataFrame,
+    label_data: pd.DataFrame,
+    show_notebook=True,
+    start_date=None,
+    end_date=None,
+) -> Iterable[go.Figure]:
+    """Backtest buy, sell, and holding cumulative return graph
+
+        Example:
+
+
+            .. code-block:: python
+
+                from qlib.data import D
+                from qlib.contrib.evaluate import risk_analysis, backtest, long_short_backtest
+                from qlib.contrib.strategy import TopkDropoutStrategy
+
+                # backtest parameters
+                bparas = {}
+                bparas['limit_threshold'] = 0.095
+                bparas['account'] = 1000000000
+
+                sparas = {}
+                sparas['topk'] = 50
+                sparas['n_drop'] = 5
+                strategy = TopkDropoutStrategy(**sparas)
+
+                report_normal_df, positions = backtest(pred_df, strategy, **bparas)
+
+                pred_df_dates = pred_df.index.get_level_values(level='datetime')
+                features_df = D.features(D.instruments('csi500'), ['Ref($close, -1)/$close - 1'], pred_df_dates.min(), pred_df_dates.max())
+                features_df.columns = ['label']
+
+                qcr.analysis_position.cumulative_return_graph(positions, report_normal_df, features_df)
+
+
+        Graph desc:
+
+            - Axis X: Trading day.
+            - Axis Y:
+            - Above axis Y: `(((Ref($close, -1)/$close - 1) * weight).sum() / weight.sum()).cumsum()`.
+            - Below axis Y: Daily weight sum.
+            - In the **sell** graph, `y < 0` stands for profit; in other cases, `y > 0` stands for profit.
+            - In the **buy_minus_sell** graph, the **y** value of the **weight** graph at the bottom is `buy_weight + sell_weight`.
+            - In each graph, the **red line** in the histogram on the right represents the average.
+
+    :param position: position data
+    :param report_normal:
+
+
+            .. code-block:: python
+
+                                return      cost        bench       turnover
+                date
+                2017-01-04  0.003421    0.000864    0.011693    0.576325
+                2017-01-05  0.000508    0.000447    0.000721    0.227882
+                2017-01-06  -0.003321   0.000212    -0.004322   0.102765
+                2017-01-09  0.006753    0.000212    0.006874    0.105864
+                2017-01-10  -0.000416   0.000440    -0.003350   0.208396
+
+
+    :param label_data: `D.features` result; index is `pd.MultiIndex`, index name is [`instrument`, `datetime`]; columns names is [`label`].
+
+        **The label T is the change from T to T+1**, it is recommended to use ``close``, example: `D.features(D.instruments('csi500'), ['Ref($close, -1)/$close-1'])`
+
+
+            .. code-block:: python
+
+                                                label
+                instrument  datetime
+                SH600004        2017-12-11  -0.013502
+                                2017-12-12  -0.072367
+                                2017-12-13  -0.068605
+                                2017-12-14  0.012440
+                                2017-12-15  -0.102778
+
+
+    :param show_notebook: True or False. If True, show graph in notebook, else return figures
+    :param start_date: start date
+    :param end_date: end date
+    :return:
+    """
+    position = copy.deepcopy(position)
+    report_normal = report_normal.copy()
+    label_data.columns = ["label"]
+    _figures = _get_figure_with_position(position, report_normal, label_data, start_date, end_date)
+    if show_notebook:
+        BaseGraph.show_graph_in_notebook(_figures)
+    else:
+        return _figures
```

## qlib/contrib/report/analysis_position/parse_position.py

 * *Ordering differences only*

```diff
@@ -1,175 +1,175 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-import pandas as pd
-
-
-from ....backtest.profit_attribution import get_stock_weight_df
-
-
-def parse_position(position: dict = None) -> pd.DataFrame:
-    """Parse position dict to position DataFrame
-
-    :param position: position data
-    :return: position DataFrame;
-
-
-        .. code-block:: python
-
-            position_df = parse_position(positions)
-            print(position_df.head())
-            # status: 0-hold, -1-sell, 1-buy
-
-                                        amount      cash      count    price status weight
-            instrument  datetime
-            SZ000547    2017-01-04  44.154290   211405.285654   1   205.189575  1   0.031255
-            SZ300202    2017-01-04  60.638845   211405.285654   1   154.356506  1   0.032290
-            SH600158    2017-01-04  46.531681   211405.285654   1   153.895142  1   0.024704
-            SH600545    2017-01-04  197.173093  211405.285654   1   48.607037   1   0.033063
-            SZ000930    2017-01-04  103.938300  211405.285654   1   80.759453   1   0.028958
-
-
-    """
-
-    position_weight_df = get_stock_weight_df(position)
-    # If the day does not exist, use the last weight
-    position_weight_df.fillna(method="ffill", inplace=True)
-
-    previous_data = {"date": None, "code_list": []}
-
-    result_df = pd.DataFrame()
-    for _trading_date, _value in position.items():
-        _value = _value.position
-        # pd_date type: pd.Timestamp
-        _cash = _value.pop("cash")
-        for _item in ["now_account_value"]:
-            if _item in _value:
-                _value.pop(_item)
-
-        _trading_day_df = pd.DataFrame.from_dict(_value, orient="index")
-        _trading_day_df["weight"] = position_weight_df.loc[_trading_date]
-        _trading_day_df["cash"] = _cash
-        _trading_day_df["date"] = _trading_date
-        # status: 0-hold, -1-sell, 1-buy
-        _trading_day_df["status"] = 0
-
-        # T not exist, T-1 exist, T sell
-        _cur_day_sell = set(previous_data["code_list"]) - set(_trading_day_df.index)
-        # T exist, T-1 not exist, T buy
-        _cur_day_buy = set(_trading_day_df.index) - set(previous_data["code_list"])
-
-        # Trading day buy
-        _trading_day_df.loc[_trading_day_df.index.isin(_cur_day_buy), "status"] = 1
-
-        # Trading day sell
-        if not result_df.empty:
-            _trading_day_sell_df = result_df.loc[
-                (result_df["date"] == previous_data["date"]) & (result_df.index.isin(_cur_day_sell))
-            ].copy()
-            if not _trading_day_sell_df.empty:
-                _trading_day_sell_df["status"] = -1
-                _trading_day_sell_df["date"] = _trading_date
-                _trading_day_df = pd.concat([_trading_day_df, _trading_day_sell_df], sort=False)
-
-        result_df = pd.concat([result_df, _trading_day_df], sort=True)
-
-        previous_data = dict(
-            date=_trading_date,
-            code_list=_trading_day_df[_trading_day_df["status"] != -1].index,
-        )
-
-    result_df.reset_index(inplace=True)
-    result_df.rename(columns={"date": "datetime", "index": "instrument"}, inplace=True)
-    return result_df.set_index(["instrument", "datetime"])
-
-
-def _add_label_to_position(position_df: pd.DataFrame, label_data: pd.DataFrame) -> pd.DataFrame:
-    """Concat position with custom label
-
-    :param position_df: position DataFrame
-    :param label_data:
-    :return: concat result
-    """
-
-    _start_time = position_df.index.get_level_values(level="datetime").min()
-    _end_time = position_df.index.get_level_values(level="datetime").max()
-    label_data = label_data.loc(axis=0)[:, pd.to_datetime(_start_time) :]
-    _result_df = pd.concat([position_df, label_data], axis=1, sort=True).reindex(label_data.index)
-    _result_df = _result_df.loc[_result_df.index.get_level_values(1) <= _end_time]
-    return _result_df
-
-
-def _add_bench_to_position(position_df: pd.DataFrame = None, bench: pd.Series = None) -> pd.DataFrame:
-    """Concat position with bench
-
-    :param position_df: position DataFrame
-    :param bench: report normal data
-    :return: concat result
-    """
-    _temp_df = position_df.reset_index(level="instrument")
-    # FIXME: After the stock is bought and sold, the rise and fall of the next trading day are calculated.
-    _temp_df["bench"] = bench.shift(-1)
-    res_df = _temp_df.set_index(["instrument", _temp_df.index])
-    return res_df
-
-
-def _calculate_label_rank(df: pd.DataFrame) -> pd.DataFrame:
-    """calculate label rank
-
-    :param df:
-    :return:
-    """
-    _label_name = "label"
-
-    def _calculate_day_value(g_df: pd.DataFrame):
-        g_df = g_df.copy()
-        g_df["rank_ratio"] = g_df[_label_name].rank(ascending=False) / len(g_df) * 100
-
-        # Sell: -1, Hold: 0, Buy: 1
-        for i in [-1, 0, 1]:
-            g_df.loc[g_df["status"] == i, "rank_label_mean"] = g_df[g_df["status"] == i]["rank_ratio"].mean()
-
-        g_df["excess_return"] = g_df[_label_name] - g_df[_label_name].mean()
-        return g_df
-
-    return df.groupby(level="datetime").apply(_calculate_day_value)
-
-
-def get_position_data(
-    position: dict,
-    label_data: pd.DataFrame,
-    report_normal: pd.DataFrame = None,
-    calculate_label_rank=False,
-    start_date=None,
-    end_date=None,
-) -> pd.DataFrame:
-    """Concat position data with pred/report_normal
-
-    :param position: position data
-    :param report_normal: report normal, must be container 'bench' column
-    :param label_data:
-    :param calculate_label_rank:
-    :param start_date: start date
-    :param end_date: end date
-    :return: concat result,
-        columns: ['amount', 'cash', 'count', 'price', 'status', 'weight', 'label',
-                    'rank_ratio', 'rank_label_mean', 'excess_return', 'score', 'bench']
-        index: ['instrument', 'date']
-    """
-    _position_df = parse_position(position)
-
-    # Add custom_label, rank_ratio, rank_mean, and excess_return field
-    _position_df = _add_label_to_position(_position_df, label_data)
-
-    if calculate_label_rank:
-        _position_df = _calculate_label_rank(_position_df)
-
-    if report_normal is not None:
-        # Add bench field
-        _position_df = _add_bench_to_position(_position_df, report_normal["bench"])
-
-    _date_list = _position_df.index.get_level_values(level="datetime")
-    start_date = _date_list.min() if start_date is None else start_date
-    end_date = _date_list.max() if end_date is None else end_date
-    _position_df = _position_df.loc[(start_date <= _date_list) & (_date_list <= end_date)]
-    return _position_df
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+import pandas as pd
+
+
+from ....backtest.profit_attribution import get_stock_weight_df
+
+
+def parse_position(position: dict = None) -> pd.DataFrame:
+    """Parse position dict to position DataFrame
+
+    :param position: position data
+    :return: position DataFrame;
+
+
+        .. code-block:: python
+
+            position_df = parse_position(positions)
+            print(position_df.head())
+            # status: 0-hold, -1-sell, 1-buy
+
+                                        amount      cash      count    price status weight
+            instrument  datetime
+            SZ000547    2017-01-04  44.154290   211405.285654   1   205.189575  1   0.031255
+            SZ300202    2017-01-04  60.638845   211405.285654   1   154.356506  1   0.032290
+            SH600158    2017-01-04  46.531681   211405.285654   1   153.895142  1   0.024704
+            SH600545    2017-01-04  197.173093  211405.285654   1   48.607037   1   0.033063
+            SZ000930    2017-01-04  103.938300  211405.285654   1   80.759453   1   0.028958
+
+
+    """
+
+    position_weight_df = get_stock_weight_df(position)
+    # If the day does not exist, use the last weight
+    position_weight_df.fillna(method="ffill", inplace=True)
+
+    previous_data = {"date": None, "code_list": []}
+
+    result_df = pd.DataFrame()
+    for _trading_date, _value in position.items():
+        _value = _value.position
+        # pd_date type: pd.Timestamp
+        _cash = _value.pop("cash")
+        for _item in ["now_account_value"]:
+            if _item in _value:
+                _value.pop(_item)
+
+        _trading_day_df = pd.DataFrame.from_dict(_value, orient="index")
+        _trading_day_df["weight"] = position_weight_df.loc[_trading_date]
+        _trading_day_df["cash"] = _cash
+        _trading_day_df["date"] = _trading_date
+        # status: 0-hold, -1-sell, 1-buy
+        _trading_day_df["status"] = 0
+
+        # T not exist, T-1 exist, T sell
+        _cur_day_sell = set(previous_data["code_list"]) - set(_trading_day_df.index)
+        # T exist, T-1 not exist, T buy
+        _cur_day_buy = set(_trading_day_df.index) - set(previous_data["code_list"])
+
+        # Trading day buy
+        _trading_day_df.loc[_trading_day_df.index.isin(_cur_day_buy), "status"] = 1
+
+        # Trading day sell
+        if not result_df.empty:
+            _trading_day_sell_df = result_df.loc[
+                (result_df["date"] == previous_data["date"]) & (result_df.index.isin(_cur_day_sell))
+            ].copy()
+            if not _trading_day_sell_df.empty:
+                _trading_day_sell_df["status"] = -1
+                _trading_day_sell_df["date"] = _trading_date
+                _trading_day_df = pd.concat([_trading_day_df, _trading_day_sell_df], sort=False)
+
+        result_df = pd.concat([result_df, _trading_day_df], sort=True)
+
+        previous_data = dict(
+            date=_trading_date,
+            code_list=_trading_day_df[_trading_day_df["status"] != -1].index,
+        )
+
+    result_df.reset_index(inplace=True)
+    result_df.rename(columns={"date": "datetime", "index": "instrument"}, inplace=True)
+    return result_df.set_index(["instrument", "datetime"])
+
+
+def _add_label_to_position(position_df: pd.DataFrame, label_data: pd.DataFrame) -> pd.DataFrame:
+    """Concat position with custom label
+
+    :param position_df: position DataFrame
+    :param label_data:
+    :return: concat result
+    """
+
+    _start_time = position_df.index.get_level_values(level="datetime").min()
+    _end_time = position_df.index.get_level_values(level="datetime").max()
+    label_data = label_data.loc(axis=0)[:, pd.to_datetime(_start_time) :]
+    _result_df = pd.concat([position_df, label_data], axis=1, sort=True).reindex(label_data.index)
+    _result_df = _result_df.loc[_result_df.index.get_level_values(1) <= _end_time]
+    return _result_df
+
+
+def _add_bench_to_position(position_df: pd.DataFrame = None, bench: pd.Series = None) -> pd.DataFrame:
+    """Concat position with bench
+
+    :param position_df: position DataFrame
+    :param bench: report normal data
+    :return: concat result
+    """
+    _temp_df = position_df.reset_index(level="instrument")
+    # FIXME: After the stock is bought and sold, the rise and fall of the next trading day are calculated.
+    _temp_df["bench"] = bench.shift(-1)
+    res_df = _temp_df.set_index(["instrument", _temp_df.index])
+    return res_df
+
+
+def _calculate_label_rank(df: pd.DataFrame) -> pd.DataFrame:
+    """calculate label rank
+
+    :param df:
+    :return:
+    """
+    _label_name = "label"
+
+    def _calculate_day_value(g_df: pd.DataFrame):
+        g_df = g_df.copy()
+        g_df["rank_ratio"] = g_df[_label_name].rank(ascending=False) / len(g_df) * 100
+
+        # Sell: -1, Hold: 0, Buy: 1
+        for i in [-1, 0, 1]:
+            g_df.loc[g_df["status"] == i, "rank_label_mean"] = g_df[g_df["status"] == i]["rank_ratio"].mean()
+
+        g_df["excess_return"] = g_df[_label_name] - g_df[_label_name].mean()
+        return g_df
+
+    return df.groupby(level="datetime").apply(_calculate_day_value)
+
+
+def get_position_data(
+    position: dict,
+    label_data: pd.DataFrame,
+    report_normal: pd.DataFrame = None,
+    calculate_label_rank=False,
+    start_date=None,
+    end_date=None,
+) -> pd.DataFrame:
+    """Concat position data with pred/report_normal
+
+    :param position: position data
+    :param report_normal: report normal, must be container 'bench' column
+    :param label_data:
+    :param calculate_label_rank:
+    :param start_date: start date
+    :param end_date: end date
+    :return: concat result,
+        columns: ['amount', 'cash', 'count', 'price', 'status', 'weight', 'label',
+                    'rank_ratio', 'rank_label_mean', 'excess_return', 'score', 'bench']
+        index: ['instrument', 'date']
+    """
+    _position_df = parse_position(position)
+
+    # Add custom_label, rank_ratio, rank_mean, and excess_return field
+    _position_df = _add_label_to_position(_position_df, label_data)
+
+    if calculate_label_rank:
+        _position_df = _calculate_label_rank(_position_df)
+
+    if report_normal is not None:
+        # Add bench field
+        _position_df = _add_bench_to_position(_position_df, report_normal["bench"])
+
+    _date_list = _position_df.index.get_level_values(level="datetime")
+    start_date = _date_list.min() if start_date is None else start_date
+    end_date = _date_list.max() if end_date is None else end_date
+    _position_df = _position_df.loc[(start_date <= _date_list) & (_date_list <= end_date)]
+    return _position_df
```

## qlib/contrib/report/analysis_position/rank_label.py

 * *Ordering differences only*

```diff
@@ -1,128 +1,128 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-import copy
-from typing import Iterable
-
-import pandas as pd
-import plotly.graph_objs as go
-
-from ..graph import ScatterGraph
-from ..analysis_position.parse_position import get_position_data
-
-
-def _get_figure_with_position(
-    position: dict, label_data: pd.DataFrame, start_date=None, end_date=None
-) -> Iterable[go.Figure]:
-    """Get average analysis figures
-
-    :param position: position
-    :param label_data:
-    :param start_date:
-    :param end_date:
-    :return:
-    """
-    _position_df = get_position_data(
-        position,
-        label_data,
-        calculate_label_rank=True,
-        start_date=start_date,
-        end_date=end_date,
-    )
-
-    res_dict = dict()
-    _pos_gp = _position_df.groupby(level=1)
-    for _item in _pos_gp:
-        _date = _item[0]
-        _day_df = _item[1]
-
-        _day_value = res_dict.setdefault(_date, {})
-        for _i, _name in {0: "Hold", 1: "Buy", -1: "Sell"}.items():
-            _temp_df = _day_df[_day_df["status"] == _i]
-            if _temp_df.empty:
-                _day_value[_name] = 0
-            else:
-                _day_value[_name] = _temp_df["rank_label_mean"].values[0]
-
-    _res_df = pd.DataFrame.from_dict(res_dict, orient="index")
-    # FIXME: support HIGH-FREQ
-    _res_df.index = _res_df.index.strftime("%Y-%m-%d")
-    for _col in _res_df.columns:
-        yield ScatterGraph(
-            _res_df.loc[:, [_col]],
-            layout=dict(
-                title=_col,
-                xaxis=dict(type="category", tickangle=45),
-                yaxis=dict(title="lable-rank-ratio: %"),
-            ),
-            graph_kwargs=dict(mode="lines+markers"),
-        ).figure
-
-
-def rank_label_graph(
-    position: dict,
-    label_data: pd.DataFrame,
-    start_date=None,
-    end_date=None,
-    show_notebook=True,
-) -> Iterable[go.Figure]:
-    """Ranking percentage of stocks buy, sell, and holding on the trading day.
-    Average rank-ratio(similar to **sell_df['label'].rank(ascending=False) / len(sell_df)**) of daily trading
-
-        Example:
-
-
-            .. code-block:: python
-
-                from qlib.data import D
-                from qlib.contrib.evaluate import backtest
-                from qlib.contrib.strategy import TopkDropoutStrategy
-
-                # backtest parameters
-                bparas = {}
-                bparas['limit_threshold'] = 0.095
-                bparas['account'] = 1000000000
-
-                sparas = {}
-                sparas['topk'] = 50
-                sparas['n_drop'] = 230
-                strategy = TopkDropoutStrategy(**sparas)
-
-                _, positions = backtest(pred_df, strategy, **bparas)
-
-                pred_df_dates = pred_df.index.get_level_values(level='datetime')
-                features_df = D.features(D.instruments('csi500'), ['Ref($close, -1)/$close-1'], pred_df_dates.min(), pred_df_dates.max())
-                features_df.columns = ['label']
-
-                qcr.analysis_position.rank_label_graph(positions, features_df, pred_df_dates.min(), pred_df_dates.max())
-
-
-    :param position: position data; **qlib.backtest.backtest** result.
-    :param label_data: **D.features** result; index is **pd.MultiIndex**, index name is **[instrument, datetime]**; columns names is **[label]**.
-
-        **The label T is the change from T to T+1**, it is recommended to use ``close``, example: `D.features(D.instruments('csi500'), ['Ref($close, -1)/$close-1'])`.
-
-
-            .. code-block:: python
-
-                                                label
-                instrument  datetime
-                SH600004        2017-12-11  -0.013502
-                                2017-12-12  -0.072367
-                                2017-12-13  -0.068605
-                                2017-12-14  0.012440
-                                2017-12-15  -0.102778
-
-
-    :param start_date: start date
-    :param end_date: end_date
-    :param show_notebook: **True** or **False**. If True, show graph in notebook, else return figures.
-    :return:
-    """
-    position = copy.deepcopy(position)
-    label_data.columns = ["label"]
-    _figures = _get_figure_with_position(position, label_data, start_date, end_date)
-    if show_notebook:
-        ScatterGraph.show_graph_in_notebook(_figures)
-    else:
-        return _figures
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+import copy
+from typing import Iterable
+
+import pandas as pd
+import plotly.graph_objs as go
+
+from ..graph import ScatterGraph
+from ..analysis_position.parse_position import get_position_data
+
+
+def _get_figure_with_position(
+    position: dict, label_data: pd.DataFrame, start_date=None, end_date=None
+) -> Iterable[go.Figure]:
+    """Get average analysis figures
+
+    :param position: position
+    :param label_data:
+    :param start_date:
+    :param end_date:
+    :return:
+    """
+    _position_df = get_position_data(
+        position,
+        label_data,
+        calculate_label_rank=True,
+        start_date=start_date,
+        end_date=end_date,
+    )
+
+    res_dict = dict()
+    _pos_gp = _position_df.groupby(level=1)
+    for _item in _pos_gp:
+        _date = _item[0]
+        _day_df = _item[1]
+
+        _day_value = res_dict.setdefault(_date, {})
+        for _i, _name in {0: "Hold", 1: "Buy", -1: "Sell"}.items():
+            _temp_df = _day_df[_day_df["status"] == _i]
+            if _temp_df.empty:
+                _day_value[_name] = 0
+            else:
+                _day_value[_name] = _temp_df["rank_label_mean"].values[0]
+
+    _res_df = pd.DataFrame.from_dict(res_dict, orient="index")
+    # FIXME: support HIGH-FREQ
+    _res_df.index = _res_df.index.strftime("%Y-%m-%d")
+    for _col in _res_df.columns:
+        yield ScatterGraph(
+            _res_df.loc[:, [_col]],
+            layout=dict(
+                title=_col,
+                xaxis=dict(type="category", tickangle=45),
+                yaxis=dict(title="lable-rank-ratio: %"),
+            ),
+            graph_kwargs=dict(mode="lines+markers"),
+        ).figure
+
+
+def rank_label_graph(
+    position: dict,
+    label_data: pd.DataFrame,
+    start_date=None,
+    end_date=None,
+    show_notebook=True,
+) -> Iterable[go.Figure]:
+    """Ranking percentage of stocks buy, sell, and holding on the trading day.
+    Average rank-ratio(similar to **sell_df['label'].rank(ascending=False) / len(sell_df)**) of daily trading
+
+        Example:
+
+
+            .. code-block:: python
+
+                from qlib.data import D
+                from qlib.contrib.evaluate import backtest
+                from qlib.contrib.strategy import TopkDropoutStrategy
+
+                # backtest parameters
+                bparas = {}
+                bparas['limit_threshold'] = 0.095
+                bparas['account'] = 1000000000
+
+                sparas = {}
+                sparas['topk'] = 50
+                sparas['n_drop'] = 230
+                strategy = TopkDropoutStrategy(**sparas)
+
+                _, positions = backtest(pred_df, strategy, **bparas)
+
+                pred_df_dates = pred_df.index.get_level_values(level='datetime')
+                features_df = D.features(D.instruments('csi500'), ['Ref($close, -1)/$close-1'], pred_df_dates.min(), pred_df_dates.max())
+                features_df.columns = ['label']
+
+                qcr.analysis_position.rank_label_graph(positions, features_df, pred_df_dates.min(), pred_df_dates.max())
+
+
+    :param position: position data; **qlib.backtest.backtest** result.
+    :param label_data: **D.features** result; index is **pd.MultiIndex**, index name is **[instrument, datetime]**; columns names is **[label]**.
+
+        **The label T is the change from T to T+1**, it is recommended to use ``close``, example: `D.features(D.instruments('csi500'), ['Ref($close, -1)/$close-1'])`.
+
+
+            .. code-block:: python
+
+                                                label
+                instrument  datetime
+                SH600004        2017-12-11  -0.013502
+                                2017-12-12  -0.072367
+                                2017-12-13  -0.068605
+                                2017-12-14  0.012440
+                                2017-12-15  -0.102778
+
+
+    :param start_date: start date
+    :param end_date: end_date
+    :param show_notebook: **True** or **False**. If True, show graph in notebook, else return figures.
+    :return:
+    """
+    position = copy.deepcopy(position)
+    label_data.columns = ["label"]
+    _figures = _get_figure_with_position(position, label_data, start_date, end_date)
+    if show_notebook:
+        ScatterGraph.show_graph_in_notebook(_figures)
+    else:
+        return _figures
```

## qlib/contrib/report/analysis_position/report.py

 * *Ordering differences only*

```diff
@@ -1,248 +1,248 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-import pandas as pd
-
-from ..graph import SubplotsGraph, BaseGraph
-
-
-def _calculate_maximum(df: pd.DataFrame, is_ex: bool = False):
-    """
-
-    :param df:
-    :param is_ex:
-    :return:
-    """
-    if is_ex:
-        end_date = df["cum_ex_return_wo_cost_mdd"].idxmin()
-        start_date = df.loc[df.index <= end_date]["cum_ex_return_wo_cost"].idxmax()
-    else:
-        end_date = df["return_wo_mdd"].idxmin()
-        start_date = df.loc[df.index <= end_date]["cum_return_wo_cost"].idxmax()
-    return start_date, end_date
-
-
-def _calculate_mdd(series):
-    """
-    Calculate mdd
-
-    :param series:
-    :return:
-    """
-    return series - series.cummax()
-
-
-def _calculate_report_data(df: pd.DataFrame) -> pd.DataFrame:
-    """
-
-    :param df:
-    :return:
-    """
-    index_names = df.index.names
-    df.index = df.index.strftime("%Y-%m-%d")
-
-    report_df = pd.DataFrame()
-
-    report_df["cum_bench"] = df["bench"].cumsum()
-    report_df["cum_return_wo_cost"] = df["return"].cumsum()
-    report_df["cum_return_w_cost"] = (df["return"] - df["cost"]).cumsum()
-    # report_df['cum_return'] - report_df['cum_return'].cummax()
-    report_df["return_wo_mdd"] = _calculate_mdd(report_df["cum_return_wo_cost"])
-    report_df["return_w_cost_mdd"] = _calculate_mdd((df["return"] - df["cost"]).cumsum())
-
-    report_df["cum_ex_return_wo_cost"] = (df["return"] - df["bench"]).cumsum()
-    report_df["cum_ex_return_w_cost"] = (df["return"] - df["bench"] - df["cost"]).cumsum()
-    report_df["cum_ex_return_wo_cost_mdd"] = _calculate_mdd((df["return"] - df["bench"]).cumsum())
-    report_df["cum_ex_return_w_cost_mdd"] = _calculate_mdd((df["return"] - df["cost"] - df["bench"]).cumsum())
-    # return_wo_mdd , return_w_cost_mdd,  cum_ex_return_wo_cost_mdd, cum_ex_return_w
-
-    report_df["turnover"] = df["turnover"]
-    report_df.sort_index(ascending=True, inplace=True)
-
-    report_df.index.names = index_names
-    return report_df
-
-
-def _report_figure(df: pd.DataFrame) -> [list, tuple]:
-    """
-
-    :param df:
-    :return:
-    """
-
-    # Get data
-    report_df = _calculate_report_data(df)
-
-    # Maximum Drawdown
-    max_start_date, max_end_date = _calculate_maximum(report_df)
-    ex_max_start_date, ex_max_end_date = _calculate_maximum(report_df, True)
-
-    index_name = report_df.index.name
-    _temp_df = report_df.reset_index()
-    _temp_df.loc[-1] = 0
-    _temp_df = _temp_df.shift(1)
-    _temp_df.loc[0, index_name] = "T0"
-    _temp_df.set_index(index_name, inplace=True)
-    _temp_df.iloc[0] = 0
-    report_df = _temp_df
-
-    # Create figure
-    _default_kind_map = dict(kind="ScatterGraph", kwargs={"mode": "lines+markers"})
-    _temp_fill_args = {"fill": "tozeroy", "mode": "lines+markers"}
-    _column_row_col_dict = [
-        ("cum_bench", dict(row=1, col=1)),
-        ("cum_return_wo_cost", dict(row=1, col=1)),
-        ("cum_return_w_cost", dict(row=1, col=1)),
-        ("return_wo_mdd", dict(row=2, col=1, graph_kwargs=_temp_fill_args)),
-        ("return_w_cost_mdd", dict(row=3, col=1, graph_kwargs=_temp_fill_args)),
-        ("cum_ex_return_wo_cost", dict(row=4, col=1)),
-        ("cum_ex_return_w_cost", dict(row=4, col=1)),
-        ("turnover", dict(row=5, col=1)),
-        ("cum_ex_return_w_cost_mdd", dict(row=6, col=1, graph_kwargs=_temp_fill_args)),
-        ("cum_ex_return_wo_cost_mdd", dict(row=7, col=1, graph_kwargs=_temp_fill_args)),
-    ]
-
-    _subplot_layout = dict()
-    for i in range(1, 8):
-        # yaxis
-        _subplot_layout.update({"yaxis{}".format(i): dict(zeroline=True, showline=True, showticklabels=True)})
-        _show_line = i == 7
-        _subplot_layout.update({"xaxis{}".format(i): dict(showline=_show_line, type="category", tickangle=45)})
-
-    _layout_style = dict(
-        height=1200,
-        title=" ",
-        shapes=[
-            {
-                "type": "rect",
-                "xref": "x",
-                "yref": "paper",
-                "x0": max_start_date,
-                "y0": 0.55,
-                "x1": max_end_date,
-                "y1": 1,
-                "fillcolor": "#d3d3d3",
-                "opacity": 0.3,
-                "line": {
-                    "width": 0,
-                },
-            },
-            {
-                "type": "rect",
-                "xref": "x",
-                "yref": "paper",
-                "x0": ex_max_start_date,
-                "y0": 0,
-                "x1": ex_max_end_date,
-                "y1": 0.55,
-                "fillcolor": "#d3d3d3",
-                "opacity": 0.3,
-                "line": {
-                    "width": 0,
-                },
-            },
-        ],
-    )
-
-    _subplot_kwargs = dict(
-        shared_xaxes=True,
-        vertical_spacing=0.01,
-        rows=7,
-        cols=1,
-        row_width=[1, 1, 1, 3, 1, 1, 3],
-        print_grid=False,
-    )
-    figure = SubplotsGraph(
-        df=report_df,
-        layout=_layout_style,
-        sub_graph_data=_column_row_col_dict,
-        subplots_kwargs=_subplot_kwargs,
-        kind_map=_default_kind_map,
-        sub_graph_layout=_subplot_layout,
-    ).figure
-    return (figure,)
-
-
-def report_graph(report_df: pd.DataFrame, show_notebook: bool = True) -> [list, tuple]:
-    """display backtest report
-
-        Example:
-
-
-            .. code-block:: python
-
-                import qlib
-                import pandas as pd
-                from qlib.utils.time import Freq
-                from qlib.utils import flatten_dict
-                from qlib.backtest import backtest, executor
-                from qlib.contrib.evaluate import risk_analysis
-                from qlib.contrib.strategy import TopkDropoutStrategy
-
-                # init qlib
-                qlib.init(provider_uri=<qlib data dir>)
-
-                CSI300_BENCH = "SH000300"
-                FREQ = "day"
-                STRATEGY_CONFIG = {
-                    "topk": 50,
-                    "n_drop": 5,
-                    # pred_score, pd.Series
-                    "signal": pred_score,
-                }
-
-                EXECUTOR_CONFIG = {
-                    "time_per_step": "day",
-                    "generate_portfolio_metrics": True,
-                }
-
-                backtest_config = {
-                    "start_time": "2017-01-01",
-                    "end_time": "2020-08-01",
-                    "account": 100000000,
-                    "benchmark": CSI300_BENCH,
-                    "exchange_kwargs": {
-                        "freq": FREQ,
-                        "limit_threshold": 0.095,
-                        "deal_price": "close",
-                        "open_cost": 0.0005,
-                        "close_cost": 0.0015,
-                        "min_cost": 5,
-                    },
-                }
-
-                # strategy object
-                strategy_obj = TopkDropoutStrategy(**STRATEGY_CONFIG)
-                # executor object
-                executor_obj = executor.SimulatorExecutor(**EXECUTOR_CONFIG)
-                # backtest
-                portfolio_metric_dict, indicator_dict = backtest(executor=executor_obj, strategy=strategy_obj, **backtest_config)
-                analysis_freq = "{0}{1}".format(*Freq.parse(FREQ))
-                # backtest info
-                report_normal_df, positions_normal = portfolio_metric_dict.get(analysis_freq)
-
-                qcr.analysis_position.report_graph(report_normal_df)
-
-    :param report_df: **df.index.name** must be **date**, **df.columns** must contain **return**, **turnover**, **cost**, **bench**.
-
-
-            .. code-block:: python
-
-                            return      cost        bench       turnover
-                date
-                2017-01-04  0.003421    0.000864    0.011693    0.576325
-                2017-01-05  0.000508    0.000447    0.000721    0.227882
-                2017-01-06  -0.003321   0.000212    -0.004322   0.102765
-                2017-01-09  0.006753    0.000212    0.006874    0.105864
-                2017-01-10  -0.000416   0.000440    -0.003350   0.208396
-
-
-    :param show_notebook: whether to display graphics in notebook, the default is **True**.
-    :return: if show_notebook is True, display in notebook; else return **plotly.graph_objs.Figure** list.
-    """
-    report_df = report_df.copy()
-    fig_list = _report_figure(report_df)
-    if show_notebook:
-        BaseGraph.show_graph_in_notebook(fig_list)
-    else:
-        return fig_list
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+import pandas as pd
+
+from ..graph import SubplotsGraph, BaseGraph
+
+
+def _calculate_maximum(df: pd.DataFrame, is_ex: bool = False):
+    """
+
+    :param df:
+    :param is_ex:
+    :return:
+    """
+    if is_ex:
+        end_date = df["cum_ex_return_wo_cost_mdd"].idxmin()
+        start_date = df.loc[df.index <= end_date]["cum_ex_return_wo_cost"].idxmax()
+    else:
+        end_date = df["return_wo_mdd"].idxmin()
+        start_date = df.loc[df.index <= end_date]["cum_return_wo_cost"].idxmax()
+    return start_date, end_date
+
+
+def _calculate_mdd(series):
+    """
+    Calculate mdd
+
+    :param series:
+    :return:
+    """
+    return series - series.cummax()
+
+
+def _calculate_report_data(df: pd.DataFrame) -> pd.DataFrame:
+    """
+
+    :param df:
+    :return:
+    """
+    index_names = df.index.names
+    df.index = df.index.strftime("%Y-%m-%d")
+
+    report_df = pd.DataFrame()
+
+    report_df["cum_bench"] = df["bench"].cumsum()
+    report_df["cum_return_wo_cost"] = df["return"].cumsum()
+    report_df["cum_return_w_cost"] = (df["return"] - df["cost"]).cumsum()
+    # report_df['cum_return'] - report_df['cum_return'].cummax()
+    report_df["return_wo_mdd"] = _calculate_mdd(report_df["cum_return_wo_cost"])
+    report_df["return_w_cost_mdd"] = _calculate_mdd((df["return"] - df["cost"]).cumsum())
+
+    report_df["cum_ex_return_wo_cost"] = (df["return"] - df["bench"]).cumsum()
+    report_df["cum_ex_return_w_cost"] = (df["return"] - df["bench"] - df["cost"]).cumsum()
+    report_df["cum_ex_return_wo_cost_mdd"] = _calculate_mdd((df["return"] - df["bench"]).cumsum())
+    report_df["cum_ex_return_w_cost_mdd"] = _calculate_mdd((df["return"] - df["cost"] - df["bench"]).cumsum())
+    # return_wo_mdd , return_w_cost_mdd,  cum_ex_return_wo_cost_mdd, cum_ex_return_w
+
+    report_df["turnover"] = df["turnover"]
+    report_df.sort_index(ascending=True, inplace=True)
+
+    report_df.index.names = index_names
+    return report_df
+
+
+def _report_figure(df: pd.DataFrame) -> [list, tuple]:
+    """
+
+    :param df:
+    :return:
+    """
+
+    # Get data
+    report_df = _calculate_report_data(df)
+
+    # Maximum Drawdown
+    max_start_date, max_end_date = _calculate_maximum(report_df)
+    ex_max_start_date, ex_max_end_date = _calculate_maximum(report_df, True)
+
+    index_name = report_df.index.name
+    _temp_df = report_df.reset_index()
+    _temp_df.loc[-1] = 0
+    _temp_df = _temp_df.shift(1)
+    _temp_df.loc[0, index_name] = "T0"
+    _temp_df.set_index(index_name, inplace=True)
+    _temp_df.iloc[0] = 0
+    report_df = _temp_df
+
+    # Create figure
+    _default_kind_map = dict(kind="ScatterGraph", kwargs={"mode": "lines+markers"})
+    _temp_fill_args = {"fill": "tozeroy", "mode": "lines+markers"}
+    _column_row_col_dict = [
+        ("cum_bench", dict(row=1, col=1)),
+        ("cum_return_wo_cost", dict(row=1, col=1)),
+        ("cum_return_w_cost", dict(row=1, col=1)),
+        ("return_wo_mdd", dict(row=2, col=1, graph_kwargs=_temp_fill_args)),
+        ("return_w_cost_mdd", dict(row=3, col=1, graph_kwargs=_temp_fill_args)),
+        ("cum_ex_return_wo_cost", dict(row=4, col=1)),
+        ("cum_ex_return_w_cost", dict(row=4, col=1)),
+        ("turnover", dict(row=5, col=1)),
+        ("cum_ex_return_w_cost_mdd", dict(row=6, col=1, graph_kwargs=_temp_fill_args)),
+        ("cum_ex_return_wo_cost_mdd", dict(row=7, col=1, graph_kwargs=_temp_fill_args)),
+    ]
+
+    _subplot_layout = dict()
+    for i in range(1, 8):
+        # yaxis
+        _subplot_layout.update({"yaxis{}".format(i): dict(zeroline=True, showline=True, showticklabels=True)})
+        _show_line = i == 7
+        _subplot_layout.update({"xaxis{}".format(i): dict(showline=_show_line, type="category", tickangle=45)})
+
+    _layout_style = dict(
+        height=1200,
+        title=" ",
+        shapes=[
+            {
+                "type": "rect",
+                "xref": "x",
+                "yref": "paper",
+                "x0": max_start_date,
+                "y0": 0.55,
+                "x1": max_end_date,
+                "y1": 1,
+                "fillcolor": "#d3d3d3",
+                "opacity": 0.3,
+                "line": {
+                    "width": 0,
+                },
+            },
+            {
+                "type": "rect",
+                "xref": "x",
+                "yref": "paper",
+                "x0": ex_max_start_date,
+                "y0": 0,
+                "x1": ex_max_end_date,
+                "y1": 0.55,
+                "fillcolor": "#d3d3d3",
+                "opacity": 0.3,
+                "line": {
+                    "width": 0,
+                },
+            },
+        ],
+    )
+
+    _subplot_kwargs = dict(
+        shared_xaxes=True,
+        vertical_spacing=0.01,
+        rows=7,
+        cols=1,
+        row_width=[1, 1, 1, 3, 1, 1, 3],
+        print_grid=False,
+    )
+    figure = SubplotsGraph(
+        df=report_df,
+        layout=_layout_style,
+        sub_graph_data=_column_row_col_dict,
+        subplots_kwargs=_subplot_kwargs,
+        kind_map=_default_kind_map,
+        sub_graph_layout=_subplot_layout,
+    ).figure
+    return (figure,)
+
+
+def report_graph(report_df: pd.DataFrame, show_notebook: bool = True) -> [list, tuple]:
+    """display backtest report
+
+        Example:
+
+
+            .. code-block:: python
+
+                import qlib
+                import pandas as pd
+                from qlib.utils.time import Freq
+                from qlib.utils import flatten_dict
+                from qlib.backtest import backtest, executor
+                from qlib.contrib.evaluate import risk_analysis
+                from qlib.contrib.strategy import TopkDropoutStrategy
+
+                # init qlib
+                qlib.init(provider_uri=<qlib data dir>)
+
+                CSI300_BENCH = "SH000300"
+                FREQ = "day"
+                STRATEGY_CONFIG = {
+                    "topk": 50,
+                    "n_drop": 5,
+                    # pred_score, pd.Series
+                    "signal": pred_score,
+                }
+
+                EXECUTOR_CONFIG = {
+                    "time_per_step": "day",
+                    "generate_portfolio_metrics": True,
+                }
+
+                backtest_config = {
+                    "start_time": "2017-01-01",
+                    "end_time": "2020-08-01",
+                    "account": 100000000,
+                    "benchmark": CSI300_BENCH,
+                    "exchange_kwargs": {
+                        "freq": FREQ,
+                        "limit_threshold": 0.095,
+                        "deal_price": "close",
+                        "open_cost": 0.0005,
+                        "close_cost": 0.0015,
+                        "min_cost": 5,
+                    },
+                }
+
+                # strategy object
+                strategy_obj = TopkDropoutStrategy(**STRATEGY_CONFIG)
+                # executor object
+                executor_obj = executor.SimulatorExecutor(**EXECUTOR_CONFIG)
+                # backtest
+                portfolio_metric_dict, indicator_dict = backtest(executor=executor_obj, strategy=strategy_obj, **backtest_config)
+                analysis_freq = "{0}{1}".format(*Freq.parse(FREQ))
+                # backtest info
+                report_normal_df, positions_normal = portfolio_metric_dict.get(analysis_freq)
+
+                qcr.analysis_position.report_graph(report_normal_df)
+
+    :param report_df: **df.index.name** must be **date**, **df.columns** must contain **return**, **turnover**, **cost**, **bench**.
+
+
+            .. code-block:: python
+
+                            return      cost        bench       turnover
+                date
+                2017-01-04  0.003421    0.000864    0.011693    0.576325
+                2017-01-05  0.000508    0.000447    0.000721    0.227882
+                2017-01-06  -0.003321   0.000212    -0.004322   0.102765
+                2017-01-09  0.006753    0.000212    0.006874    0.105864
+                2017-01-10  -0.000416   0.000440    -0.003350   0.208396
+
+
+    :param show_notebook: whether to display graphics in notebook, the default is **True**.
+    :return: if show_notebook is True, display in notebook; else return **plotly.graph_objs.Figure** list.
+    """
+    report_df = report_df.copy()
+    fig_list = _report_figure(report_df)
+    if show_notebook:
+        BaseGraph.show_graph_in_notebook(fig_list)
+    else:
+        return fig_list
```

## qlib/contrib/report/analysis_position/risk_analysis.py

 * *Ordering differences only*

```diff
@@ -1,295 +1,295 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-from typing import Iterable
-
-import pandas as pd
-
-import plotly.graph_objs as py
-
-from ...evaluate import risk_analysis
-
-from ..graph import SubplotsGraph, ScatterGraph
-
-
-def _get_risk_analysis_data_with_report(
-    report_normal_df: pd.DataFrame,
-    # report_long_short_df: pd.DataFrame,
-    date: pd.Timestamp,
-) -> pd.DataFrame:
-    """Get risk analysis data with report
-
-    :param report_normal_df: report data
-    :param report_long_short_df: report data
-    :param date: date string
-    :return:
-    """
-
-    analysis = dict()
-    # if not report_long_short_df.empty:
-    #     analysis["pred_long"] = risk_analysis(report_long_short_df["long"])
-    #     analysis["pred_short"] = risk_analysis(report_long_short_df["short"])
-    #     analysis["pred_long_short"] = risk_analysis(report_long_short_df["long_short"])
-
-    if not report_normal_df.empty:
-        analysis["excess_return_without_cost"] = risk_analysis(report_normal_df["return"] - report_normal_df["bench"])
-        analysis["excess_return_with_cost"] = risk_analysis(
-            report_normal_df["return"] - report_normal_df["bench"] - report_normal_df["cost"]
-        )
-    analysis_df = pd.concat(analysis)  # type: pd.DataFrame
-    analysis_df["date"] = date
-    return analysis_df
-
-
-def _get_all_risk_analysis(risk_df: pd.DataFrame) -> pd.DataFrame:
-    """risk_df to standard
-
-    :param risk_df: risk data
-    :return:
-    """
-    if risk_df is None:
-        return pd.DataFrame()
-    risk_df = risk_df.unstack()
-    risk_df.columns = risk_df.columns.droplevel(0)
-    return risk_df.drop("mean", axis=1)
-
-
-def _get_monthly_risk_analysis_with_report(report_normal_df: pd.DataFrame) -> pd.DataFrame:
-    """Get monthly analysis data
-
-    :param report_normal_df:
-    # :param report_long_short_df:
-    :return:
-    """
-
-    # Group by month
-    report_normal_gp = report_normal_df.groupby([report_normal_df.index.year, report_normal_df.index.month])
-    # report_long_short_gp = report_long_short_df.groupby(
-    #     [report_long_short_df.index.year, report_long_short_df.index.month]
-    # )
-
-    gp_month = sorted(set(report_normal_gp.size().index))
-
-    _monthly_df = pd.DataFrame()
-    for gp_m in gp_month:
-        _m_report_normal = report_normal_gp.get_group(gp_m)
-        # _m_report_long_short = report_long_short_gp.get_group(gp_m)
-
-        if len(_m_report_normal) < 3:
-            # The month's data is less than 3, not displayed
-            # FIXME: If the trading day of a month is less than 3 days, a breakpoint will appear in the graph
-            continue
-        month_days = pd.Timestamp(year=gp_m[0], month=gp_m[1], day=1).days_in_month
-        _temp_df = _get_risk_analysis_data_with_report(
-            _m_report_normal,
-            # _m_report_long_short,
-            pd.Timestamp(year=gp_m[0], month=gp_m[1], day=month_days),
-        )
-        _monthly_df = pd.concat([_monthly_df, _temp_df], sort=False)
-
-    return _monthly_df
-
-
-def _get_monthly_analysis_with_feature(monthly_df: pd.DataFrame, feature: str = "annualized_return") -> pd.DataFrame:
-    """
-
-    :param monthly_df:
-    :param feature:
-    :return:
-    """
-    _monthly_df_gp = monthly_df.reset_index().groupby(["level_1"])
-
-    _name_df = _monthly_df_gp.get_group(feature).set_index(["level_0", "level_1"])
-    _temp_df = _name_df.pivot_table(index="date", values=["risk"], columns=_name_df.index)
-    _temp_df.columns = map(lambda x: "_".join(x[-1]), _temp_df.columns)
-    _temp_df.index = _temp_df.index.strftime("%Y-%m")
-
-    return _temp_df
-
-
-def _get_risk_analysis_figure(analysis_df: pd.DataFrame) -> Iterable[py.Figure]:
-    """Get analysis graph figure
-
-    :param analysis_df:
-    :return:
-    """
-    if analysis_df is None:
-        return []
-
-    _figure = SubplotsGraph(
-        _get_all_risk_analysis(analysis_df),
-        kind_map=dict(kind="BarGraph", kwargs={}),
-        subplots_kwargs={"rows": 1, "cols": 4},
-    ).figure
-    return (_figure,)
-
-
-def _get_monthly_risk_analysis_figure(report_normal_df: pd.DataFrame) -> Iterable[py.Figure]:
-    """Get analysis monthly graph figure
-
-    :param report_normal_df:
-    :param report_long_short_df:
-    :return:
-    """
-
-    # if report_normal_df is None and report_long_short_df is None:
-    #     return []
-    if report_normal_df is None:
-        return []
-
-    # if report_normal_df is None:
-    #     report_normal_df = pd.DataFrame(index=report_long_short_df.index)
-
-    # if report_long_short_df is None:
-    #     report_long_short_df = pd.DataFrame(index=report_normal_df.index)
-
-    _monthly_df = _get_monthly_risk_analysis_with_report(
-        report_normal_df=report_normal_df,
-        # report_long_short_df=report_long_short_df,
-    )
-
-    for _feature in ["annualized_return", "max_drawdown", "information_ratio", "std"]:
-        _temp_df = _get_monthly_analysis_with_feature(_monthly_df, _feature)
-        yield ScatterGraph(
-            _temp_df,
-            layout=dict(title=_feature, xaxis=dict(type="category", tickangle=45)),
-            graph_kwargs={"mode": "lines+markers"},
-        ).figure
-
-
-def risk_analysis_graph(
-    analysis_df: pd.DataFrame = None,
-    report_normal_df: pd.DataFrame = None,
-    report_long_short_df: pd.DataFrame = None,
-    show_notebook: bool = True,
-) -> Iterable[py.Figure]:
-    """Generate analysis graph and monthly analysis
-
-        Example:
-
-
-            .. code-block:: python
-
-                import qlib
-                import pandas as pd
-                from qlib.utils.time import Freq
-                from qlib.utils import flatten_dict
-                from qlib.backtest import backtest, executor
-                from qlib.contrib.evaluate import risk_analysis
-                from qlib.contrib.strategy import TopkDropoutStrategy
-
-                # init qlib
-                qlib.init(provider_uri=<qlib data dir>)
-
-                CSI300_BENCH = "SH000300"
-                FREQ = "day"
-                STRATEGY_CONFIG = {
-                    "topk": 50,
-                    "n_drop": 5,
-                    # pred_score, pd.Series
-                    "signal": pred_score,
-                }
-
-                EXECUTOR_CONFIG = {
-                    "time_per_step": "day",
-                    "generate_portfolio_metrics": True,
-                }
-
-                backtest_config = {
-                    "start_time": "2017-01-01",
-                    "end_time": "2020-08-01",
-                    "account": 100000000,
-                    "benchmark": CSI300_BENCH,
-                    "exchange_kwargs": {
-                        "freq": FREQ,
-                        "limit_threshold": 0.095,
-                        "deal_price": "close",
-                        "open_cost": 0.0005,
-                        "close_cost": 0.0015,
-                        "min_cost": 5,
-                    },
-                }
-
-                # strategy object
-                strategy_obj = TopkDropoutStrategy(**STRATEGY_CONFIG)
-                # executor object
-                executor_obj = executor.SimulatorExecutor(**EXECUTOR_CONFIG)
-                # backtest
-                portfolio_metric_dict, indicator_dict = backtest(executor=executor_obj, strategy=strategy_obj, **backtest_config)
-                analysis_freq = "{0}{1}".format(*Freq.parse(FREQ))
-                # backtest info
-                report_normal_df, positions_normal = portfolio_metric_dict.get(analysis_freq)
-                analysis = dict()
-                analysis["excess_return_without_cost"] = risk_analysis(
-                    report_normal_df["return"] - report_normal_df["bench"], freq=analysis_freq
-                )
-                analysis["excess_return_with_cost"] = risk_analysis(
-                    report_normal_df["return"] - report_normal_df["bench"] - report_normal_df["cost"], freq=analysis_freq
-                )
-
-                analysis_df = pd.concat(analysis)  # type: pd.DataFrame
-                analysis_position.risk_analysis_graph(analysis_df, report_normal_df)
-
-
-
-    :param analysis_df: analysis data, index is **pd.MultiIndex**; columns names is **[risk]**.
-
-
-            .. code-block:: python
-
-                                                                  risk
-                excess_return_without_cost mean               0.000692
-                                           std                0.005374
-                                           annualized_return  0.174495
-                                           information_ratio  2.045576
-                                           max_drawdown      -0.079103
-                excess_return_with_cost    mean               0.000499
-                                           std                0.005372
-                                           annualized_return  0.125625
-                                           information_ratio  1.473152
-                                           max_drawdown      -0.088263
-
-
-    :param report_normal_df: **df.index.name** must be **date**, df.columns must contain **return**, **turnover**, **cost**, **bench**.
-
-
-            .. code-block:: python
-
-                            return      cost        bench       turnover
-                date
-                2017-01-04  0.003421    0.000864    0.011693    0.576325
-                2017-01-05  0.000508    0.000447    0.000721    0.227882
-                2017-01-06  -0.003321   0.000212    -0.004322   0.102765
-                2017-01-09  0.006753    0.000212    0.006874    0.105864
-                2017-01-10  -0.000416   0.000440    -0.003350   0.208396
-
-
-    :param report_long_short_df: **df.index.name** must be **date**, df.columns contain **long**, **short**, **long_short**.
-
-
-            .. code-block:: python
-
-                            long        short       long_short
-                date
-                2017-01-04  -0.001360   0.001394    0.000034
-                2017-01-05  0.002456    0.000058    0.002514
-                2017-01-06  0.000120    0.002739    0.002859
-                2017-01-09  0.001436    0.001838    0.003273
-                2017-01-10  0.000824    -0.001944   -0.001120
-
-
-    :param show_notebook: Whether to display graphics in a notebook, default **True**.
-        If True, show graph in notebook
-        If False, return graph figure
-    :return:
-    """
-    _figure_list = list(_get_risk_analysis_figure(analysis_df)) + list(
-        _get_monthly_risk_analysis_figure(
-            report_normal_df,
-            # report_long_short_df,
-        )
-    )
-    if show_notebook:
-        ScatterGraph.show_graph_in_notebook(_figure_list)
-    else:
-        return _figure_list
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+from typing import Iterable
+
+import pandas as pd
+
+import plotly.graph_objs as py
+
+from ...evaluate import risk_analysis
+
+from ..graph import SubplotsGraph, ScatterGraph
+
+
+def _get_risk_analysis_data_with_report(
+    report_normal_df: pd.DataFrame,
+    # report_long_short_df: pd.DataFrame,
+    date: pd.Timestamp,
+) -> pd.DataFrame:
+    """Get risk analysis data with report
+
+    :param report_normal_df: report data
+    :param report_long_short_df: report data
+    :param date: date string
+    :return:
+    """
+
+    analysis = dict()
+    # if not report_long_short_df.empty:
+    #     analysis["pred_long"] = risk_analysis(report_long_short_df["long"])
+    #     analysis["pred_short"] = risk_analysis(report_long_short_df["short"])
+    #     analysis["pred_long_short"] = risk_analysis(report_long_short_df["long_short"])
+
+    if not report_normal_df.empty:
+        analysis["excess_return_without_cost"] = risk_analysis(report_normal_df["return"] - report_normal_df["bench"])
+        analysis["excess_return_with_cost"] = risk_analysis(
+            report_normal_df["return"] - report_normal_df["bench"] - report_normal_df["cost"]
+        )
+    analysis_df = pd.concat(analysis)  # type: pd.DataFrame
+    analysis_df["date"] = date
+    return analysis_df
+
+
+def _get_all_risk_analysis(risk_df: pd.DataFrame) -> pd.DataFrame:
+    """risk_df to standard
+
+    :param risk_df: risk data
+    :return:
+    """
+    if risk_df is None:
+        return pd.DataFrame()
+    risk_df = risk_df.unstack()
+    risk_df.columns = risk_df.columns.droplevel(0)
+    return risk_df.drop("mean", axis=1)
+
+
+def _get_monthly_risk_analysis_with_report(report_normal_df: pd.DataFrame) -> pd.DataFrame:
+    """Get monthly analysis data
+
+    :param report_normal_df:
+    # :param report_long_short_df:
+    :return:
+    """
+
+    # Group by month
+    report_normal_gp = report_normal_df.groupby([report_normal_df.index.year, report_normal_df.index.month])
+    # report_long_short_gp = report_long_short_df.groupby(
+    #     [report_long_short_df.index.year, report_long_short_df.index.month]
+    # )
+
+    gp_month = sorted(set(report_normal_gp.size().index))
+
+    _monthly_df = pd.DataFrame()
+    for gp_m in gp_month:
+        _m_report_normal = report_normal_gp.get_group(gp_m)
+        # _m_report_long_short = report_long_short_gp.get_group(gp_m)
+
+        if len(_m_report_normal) < 3:
+            # The month's data is less than 3, not displayed
+            # FIXME: If the trading day of a month is less than 3 days, a breakpoint will appear in the graph
+            continue
+        month_days = pd.Timestamp(year=gp_m[0], month=gp_m[1], day=1).days_in_month
+        _temp_df = _get_risk_analysis_data_with_report(
+            _m_report_normal,
+            # _m_report_long_short,
+            pd.Timestamp(year=gp_m[0], month=gp_m[1], day=month_days),
+        )
+        _monthly_df = pd.concat([_monthly_df, _temp_df], sort=False)
+
+    return _monthly_df
+
+
+def _get_monthly_analysis_with_feature(monthly_df: pd.DataFrame, feature: str = "annualized_return") -> pd.DataFrame:
+    """
+
+    :param monthly_df:
+    :param feature:
+    :return:
+    """
+    _monthly_df_gp = monthly_df.reset_index().groupby(["level_1"])
+
+    _name_df = _monthly_df_gp.get_group(feature).set_index(["level_0", "level_1"])
+    _temp_df = _name_df.pivot_table(index="date", values=["risk"], columns=_name_df.index)
+    _temp_df.columns = map(lambda x: "_".join(x[-1]), _temp_df.columns)
+    _temp_df.index = _temp_df.index.strftime("%Y-%m")
+
+    return _temp_df
+
+
+def _get_risk_analysis_figure(analysis_df: pd.DataFrame) -> Iterable[py.Figure]:
+    """Get analysis graph figure
+
+    :param analysis_df:
+    :return:
+    """
+    if analysis_df is None:
+        return []
+
+    _figure = SubplotsGraph(
+        _get_all_risk_analysis(analysis_df),
+        kind_map=dict(kind="BarGraph", kwargs={}),
+        subplots_kwargs={"rows": 1, "cols": 4},
+    ).figure
+    return (_figure,)
+
+
+def _get_monthly_risk_analysis_figure(report_normal_df: pd.DataFrame) -> Iterable[py.Figure]:
+    """Get analysis monthly graph figure
+
+    :param report_normal_df:
+    :param report_long_short_df:
+    :return:
+    """
+
+    # if report_normal_df is None and report_long_short_df is None:
+    #     return []
+    if report_normal_df is None:
+        return []
+
+    # if report_normal_df is None:
+    #     report_normal_df = pd.DataFrame(index=report_long_short_df.index)
+
+    # if report_long_short_df is None:
+    #     report_long_short_df = pd.DataFrame(index=report_normal_df.index)
+
+    _monthly_df = _get_monthly_risk_analysis_with_report(
+        report_normal_df=report_normal_df,
+        # report_long_short_df=report_long_short_df,
+    )
+
+    for _feature in ["annualized_return", "max_drawdown", "information_ratio", "std"]:
+        _temp_df = _get_monthly_analysis_with_feature(_monthly_df, _feature)
+        yield ScatterGraph(
+            _temp_df,
+            layout=dict(title=_feature, xaxis=dict(type="category", tickangle=45)),
+            graph_kwargs={"mode": "lines+markers"},
+        ).figure
+
+
+def risk_analysis_graph(
+    analysis_df: pd.DataFrame = None,
+    report_normal_df: pd.DataFrame = None,
+    report_long_short_df: pd.DataFrame = None,
+    show_notebook: bool = True,
+) -> Iterable[py.Figure]:
+    """Generate analysis graph and monthly analysis
+
+        Example:
+
+
+            .. code-block:: python
+
+                import qlib
+                import pandas as pd
+                from qlib.utils.time import Freq
+                from qlib.utils import flatten_dict
+                from qlib.backtest import backtest, executor
+                from qlib.contrib.evaluate import risk_analysis
+                from qlib.contrib.strategy import TopkDropoutStrategy
+
+                # init qlib
+                qlib.init(provider_uri=<qlib data dir>)
+
+                CSI300_BENCH = "SH000300"
+                FREQ = "day"
+                STRATEGY_CONFIG = {
+                    "topk": 50,
+                    "n_drop": 5,
+                    # pred_score, pd.Series
+                    "signal": pred_score,
+                }
+
+                EXECUTOR_CONFIG = {
+                    "time_per_step": "day",
+                    "generate_portfolio_metrics": True,
+                }
+
+                backtest_config = {
+                    "start_time": "2017-01-01",
+                    "end_time": "2020-08-01",
+                    "account": 100000000,
+                    "benchmark": CSI300_BENCH,
+                    "exchange_kwargs": {
+                        "freq": FREQ,
+                        "limit_threshold": 0.095,
+                        "deal_price": "close",
+                        "open_cost": 0.0005,
+                        "close_cost": 0.0015,
+                        "min_cost": 5,
+                    },
+                }
+
+                # strategy object
+                strategy_obj = TopkDropoutStrategy(**STRATEGY_CONFIG)
+                # executor object
+                executor_obj = executor.SimulatorExecutor(**EXECUTOR_CONFIG)
+                # backtest
+                portfolio_metric_dict, indicator_dict = backtest(executor=executor_obj, strategy=strategy_obj, **backtest_config)
+                analysis_freq = "{0}{1}".format(*Freq.parse(FREQ))
+                # backtest info
+                report_normal_df, positions_normal = portfolio_metric_dict.get(analysis_freq)
+                analysis = dict()
+                analysis["excess_return_without_cost"] = risk_analysis(
+                    report_normal_df["return"] - report_normal_df["bench"], freq=analysis_freq
+                )
+                analysis["excess_return_with_cost"] = risk_analysis(
+                    report_normal_df["return"] - report_normal_df["bench"] - report_normal_df["cost"], freq=analysis_freq
+                )
+
+                analysis_df = pd.concat(analysis)  # type: pd.DataFrame
+                analysis_position.risk_analysis_graph(analysis_df, report_normal_df)
+
+
+
+    :param analysis_df: analysis data, index is **pd.MultiIndex**; columns names is **[risk]**.
+
+
+            .. code-block:: python
+
+                                                                  risk
+                excess_return_without_cost mean               0.000692
+                                           std                0.005374
+                                           annualized_return  0.174495
+                                           information_ratio  2.045576
+                                           max_drawdown      -0.079103
+                excess_return_with_cost    mean               0.000499
+                                           std                0.005372
+                                           annualized_return  0.125625
+                                           information_ratio  1.473152
+                                           max_drawdown      -0.088263
+
+
+    :param report_normal_df: **df.index.name** must be **date**, df.columns must contain **return**, **turnover**, **cost**, **bench**.
+
+
+            .. code-block:: python
+
+                            return      cost        bench       turnover
+                date
+                2017-01-04  0.003421    0.000864    0.011693    0.576325
+                2017-01-05  0.000508    0.000447    0.000721    0.227882
+                2017-01-06  -0.003321   0.000212    -0.004322   0.102765
+                2017-01-09  0.006753    0.000212    0.006874    0.105864
+                2017-01-10  -0.000416   0.000440    -0.003350   0.208396
+
+
+    :param report_long_short_df: **df.index.name** must be **date**, df.columns contain **long**, **short**, **long_short**.
+
+
+            .. code-block:: python
+
+                            long        short       long_short
+                date
+                2017-01-04  -0.001360   0.001394    0.000034
+                2017-01-05  0.002456    0.000058    0.002514
+                2017-01-06  0.000120    0.002739    0.002859
+                2017-01-09  0.001436    0.001838    0.003273
+                2017-01-10  0.000824    -0.001944   -0.001120
+
+
+    :param show_notebook: Whether to display graphics in a notebook, default **True**.
+        If True, show graph in notebook
+        If False, return graph figure
+    :return:
+    """
+    _figure_list = list(_get_risk_analysis_figure(analysis_df)) + list(
+        _get_monthly_risk_analysis_figure(
+            report_normal_df,
+            # report_long_short_df,
+        )
+    )
+    if show_notebook:
+        ScatterGraph.show_graph_in_notebook(_figure_list)
+    else:
+        return _figure_list
```

## qlib/contrib/report/analysis_position/score_ic.py

 * *Ordering differences only*

```diff
@@ -1,69 +1,69 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-import pandas as pd
-
-from ..graph import ScatterGraph
-from ..utils import guess_plotly_rangebreaks
-
-
-def _get_score_ic(pred_label: pd.DataFrame):
-    """
-
-    :param pred_label:
-    :return:
-    """
-    concat_data = pred_label.copy()
-    concat_data.dropna(axis=0, how="any", inplace=True)
-    _ic = concat_data.groupby(level="datetime").apply(lambda x: x["label"].corr(x["score"]))
-    _rank_ic = concat_data.groupby(level="datetime").apply(lambda x: x["label"].corr(x["score"], method="spearman"))
-    return pd.DataFrame({"ic": _ic, "rank_ic": _rank_ic})
-
-
-def score_ic_graph(pred_label: pd.DataFrame, show_notebook: bool = True, **kwargs) -> [list, tuple]:
-    """score IC
-
-        Example:
-
-
-            .. code-block:: python
-
-                from qlib.data import D
-                from qlib.contrib.report import analysis_position
-                pred_df_dates = pred_df.index.get_level_values(level='datetime')
-                features_df = D.features(D.instruments('csi500'), ['Ref($close, -2)/Ref($close, -1)-1'], pred_df_dates.min(), pred_df_dates.max())
-                features_df.columns = ['label']
-                pred_label = pd.concat([features_df, pred], axis=1, sort=True).reindex(features_df.index)
-                analysis_position.score_ic_graph(pred_label)
-
-
-    :param pred_label: index is **pd.MultiIndex**, index name is **[instrument, datetime]**; columns names is **[score, label]**.
-
-
-            .. code-block:: python
-
-                instrument  datetime        score         label
-                SH600004  2017-12-11     -0.013502       -0.013502
-                            2017-12-12   -0.072367       -0.072367
-                            2017-12-13   -0.068605       -0.068605
-                            2017-12-14    0.012440        0.012440
-                            2017-12-15   -0.102778       -0.102778
-
-
-    :param show_notebook: whether to display graphics in notebook, the default is **True**.
-    :return: if show_notebook is True, display in notebook; else return **plotly.graph_objs.Figure** list.
-    """
-    _ic_df = _get_score_ic(pred_label)
-
-    _figure = ScatterGraph(
-        _ic_df,
-        layout=dict(
-            title="Score IC",
-            xaxis=dict(tickangle=45, rangebreaks=kwargs.get("rangebreaks", guess_plotly_rangebreaks(_ic_df.index))),
-        ),
-        graph_kwargs={"mode": "lines+markers"},
-    ).figure
-    if show_notebook:
-        ScatterGraph.show_graph_in_notebook([_figure])
-    else:
-        return (_figure,)
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+import pandas as pd
+
+from ..graph import ScatterGraph
+from ..utils import guess_plotly_rangebreaks
+
+
+def _get_score_ic(pred_label: pd.DataFrame):
+    """
+
+    :param pred_label:
+    :return:
+    """
+    concat_data = pred_label.copy()
+    concat_data.dropna(axis=0, how="any", inplace=True)
+    _ic = concat_data.groupby(level="datetime").apply(lambda x: x["label"].corr(x["score"]))
+    _rank_ic = concat_data.groupby(level="datetime").apply(lambda x: x["label"].corr(x["score"], method="spearman"))
+    return pd.DataFrame({"ic": _ic, "rank_ic": _rank_ic})
+
+
+def score_ic_graph(pred_label: pd.DataFrame, show_notebook: bool = True, **kwargs) -> [list, tuple]:
+    """score IC
+
+        Example:
+
+
+            .. code-block:: python
+
+                from qlib.data import D
+                from qlib.contrib.report import analysis_position
+                pred_df_dates = pred_df.index.get_level_values(level='datetime')
+                features_df = D.features(D.instruments('csi500'), ['Ref($close, -2)/Ref($close, -1)-1'], pred_df_dates.min(), pred_df_dates.max())
+                features_df.columns = ['label']
+                pred_label = pd.concat([features_df, pred], axis=1, sort=True).reindex(features_df.index)
+                analysis_position.score_ic_graph(pred_label)
+
+
+    :param pred_label: index is **pd.MultiIndex**, index name is **[instrument, datetime]**; columns names is **[score, label]**.
+
+
+            .. code-block:: python
+
+                instrument  datetime        score         label
+                SH600004  2017-12-11     -0.013502       -0.013502
+                            2017-12-12   -0.072367       -0.072367
+                            2017-12-13   -0.068605       -0.068605
+                            2017-12-14    0.012440        0.012440
+                            2017-12-15   -0.102778       -0.102778
+
+
+    :param show_notebook: whether to display graphics in notebook, the default is **True**.
+    :return: if show_notebook is True, display in notebook; else return **plotly.graph_objs.Figure** list.
+    """
+    _ic_df = _get_score_ic(pred_label)
+
+    _figure = ScatterGraph(
+        _ic_df,
+        layout=dict(
+            title="Score IC",
+            xaxis=dict(tickangle=45, rangebreaks=kwargs.get("rangebreaks", guess_plotly_rangebreaks(_ic_df.index))),
+        ),
+        graph_kwargs={"mode": "lines+markers"},
+    ).figure
+    if show_notebook:
+        ScatterGraph.show_graph_in_notebook([_figure])
+    else:
+        return (_figure,)
```

## qlib/contrib/report/data/__init__.py

 * *Ordering differences only*

```diff
@@ -1,7 +1,7 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-"""
-This module is designed to analysis data
-
-"""
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+"""
+This module is designed to analysis data
+
+"""
```

## qlib/contrib/report/data/ana.py

```diff
@@ -1,202 +1,201 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-import pandas as pd
-import numpy as np
-from qlib.contrib.report.data.base import FeaAnalyser
-from qlib.contrib.report.utils import sub_fig_generator
-from qlib.utils.paral import datetime_groupby_apply
-from qlib.contrib.eva.alpha import pred_autocorr_all
-from loguru import logger
-import seaborn as sns
-
-DT_COL_NAME = "datetime"
-
-
-class CombFeaAna(FeaAnalyser):
-    """
-    Combine the sub feature analysers and plot then in a single graph
-    """
-
-    def __init__(self, dataset: pd.DataFrame, *fea_ana_cls):
-        if len(fea_ana_cls) <= 1:
-            raise NotImplementedError(f"This type of input is not supported")
-        self._fea_ana_l = [fcls(dataset) for fcls in fea_ana_cls]
-        super().__init__(dataset=dataset)
-
-    def skip(self, col):
-        return np.all(list(map(lambda fa: fa.skip(col), self._fea_ana_l)))
-
-    def calc_stat_values(self):
-        """The statistics of features are finished in the underlying analysers"""
-
-    def plot_all(self, *args, **kwargs):
-
-        ax_gen = iter(sub_fig_generator(row_n=len(self._fea_ana_l), *args, **kwargs))
-
-        for col in self._dataset:
-            if not self.skip(col):
-                axes = next(ax_gen)
-                for fa, ax in zip(self._fea_ana_l, axes):
-                    if not fa.skip(col):
-                        fa.plot_single(col, ax)
-                    ax.set_xlabel("")
-                    ax.set_title("")
-                axes[0].set_title(col)
-
-
-class NumFeaAnalyser(FeaAnalyser):
-    def skip(self, col):
-        is_obj = np.issubdtype(self._dataset[col], np.dtype("O"))
-        if is_obj:
-            logger.info(f"{col} is not numeric and is skipped")
-        return is_obj
-
-
-class ValueCNT(FeaAnalyser):
-    def __init__(self, dataset: pd.DataFrame, ratio=False):
-        self.ratio = ratio
-        super().__init__(dataset)
-
-    def calc_stat_values(self):
-        self._val_cnt = {}
-        for col, item in self._dataset.items():
-            if not super().skip(col):
-                self._val_cnt[col] = item.groupby(DT_COL_NAME).apply(lambda s: len(s.unique()))
-        self._val_cnt = pd.DataFrame(self._val_cnt)
-        if self.ratio:
-            self._val_cnt = self._val_cnt.div(self._dataset.groupby(DT_COL_NAME).size(), axis=0)
-
-        # TODO: transfer this feature to other analysers
-        ymin, ymax = self._val_cnt.min().min(), self._val_cnt.max().max()
-        self.ylim = (ymin - 0.05 * (ymax - ymin), ymax + 0.05 * (ymax - ymin))
-
-    def plot_single(self, col, ax):
-        self._val_cnt[col].plot(ax=ax, title=col, ylim=self.ylim)
-        ax.set_xlabel("")
-
-
-class FeaDistAna(NumFeaAnalyser):
-    def plot_single(self, col, ax):
-        sns.histplot(self._dataset[col], ax=ax, kde=False, bins=100)
-        ax.set_xlabel("")
-        ax.set_title(col)
-
-
-class FeaInfAna(NumFeaAnalyser):
-    def calc_stat_values(self):
-        self._inf_cnt = {}
-        for col, item in self._dataset.items():
-            if not super().skip(col):
-                self._inf_cnt[col] = item.apply(np.isinf).astype(np.int).groupby(DT_COL_NAME).sum()
-        self._inf_cnt = pd.DataFrame(self._inf_cnt)
-
-    def skip(self, col):
-        return (col not in self._inf_cnt) or (self._inf_cnt[col].sum() == 0)
-
-    def plot_single(self, col, ax):
-        self._inf_cnt[col].plot(ax=ax, title=col)
-        ax.set_xlabel("")
-
-
-class FeaNanAna(FeaAnalyser):
-    def calc_stat_values(self):
-        self._nan_cnt = self._dataset.isna().groupby(DT_COL_NAME).sum()
-
-    def skip(self, col):
-        return (col not in self._nan_cnt) or (self._nan_cnt[col].sum() == 0)
-
-    def plot_single(self, col, ax):
-        self._nan_cnt[col].plot(ax=ax, title=col)
-        ax.set_xlabel("")
-
-
-class FeaNanAnaRatio(FeaAnalyser):
-    def calc_stat_values(self):
-        self._nan_cnt = self._dataset.isna().groupby(DT_COL_NAME).sum()
-        self._total_cnt = self._dataset.groupby(DT_COL_NAME).size()
-
-    def skip(self, col):
-        return (col not in self._nan_cnt) or (self._nan_cnt[col].sum() == 0)
-
-    def plot_single(self, col, ax):
-        (self._nan_cnt[col] / self._total_cnt).plot(ax=ax, title=col)
-        ax.set_xlabel("")
-
-
-class FeaACAna(FeaAnalyser):
-    """Analysis the auto-correlation of features"""
-
-    def calc_stat_values(self):
-        self._fea_corr = pred_autocorr_all(self._dataset.to_dict("series"))
-        df = pd.DataFrame(self._fea_corr)
-        ymin, ymax = df.min().min(), df.max().max()
-        self.ylim = (ymin - 0.05 * (ymax - ymin), ymax + 0.05 * (ymax - ymin))
-
-    def plot_single(self, col, ax):
-        self._fea_corr[col].plot(ax=ax, title=col, ylim=self.ylim)
-        ax.set_xlabel("")
-
-
-class FeaSkewTurt(NumFeaAnalyser):
-    def calc_stat_values(self):
-        self._skew = datetime_groupby_apply(self._dataset, "skew")
-        self._kurt = datetime_groupby_apply(self._dataset, pd.DataFrame.kurt)
-
-    def plot_single(self, col, ax):
-        self._skew[col].plot(ax=ax, label="skew")
-        ax.set_xlabel("")
-        ax.set_ylabel("skew")
-        ax.legend()
-
-        right_ax = ax.twinx()
-
-        self._kurt[col].plot(ax=right_ax, label="kurt", color="green")
-        right_ax.set_xlabel("")
-        right_ax.set_ylabel("kurt")
-
-        h1, l1 = ax.get_legend_handles_labels()
-        h2, l2 = right_ax.get_legend_handles_labels()
-
-        ax.legend().set_visible(False)
-        right_ax.legend(h1 + h2, l1 + l2)
-        ax.set_title(col)
-
-
-class FeaMeanStd(NumFeaAnalyser):
-    def calc_stat_values(self):
-        self._std = self._dataset.groupby(DT_COL_NAME).std()
-        self._mean = self._dataset.groupby(DT_COL_NAME).mean()
-
-    def plot_single(self, col, ax):
-        self._mean[col].plot(ax=ax, label="mean")
-        ax.set_xlabel("")
-        ax.set_ylabel("mean")
-        ax.legend()
-
-        right_ax = ax.twinx()
-
-        self._std[col].plot(ax=right_ax, label="std", color="green")
-        right_ax.set_xlabel("")
-        right_ax.set_ylabel("std")
-
-        h1, l1 = ax.get_legend_handles_labels()
-        h2, l2 = right_ax.get_legend_handles_labels()
-
-        ax.legend().set_visible(False)
-        right_ax.legend(h1 + h2, l1 + l2)
-        ax.set_title(col)
-
-
-class RawFeaAna(FeaAnalyser):
-    """
-    Motivation:
-    - display the values without further analysis
-    """
-
-    def calc_stat_values(self):
-        ymin, ymax = self._dataset.min().min(), self._dataset.max().max()
-        self.ylim = (ymin - 0.05 * (ymax - ymin), ymax + 0.05 * (ymax - ymin))
-
-    def plot_single(self, col, ax):
-        self._dataset[col].plot(ax=ax, title=col, ylim=self.ylim)
-        ax.set_xlabel("")
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+import pandas as pd
+import numpy as np
+from qlib.contrib.report.data.base import FeaAnalyser
+from qlib.contrib.report.utils import sub_fig_generator
+from qlib.utils.paral import datetime_groupby_apply
+from qlib.contrib.eva.alpha import pred_autocorr_all
+from loguru import logger
+import seaborn as sns
+
+DT_COL_NAME = "datetime"
+
+
+class CombFeaAna(FeaAnalyser):
+    """
+    Combine the sub feature analysers and plot then in a single graph
+    """
+
+    def __init__(self, dataset: pd.DataFrame, *fea_ana_cls):
+        if len(fea_ana_cls) <= 1:
+            raise NotImplementedError(f"This type of input is not supported")
+        self._fea_ana_l = [fcls(dataset) for fcls in fea_ana_cls]
+        super().__init__(dataset=dataset)
+
+    def skip(self, col):
+        return np.all(list(map(lambda fa: fa.skip(col), self._fea_ana_l)))
+
+    def calc_stat_values(self):
+        """The statistics of features are finished in the underlying analysers"""
+
+    def plot_all(self, *args, **kwargs):
+        ax_gen = iter(sub_fig_generator(row_n=len(self._fea_ana_l), *args, **kwargs))
+
+        for col in self._dataset:
+            if not self.skip(col):
+                axes = next(ax_gen)
+                for fa, ax in zip(self._fea_ana_l, axes):
+                    if not fa.skip(col):
+                        fa.plot_single(col, ax)
+                    ax.set_xlabel("")
+                    ax.set_title("")
+                axes[0].set_title(col)
+
+
+class NumFeaAnalyser(FeaAnalyser):
+    def skip(self, col):
+        is_obj = np.issubdtype(self._dataset[col], np.dtype("O"))
+        if is_obj:
+            logger.info(f"{col} is not numeric and is skipped")
+        return is_obj
+
+
+class ValueCNT(FeaAnalyser):
+    def __init__(self, dataset: pd.DataFrame, ratio=False):
+        self.ratio = ratio
+        super().__init__(dataset)
+
+    def calc_stat_values(self):
+        self._val_cnt = {}
+        for col, item in self._dataset.items():
+            if not super().skip(col):
+                self._val_cnt[col] = item.groupby(DT_COL_NAME).apply(lambda s: len(s.unique()))
+        self._val_cnt = pd.DataFrame(self._val_cnt)
+        if self.ratio:
+            self._val_cnt = self._val_cnt.div(self._dataset.groupby(DT_COL_NAME).size(), axis=0)
+
+        # TODO: transfer this feature to other analysers
+        ymin, ymax = self._val_cnt.min().min(), self._val_cnt.max().max()
+        self.ylim = (ymin - 0.05 * (ymax - ymin), ymax + 0.05 * (ymax - ymin))
+
+    def plot_single(self, col, ax):
+        self._val_cnt[col].plot(ax=ax, title=col, ylim=self.ylim)
+        ax.set_xlabel("")
+
+
+class FeaDistAna(NumFeaAnalyser):
+    def plot_single(self, col, ax):
+        sns.histplot(self._dataset[col], ax=ax, kde=False, bins=100)
+        ax.set_xlabel("")
+        ax.set_title(col)
+
+
+class FeaInfAna(NumFeaAnalyser):
+    def calc_stat_values(self):
+        self._inf_cnt = {}
+        for col, item in self._dataset.items():
+            if not super().skip(col):
+                self._inf_cnt[col] = item.apply(np.isinf).astype(np.int).groupby(DT_COL_NAME).sum()
+        self._inf_cnt = pd.DataFrame(self._inf_cnt)
+
+    def skip(self, col):
+        return (col not in self._inf_cnt) or (self._inf_cnt[col].sum() == 0)
+
+    def plot_single(self, col, ax):
+        self._inf_cnt[col].plot(ax=ax, title=col)
+        ax.set_xlabel("")
+
+
+class FeaNanAna(FeaAnalyser):
+    def calc_stat_values(self):
+        self._nan_cnt = self._dataset.isna().groupby(DT_COL_NAME).sum()
+
+    def skip(self, col):
+        return (col not in self._nan_cnt) or (self._nan_cnt[col].sum() == 0)
+
+    def plot_single(self, col, ax):
+        self._nan_cnt[col].plot(ax=ax, title=col)
+        ax.set_xlabel("")
+
+
+class FeaNanAnaRatio(FeaAnalyser):
+    def calc_stat_values(self):
+        self._nan_cnt = self._dataset.isna().groupby(DT_COL_NAME).sum()
+        self._total_cnt = self._dataset.groupby(DT_COL_NAME).size()
+
+    def skip(self, col):
+        return (col not in self._nan_cnt) or (self._nan_cnt[col].sum() == 0)
+
+    def plot_single(self, col, ax):
+        (self._nan_cnt[col] / self._total_cnt).plot(ax=ax, title=col)
+        ax.set_xlabel("")
+
+
+class FeaACAna(FeaAnalyser):
+    """Analysis the auto-correlation of features"""
+
+    def calc_stat_values(self):
+        self._fea_corr = pred_autocorr_all(self._dataset.to_dict("series"))
+        df = pd.DataFrame(self._fea_corr)
+        ymin, ymax = df.min().min(), df.max().max()
+        self.ylim = (ymin - 0.05 * (ymax - ymin), ymax + 0.05 * (ymax - ymin))
+
+    def plot_single(self, col, ax):
+        self._fea_corr[col].plot(ax=ax, title=col, ylim=self.ylim)
+        ax.set_xlabel("")
+
+
+class FeaSkewTurt(NumFeaAnalyser):
+    def calc_stat_values(self):
+        self._skew = datetime_groupby_apply(self._dataset, "skew")
+        self._kurt = datetime_groupby_apply(self._dataset, pd.DataFrame.kurt)
+
+    def plot_single(self, col, ax):
+        self._skew[col].plot(ax=ax, label="skew")
+        ax.set_xlabel("")
+        ax.set_ylabel("skew")
+        ax.legend()
+
+        right_ax = ax.twinx()
+
+        self._kurt[col].plot(ax=right_ax, label="kurt", color="green")
+        right_ax.set_xlabel("")
+        right_ax.set_ylabel("kurt")
+
+        h1, l1 = ax.get_legend_handles_labels()
+        h2, l2 = right_ax.get_legend_handles_labels()
+
+        ax.legend().set_visible(False)
+        right_ax.legend(h1 + h2, l1 + l2)
+        ax.set_title(col)
+
+
+class FeaMeanStd(NumFeaAnalyser):
+    def calc_stat_values(self):
+        self._std = self._dataset.groupby(DT_COL_NAME).std()
+        self._mean = self._dataset.groupby(DT_COL_NAME).mean()
+
+    def plot_single(self, col, ax):
+        self._mean[col].plot(ax=ax, label="mean")
+        ax.set_xlabel("")
+        ax.set_ylabel("mean")
+        ax.legend()
+
+        right_ax = ax.twinx()
+
+        self._std[col].plot(ax=right_ax, label="std", color="green")
+        right_ax.set_xlabel("")
+        right_ax.set_ylabel("std")
+
+        h1, l1 = ax.get_legend_handles_labels()
+        h2, l2 = right_ax.get_legend_handles_labels()
+
+        ax.legend().set_visible(False)
+        right_ax.legend(h1 + h2, l1 + l2)
+        ax.set_title(col)
+
+
+class RawFeaAna(FeaAnalyser):
+    """
+    Motivation:
+    - display the values without further analysis
+    """
+
+    def calc_stat_values(self):
+        ymin, ymax = self._dataset.min().min(), self._dataset.max().max()
+        self.ylim = (ymin - 0.05 * (ymax - ymin), ymax + 0.05 * (ymax - ymin))
+
+    def plot_single(self, col, ax):
+        self._dataset[col].plot(ax=ax, title=col, ylim=self.ylim)
+        ax.set_xlabel("")
```

## qlib/contrib/report/data/base.py

```diff
@@ -1,36 +1,35 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-"""
-This module is responsible for analysing data
-
-Assumptions
-- The analyse each feature individually
-
-"""
-import pandas as pd
-from qlib.log import TimeInspector
-from qlib.contrib.report.utils import sub_fig_generator
-
-
-class FeaAnalyser:
-    def __init__(self, dataset: pd.DataFrame):
-        self._dataset = dataset
-        with TimeInspector.logt("calc_stat_values"):
-            self.calc_stat_values()
-
-    def calc_stat_values(self):
-        pass
-
-    def plot_single(self, col, ax):
-        raise NotImplementedError(f"This type of input is not supported")
-
-    def skip(self, col):
-        return False
-
-    def plot_all(self, *args, **kwargs):
-
-        ax_gen = iter(sub_fig_generator(*args, **kwargs))
-        for col in self._dataset:
-            if not self.skip(col):
-                ax = next(ax_gen)
-                self.plot_single(col, ax)
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+"""
+This module is responsible for analysing data
+
+Assumptions
+- The analyse each feature individually
+
+"""
+import pandas as pd
+from qlib.log import TimeInspector
+from qlib.contrib.report.utils import sub_fig_generator
+
+
+class FeaAnalyser:
+    def __init__(self, dataset: pd.DataFrame):
+        self._dataset = dataset
+        with TimeInspector.logt("calc_stat_values"):
+            self.calc_stat_values()
+
+    def calc_stat_values(self):
+        pass
+
+    def plot_single(self, col, ax):
+        raise NotImplementedError(f"This type of input is not supported")
+
+    def skip(self, col):
+        return False
+
+    def plot_all(self, *args, **kwargs):
+        ax_gen = iter(sub_fig_generator(*args, **kwargs))
+        for col in self._dataset:
+            if not self.skip(col):
+                ax = next(ax_gen)
+                self.plot_single(col, ax)
```

## qlib/contrib/strategy/__init__.py

 * *Ordering differences only*

```diff
@@ -1,28 +1,28 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-
-from .signal_strategy import (
-    TopkDropoutStrategy,
-    WeightStrategyBase,
-    EnhancedIndexingStrategy,
-)
-
-from .rule_strategy import (
-    TWAPStrategy,
-    SBBStrategyBase,
-    SBBStrategyEMA,
-)
-
-from .cost_control import SoftTopkStrategy
-
-
-__all__ = [
-    "TopkDropoutStrategy",
-    "WeightStrategyBase",
-    "EnhancedIndexingStrategy",
-    "TWAPStrategy",
-    "SBBStrategyBase",
-    "SBBStrategyEMA",
-    "SoftTopkStrategy",
-]
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+
+from .signal_strategy import (
+    TopkDropoutStrategy,
+    WeightStrategyBase,
+    EnhancedIndexingStrategy,
+)
+
+from .rule_strategy import (
+    TWAPStrategy,
+    SBBStrategyBase,
+    SBBStrategyEMA,
+)
+
+from .cost_control import SoftTopkStrategy
+
+
+__all__ = [
+    "TopkDropoutStrategy",
+    "WeightStrategyBase",
+    "EnhancedIndexingStrategy",
+    "TWAPStrategy",
+    "SBBStrategyBase",
+    "SBBStrategyEMA",
+    "SoftTopkStrategy",
+]
```

## qlib/contrib/strategy/cost_control.py

 * *Ordering differences only*

```diff
@@ -1,101 +1,101 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-"""
-This strategy is not well maintained
-"""
-
-
-from .order_generator import OrderGenWInteract
-from .signal_strategy import WeightStrategyBase
-import copy
-
-
-class SoftTopkStrategy(WeightStrategyBase):
-    def __init__(
-        self,
-        model,
-        dataset,
-        topk,
-        order_generator_cls_or_obj=OrderGenWInteract,
-        max_sold_weight=1.0,
-        risk_degree=0.95,
-        buy_method="first_fill",
-        trade_exchange=None,
-        level_infra=None,
-        common_infra=None,
-        **kwargs,
-    ):
-        """
-        Parameters
-        ----------
-        topk : int
-            top-N stocks to buy
-        risk_degree : float
-            position percentage of total value buy_method:
-
-                rank_fill: assign the weight stocks that rank high first(1/topk max)
-                average_fill: assign the weight to the stocks rank high averagely.
-        """
-        super(SoftTopkStrategy, self).__init__(
-            model, dataset, order_generator_cls_or_obj, trade_exchange, level_infra, common_infra, **kwargs
-        )
-        self.topk = topk
-        self.max_sold_weight = max_sold_weight
-        self.risk_degree = risk_degree
-        self.buy_method = buy_method
-
-    def get_risk_degree(self, trade_step=None):
-        """get_risk_degree
-        Return the proportion of your total value you will used in investment.
-        Dynamically risk_degree will result in Market timing
-        """
-        # It will use 95% amount of your total value by default
-        return self.risk_degree
-
-    def generate_target_weight_position(self, score, current, trade_start_time, trade_end_time):
-        """
-        Parameters
-        ----------
-        score:
-            pred score for this trade date, pd.Series, index is stock_id, contain 'score' column
-        current:
-            current position, use Position() class
-        trade_date:
-            trade date
-
-            generate target position from score for this date and the current position
-
-            The cache is not considered in the position
-        """
-        # TODO:
-        # If the current stock list is more than topk(eg. The weights are modified
-        # by risk control), the weight will not be handled correctly.
-        buy_signal_stocks = set(score.sort_values(ascending=False).iloc[: self.topk].index)
-        cur_stock_weight = current.get_stock_weight_dict(only_stock=True)
-
-        if len(cur_stock_weight) == 0:
-            final_stock_weight = {code: 1 / self.topk for code in buy_signal_stocks}
-        else:
-            final_stock_weight = copy.deepcopy(cur_stock_weight)
-            sold_stock_weight = 0.0
-            for stock_id in final_stock_weight:
-                if stock_id not in buy_signal_stocks:
-                    sw = min(self.max_sold_weight, final_stock_weight[stock_id])
-                    sold_stock_weight += sw
-                    final_stock_weight[stock_id] -= sw
-            if self.buy_method == "first_fill":
-                for stock_id in buy_signal_stocks:
-                    add_weight = min(
-                        max(1 / self.topk - final_stock_weight.get(stock_id, 0), 0.0),
-                        sold_stock_weight,
-                    )
-                    final_stock_weight[stock_id] = final_stock_weight.get(stock_id, 0.0) + add_weight
-                    sold_stock_weight -= add_weight
-            elif self.buy_method == "average_fill":
-                for stock_id in buy_signal_stocks:
-                    final_stock_weight[stock_id] = final_stock_weight.get(stock_id, 0.0) + sold_stock_weight / len(
-                        buy_signal_stocks
-                    )
-            else:
-                raise ValueError("Buy method not found")
-        return final_stock_weight
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+"""
+This strategy is not well maintained
+"""
+
+
+from .order_generator import OrderGenWInteract
+from .signal_strategy import WeightStrategyBase
+import copy
+
+
+class SoftTopkStrategy(WeightStrategyBase):
+    def __init__(
+        self,
+        model,
+        dataset,
+        topk,
+        order_generator_cls_or_obj=OrderGenWInteract,
+        max_sold_weight=1.0,
+        risk_degree=0.95,
+        buy_method="first_fill",
+        trade_exchange=None,
+        level_infra=None,
+        common_infra=None,
+        **kwargs,
+    ):
+        """
+        Parameters
+        ----------
+        topk : int
+            top-N stocks to buy
+        risk_degree : float
+            position percentage of total value buy_method:
+
+                rank_fill: assign the weight stocks that rank high first(1/topk max)
+                average_fill: assign the weight to the stocks rank high averagely.
+        """
+        super(SoftTopkStrategy, self).__init__(
+            model, dataset, order_generator_cls_or_obj, trade_exchange, level_infra, common_infra, **kwargs
+        )
+        self.topk = topk
+        self.max_sold_weight = max_sold_weight
+        self.risk_degree = risk_degree
+        self.buy_method = buy_method
+
+    def get_risk_degree(self, trade_step=None):
+        """get_risk_degree
+        Return the proportion of your total value you will used in investment.
+        Dynamically risk_degree will result in Market timing
+        """
+        # It will use 95% amount of your total value by default
+        return self.risk_degree
+
+    def generate_target_weight_position(self, score, current, trade_start_time, trade_end_time):
+        """
+        Parameters
+        ----------
+        score:
+            pred score for this trade date, pd.Series, index is stock_id, contain 'score' column
+        current:
+            current position, use Position() class
+        trade_date:
+            trade date
+
+            generate target position from score for this date and the current position
+
+            The cache is not considered in the position
+        """
+        # TODO:
+        # If the current stock list is more than topk(eg. The weights are modified
+        # by risk control), the weight will not be handled correctly.
+        buy_signal_stocks = set(score.sort_values(ascending=False).iloc[: self.topk].index)
+        cur_stock_weight = current.get_stock_weight_dict(only_stock=True)
+
+        if len(cur_stock_weight) == 0:
+            final_stock_weight = {code: 1 / self.topk for code in buy_signal_stocks}
+        else:
+            final_stock_weight = copy.deepcopy(cur_stock_weight)
+            sold_stock_weight = 0.0
+            for stock_id in final_stock_weight:
+                if stock_id not in buy_signal_stocks:
+                    sw = min(self.max_sold_weight, final_stock_weight[stock_id])
+                    sold_stock_weight += sw
+                    final_stock_weight[stock_id] -= sw
+            if self.buy_method == "first_fill":
+                for stock_id in buy_signal_stocks:
+                    add_weight = min(
+                        max(1 / self.topk - final_stock_weight.get(stock_id, 0), 0.0),
+                        sold_stock_weight,
+                    )
+                    final_stock_weight[stock_id] = final_stock_weight.get(stock_id, 0.0) + add_weight
+                    sold_stock_weight -= add_weight
+            elif self.buy_method == "average_fill":
+                for stock_id in buy_signal_stocks:
+                    final_stock_weight[stock_id] = final_stock_weight.get(stock_id, 0.0) + sold_stock_weight / len(
+                        buy_signal_stocks
+                    )
+            else:
+                raise ValueError("Buy method not found")
+        return final_stock_weight
```

## qlib/contrib/strategy/order_generator.py

 * *Ordering differences only*

```diff
@@ -1,218 +1,218 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-"""
-This order generator is for strategies based on WeightStrategyBase
-"""
-from ...backtest.position import Position
-from ...backtest.exchange import Exchange
-
-import pandas as pd
-import copy
-
-
-class OrderGenerator:
-    def generate_order_list_from_target_weight_position(
-        self,
-        current: Position,
-        trade_exchange: Exchange,
-        target_weight_position: dict,
-        risk_degree: float,
-        pred_start_time: pd.Timestamp,
-        pred_end_time: pd.Timestamp,
-        trade_start_time: pd.Timestamp,
-        trade_end_time: pd.Timestamp,
-    ) -> list:
-        """generate_order_list_from_target_weight_position
-
-        :param current: The current position
-        :type current: Position
-        :param trade_exchange:
-        :type trade_exchange: Exchange
-        :param target_weight_position: {stock_id : weight}
-        :type target_weight_position: dict
-        :param risk_degree:
-        :type risk_degree: float
-        :param pred_start_time:
-        :type pred_start_time: pd.Timestamp
-        :param pred_end_time:
-        :type pred_end_time: pd.Timestamp
-        :param trade_start_time:
-        :type trade_start_time: pd.Timestamp
-        :param trade_end_time:
-        :type trade_end_time: pd.Timestamp
-
-        :rtype: list
-        """
-        raise NotImplementedError()
-
-
-class OrderGenWInteract(OrderGenerator):
-    """Order Generator With Interact"""
-
-    def generate_order_list_from_target_weight_position(
-        self,
-        current: Position,
-        trade_exchange: Exchange,
-        target_weight_position: dict,
-        risk_degree: float,
-        pred_start_time: pd.Timestamp,
-        pred_end_time: pd.Timestamp,
-        trade_start_time: pd.Timestamp,
-        trade_end_time: pd.Timestamp,
-    ) -> list:
-        """generate_order_list_from_target_weight_position
-
-        No adjustment for for the nontradable share.
-        All the tadable value is assigned to the tadable stock according to the weight.
-        if interact == True, will use the price at trade date to generate order list
-        else, will only use the price before the trade date to generate order list
-
-        :param current:
-        :type current: Position
-        :param trade_exchange:
-        :type trade_exchange: Exchange
-        :param target_weight_position:
-        :type target_weight_position: dict
-        :param risk_degree:
-        :type risk_degree: float
-        :param pred_start_time:
-        :type pred_start_time: pd.Timestamp
-        :param pred_end_time:
-        :type pred_end_time: pd.Timestamp
-        :param trade_start_time:
-        :type trade_start_time: pd.Timestamp
-        :param trade_end_time:
-        :type trade_end_time: pd.Timestamp
-
-        :rtype: list
-        """
-        if target_weight_position is None:
-            return []
-
-        # calculate current_tradable_value
-        current_amount_dict = current.get_stock_amount_dict()
-
-        current_total_value = trade_exchange.calculate_amount_position_value(
-            amount_dict=current_amount_dict,
-            start_time=trade_start_time,
-            end_time=trade_end_time,
-            only_tradable=False,
-        )
-        current_tradable_value = trade_exchange.calculate_amount_position_value(
-            amount_dict=current_amount_dict,
-            start_time=trade_start_time,
-            end_time=trade_end_time,
-            only_tradable=True,
-        )
-        # add cash
-        current_tradable_value += current.get_cash()
-
-        reserved_cash = (1.0 - risk_degree) * (current_total_value + current.get_cash())
-        current_tradable_value -= reserved_cash
-
-        if current_tradable_value < 0:
-            # if you sell all the tradable stock can not meet the reserved
-            # value. Then just sell all the stocks
-            target_amount_dict = copy.deepcopy(current_amount_dict.copy())
-            for stock_id in list(target_amount_dict.keys()):
-                if trade_exchange.is_stock_tradable(stock_id, start_time=trade_start_time, end_time=trade_end_time):
-                    del target_amount_dict[stock_id]
-        else:
-            # consider cost rate
-            current_tradable_value /= 1 + max(trade_exchange.close_cost, trade_exchange.open_cost)
-
-            # strategy 1 : generate amount_position by weight_position
-            # Use API in Exchange()
-            target_amount_dict = trade_exchange.generate_amount_position_from_weight_position(
-                weight_position=target_weight_position,
-                cash=current_tradable_value,
-                start_time=trade_start_time,
-                end_time=trade_end_time,
-            )
-        order_list = trade_exchange.generate_order_for_target_amount_position(
-            target_position=target_amount_dict,
-            current_position=current_amount_dict,
-            start_time=trade_start_time,
-            end_time=trade_end_time,
-        )
-        return order_list
-
-
-class OrderGenWOInteract(OrderGenerator):
-    """Order Generator Without Interact"""
-
-    def generate_order_list_from_target_weight_position(
-        self,
-        current: Position,
-        trade_exchange: Exchange,
-        target_weight_position: dict,
-        risk_degree: float,
-        pred_start_time: pd.Timestamp,
-        pred_end_time: pd.Timestamp,
-        trade_start_time: pd.Timestamp,
-        trade_end_time: pd.Timestamp,
-    ) -> list:
-        """generate_order_list_from_target_weight_position
-
-        generate order list directly not using the information (e.g. whether can be traded, the accurate trade price)
-         at trade date.
-        In target weight position, generating order list need to know the price of objective stock in trade date,
-        but we cannot get that
-        value when do not interact with exchange, so we check the %close price at pred_date or price recorded
-        in current position.
-
-        :param current:
-        :type current: Position
-        :param trade_exchange:
-        :type trade_exchange: Exchange
-        :param target_weight_position:
-        :type target_weight_position: dict
-        :param risk_degree:
-        :type risk_degree: float
-        :param pred_start_time:
-        :type pred_start_time: pd.Timestamp
-        :param pred_end_time:
-        :type pred_end_time: pd.Timestamp
-        :param trade_start_time:
-        :type trade_start_time: pd.Timestamp
-        :param trade_end_time:
-        :type trade_end_time: pd.Timestamp
-
-        :rtype: list of generated orders
-        """
-        if target_weight_position is None:
-            return []
-
-        risk_total_value = risk_degree * current.calculate_value()
-
-        current_stock = current.get_stock_list()
-        amount_dict = {}
-        for stock_id in target_weight_position:
-            # Current rule will ignore the stock that not hold and cannot be traded at predict date
-            if trade_exchange.is_stock_tradable(
-                stock_id=stock_id, start_time=trade_start_time, end_time=trade_end_time
-            ) and trade_exchange.is_stock_tradable(
-                stock_id=stock_id, start_time=pred_start_time, end_time=pred_end_time
-            ):
-                amount_dict[stock_id] = (
-                    risk_total_value
-                    * target_weight_position[stock_id]
-                    / trade_exchange.get_close(stock_id, start_time=pred_start_time, end_time=pred_end_time)
-                )
-                # TODO: Qlib use None to represent trading suspension.
-                #  So last close price can't be the estimated trading price.
-                # Maybe a close price with forward fill will be a better solution.
-            elif stock_id in current_stock:
-                amount_dict[stock_id] = (
-                    risk_total_value * target_weight_position[stock_id] / current.get_stock_price(stock_id)
-                )
-            else:
-                continue
-        order_list = trade_exchange.generate_order_for_target_amount_position(
-            target_position=amount_dict,
-            current_position=current.get_stock_amount_dict(),
-            start_time=trade_start_time,
-            end_time=trade_end_time,
-        )
-        return order_list
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+"""
+This order generator is for strategies based on WeightStrategyBase
+"""
+from ...backtest.position import Position
+from ...backtest.exchange import Exchange
+
+import pandas as pd
+import copy
+
+
+class OrderGenerator:
+    def generate_order_list_from_target_weight_position(
+        self,
+        current: Position,
+        trade_exchange: Exchange,
+        target_weight_position: dict,
+        risk_degree: float,
+        pred_start_time: pd.Timestamp,
+        pred_end_time: pd.Timestamp,
+        trade_start_time: pd.Timestamp,
+        trade_end_time: pd.Timestamp,
+    ) -> list:
+        """generate_order_list_from_target_weight_position
+
+        :param current: The current position
+        :type current: Position
+        :param trade_exchange:
+        :type trade_exchange: Exchange
+        :param target_weight_position: {stock_id : weight}
+        :type target_weight_position: dict
+        :param risk_degree:
+        :type risk_degree: float
+        :param pred_start_time:
+        :type pred_start_time: pd.Timestamp
+        :param pred_end_time:
+        :type pred_end_time: pd.Timestamp
+        :param trade_start_time:
+        :type trade_start_time: pd.Timestamp
+        :param trade_end_time:
+        :type trade_end_time: pd.Timestamp
+
+        :rtype: list
+        """
+        raise NotImplementedError()
+
+
+class OrderGenWInteract(OrderGenerator):
+    """Order Generator With Interact"""
+
+    def generate_order_list_from_target_weight_position(
+        self,
+        current: Position,
+        trade_exchange: Exchange,
+        target_weight_position: dict,
+        risk_degree: float,
+        pred_start_time: pd.Timestamp,
+        pred_end_time: pd.Timestamp,
+        trade_start_time: pd.Timestamp,
+        trade_end_time: pd.Timestamp,
+    ) -> list:
+        """generate_order_list_from_target_weight_position
+
+        No adjustment for for the nontradable share.
+        All the tadable value is assigned to the tadable stock according to the weight.
+        if interact == True, will use the price at trade date to generate order list
+        else, will only use the price before the trade date to generate order list
+
+        :param current:
+        :type current: Position
+        :param trade_exchange:
+        :type trade_exchange: Exchange
+        :param target_weight_position:
+        :type target_weight_position: dict
+        :param risk_degree:
+        :type risk_degree: float
+        :param pred_start_time:
+        :type pred_start_time: pd.Timestamp
+        :param pred_end_time:
+        :type pred_end_time: pd.Timestamp
+        :param trade_start_time:
+        :type trade_start_time: pd.Timestamp
+        :param trade_end_time:
+        :type trade_end_time: pd.Timestamp
+
+        :rtype: list
+        """
+        if target_weight_position is None:
+            return []
+
+        # calculate current_tradable_value
+        current_amount_dict = current.get_stock_amount_dict()
+
+        current_total_value = trade_exchange.calculate_amount_position_value(
+            amount_dict=current_amount_dict,
+            start_time=trade_start_time,
+            end_time=trade_end_time,
+            only_tradable=False,
+        )
+        current_tradable_value = trade_exchange.calculate_amount_position_value(
+            amount_dict=current_amount_dict,
+            start_time=trade_start_time,
+            end_time=trade_end_time,
+            only_tradable=True,
+        )
+        # add cash
+        current_tradable_value += current.get_cash()
+
+        reserved_cash = (1.0 - risk_degree) * (current_total_value + current.get_cash())
+        current_tradable_value -= reserved_cash
+
+        if current_tradable_value < 0:
+            # if you sell all the tradable stock can not meet the reserved
+            # value. Then just sell all the stocks
+            target_amount_dict = copy.deepcopy(current_amount_dict.copy())
+            for stock_id in list(target_amount_dict.keys()):
+                if trade_exchange.is_stock_tradable(stock_id, start_time=trade_start_time, end_time=trade_end_time):
+                    del target_amount_dict[stock_id]
+        else:
+            # consider cost rate
+            current_tradable_value /= 1 + max(trade_exchange.close_cost, trade_exchange.open_cost)
+
+            # strategy 1 : generate amount_position by weight_position
+            # Use API in Exchange()
+            target_amount_dict = trade_exchange.generate_amount_position_from_weight_position(
+                weight_position=target_weight_position,
+                cash=current_tradable_value,
+                start_time=trade_start_time,
+                end_time=trade_end_time,
+            )
+        order_list = trade_exchange.generate_order_for_target_amount_position(
+            target_position=target_amount_dict,
+            current_position=current_amount_dict,
+            start_time=trade_start_time,
+            end_time=trade_end_time,
+        )
+        return order_list
+
+
+class OrderGenWOInteract(OrderGenerator):
+    """Order Generator Without Interact"""
+
+    def generate_order_list_from_target_weight_position(
+        self,
+        current: Position,
+        trade_exchange: Exchange,
+        target_weight_position: dict,
+        risk_degree: float,
+        pred_start_time: pd.Timestamp,
+        pred_end_time: pd.Timestamp,
+        trade_start_time: pd.Timestamp,
+        trade_end_time: pd.Timestamp,
+    ) -> list:
+        """generate_order_list_from_target_weight_position
+
+        generate order list directly not using the information (e.g. whether can be traded, the accurate trade price)
+         at trade date.
+        In target weight position, generating order list need to know the price of objective stock in trade date,
+        but we cannot get that
+        value when do not interact with exchange, so we check the %close price at pred_date or price recorded
+        in current position.
+
+        :param current:
+        :type current: Position
+        :param trade_exchange:
+        :type trade_exchange: Exchange
+        :param target_weight_position:
+        :type target_weight_position: dict
+        :param risk_degree:
+        :type risk_degree: float
+        :param pred_start_time:
+        :type pred_start_time: pd.Timestamp
+        :param pred_end_time:
+        :type pred_end_time: pd.Timestamp
+        :param trade_start_time:
+        :type trade_start_time: pd.Timestamp
+        :param trade_end_time:
+        :type trade_end_time: pd.Timestamp
+
+        :rtype: list of generated orders
+        """
+        if target_weight_position is None:
+            return []
+
+        risk_total_value = risk_degree * current.calculate_value()
+
+        current_stock = current.get_stock_list()
+        amount_dict = {}
+        for stock_id in target_weight_position:
+            # Current rule will ignore the stock that not hold and cannot be traded at predict date
+            if trade_exchange.is_stock_tradable(
+                stock_id=stock_id, start_time=trade_start_time, end_time=trade_end_time
+            ) and trade_exchange.is_stock_tradable(
+                stock_id=stock_id, start_time=pred_start_time, end_time=pred_end_time
+            ):
+                amount_dict[stock_id] = (
+                    risk_total_value
+                    * target_weight_position[stock_id]
+                    / trade_exchange.get_close(stock_id, start_time=pred_start_time, end_time=pred_end_time)
+                )
+                # TODO: Qlib use None to represent trading suspension.
+                #  So last close price can't be the estimated trading price.
+                # Maybe a close price with forward fill will be a better solution.
+            elif stock_id in current_stock:
+                amount_dict[stock_id] = (
+                    risk_total_value * target_weight_position[stock_id] / current.get_stock_price(stock_id)
+                )
+            else:
+                continue
+        order_list = trade_exchange.generate_order_for_target_amount_position(
+            target_position=amount_dict,
+            current_position=current.get_stock_amount_dict(),
+            start_time=trade_start_time,
+            end_time=trade_end_time,
+        )
+        return order_list
```

## qlib/contrib/strategy/rule_strategy.py

```diff
@@ -1,671 +1,670 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-from pathlib import Path
-import warnings
-import numpy as np
-import pandas as pd
-from typing import IO, List, Tuple, Union
-from qlib.data.dataset.utils import convert_index_format
-
-from qlib.utils import lazy_sort_index
-
-from ...utils.resam import resam_ts_data, ts_data_last
-from ...data.data import D
-from ...strategy.base import BaseStrategy
-from ...backtest.decision import BaseTradeDecision, Order, TradeDecisionWO, TradeRange
-from ...backtest.exchange import Exchange, OrderHelper
-from ...backtest.utils import CommonInfrastructure, LevelInfrastructure
-from qlib.utils.file import get_io_object
-from qlib.backtest.utils import get_start_end_idx
-
-
-class TWAPStrategy(BaseStrategy):
-    """TWAP Strategy for trading
-
-    NOTE:
-        - This TWAP strategy will celling round when trading. This will make the TWAP trading strategy produce the order
-          earlier when the total trade unit of amount is less than the trading step
-    """
-
-    def reset(self, outer_trade_decision: BaseTradeDecision = None, **kwargs):
-        """
-        Parameters
-        ----------
-        outer_trade_decision : BaseTradeDecision, optional
-        """
-
-        super(TWAPStrategy, self).reset(outer_trade_decision=outer_trade_decision, **kwargs)
-        if outer_trade_decision is not None:
-            self.trade_amount_remain = {}
-            for order in outer_trade_decision.get_decision():
-                self.trade_amount_remain[order.stock_id] = order.amount
-
-    def generate_trade_decision(self, execute_result=None):
-        # NOTE:  corner cases!!!
-        # - If using upperbound round, please don't sell the amount which should in next step
-        #   - the coordinate of the amount between steps is hard to be dealt between steps in the same level. It
-        #     is easier to be dealt in upper steps
-
-        # strategy is not available. Give an empty decision
-        if len(self.outer_trade_decision.get_decision()) == 0:
-            return TradeDecisionWO(order_list=[], strategy=self)
-
-        # get the number of trading step finished, trade_step can be [0, 1, 2, ..., trade_len - 1]
-        trade_step = self.trade_calendar.get_trade_step()
-        # get the total count of trading step
-        start_idx, end_idx = get_start_end_idx(self.trade_calendar, self.outer_trade_decision)
-        trade_len = end_idx - start_idx + 1
-
-        if trade_step < start_idx or trade_step > end_idx:
-            # It is not time to start trading or trading has ended.
-            return TradeDecisionWO(order_list=[], strategy=self)
-
-        rel_trade_step = trade_step - start_idx  # trade_step relative to start_idx (number of steps has already passed)
-
-        # update the order amount
-        if execute_result is not None:
-            for order, _, _, _ in execute_result:
-                self.trade_amount_remain[order.stock_id] -= order.deal_amount
-
-        trade_start_time, trade_end_time = self.trade_calendar.get_step_time(trade_step)
-        order_list = []
-        for order in self.outer_trade_decision.get_decision():
-            # Don't peek the future information, so we use check_stock_suspended instead of is_stock_tradable
-            # necessity of this
-            # - if stock is suspended, the quote values of stocks is NaN. The following code will raise error when
-            # encountering NaN factor
-            if self.trade_exchange.check_stock_suspended(
-                stock_id=order.stock_id, start_time=trade_start_time, end_time=trade_end_time
-            ):
-                continue
-
-            # the expected trade amount after current step
-            amount_expect = order.amount / trade_len * (rel_trade_step + 1)
-
-            # remain amount
-            amount_remain = self.trade_amount_remain[order.stock_id]
-
-            # the amount has already been finished now.
-            amount_finished = order.amount - amount_remain
-
-            # the expected amount of current step
-            amount_delta = amount_expect - amount_finished
-
-            _amount_trade_unit = self.trade_exchange.get_amount_of_trade_unit(
-                stock_id=order.stock_id, start_time=order.start_time, end_time=order.end_time
-            )
-
-            # round the amount_delta by trade_unit and clip by remain
-            # NOTE: this could be more than expected.
-            if _amount_trade_unit is None:
-                # divide the order into equal parts, and trade one part
-                amount_delta_target = amount_delta
-            else:
-                amount_delta_target = min(
-                    np.round(amount_delta / _amount_trade_unit) * _amount_trade_unit, amount_remain
-                )
-
-            # handle last step to make sure all positions have gone
-            # necessity: the last step can't be rounded to the a unit (e.g. reminder < 0.5 unit)
-            if rel_trade_step == trade_len - 1:
-                amount_delta_target = amount_remain
-
-            if amount_delta_target > 1e-5:
-                _order = Order(
-                    stock_id=order.stock_id,
-                    amount=amount_delta_target,
-                    start_time=trade_start_time,
-                    end_time=trade_end_time,
-                    direction=order.direction,  # 1 for buy
-                )
-                order_list.append(_order)
-        return TradeDecisionWO(order_list=order_list, strategy=self)
-
-
-class SBBStrategyBase(BaseStrategy):
-    """
-    (S)elect the (B)etter one among every two adjacent trading (B)ars to sell or buy.
-    """
-
-    TREND_MID = 0
-    TREND_SHORT = 1
-    TREND_LONG = 2
-
-    # TODO:
-    # 1. Supporting leverage the get_range_limit result from the decision
-    # 2. Supporting alter_outer_trade_decision
-    # 3. Supporting checking the availability of trade decision
-
-    def reset(self, outer_trade_decision: BaseTradeDecision = None, **kwargs):
-        """
-        Parameters
-        ----------
-        outer_trade_decision : BaseTradeDecision, optional
-        """
-        super(SBBStrategyBase, self).reset(outer_trade_decision=outer_trade_decision, **kwargs)
-        if outer_trade_decision is not None:
-            self.trade_trend = {}
-            self.trade_amount = {}
-            # init the trade amount of order and  predicted trade trend
-            for order in outer_trade_decision.get_decision():
-                self.trade_trend[order.stock_id] = self.TREND_MID
-                self.trade_amount[order.stock_id] = order.amount
-
-    def _pred_price_trend(self, stock_id, pred_start_time=None, pred_end_time=None):
-        raise NotImplementedError("pred_price_trend method is not implemented!")
-
-    def generate_trade_decision(self, execute_result=None):
-        # get the number of trading step finished, trade_step can be [0, 1, 2, ..., trade_len - 1]
-        trade_step = self.trade_calendar.get_trade_step()
-        # get the total count of trading step
-        trade_len = self.trade_calendar.get_trade_len()
-
-        # update the order amount
-        if execute_result is not None:
-            for order, _, _, _ in execute_result:
-                self.trade_amount[order.stock_id] -= order.deal_amount
-
-        trade_start_time, trade_end_time = self.trade_calendar.get_step_time(trade_step)
-        pred_start_time, pred_end_time = self.trade_calendar.get_step_time(trade_step, shift=1)
-        order_list = []
-        # for each order in in self.outer_trade_decision
-        for order in self.outer_trade_decision.get_decision():
-            # get the price trend
-            if trade_step % 2 == 0:
-                # in the first of two adjacent bars, predict the price trend
-                _pred_trend = self._pred_price_trend(order.stock_id, pred_start_time, pred_end_time)
-            else:
-                # in the second of two adjacent bars, use the trend predicted in the first one
-                _pred_trend = self.trade_trend[order.stock_id]
-            # if not tradable, continue
-            if not self.trade_exchange.is_stock_tradable(
-                stock_id=order.stock_id, start_time=trade_start_time, end_time=trade_end_time
-            ):
-                if trade_step % 2 == 0:
-                    self.trade_trend[order.stock_id] = _pred_trend
-                continue
-            # get amount of one trade unit
-            _amount_trade_unit = self.trade_exchange.get_amount_of_trade_unit(
-                stock_id=order.stock_id, start_time=order.start_time, end_time=order.end_time
-            )
-            if _pred_trend == self.TREND_MID:
-                _order_amount = None
-                # considering trade unit
-                if _amount_trade_unit is None:
-                    # divide the order into equal parts, and trade one part
-                    _order_amount = self.trade_amount[order.stock_id] / (trade_len - trade_step)
-                # without considering trade unit
-                else:
-                    # divide the order into equal parts, and trade one part
-                    # calculate the total count of trade units to trade
-                    trade_unit_cnt = int(self.trade_amount[order.stock_id] // _amount_trade_unit)
-                    # calculate the amount of one part, ceil the amount
-                    # floor((trade_unit_cnt + trade_len - trade_step - 1) / (trade_len - trade_step)) == ceil(trade_unit_cnt / (trade_len - trade_step))
-                    _order_amount = (
-                        (trade_unit_cnt + trade_len - trade_step - 1) // (trade_len - trade_step) * _amount_trade_unit
-                    )
-                if order.direction == order.SELL:
-                    # sell all amount at last
-                    if self.trade_amount[order.stock_id] > 1e-5 and (
-                        _order_amount < 1e-5 or trade_step == trade_len - 1
-                    ):
-                        _order_amount = self.trade_amount[order.stock_id]
-
-                _order_amount = min(_order_amount, self.trade_amount[order.stock_id])
-
-                if _order_amount > 1e-5:
-                    _order = Order(
-                        stock_id=order.stock_id,
-                        amount=_order_amount,
-                        start_time=trade_start_time,
-                        end_time=trade_end_time,
-                        direction=order.direction,
-                    )
-                    order_list.append(_order)
-
-            else:
-                _order_amount = None
-                # considering trade unit
-                if _amount_trade_unit is None:
-                    # N trade day left, divide the order into N + 1 parts, and trade 2 parts
-                    _order_amount = 2 * self.trade_amount[order.stock_id] / (trade_len - trade_step + 1)
-                # without considering trade unit
-                else:
-                    # cal how many trade unit
-                    trade_unit_cnt = int(self.trade_amount[order.stock_id] // _amount_trade_unit)
-                    # N trade day left, divide the order into N + 1 parts, and trade 2 parts
-                    _order_amount = (
-                        (trade_unit_cnt + trade_len - trade_step)
-                        // (trade_len - trade_step + 1)
-                        * 2
-                        * _amount_trade_unit
-                    )
-                if order.direction == order.SELL:
-                    # sell all amount at last
-                    if self.trade_amount[order.stock_id] > 1e-5 and (
-                        _order_amount < 1e-5 or trade_step == trade_len - 1
-                    ):
-                        _order_amount = self.trade_amount[order.stock_id]
-
-                _order_amount = min(_order_amount, self.trade_amount[order.stock_id])
-
-                if _order_amount > 1e-5:
-                    if trade_step % 2 == 0:
-                        # in the first one of two adjacent bars
-                        # if look short on the price, sell the stock more
-                        # if look long on the price, buy the stock more
-                        if (
-                            _pred_trend == self.TREND_SHORT
-                            and order.direction == order.SELL
-                            or _pred_trend == self.TREND_LONG
-                            and order.direction == order.BUY
-                        ):
-                            _order = Order(
-                                stock_id=order.stock_id,
-                                amount=_order_amount,
-                                start_time=trade_start_time,
-                                end_time=trade_end_time,
-                                direction=order.direction,  # 1 for buy
-                            )
-                            order_list.append(_order)
-                    else:
-                        # in the second one of two adjacent bars
-                        # if look short on the price, buy the stock more
-                        # if look long on the price, sell the stock more
-                        if (
-                            _pred_trend == self.TREND_SHORT
-                            and order.direction == order.BUY
-                            or _pred_trend == self.TREND_LONG
-                            and order.direction == order.SELL
-                        ):
-                            _order = Order(
-                                stock_id=order.stock_id,
-                                amount=_order_amount,
-                                start_time=trade_start_time,
-                                end_time=trade_end_time,
-                                direction=order.direction,  # 1 for buy
-                            )
-                            order_list.append(_order)
-
-            if trade_step % 2 == 0:
-                # in the first one of two adjacent bars, store the trend for the second one to use
-                self.trade_trend[order.stock_id] = _pred_trend
-
-        return TradeDecisionWO(order_list, self)
-
-
-class SBBStrategyEMA(SBBStrategyBase):
-    """
-    (S)elect the (B)etter one among every two adjacent trading (B)ars to sell or buy with (EMA) signal.
-    """
-
-    # TODO:
-    # 1. Supporting leverage the get_range_limit result from the decision
-    # 2. Supporting alter_outer_trade_decision
-    # 3. Supporting checking the availability of trade decision
-
-    def __init__(
-        self,
-        outer_trade_decision: BaseTradeDecision = None,
-        instruments: Union[List, str] = "csi300",
-        freq: str = "day",
-        trade_exchange: Exchange = None,
-        level_infra: LevelInfrastructure = None,
-        common_infra: CommonInfrastructure = None,
-        **kwargs,
-    ):
-        """
-        Parameters
-        ----------
-        instruments : Union[List, str], optional
-            instruments of EMA signal, by default "csi300"
-        freq : str, optional
-            freq of EMA signal, by default "day"
-            Note: `freq` may be different from `time_per_step`
-        """
-        if instruments is None:
-            warnings.warn("`instruments` is not set, will load all stocks")
-            self.instruments = "all"
-        if isinstance(instruments, str):
-            self.instruments = D.instruments(instruments)
-        self.freq = freq
-        super(SBBStrategyEMA, self).__init__(
-            outer_trade_decision, level_infra, common_infra, trade_exchange=trade_exchange, **kwargs
-        )
-
-    def _reset_signal(self):
-        trade_len = self.trade_calendar.get_trade_len()
-        fields = ["EMA($close, 10)-EMA($close, 20)"]
-        signal_start_time, _ = self.trade_calendar.get_step_time(trade_step=0, shift=1)
-        _, signal_end_time = self.trade_calendar.get_step_time(trade_step=trade_len - 1, shift=1)
-        signal_df = D.features(
-            self.instruments, fields, start_time=signal_start_time, end_time=signal_end_time, freq=self.freq
-        )
-        signal_df.columns = ["signal"]
-        self.signal = {}
-
-        if not signal_df.empty:
-            for stock_id, stock_val in signal_df.groupby(level="instrument"):
-                self.signal[stock_id] = stock_val["signal"].droplevel(level="instrument")
-
-    def reset_level_infra(self, level_infra):
-        """
-        reset level-shared infra
-        - After reset the trade calendar, the signal will be changed
-        """
-        super().reset_level_infra(level_infra)
-        self._reset_signal()
-
-    def _pred_price_trend(self, stock_id, pred_start_time=None, pred_end_time=None):
-        # if no signal, return mid trend
-        if stock_id not in self.signal:
-            return self.TREND_MID
-        else:
-            _sample_signal = resam_ts_data(
-                self.signal[stock_id],
-                pred_start_time,
-                pred_end_time,
-                method=ts_data_last,
-            )
-            # if EMA signal == 0 or None, return mid trend
-            if _sample_signal is None or np.isnan(_sample_signal) or _sample_signal == 0:
-                return self.TREND_MID
-            # if EMA signal > 0, return long trend
-            elif _sample_signal > 0:
-                return self.TREND_LONG
-            # if EMA signal < 0, return short trend
-            else:
-                return self.TREND_SHORT
-
-
-class ACStrategy(BaseStrategy):
-    # TODO:
-    # 1. Supporting leverage the get_range_limit result from the decision
-    # 2. Supporting alter_outer_trade_decision
-    # 3. Supporting checking the availability of trade decision
-    def __init__(
-        self,
-        lamb: float = 1e-6,
-        eta: float = 2.5e-6,
-        window_size: int = 20,
-        outer_trade_decision: BaseTradeDecision = None,
-        instruments: Union[List, str] = "csi300",
-        freq: str = "day",
-        trade_exchange: Exchange = None,
-        level_infra: LevelInfrastructure = None,
-        common_infra: CommonInfrastructure = None,
-        **kwargs,
-    ):
-        """
-        Parameters
-        ----------
-        instruments : Union[List, str], optional
-            instruments of Volatility, by default "csi300"
-        freq : str, optional
-            freq of Volatility, by default "day"
-            Note: `freq` may be different from `time_per_step`
-        """
-        self.lamb = lamb
-        self.eta = eta
-        self.window_size = window_size
-        if instruments is None:
-            warnings.warn("`instruments` is not set, will load all stocks")
-            self.instruments = "all"
-        if isinstance(instruments, str):
-            self.instruments = D.instruments(instruments)
-        self.freq = freq
-        super(ACStrategy, self).__init__(
-            outer_trade_decision, level_infra, common_infra, trade_exchange=trade_exchange, **kwargs
-        )
-
-    def _reset_signal(self):
-        trade_len = self.trade_calendar.get_trade_len()
-        fields = [
-            f"Power(Sum(Power(Log($close/Ref($close, 1)), 2), {self.window_size})/{self.window_size - 1}-Power(Sum(Log($close/Ref($close, 1)), {self.window_size}), 2)/({self.window_size}*{self.window_size - 1}), 0.5)"
-        ]
-        signal_start_time, _ = self.trade_calendar.get_step_time(trade_step=0, shift=1)
-        _, signal_end_time = self.trade_calendar.get_step_time(trade_step=trade_len - 1, shift=1)
-        signal_df = D.features(
-            self.instruments, fields, start_time=signal_start_time, end_time=signal_end_time, freq=self.freq
-        )
-        signal_df.columns = ["volatility"]
-        self.signal = {}
-
-        if not signal_df.empty:
-            for stock_id, stock_val in signal_df.groupby(level="instrument"):
-                self.signal[stock_id] = stock_val["volatility"].droplevel(level="instrument")
-
-    def reset_level_infra(self, level_infra):
-        """
-        reset level-shared infra
-        - After reset the trade calendar, the signal will be changed
-        """
-        super().reset_level_infra(level_infra)
-        self._reset_signal()
-
-    def reset(self, outer_trade_decision: BaseTradeDecision = None, **kwargs):
-        """
-        Parameters
-        ----------
-        outer_trade_decision : BaseTradeDecision, optional
-        """
-        super(ACStrategy, self).reset(outer_trade_decision=outer_trade_decision, **kwargs)
-        if outer_trade_decision is not None:
-            self.trade_amount = {}
-            # init the trade amount of order and  predicted trade trend
-            for order in outer_trade_decision.get_decision():
-                self.trade_amount[order.stock_id] = order.amount
-
-    def generate_trade_decision(self, execute_result=None):
-        # get the number of trading step finished, trade_step can be [0, 1, 2, ..., trade_len - 1]
-        trade_step = self.trade_calendar.get_trade_step()
-        # get the total count of trading step
-        trade_len = self.trade_calendar.get_trade_len()
-
-        # update the order amount
-        if execute_result is not None:
-            for order, _, _, _ in execute_result:
-                self.trade_amount[order.stock_id] -= order.deal_amount
-
-        trade_start_time, trade_end_time = self.trade_calendar.get_step_time(trade_step)
-        pred_start_time, pred_end_time = self.trade_calendar.get_step_time(trade_step, shift=1)
-        order_list = []
-        for order in self.outer_trade_decision.get_decision():
-            # if not tradable, continue
-            if not self.trade_exchange.is_stock_tradable(
-                stock_id=order.stock_id, start_time=trade_start_time, end_time=trade_end_time
-            ):
-                continue
-            _order_amount = None
-            # considering trade unit
-
-            sig_sam = (
-                resam_ts_data(self.signal[order.stock_id], pred_start_time, pred_end_time, method=ts_data_last)
-                if order.stock_id in self.signal
-                else None
-            )
-
-            if sig_sam is None or np.isnan(sig_sam):
-                # no signal, TWAP
-                _amount_trade_unit = self.trade_exchange.get_amount_of_trade_unit(
-                    stock_id=order.stock_id, start_time=order.start_time, end_time=order.end_time
-                )
-                if _amount_trade_unit is None:
-                    # divide the order into equal parts, and trade one part
-                    _order_amount = self.trade_amount[order.stock_id] / (trade_len - trade_step)
-                else:
-                    # divide the order into equal parts, and trade one part
-                    # calculate the total count of trade units to trade
-                    trade_unit_cnt = int(self.trade_amount[order.stock_id] // _amount_trade_unit)
-                    # calculate the amount of one part, ceil the amount
-                    # floor((trade_unit_cnt + trade_len - trade_step - 1) / (trade_len - trade_step)) == ceil(trade_unit_cnt / (trade_len - trade_step))
-                    _order_amount = (
-                        (trade_unit_cnt + trade_len - trade_step - 1) // (trade_len - trade_step) * _amount_trade_unit
-                    )
-            else:
-                # VA strategy
-                kappa_tild = self.lamb / self.eta * sig_sam * sig_sam
-                kappa = np.arccosh(kappa_tild / 2 + 1)
-                amount_ratio = (
-                    np.sinh(kappa * (trade_len - trade_step)) - np.sinh(kappa * (trade_len - trade_step - 1))
-                ) / np.sinh(kappa * trade_len)
-                _order_amount = order.amount * amount_ratio
-                _order_amount = self.trade_exchange.round_amount_by_trade_unit(
-                    _order_amount, stock_id=order.stock_id, start_time=order.start_time, end_time=order.end_time
-                )
-
-            if order.direction == order.SELL:
-                # sell all amount at last
-                if self.trade_amount[order.stock_id] > 1e-5 and (_order_amount < 1e-5 or trade_step == trade_len - 1):
-                    _order_amount = self.trade_amount[order.stock_id]
-
-            _order_amount = min(_order_amount, self.trade_amount[order.stock_id])
-
-            if _order_amount > 1e-5:
-
-                _order = Order(
-                    stock_id=order.stock_id,
-                    amount=_order_amount,
-                    start_time=trade_start_time,
-                    end_time=trade_end_time,
-                    direction=order.direction,  # 1 for buy
-                    factor=order.factor,
-                )
-                order_list.append(_order)
-        return TradeDecisionWO(order_list, self)
-
-
-class RandomOrderStrategy(BaseStrategy):
-    def __init__(
-        self,
-        trade_range: Union[Tuple[int, int], TradeRange],  # The range is closed on both left and right.
-        sample_ratio: float = 1.0,
-        volume_ratio: float = 0.01,
-        market: str = "all",
-        direction: int = Order.BUY,
-        *args,
-        **kwargs,
-    ):
-        """
-        Parameters
-        ----------
-        trade_range : Tuple
-            please refer to the `trade_range` parameter of BaseStrategy
-        sample_ratio : float
-            the ratio of all orders are sampled
-        volume_ratio : float
-            the volume of the total day
-            raito of the total volume of a specific day
-        market : str
-            stock pool for sampling
-        """
-
-        super().__init__(*args, **kwargs)
-        self.sample_ratio = sample_ratio
-        self.volume_ratio = volume_ratio
-        self.market = market
-        self.direction = direction
-        exch: Exchange = self.common_infra.get("trade_exchange")
-        # TODO: this can't be online
-        self.volume = D.features(
-            D.instruments(market), ["Mean(Ref($volume, 1), 10)"], start_time=exch.start_time, end_time=exch.end_time
-        )
-        self.volume_df = self.volume.iloc[:, 0].unstack()
-        self.trade_range = trade_range
-
-    def generate_trade_decision(self, execute_result=None):
-        trade_step = self.trade_calendar.get_trade_step()
-        step_time_start, step_time_end = self.trade_calendar.get_step_time(trade_step)
-
-        order_list = []
-        if step_time_start in self.volume_df:
-            for stock_id, volume in self.volume_df[step_time_start].dropna().sample(frac=self.sample_ratio).items():
-                order_list.append(
-                    self.common_infra.get("trade_exchange")
-                    .get_order_helper()
-                    .create(
-                        code=stock_id,
-                        amount=volume * self.volume_ratio,
-                        direction=self.direction,
-                    )
-                )
-        return TradeDecisionWO(order_list, self, self.trade_range)
-
-
-class FileOrderStrategy(BaseStrategy):
-    """
-    Motivation:
-    - This class provides an interface for user to read orders from csv files.
-    """
-
-    def __init__(
-        self,
-        file: Union[IO, str, Path, pd.DataFrame],
-        trade_range: Union[Tuple[int, int], TradeRange] = None,
-        *args,
-        **kwargs,
-    ):
-        """
-
-        Parameters
-        ----------
-        file : Union[IO, str, Path, pd.DataFrame]
-            this parameters will specify the info of expected orders
-
-            Here is an example of the content
-
-            1) Amount (**adjusted**) based strategy
-
-                datetime,instrument,amount,direction
-                20200102,  SH600519,  1000,     sell
-                20200103,  SH600519,  1000,      buy
-                20200106,  SH600519,  1000,     sell
-
-        trade_range : Tuple[int, int]
-            the intra day time index range of the orders
-            the left and right is closed.
-
-            If you want to get the trade_range in intra-day
-            - `qlib/utils/time.py:def get_day_min_idx_range` can help you create the index range easier
-            # TODO: this is a trade_range level limitation. We'll implement a more detailed limitation later.
-
-        """
-        super().__init__(*args, **kwargs)
-        if isinstance(file, pd.DataFrame):
-            self.order_df = file
-        else:
-            with get_io_object(file) as f:
-                self.order_df = pd.read_csv(f, dtype={"datetime": str})
-
-        self.order_df["datetime"] = self.order_df["datetime"].apply(pd.Timestamp)
-        self.order_df = self.order_df.set_index(["datetime", "instrument"])
-
-        # make sure the datetime is the first level for fast indexing
-        self.order_df = lazy_sort_index(convert_index_format(self.order_df, level="datetime"))
-        self.trade_range = trade_range
-
-    def generate_trade_decision(self, execute_result=None) -> TradeDecisionWO:
-        """
-        Parameters
-        ----------
-        execute_result :
-            execute_result will be ignored in FileOrderStrategy
-        """
-        oh: OrderHelper = self.common_infra.get("trade_exchange").get_order_helper()
-        start, _ = self.trade_calendar.get_step_time()
-        # CONVERSION: the bar is indexed by the time
-        try:
-            df = self.order_df.loc(axis=0)[start]
-        except KeyError:
-            return TradeDecisionWO([], self)
-        else:
-            order_list = []
-            for idx, row in df.iterrows():
-                order_list.append(
-                    oh.create(
-                        code=idx,
-                        amount=row["amount"],
-                        direction=Order.parse_dir(row["direction"]),
-                    )
-                )
-            return TradeDecisionWO(order_list, self, self.trade_range)
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+from pathlib import Path
+import warnings
+import numpy as np
+import pandas as pd
+from typing import IO, List, Tuple, Union
+from qlib.data.dataset.utils import convert_index_format
+
+from qlib.utils import lazy_sort_index
+
+from ...utils.resam import resam_ts_data, ts_data_last
+from ...data.data import D
+from ...strategy.base import BaseStrategy
+from ...backtest.decision import BaseTradeDecision, Order, TradeDecisionWO, TradeRange
+from ...backtest.exchange import Exchange, OrderHelper
+from ...backtest.utils import CommonInfrastructure, LevelInfrastructure
+from qlib.utils.file import get_io_object
+from qlib.backtest.utils import get_start_end_idx
+
+
+class TWAPStrategy(BaseStrategy):
+    """TWAP Strategy for trading
+
+    NOTE:
+        - This TWAP strategy will celling round when trading. This will make the TWAP trading strategy produce the order
+          earlier when the total trade unit of amount is less than the trading step
+    """
+
+    def reset(self, outer_trade_decision: BaseTradeDecision = None, **kwargs):
+        """
+        Parameters
+        ----------
+        outer_trade_decision : BaseTradeDecision, optional
+        """
+
+        super(TWAPStrategy, self).reset(outer_trade_decision=outer_trade_decision, **kwargs)
+        if outer_trade_decision is not None:
+            self.trade_amount_remain = {}
+            for order in outer_trade_decision.get_decision():
+                self.trade_amount_remain[order.stock_id] = order.amount
+
+    def generate_trade_decision(self, execute_result=None):
+        # NOTE:  corner cases!!!
+        # - If using upperbound round, please don't sell the amount which should in next step
+        #   - the coordinate of the amount between steps is hard to be dealt between steps in the same level. It
+        #     is easier to be dealt in upper steps
+
+        # strategy is not available. Give an empty decision
+        if len(self.outer_trade_decision.get_decision()) == 0:
+            return TradeDecisionWO(order_list=[], strategy=self)
+
+        # get the number of trading step finished, trade_step can be [0, 1, 2, ..., trade_len - 1]
+        trade_step = self.trade_calendar.get_trade_step()
+        # get the total count of trading step
+        start_idx, end_idx = get_start_end_idx(self.trade_calendar, self.outer_trade_decision)
+        trade_len = end_idx - start_idx + 1
+
+        if trade_step < start_idx or trade_step > end_idx:
+            # It is not time to start trading or trading has ended.
+            return TradeDecisionWO(order_list=[], strategy=self)
+
+        rel_trade_step = trade_step - start_idx  # trade_step relative to start_idx (number of steps has already passed)
+
+        # update the order amount
+        if execute_result is not None:
+            for order, _, _, _ in execute_result:
+                self.trade_amount_remain[order.stock_id] -= order.deal_amount
+
+        trade_start_time, trade_end_time = self.trade_calendar.get_step_time(trade_step)
+        order_list = []
+        for order in self.outer_trade_decision.get_decision():
+            # Don't peek the future information, so we use check_stock_suspended instead of is_stock_tradable
+            # necessity of this
+            # - if stock is suspended, the quote values of stocks is NaN. The following code will raise error when
+            # encountering NaN factor
+            if self.trade_exchange.check_stock_suspended(
+                stock_id=order.stock_id, start_time=trade_start_time, end_time=trade_end_time
+            ):
+                continue
+
+            # the expected trade amount after current step
+            amount_expect = order.amount / trade_len * (rel_trade_step + 1)
+
+            # remain amount
+            amount_remain = self.trade_amount_remain[order.stock_id]
+
+            # the amount has already been finished now.
+            amount_finished = order.amount - amount_remain
+
+            # the expected amount of current step
+            amount_delta = amount_expect - amount_finished
+
+            _amount_trade_unit = self.trade_exchange.get_amount_of_trade_unit(
+                stock_id=order.stock_id, start_time=order.start_time, end_time=order.end_time
+            )
+
+            # round the amount_delta by trade_unit and clip by remain
+            # NOTE: this could be more than expected.
+            if _amount_trade_unit is None:
+                # divide the order into equal parts, and trade one part
+                amount_delta_target = amount_delta
+            else:
+                amount_delta_target = min(
+                    np.round(amount_delta / _amount_trade_unit) * _amount_trade_unit, amount_remain
+                )
+
+            # handle last step to make sure all positions have gone
+            # necessity: the last step can't be rounded to the a unit (e.g. reminder < 0.5 unit)
+            if rel_trade_step == trade_len - 1:
+                amount_delta_target = amount_remain
+
+            if amount_delta_target > 1e-5:
+                _order = Order(
+                    stock_id=order.stock_id,
+                    amount=amount_delta_target,
+                    start_time=trade_start_time,
+                    end_time=trade_end_time,
+                    direction=order.direction,  # 1 for buy
+                )
+                order_list.append(_order)
+        return TradeDecisionWO(order_list=order_list, strategy=self)
+
+
+class SBBStrategyBase(BaseStrategy):
+    """
+    (S)elect the (B)etter one among every two adjacent trading (B)ars to sell or buy.
+    """
+
+    TREND_MID = 0
+    TREND_SHORT = 1
+    TREND_LONG = 2
+
+    # TODO:
+    # 1. Supporting leverage the get_range_limit result from the decision
+    # 2. Supporting alter_outer_trade_decision
+    # 3. Supporting checking the availability of trade decision
+
+    def reset(self, outer_trade_decision: BaseTradeDecision = None, **kwargs):
+        """
+        Parameters
+        ----------
+        outer_trade_decision : BaseTradeDecision, optional
+        """
+        super(SBBStrategyBase, self).reset(outer_trade_decision=outer_trade_decision, **kwargs)
+        if outer_trade_decision is not None:
+            self.trade_trend = {}
+            self.trade_amount = {}
+            # init the trade amount of order and  predicted trade trend
+            for order in outer_trade_decision.get_decision():
+                self.trade_trend[order.stock_id] = self.TREND_MID
+                self.trade_amount[order.stock_id] = order.amount
+
+    def _pred_price_trend(self, stock_id, pred_start_time=None, pred_end_time=None):
+        raise NotImplementedError("pred_price_trend method is not implemented!")
+
+    def generate_trade_decision(self, execute_result=None):
+        # get the number of trading step finished, trade_step can be [0, 1, 2, ..., trade_len - 1]
+        trade_step = self.trade_calendar.get_trade_step()
+        # get the total count of trading step
+        trade_len = self.trade_calendar.get_trade_len()
+
+        # update the order amount
+        if execute_result is not None:
+            for order, _, _, _ in execute_result:
+                self.trade_amount[order.stock_id] -= order.deal_amount
+
+        trade_start_time, trade_end_time = self.trade_calendar.get_step_time(trade_step)
+        pred_start_time, pred_end_time = self.trade_calendar.get_step_time(trade_step, shift=1)
+        order_list = []
+        # for each order in in self.outer_trade_decision
+        for order in self.outer_trade_decision.get_decision():
+            # get the price trend
+            if trade_step % 2 == 0:
+                # in the first of two adjacent bars, predict the price trend
+                _pred_trend = self._pred_price_trend(order.stock_id, pred_start_time, pred_end_time)
+            else:
+                # in the second of two adjacent bars, use the trend predicted in the first one
+                _pred_trend = self.trade_trend[order.stock_id]
+            # if not tradable, continue
+            if not self.trade_exchange.is_stock_tradable(
+                stock_id=order.stock_id, start_time=trade_start_time, end_time=trade_end_time
+            ):
+                if trade_step % 2 == 0:
+                    self.trade_trend[order.stock_id] = _pred_trend
+                continue
+            # get amount of one trade unit
+            _amount_trade_unit = self.trade_exchange.get_amount_of_trade_unit(
+                stock_id=order.stock_id, start_time=order.start_time, end_time=order.end_time
+            )
+            if _pred_trend == self.TREND_MID:
+                _order_amount = None
+                # considering trade unit
+                if _amount_trade_unit is None:
+                    # divide the order into equal parts, and trade one part
+                    _order_amount = self.trade_amount[order.stock_id] / (trade_len - trade_step)
+                # without considering trade unit
+                else:
+                    # divide the order into equal parts, and trade one part
+                    # calculate the total count of trade units to trade
+                    trade_unit_cnt = int(self.trade_amount[order.stock_id] // _amount_trade_unit)
+                    # calculate the amount of one part, ceil the amount
+                    # floor((trade_unit_cnt + trade_len - trade_step - 1) / (trade_len - trade_step)) == ceil(trade_unit_cnt / (trade_len - trade_step))
+                    _order_amount = (
+                        (trade_unit_cnt + trade_len - trade_step - 1) // (trade_len - trade_step) * _amount_trade_unit
+                    )
+                if order.direction == order.SELL:
+                    # sell all amount at last
+                    if self.trade_amount[order.stock_id] > 1e-5 and (
+                        _order_amount < 1e-5 or trade_step == trade_len - 1
+                    ):
+                        _order_amount = self.trade_amount[order.stock_id]
+
+                _order_amount = min(_order_amount, self.trade_amount[order.stock_id])
+
+                if _order_amount > 1e-5:
+                    _order = Order(
+                        stock_id=order.stock_id,
+                        amount=_order_amount,
+                        start_time=trade_start_time,
+                        end_time=trade_end_time,
+                        direction=order.direction,
+                    )
+                    order_list.append(_order)
+
+            else:
+                _order_amount = None
+                # considering trade unit
+                if _amount_trade_unit is None:
+                    # N trade day left, divide the order into N + 1 parts, and trade 2 parts
+                    _order_amount = 2 * self.trade_amount[order.stock_id] / (trade_len - trade_step + 1)
+                # without considering trade unit
+                else:
+                    # cal how many trade unit
+                    trade_unit_cnt = int(self.trade_amount[order.stock_id] // _amount_trade_unit)
+                    # N trade day left, divide the order into N + 1 parts, and trade 2 parts
+                    _order_amount = (
+                        (trade_unit_cnt + trade_len - trade_step)
+                        // (trade_len - trade_step + 1)
+                        * 2
+                        * _amount_trade_unit
+                    )
+                if order.direction == order.SELL:
+                    # sell all amount at last
+                    if self.trade_amount[order.stock_id] > 1e-5 and (
+                        _order_amount < 1e-5 or trade_step == trade_len - 1
+                    ):
+                        _order_amount = self.trade_amount[order.stock_id]
+
+                _order_amount = min(_order_amount, self.trade_amount[order.stock_id])
+
+                if _order_amount > 1e-5:
+                    if trade_step % 2 == 0:
+                        # in the first one of two adjacent bars
+                        # if look short on the price, sell the stock more
+                        # if look long on the price, buy the stock more
+                        if (
+                            _pred_trend == self.TREND_SHORT
+                            and order.direction == order.SELL
+                            or _pred_trend == self.TREND_LONG
+                            and order.direction == order.BUY
+                        ):
+                            _order = Order(
+                                stock_id=order.stock_id,
+                                amount=_order_amount,
+                                start_time=trade_start_time,
+                                end_time=trade_end_time,
+                                direction=order.direction,  # 1 for buy
+                            )
+                            order_list.append(_order)
+                    else:
+                        # in the second one of two adjacent bars
+                        # if look short on the price, buy the stock more
+                        # if look long on the price, sell the stock more
+                        if (
+                            _pred_trend == self.TREND_SHORT
+                            and order.direction == order.BUY
+                            or _pred_trend == self.TREND_LONG
+                            and order.direction == order.SELL
+                        ):
+                            _order = Order(
+                                stock_id=order.stock_id,
+                                amount=_order_amount,
+                                start_time=trade_start_time,
+                                end_time=trade_end_time,
+                                direction=order.direction,  # 1 for buy
+                            )
+                            order_list.append(_order)
+
+            if trade_step % 2 == 0:
+                # in the first one of two adjacent bars, store the trend for the second one to use
+                self.trade_trend[order.stock_id] = _pred_trend
+
+        return TradeDecisionWO(order_list, self)
+
+
+class SBBStrategyEMA(SBBStrategyBase):
+    """
+    (S)elect the (B)etter one among every two adjacent trading (B)ars to sell or buy with (EMA) signal.
+    """
+
+    # TODO:
+    # 1. Supporting leverage the get_range_limit result from the decision
+    # 2. Supporting alter_outer_trade_decision
+    # 3. Supporting checking the availability of trade decision
+
+    def __init__(
+        self,
+        outer_trade_decision: BaseTradeDecision = None,
+        instruments: Union[List, str] = "csi300",
+        freq: str = "day",
+        trade_exchange: Exchange = None,
+        level_infra: LevelInfrastructure = None,
+        common_infra: CommonInfrastructure = None,
+        **kwargs,
+    ):
+        """
+        Parameters
+        ----------
+        instruments : Union[List, str], optional
+            instruments of EMA signal, by default "csi300"
+        freq : str, optional
+            freq of EMA signal, by default "day"
+            Note: `freq` may be different from `time_per_step`
+        """
+        if instruments is None:
+            warnings.warn("`instruments` is not set, will load all stocks")
+            self.instruments = "all"
+        if isinstance(instruments, str):
+            self.instruments = D.instruments(instruments)
+        self.freq = freq
+        super(SBBStrategyEMA, self).__init__(
+            outer_trade_decision, level_infra, common_infra, trade_exchange=trade_exchange, **kwargs
+        )
+
+    def _reset_signal(self):
+        trade_len = self.trade_calendar.get_trade_len()
+        fields = ["EMA($close, 10)-EMA($close, 20)"]
+        signal_start_time, _ = self.trade_calendar.get_step_time(trade_step=0, shift=1)
+        _, signal_end_time = self.trade_calendar.get_step_time(trade_step=trade_len - 1, shift=1)
+        signal_df = D.features(
+            self.instruments, fields, start_time=signal_start_time, end_time=signal_end_time, freq=self.freq
+        )
+        signal_df.columns = ["signal"]
+        self.signal = {}
+
+        if not signal_df.empty:
+            for stock_id, stock_val in signal_df.groupby(level="instrument"):
+                self.signal[stock_id] = stock_val["signal"].droplevel(level="instrument")
+
+    def reset_level_infra(self, level_infra):
+        """
+        reset level-shared infra
+        - After reset the trade calendar, the signal will be changed
+        """
+        super().reset_level_infra(level_infra)
+        self._reset_signal()
+
+    def _pred_price_trend(self, stock_id, pred_start_time=None, pred_end_time=None):
+        # if no signal, return mid trend
+        if stock_id not in self.signal:
+            return self.TREND_MID
+        else:
+            _sample_signal = resam_ts_data(
+                self.signal[stock_id],
+                pred_start_time,
+                pred_end_time,
+                method=ts_data_last,
+            )
+            # if EMA signal == 0 or None, return mid trend
+            if _sample_signal is None or np.isnan(_sample_signal) or _sample_signal == 0:
+                return self.TREND_MID
+            # if EMA signal > 0, return long trend
+            elif _sample_signal > 0:
+                return self.TREND_LONG
+            # if EMA signal < 0, return short trend
+            else:
+                return self.TREND_SHORT
+
+
+class ACStrategy(BaseStrategy):
+    # TODO:
+    # 1. Supporting leverage the get_range_limit result from the decision
+    # 2. Supporting alter_outer_trade_decision
+    # 3. Supporting checking the availability of trade decision
+    def __init__(
+        self,
+        lamb: float = 1e-6,
+        eta: float = 2.5e-6,
+        window_size: int = 20,
+        outer_trade_decision: BaseTradeDecision = None,
+        instruments: Union[List, str] = "csi300",
+        freq: str = "day",
+        trade_exchange: Exchange = None,
+        level_infra: LevelInfrastructure = None,
+        common_infra: CommonInfrastructure = None,
+        **kwargs,
+    ):
+        """
+        Parameters
+        ----------
+        instruments : Union[List, str], optional
+            instruments of Volatility, by default "csi300"
+        freq : str, optional
+            freq of Volatility, by default "day"
+            Note: `freq` may be different from `time_per_step`
+        """
+        self.lamb = lamb
+        self.eta = eta
+        self.window_size = window_size
+        if instruments is None:
+            warnings.warn("`instruments` is not set, will load all stocks")
+            self.instruments = "all"
+        if isinstance(instruments, str):
+            self.instruments = D.instruments(instruments)
+        self.freq = freq
+        super(ACStrategy, self).__init__(
+            outer_trade_decision, level_infra, common_infra, trade_exchange=trade_exchange, **kwargs
+        )
+
+    def _reset_signal(self):
+        trade_len = self.trade_calendar.get_trade_len()
+        fields = [
+            f"Power(Sum(Power(Log($close/Ref($close, 1)), 2), {self.window_size})/{self.window_size - 1}-Power(Sum(Log($close/Ref($close, 1)), {self.window_size}), 2)/({self.window_size}*{self.window_size - 1}), 0.5)"
+        ]
+        signal_start_time, _ = self.trade_calendar.get_step_time(trade_step=0, shift=1)
+        _, signal_end_time = self.trade_calendar.get_step_time(trade_step=trade_len - 1, shift=1)
+        signal_df = D.features(
+            self.instruments, fields, start_time=signal_start_time, end_time=signal_end_time, freq=self.freq
+        )
+        signal_df.columns = ["volatility"]
+        self.signal = {}
+
+        if not signal_df.empty:
+            for stock_id, stock_val in signal_df.groupby(level="instrument"):
+                self.signal[stock_id] = stock_val["volatility"].droplevel(level="instrument")
+
+    def reset_level_infra(self, level_infra):
+        """
+        reset level-shared infra
+        - After reset the trade calendar, the signal will be changed
+        """
+        super().reset_level_infra(level_infra)
+        self._reset_signal()
+
+    def reset(self, outer_trade_decision: BaseTradeDecision = None, **kwargs):
+        """
+        Parameters
+        ----------
+        outer_trade_decision : BaseTradeDecision, optional
+        """
+        super(ACStrategy, self).reset(outer_trade_decision=outer_trade_decision, **kwargs)
+        if outer_trade_decision is not None:
+            self.trade_amount = {}
+            # init the trade amount of order and  predicted trade trend
+            for order in outer_trade_decision.get_decision():
+                self.trade_amount[order.stock_id] = order.amount
+
+    def generate_trade_decision(self, execute_result=None):
+        # get the number of trading step finished, trade_step can be [0, 1, 2, ..., trade_len - 1]
+        trade_step = self.trade_calendar.get_trade_step()
+        # get the total count of trading step
+        trade_len = self.trade_calendar.get_trade_len()
+
+        # update the order amount
+        if execute_result is not None:
+            for order, _, _, _ in execute_result:
+                self.trade_amount[order.stock_id] -= order.deal_amount
+
+        trade_start_time, trade_end_time = self.trade_calendar.get_step_time(trade_step)
+        pred_start_time, pred_end_time = self.trade_calendar.get_step_time(trade_step, shift=1)
+        order_list = []
+        for order in self.outer_trade_decision.get_decision():
+            # if not tradable, continue
+            if not self.trade_exchange.is_stock_tradable(
+                stock_id=order.stock_id, start_time=trade_start_time, end_time=trade_end_time
+            ):
+                continue
+            _order_amount = None
+            # considering trade unit
+
+            sig_sam = (
+                resam_ts_data(self.signal[order.stock_id], pred_start_time, pred_end_time, method=ts_data_last)
+                if order.stock_id in self.signal
+                else None
+            )
+
+            if sig_sam is None or np.isnan(sig_sam):
+                # no signal, TWAP
+                _amount_trade_unit = self.trade_exchange.get_amount_of_trade_unit(
+                    stock_id=order.stock_id, start_time=order.start_time, end_time=order.end_time
+                )
+                if _amount_trade_unit is None:
+                    # divide the order into equal parts, and trade one part
+                    _order_amount = self.trade_amount[order.stock_id] / (trade_len - trade_step)
+                else:
+                    # divide the order into equal parts, and trade one part
+                    # calculate the total count of trade units to trade
+                    trade_unit_cnt = int(self.trade_amount[order.stock_id] // _amount_trade_unit)
+                    # calculate the amount of one part, ceil the amount
+                    # floor((trade_unit_cnt + trade_len - trade_step - 1) / (trade_len - trade_step)) == ceil(trade_unit_cnt / (trade_len - trade_step))
+                    _order_amount = (
+                        (trade_unit_cnt + trade_len - trade_step - 1) // (trade_len - trade_step) * _amount_trade_unit
+                    )
+            else:
+                # VA strategy
+                kappa_tild = self.lamb / self.eta * sig_sam * sig_sam
+                kappa = np.arccosh(kappa_tild / 2 + 1)
+                amount_ratio = (
+                    np.sinh(kappa * (trade_len - trade_step)) - np.sinh(kappa * (trade_len - trade_step - 1))
+                ) / np.sinh(kappa * trade_len)
+                _order_amount = order.amount * amount_ratio
+                _order_amount = self.trade_exchange.round_amount_by_trade_unit(
+                    _order_amount, stock_id=order.stock_id, start_time=order.start_time, end_time=order.end_time
+                )
+
+            if order.direction == order.SELL:
+                # sell all amount at last
+                if self.trade_amount[order.stock_id] > 1e-5 and (_order_amount < 1e-5 or trade_step == trade_len - 1):
+                    _order_amount = self.trade_amount[order.stock_id]
+
+            _order_amount = min(_order_amount, self.trade_amount[order.stock_id])
+
+            if _order_amount > 1e-5:
+                _order = Order(
+                    stock_id=order.stock_id,
+                    amount=_order_amount,
+                    start_time=trade_start_time,
+                    end_time=trade_end_time,
+                    direction=order.direction,  # 1 for buy
+                    factor=order.factor,
+                )
+                order_list.append(_order)
+        return TradeDecisionWO(order_list, self)
+
+
+class RandomOrderStrategy(BaseStrategy):
+    def __init__(
+        self,
+        trade_range: Union[Tuple[int, int], TradeRange],  # The range is closed on both left and right.
+        sample_ratio: float = 1.0,
+        volume_ratio: float = 0.01,
+        market: str = "all",
+        direction: int = Order.BUY,
+        *args,
+        **kwargs,
+    ):
+        """
+        Parameters
+        ----------
+        trade_range : Tuple
+            please refer to the `trade_range` parameter of BaseStrategy
+        sample_ratio : float
+            the ratio of all orders are sampled
+        volume_ratio : float
+            the volume of the total day
+            raito of the total volume of a specific day
+        market : str
+            stock pool for sampling
+        """
+
+        super().__init__(*args, **kwargs)
+        self.sample_ratio = sample_ratio
+        self.volume_ratio = volume_ratio
+        self.market = market
+        self.direction = direction
+        exch: Exchange = self.common_infra.get("trade_exchange")
+        # TODO: this can't be online
+        self.volume = D.features(
+            D.instruments(market), ["Mean(Ref($volume, 1), 10)"], start_time=exch.start_time, end_time=exch.end_time
+        )
+        self.volume_df = self.volume.iloc[:, 0].unstack()
+        self.trade_range = trade_range
+
+    def generate_trade_decision(self, execute_result=None):
+        trade_step = self.trade_calendar.get_trade_step()
+        step_time_start, step_time_end = self.trade_calendar.get_step_time(trade_step)
+
+        order_list = []
+        if step_time_start in self.volume_df:
+            for stock_id, volume in self.volume_df[step_time_start].dropna().sample(frac=self.sample_ratio).items():
+                order_list.append(
+                    self.common_infra.get("trade_exchange")
+                    .get_order_helper()
+                    .create(
+                        code=stock_id,
+                        amount=volume * self.volume_ratio,
+                        direction=self.direction,
+                    )
+                )
+        return TradeDecisionWO(order_list, self, self.trade_range)
+
+
+class FileOrderStrategy(BaseStrategy):
+    """
+    Motivation:
+    - This class provides an interface for user to read orders from csv files.
+    """
+
+    def __init__(
+        self,
+        file: Union[IO, str, Path, pd.DataFrame],
+        trade_range: Union[Tuple[int, int], TradeRange] = None,
+        *args,
+        **kwargs,
+    ):
+        """
+
+        Parameters
+        ----------
+        file : Union[IO, str, Path, pd.DataFrame]
+            this parameters will specify the info of expected orders
+
+            Here is an example of the content
+
+            1) Amount (**adjusted**) based strategy
+
+                datetime,instrument,amount,direction
+                20200102,  SH600519,  1000,     sell
+                20200103,  SH600519,  1000,      buy
+                20200106,  SH600519,  1000,     sell
+
+        trade_range : Tuple[int, int]
+            the intra day time index range of the orders
+            the left and right is closed.
+
+            If you want to get the trade_range in intra-day
+            - `qlib/utils/time.py:def get_day_min_idx_range` can help you create the index range easier
+            # TODO: this is a trade_range level limitation. We'll implement a more detailed limitation later.
+
+        """
+        super().__init__(*args, **kwargs)
+        if isinstance(file, pd.DataFrame):
+            self.order_df = file
+        else:
+            with get_io_object(file) as f:
+                self.order_df = pd.read_csv(f, dtype={"datetime": str})
+
+        self.order_df["datetime"] = self.order_df["datetime"].apply(pd.Timestamp)
+        self.order_df = self.order_df.set_index(["datetime", "instrument"])
+
+        # make sure the datetime is the first level for fast indexing
+        self.order_df = lazy_sort_index(convert_index_format(self.order_df, level="datetime"))
+        self.trade_range = trade_range
+
+    def generate_trade_decision(self, execute_result=None) -> TradeDecisionWO:
+        """
+        Parameters
+        ----------
+        execute_result :
+            execute_result will be ignored in FileOrderStrategy
+        """
+        oh: OrderHelper = self.common_infra.get("trade_exchange").get_order_helper()
+        start, _ = self.trade_calendar.get_step_time()
+        # CONVERSION: the bar is indexed by the time
+        try:
+            df = self.order_df.loc(axis=0)[start]
+        except KeyError:
+            return TradeDecisionWO([], self)
+        else:
+            order_list = []
+            for idx, row in df.iterrows():
+                order_list.append(
+                    oh.create(
+                        code=idx,
+                        amount=row["amount"],
+                        direction=Order.parse_dir(row["direction"]),
+                    )
+                )
+            return TradeDecisionWO(order_list, self, self.trade_range)
```

## qlib/contrib/strategy/signal_strategy.py

```diff
@@ -1,525 +1,523 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-import os
-import copy
-import warnings
-import numpy as np
-import pandas as pd
-
-from typing import Dict, List, Text, Tuple, Union
-from abc import ABC
-
-from qlib.data import D
-from qlib.data.dataset import Dataset
-from qlib.model.base import BaseModel
-from qlib.strategy.base import BaseStrategy
-from qlib.backtest.position import Position
-from qlib.backtest.signal import Signal, create_signal_from
-from qlib.backtest.decision import Order, OrderDir, TradeDecisionWO
-from qlib.log import get_module_logger
-from qlib.utils import get_pre_trading_date, load_dataset
-from qlib.contrib.strategy.order_generator import OrderGenerator, OrderGenWOInteract
-from qlib.contrib.strategy.optimizer import EnhancedIndexingOptimizer
-
-
-class BaseSignalStrategy(BaseStrategy, ABC):
-    def __init__(
-        self,
-        *,
-        signal: Union[Signal, Tuple[BaseModel, Dataset], List, Dict, Text, pd.Series, pd.DataFrame] = None,
-        model=None,
-        dataset=None,
-        risk_degree: float = 0.95,
-        trade_exchange=None,
-        level_infra=None,
-        common_infra=None,
-        **kwargs,
-    ):
-        """
-        Parameters
-        -----------
-        signal :
-            the information to describe a signal. Please refer to the docs of `qlib.backtest.signal.create_signal_from`
-            the decision of the strategy will base on the given signal
-        risk_degree : float
-            position percentage of total value.
-        trade_exchange : Exchange
-            exchange that provides market info, used to deal order and generate report
-            - If `trade_exchange` is None, self.trade_exchange will be set with common_infra
-            - It allowes different trade_exchanges is used in different executions.
-            - For example:
-                - In daily execution, both daily exchange and minutely are usable, but the daily exchange is recommended because it runs faster.
-                - In minutely execution, the daily exchange is not usable, only the minutely exchange is recommended.
-
-        """
-        super().__init__(level_infra=level_infra, common_infra=common_infra, trade_exchange=trade_exchange, **kwargs)
-
-        self.risk_degree = risk_degree
-
-        # This is trying to be compatible with previous version of qlib task config
-        if model is not None and dataset is not None:
-            warnings.warn("`model` `dataset` is deprecated; use `signal`.", DeprecationWarning)
-            signal = model, dataset
-
-        self.signal: Signal = create_signal_from(signal)
-
-    def get_risk_degree(self, trade_step=None):
-        """get_risk_degree
-        Return the proportion of your total value you will use in investment.
-        Dynamically risk_degree will result in Market timing.
-        """
-        # It will use 95% amount of your total value by default
-        return self.risk_degree
-
-
-class TopkDropoutStrategy(BaseSignalStrategy):
-    # TODO:
-    # 1. Supporting leverage the get_range_limit result from the decision
-    # 2. Supporting alter_outer_trade_decision
-    # 3. Supporting checking the availability of trade decision
-    # 4. Regenerate results with forbid_all_trade_at_limit set to false and flip the default to false, as it is consistent with reality.
-    def __init__(
-        self,
-        *,
-        topk,
-        n_drop,
-        method_sell="bottom",
-        method_buy="top",
-        hold_thresh=1,
-        only_tradable=False,
-        forbid_all_trade_at_limit=True,
-        **kwargs,
-    ):
-        """
-        Parameters
-        -----------
-        topk : int
-            the number of stocks in the portfolio.
-        n_drop : int
-            number of stocks to be replaced in each trading date.
-        method_sell : str
-            dropout method_sell, random/bottom.
-        method_buy : str
-            dropout method_buy, random/top.
-        hold_thresh : int
-            minimum holding days
-            before sell stock , will check current.get_stock_count(order.stock_id) >= self.hold_thresh.
-        only_tradable : bool
-            will the strategy only consider the tradable stock when buying and selling.
-
-            if only_tradable:
-
-                strategy will make decision with the tradable state of the stock info and avoid buy and sell them.
-
-            else:
-
-                strategy will make buy sell decision without checking the tradable state of the stock.
-        forbid_all_trade_at_limit : bool
-            if forbid all trades when limit_up or limit_down reached.
-
-            if forbid_all_trade_at_limit:
-
-                strategy will not do any trade when price reaches limit up/down, even not sell at limit up nor buy at
-                limit down, though allowed in reality.
-
-            else:
-
-                strategy will sell at limit up and buy ad limit down.
-        """
-        super().__init__(**kwargs)
-        self.topk = topk
-        self.n_drop = n_drop
-        self.method_sell = method_sell
-        self.method_buy = method_buy
-        self.hold_thresh = hold_thresh
-        self.only_tradable = only_tradable
-        self.forbid_all_trade_at_limit = forbid_all_trade_at_limit
-
-    def generate_trade_decision(self, execute_result=None):
-        # get the number of trading step finished, trade_step can be [0, 1, 2, ..., trade_len - 1]
-        trade_step = self.trade_calendar.get_trade_step()
-        trade_start_time, trade_end_time = self.trade_calendar.get_step_time(trade_step)
-        pred_start_time, pred_end_time = self.trade_calendar.get_step_time(trade_step, shift=1)
-        pred_score = self.signal.get_signal(start_time=pred_start_time, end_time=pred_end_time)
-        # NOTE: the current version of topk dropout strategy can't handle pd.DataFrame(multiple signal)
-        # So it only leverage the first col of signal
-        if isinstance(pred_score, pd.DataFrame):
-            pred_score = pred_score.iloc[:, 0]
-        if pred_score is None:
-            return TradeDecisionWO([], self)
-        if self.only_tradable:
-            # If The strategy only consider tradable stock when make decision
-            # It needs following actions to filter stocks
-            def get_first_n(li, n, reverse=False):
-                cur_n = 0
-                res = []
-                for si in reversed(li) if reverse else li:
-                    if self.trade_exchange.is_stock_tradable(
-                        stock_id=si, start_time=trade_start_time, end_time=trade_end_time
-                    ):
-                        res.append(si)
-                        cur_n += 1
-                        if cur_n >= n:
-                            break
-                return res[::-1] if reverse else res
-
-            def get_last_n(li, n):
-                return get_first_n(li, n, reverse=True)
-
-            def filter_stock(li):
-                return [
-                    si
-                    for si in li
-                    if self.trade_exchange.is_stock_tradable(
-                        stock_id=si, start_time=trade_start_time, end_time=trade_end_time
-                    )
-                ]
-
-        else:
-            # Otherwise, the stock will make decision without the stock tradable info
-            def get_first_n(li, n):
-                return list(li)[:n]
-
-            def get_last_n(li, n):
-                return list(li)[-n:]
-
-            def filter_stock(li):
-                return li
-
-        current_temp: Position = copy.deepcopy(self.trade_position)
-        # generate order list for this adjust date
-        sell_order_list = []
-        buy_order_list = []
-        # load score
-        cash = current_temp.get_cash()
-        current_stock_list = current_temp.get_stock_list()
-        # last position (sorted by score)
-        last = pred_score.reindex(current_stock_list).sort_values(ascending=False).index
-        # The new stocks today want to buy **at most**
-        if self.method_buy == "top":
-            today = get_first_n(
-                pred_score[~pred_score.index.isin(last)].sort_values(ascending=False).index,
-                self.n_drop + self.topk - len(last),
-            )
-        elif self.method_buy == "random":
-            topk_candi = get_first_n(pred_score.sort_values(ascending=False).index, self.topk)
-            candi = list(filter(lambda x: x not in last, topk_candi))
-            n = self.n_drop + self.topk - len(last)
-            try:
-                today = np.random.choice(candi, n, replace=False)
-            except ValueError:
-                today = candi
-        else:
-            raise NotImplementedError(f"This type of input is not supported")
-        # combine(new stocks + last stocks),  we will drop stocks from this list
-        # In case of dropping higher score stock and buying lower score stock.
-        comb = pred_score.reindex(last.union(pd.Index(today))).sort_values(ascending=False).index
-
-        # Get the stock list we really want to sell (After filtering the case that we sell high and buy low)
-        if self.method_sell == "bottom":
-            sell = last[last.isin(get_last_n(comb, self.n_drop))]
-        elif self.method_sell == "random":
-            candi = filter_stock(last)
-            try:
-                sell = pd.Index(np.random.choice(candi, self.n_drop, replace=False) if len(last) else [])
-            except ValueError:  # No enough candidates
-                sell = candi
-        else:
-            raise NotImplementedError(f"This type of input is not supported")
-
-        # Get the stock list we really want to buy
-        buy = today[: len(sell) + self.topk - len(last)]
-        for code in current_stock_list:
-            if not self.trade_exchange.is_stock_tradable(
-                stock_id=code,
-                start_time=trade_start_time,
-                end_time=trade_end_time,
-                direction=None if self.forbid_all_trade_at_limit else OrderDir.SELL,
-            ):
-                continue
-            if code in sell:
-                # check hold limit
-                time_per_step = self.trade_calendar.get_freq()
-                if current_temp.get_stock_count(code, bar=time_per_step) < self.hold_thresh:
-                    continue
-                # sell order
-                sell_amount = current_temp.get_stock_amount(code=code)
-                # sell_amount = self.trade_exchange.round_amount_by_trade_unit(sell_amount, factor)
-                sell_order = Order(
-                    stock_id=code,
-                    amount=sell_amount,
-                    start_time=trade_start_time,
-                    end_time=trade_end_time,
-                    direction=Order.SELL,  # 0 for sell, 1 for buy
-                )
-                # is order executable
-                if self.trade_exchange.check_order(sell_order):
-                    sell_order_list.append(sell_order)
-                    trade_val, trade_cost, trade_price = self.trade_exchange.deal_order(
-                        sell_order, position=current_temp
-                    )
-                    # update cash
-                    cash += trade_val - trade_cost
-        # buy new stock
-        # note the current has been changed
-        # current_stock_list = current_temp.get_stock_list()
-        value = cash * self.risk_degree / len(buy) if len(buy) > 0 else 0
-
-        # open_cost should be considered in the real trading environment, while the backtest in evaluate.py does not
-        # consider it as the aim of demo is to accomplish same strategy as evaluate.py, so comment out this line
-        # value = value / (1+self.trade_exchange.open_cost) # set open_cost limit
-        for code in buy:
-            # check is stock suspended
-            if not self.trade_exchange.is_stock_tradable(
-                stock_id=code,
-                start_time=trade_start_time,
-                end_time=trade_end_time,
-                direction=None if self.forbid_all_trade_at_limit else OrderDir.BUY,
-            ):
-                continue
-            # buy order
-            buy_price = self.trade_exchange.get_deal_price(
-                stock_id=code, start_time=trade_start_time, end_time=trade_end_time, direction=OrderDir.BUY
-            )
-            buy_amount = value / buy_price
-            factor = self.trade_exchange.get_factor(stock_id=code, start_time=trade_start_time, end_time=trade_end_time)
-            buy_amount = self.trade_exchange.round_amount_by_trade_unit(buy_amount, factor)
-            buy_order = Order(
-                stock_id=code,
-                amount=buy_amount,
-                start_time=trade_start_time,
-                end_time=trade_end_time,
-                direction=Order.BUY,  # 1 for buy
-            )
-            buy_order_list.append(buy_order)
-        return TradeDecisionWO(sell_order_list + buy_order_list, self)
-
-
-class WeightStrategyBase(BaseSignalStrategy):
-    # TODO:
-    # 1. Supporting leverage the get_range_limit result from the decision
-    # 2. Supporting alter_outer_trade_decision
-    # 3. Supporting checking the availability of trade decision
-    def __init__(
-        self,
-        *,
-        order_generator_cls_or_obj=OrderGenWOInteract,
-        **kwargs,
-    ):
-        """
-        signal :
-            the information to describe a signal. Please refer to the docs of `qlib.backtest.signal.create_signal_from`
-            the decision of the strategy will base on the given signal
-        trade_exchange : Exchange
-            exchange that provides market info, used to deal order and generate report
-
-            - If `trade_exchange` is None, self.trade_exchange will be set with common_infra
-            - It allowes different trade_exchanges is used in different executions.
-            - For example:
-
-                - In daily execution, both daily exchange and minutely are usable, but the daily exchange is recommended because it runs faster.
-                - In minutely execution, the daily exchange is not usable, only the minutely exchange is recommended.
-        """
-        super().__init__(**kwargs)
-
-        if isinstance(order_generator_cls_or_obj, type):
-            self.order_generator: OrderGenerator = order_generator_cls_or_obj()
-        else:
-            self.order_generator: OrderGenerator = order_generator_cls_or_obj
-
-    def generate_target_weight_position(self, score, current, trade_start_time, trade_end_time):
-        """
-        Generate target position from score for this date and the current position.The cash is not considered in the position
-
-        Parameters
-        -----------
-        score : pd.Series
-            pred score for this trade date, index is stock_id, contain 'score' column.
-        current : Position()
-            current position.
-        trade_start_time: pd.Timestamp
-        trade_end_time: pd.Timestamp
-        """
-        raise NotImplementedError()
-
-    def generate_trade_decision(self, execute_result=None):
-        # generate_trade_decision
-        # generate_target_weight_position() and generate_order_list_from_target_weight_position() to generate order_list
-
-        # get the number of trading step finished, trade_step can be [0, 1, 2, ..., trade_len - 1]
-        trade_step = self.trade_calendar.get_trade_step()
-        trade_start_time, trade_end_time = self.trade_calendar.get_step_time(trade_step)
-        pred_start_time, pred_end_time = self.trade_calendar.get_step_time(trade_step, shift=1)
-        pred_score = self.signal.get_signal(start_time=pred_start_time, end_time=pred_end_time)
-        if pred_score is None:
-            return TradeDecisionWO([], self)
-        current_temp = copy.deepcopy(self.trade_position)
-        assert isinstance(current_temp, Position)  # Avoid InfPosition
-
-        target_weight_position = self.generate_target_weight_position(
-            score=pred_score, current=current_temp, trade_start_time=trade_start_time, trade_end_time=trade_end_time
-        )
-        order_list = self.order_generator.generate_order_list_from_target_weight_position(
-            current=current_temp,
-            trade_exchange=self.trade_exchange,
-            risk_degree=self.get_risk_degree(trade_step),
-            target_weight_position=target_weight_position,
-            pred_start_time=pred_start_time,
-            pred_end_time=pred_end_time,
-            trade_start_time=trade_start_time,
-            trade_end_time=trade_end_time,
-        )
-        return TradeDecisionWO(order_list, self)
-
-
-class EnhancedIndexingStrategy(WeightStrategyBase):
-
-    """Enhanced Indexing Strategy
-
-    Enhanced indexing combines the arts of active management and passive management,
-    with the aim of outperforming a benchmark index (e.g., S&P 500) in terms of
-    portfolio return while controlling the risk exposure (a.k.a. tracking error).
-
-    Users need to prepare their risk model data like below:
-
-    .. code-block:: text
-
-        ├── /path/to/riskmodel
-        ├──── 20210101
-        ├────── factor_exp.{csv|pkl|h5}
-        ├────── factor_cov.{csv|pkl|h5}
-        ├────── specific_risk.{csv|pkl|h5}
-        ├────── blacklist.{csv|pkl|h5}  # optional
-
-    The risk model data can be obtained from risk data provider. You can also use
-    `qlib.model.riskmodel.structured.StructuredCovEstimator` to prepare these data.
-
-    Args:
-        riskmodel_path (str): risk model path
-        name_mapping (dict): alternative file names
-    """
-
-    FACTOR_EXP_NAME = "factor_exp.pkl"
-    FACTOR_COV_NAME = "factor_cov.pkl"
-    SPECIFIC_RISK_NAME = "specific_risk.pkl"
-    BLACKLIST_NAME = "blacklist.pkl"
-
-    def __init__(
-        self,
-        *,
-        riskmodel_root,
-        market="csi500",
-        turn_limit=None,
-        name_mapping={},
-        optimizer_kwargs={},
-        verbose=False,
-        **kwargs,
-    ):
-        super().__init__(**kwargs)
-
-        self.logger = get_module_logger("EnhancedIndexingStrategy")
-
-        self.riskmodel_root = riskmodel_root
-        self.market = market
-        self.turn_limit = turn_limit
-
-        self.factor_exp_path = name_mapping.get("factor_exp", self.FACTOR_EXP_NAME)
-        self.factor_cov_path = name_mapping.get("factor_cov", self.FACTOR_COV_NAME)
-        self.specific_risk_path = name_mapping.get("specific_risk", self.SPECIFIC_RISK_NAME)
-        self.blacklist_path = name_mapping.get("blacklist", self.BLACKLIST_NAME)
-
-        self.optimizer = EnhancedIndexingOptimizer(**optimizer_kwargs)
-
-        self.verbose = verbose
-
-        self._riskdata_cache = {}
-
-    def get_risk_data(self, date):
-
-        if date in self._riskdata_cache:
-            return self._riskdata_cache[date]
-
-        root = self.riskmodel_root + "/" + date.strftime("%Y%m%d")
-        if not os.path.exists(root):
-            return None
-
-        factor_exp = load_dataset(root + "/" + self.factor_exp_path, index_col=[0])
-        factor_cov = load_dataset(root + "/" + self.factor_cov_path, index_col=[0])
-        specific_risk = load_dataset(root + "/" + self.specific_risk_path, index_col=[0])
-
-        if not factor_exp.index.equals(specific_risk.index):
-            # NOTE: for stocks missing specific_risk, we always assume it has the highest volatility
-            specific_risk = specific_risk.reindex(factor_exp.index, fill_value=specific_risk.max())
-
-        universe = factor_exp.index.tolist()
-
-        blacklist = []
-        if os.path.exists(root + "/" + self.blacklist_path):
-            blacklist = load_dataset(root + "/" + self.blacklist_path).index.tolist()
-
-        self._riskdata_cache[date] = factor_exp.values, factor_cov.values, specific_risk.values, universe, blacklist
-
-        return self._riskdata_cache[date]
-
-    def generate_target_weight_position(self, score, current, trade_start_time, trade_end_time):
-
-        trade_date = trade_start_time
-        pre_date = get_pre_trading_date(trade_date, future=True)  # previous trade date
-
-        # load risk data
-        outs = self.get_risk_data(pre_date)
-        if outs is None:
-            self.logger.warning(f"no risk data for {pre_date:%Y-%m-%d}, skip optimization")
-            return None
-        factor_exp, factor_cov, specific_risk, universe, blacklist = outs
-
-        # transform score
-        # NOTE: for stocks missing score, we always assume they have the lowest score
-        score = score.reindex(universe).fillna(score.min()).values
-
-        # get current weight
-        # NOTE: if a stock is not in universe, its current weight will be zero
-        cur_weight = current.get_stock_weight_dict(only_stock=False)
-        cur_weight = np.array([cur_weight.get(stock, 0) for stock in universe])
-        assert all(cur_weight >= 0), "current weight has negative values"
-        cur_weight = cur_weight / self.get_risk_degree(trade_date)  # sum of weight should be risk_degree
-        if cur_weight.sum() > 1 and self.verbose:
-            self.logger.warning(f"previous total holdings excess risk degree (current: {cur_weight.sum()})")
-
-        # load bench weight
-        bench_weight = D.features(
-            D.instruments("all"), [f"${self.market}_weight"], start_time=pre_date, end_time=pre_date
-        ).squeeze()
-        bench_weight.index = bench_weight.index.droplevel(level="datetime")
-        bench_weight = bench_weight.reindex(universe).fillna(0).values
-
-        # whether stock tradable
-        # NOTE: currently we use last day volume to check whether tradable
-        tradable = D.features(D.instruments("all"), ["$volume"], start_time=pre_date, end_time=pre_date).squeeze()
-        tradable.index = tradable.index.droplevel(level="datetime")
-        tradable = tradable.reindex(universe).gt(0).values
-        mask_force_hold = ~tradable
-
-        # mask force sell
-        mask_force_sell = np.array([stock in blacklist for stock in universe], dtype=bool)
-
-        # optimize
-        weight = self.optimizer(
-            r=score,
-            F=factor_exp,
-            cov_b=factor_cov,
-            var_u=specific_risk**2,
-            w0=cur_weight,
-            wb=bench_weight,
-            mfh=mask_force_hold,
-            mfs=mask_force_sell,
-        )
-
-        target_weight_position = {stock: weight for stock, weight in zip(universe, weight) if weight > 0}
-
-        if self.verbose:
-            self.logger.info("trade date: {:%Y-%m-%d}".format(trade_date))
-            self.logger.info("number of holding stocks: {}".format(len(target_weight_position)))
-            self.logger.info("total holding weight: {:.6f}".format(weight.sum()))
-
-        return target_weight_position
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+import os
+import copy
+import warnings
+import numpy as np
+import pandas as pd
+
+from typing import Dict, List, Text, Tuple, Union
+from abc import ABC
+
+from qlib.data import D
+from qlib.data.dataset import Dataset
+from qlib.model.base import BaseModel
+from qlib.strategy.base import BaseStrategy
+from qlib.backtest.position import Position
+from qlib.backtest.signal import Signal, create_signal_from
+from qlib.backtest.decision import Order, OrderDir, TradeDecisionWO
+from qlib.log import get_module_logger
+from qlib.utils import get_pre_trading_date, load_dataset
+from qlib.contrib.strategy.order_generator import OrderGenerator, OrderGenWOInteract
+from qlib.contrib.strategy.optimizer import EnhancedIndexingOptimizer
+
+
+class BaseSignalStrategy(BaseStrategy, ABC):
+    def __init__(
+        self,
+        *,
+        signal: Union[Signal, Tuple[BaseModel, Dataset], List, Dict, Text, pd.Series, pd.DataFrame] = None,
+        model=None,
+        dataset=None,
+        risk_degree: float = 0.95,
+        trade_exchange=None,
+        level_infra=None,
+        common_infra=None,
+        **kwargs,
+    ):
+        """
+        Parameters
+        -----------
+        signal :
+            the information to describe a signal. Please refer to the docs of `qlib.backtest.signal.create_signal_from`
+            the decision of the strategy will base on the given signal
+        risk_degree : float
+            position percentage of total value.
+        trade_exchange : Exchange
+            exchange that provides market info, used to deal order and generate report
+            - If `trade_exchange` is None, self.trade_exchange will be set with common_infra
+            - It allowes different trade_exchanges is used in different executions.
+            - For example:
+                - In daily execution, both daily exchange and minutely are usable, but the daily exchange is recommended because it runs faster.
+                - In minutely execution, the daily exchange is not usable, only the minutely exchange is recommended.
+
+        """
+        super().__init__(level_infra=level_infra, common_infra=common_infra, trade_exchange=trade_exchange, **kwargs)
+
+        self.risk_degree = risk_degree
+
+        # This is trying to be compatible with previous version of qlib task config
+        if model is not None and dataset is not None:
+            warnings.warn("`model` `dataset` is deprecated; use `signal`.", DeprecationWarning)
+            signal = model, dataset
+
+        self.signal: Signal = create_signal_from(signal)
+
+    def get_risk_degree(self, trade_step=None):
+        """get_risk_degree
+        Return the proportion of your total value you will use in investment.
+        Dynamically risk_degree will result in Market timing.
+        """
+        # It will use 95% amount of your total value by default
+        return self.risk_degree
+
+
+class TopkDropoutStrategy(BaseSignalStrategy):
+    # TODO:
+    # 1. Supporting leverage the get_range_limit result from the decision
+    # 2. Supporting alter_outer_trade_decision
+    # 3. Supporting checking the availability of trade decision
+    # 4. Regenerate results with forbid_all_trade_at_limit set to false and flip the default to false, as it is consistent with reality.
+    def __init__(
+        self,
+        *,
+        topk,
+        n_drop,
+        method_sell="bottom",
+        method_buy="top",
+        hold_thresh=1,
+        only_tradable=False,
+        forbid_all_trade_at_limit=True,
+        **kwargs,
+    ):
+        """
+        Parameters
+        -----------
+        topk : int
+            the number of stocks in the portfolio.
+        n_drop : int
+            number of stocks to be replaced in each trading date.
+        method_sell : str
+            dropout method_sell, random/bottom.
+        method_buy : str
+            dropout method_buy, random/top.
+        hold_thresh : int
+            minimum holding days
+            before sell stock , will check current.get_stock_count(order.stock_id) >= self.hold_thresh.
+        only_tradable : bool
+            will the strategy only consider the tradable stock when buying and selling.
+
+            if only_tradable:
+
+                strategy will make decision with the tradable state of the stock info and avoid buy and sell them.
+
+            else:
+
+                strategy will make buy sell decision without checking the tradable state of the stock.
+        forbid_all_trade_at_limit : bool
+            if forbid all trades when limit_up or limit_down reached.
+
+            if forbid_all_trade_at_limit:
+
+                strategy will not do any trade when price reaches limit up/down, even not sell at limit up nor buy at
+                limit down, though allowed in reality.
+
+            else:
+
+                strategy will sell at limit up and buy ad limit down.
+        """
+        super().__init__(**kwargs)
+        self.topk = topk
+        self.n_drop = n_drop
+        self.method_sell = method_sell
+        self.method_buy = method_buy
+        self.hold_thresh = hold_thresh
+        self.only_tradable = only_tradable
+        self.forbid_all_trade_at_limit = forbid_all_trade_at_limit
+
+    def generate_trade_decision(self, execute_result=None):
+        # get the number of trading step finished, trade_step can be [0, 1, 2, ..., trade_len - 1]
+        trade_step = self.trade_calendar.get_trade_step()
+        trade_start_time, trade_end_time = self.trade_calendar.get_step_time(trade_step)
+        pred_start_time, pred_end_time = self.trade_calendar.get_step_time(trade_step, shift=1)
+        pred_score = self.signal.get_signal(start_time=pred_start_time, end_time=pred_end_time)
+        # NOTE: the current version of topk dropout strategy can't handle pd.DataFrame(multiple signal)
+        # So it only leverage the first col of signal
+        if isinstance(pred_score, pd.DataFrame):
+            pred_score = pred_score.iloc[:, 0]
+        if pred_score is None:
+            return TradeDecisionWO([], self)
+        if self.only_tradable:
+            # If The strategy only consider tradable stock when make decision
+            # It needs following actions to filter stocks
+            def get_first_n(li, n, reverse=False):
+                cur_n = 0
+                res = []
+                for si in reversed(li) if reverse else li:
+                    if self.trade_exchange.is_stock_tradable(
+                        stock_id=si, start_time=trade_start_time, end_time=trade_end_time
+                    ):
+                        res.append(si)
+                        cur_n += 1
+                        if cur_n >= n:
+                            break
+                return res[::-1] if reverse else res
+
+            def get_last_n(li, n):
+                return get_first_n(li, n, reverse=True)
+
+            def filter_stock(li):
+                return [
+                    si
+                    for si in li
+                    if self.trade_exchange.is_stock_tradable(
+                        stock_id=si, start_time=trade_start_time, end_time=trade_end_time
+                    )
+                ]
+
+        else:
+            # Otherwise, the stock will make decision without the stock tradable info
+            def get_first_n(li, n):
+                return list(li)[:n]
+
+            def get_last_n(li, n):
+                return list(li)[-n:]
+
+            def filter_stock(li):
+                return li
+
+        current_temp: Position = copy.deepcopy(self.trade_position)
+        # generate order list for this adjust date
+        sell_order_list = []
+        buy_order_list = []
+        # load score
+        cash = current_temp.get_cash()
+        current_stock_list = current_temp.get_stock_list()
+        # last position (sorted by score)
+        last = pred_score.reindex(current_stock_list).sort_values(ascending=False).index
+        # The new stocks today want to buy **at most**
+        if self.method_buy == "top":
+            today = get_first_n(
+                pred_score[~pred_score.index.isin(last)].sort_values(ascending=False).index,
+                self.n_drop + self.topk - len(last),
+            )
+        elif self.method_buy == "random":
+            topk_candi = get_first_n(pred_score.sort_values(ascending=False).index, self.topk)
+            candi = list(filter(lambda x: x not in last, topk_candi))
+            n = self.n_drop + self.topk - len(last)
+            try:
+                today = np.random.choice(candi, n, replace=False)
+            except ValueError:
+                today = candi
+        else:
+            raise NotImplementedError(f"This type of input is not supported")
+        # combine(new stocks + last stocks),  we will drop stocks from this list
+        # In case of dropping higher score stock and buying lower score stock.
+        comb = pred_score.reindex(last.union(pd.Index(today))).sort_values(ascending=False).index
+
+        # Get the stock list we really want to sell (After filtering the case that we sell high and buy low)
+        if self.method_sell == "bottom":
+            sell = last[last.isin(get_last_n(comb, self.n_drop))]
+        elif self.method_sell == "random":
+            candi = filter_stock(last)
+            try:
+                sell = pd.Index(np.random.choice(candi, self.n_drop, replace=False) if len(last) else [])
+            except ValueError:  # No enough candidates
+                sell = candi
+        else:
+            raise NotImplementedError(f"This type of input is not supported")
+
+        # Get the stock list we really want to buy
+        buy = today[: len(sell) + self.topk - len(last)]
+        for code in current_stock_list:
+            if not self.trade_exchange.is_stock_tradable(
+                stock_id=code,
+                start_time=trade_start_time,
+                end_time=trade_end_time,
+                direction=None if self.forbid_all_trade_at_limit else OrderDir.SELL,
+            ):
+                continue
+            if code in sell:
+                # check hold limit
+                time_per_step = self.trade_calendar.get_freq()
+                if current_temp.get_stock_count(code, bar=time_per_step) < self.hold_thresh:
+                    continue
+                # sell order
+                sell_amount = current_temp.get_stock_amount(code=code)
+                # sell_amount = self.trade_exchange.round_amount_by_trade_unit(sell_amount, factor)
+                sell_order = Order(
+                    stock_id=code,
+                    amount=sell_amount,
+                    start_time=trade_start_time,
+                    end_time=trade_end_time,
+                    direction=Order.SELL,  # 0 for sell, 1 for buy
+                )
+                # is order executable
+                if self.trade_exchange.check_order(sell_order):
+                    sell_order_list.append(sell_order)
+                    trade_val, trade_cost, trade_price = self.trade_exchange.deal_order(
+                        sell_order, position=current_temp
+                    )
+                    # update cash
+                    cash += trade_val - trade_cost
+        # buy new stock
+        # note the current has been changed
+        # current_stock_list = current_temp.get_stock_list()
+        value = cash * self.risk_degree / len(buy) if len(buy) > 0 else 0
+
+        # open_cost should be considered in the real trading environment, while the backtest in evaluate.py does not
+        # consider it as the aim of demo is to accomplish same strategy as evaluate.py, so comment out this line
+        # value = value / (1+self.trade_exchange.open_cost) # set open_cost limit
+        for code in buy:
+            # check is stock suspended
+            if not self.trade_exchange.is_stock_tradable(
+                stock_id=code,
+                start_time=trade_start_time,
+                end_time=trade_end_time,
+                direction=None if self.forbid_all_trade_at_limit else OrderDir.BUY,
+            ):
+                continue
+            # buy order
+            buy_price = self.trade_exchange.get_deal_price(
+                stock_id=code, start_time=trade_start_time, end_time=trade_end_time, direction=OrderDir.BUY
+            )
+            buy_amount = value / buy_price
+            factor = self.trade_exchange.get_factor(stock_id=code, start_time=trade_start_time, end_time=trade_end_time)
+            buy_amount = self.trade_exchange.round_amount_by_trade_unit(buy_amount, factor)
+            buy_order = Order(
+                stock_id=code,
+                amount=buy_amount,
+                start_time=trade_start_time,
+                end_time=trade_end_time,
+                direction=Order.BUY,  # 1 for buy
+            )
+            buy_order_list.append(buy_order)
+        return TradeDecisionWO(sell_order_list + buy_order_list, self)
+
+
+class WeightStrategyBase(BaseSignalStrategy):
+    # TODO:
+    # 1. Supporting leverage the get_range_limit result from the decision
+    # 2. Supporting alter_outer_trade_decision
+    # 3. Supporting checking the availability of trade decision
+    def __init__(
+        self,
+        *,
+        order_generator_cls_or_obj=OrderGenWOInteract,
+        **kwargs,
+    ):
+        """
+        signal :
+            the information to describe a signal. Please refer to the docs of `qlib.backtest.signal.create_signal_from`
+            the decision of the strategy will base on the given signal
+        trade_exchange : Exchange
+            exchange that provides market info, used to deal order and generate report
+
+            - If `trade_exchange` is None, self.trade_exchange will be set with common_infra
+            - It allowes different trade_exchanges is used in different executions.
+            - For example:
+
+                - In daily execution, both daily exchange and minutely are usable, but the daily exchange is recommended because it runs faster.
+                - In minutely execution, the daily exchange is not usable, only the minutely exchange is recommended.
+        """
+        super().__init__(**kwargs)
+
+        if isinstance(order_generator_cls_or_obj, type):
+            self.order_generator: OrderGenerator = order_generator_cls_or_obj()
+        else:
+            self.order_generator: OrderGenerator = order_generator_cls_or_obj
+
+    def generate_target_weight_position(self, score, current, trade_start_time, trade_end_time):
+        """
+        Generate target position from score for this date and the current position.The cash is not considered in the position
+
+        Parameters
+        -----------
+        score : pd.Series
+            pred score for this trade date, index is stock_id, contain 'score' column.
+        current : Position()
+            current position.
+        trade_start_time: pd.Timestamp
+        trade_end_time: pd.Timestamp
+        """
+        raise NotImplementedError()
+
+    def generate_trade_decision(self, execute_result=None):
+        # generate_trade_decision
+        # generate_target_weight_position() and generate_order_list_from_target_weight_position() to generate order_list
+
+        # get the number of trading step finished, trade_step can be [0, 1, 2, ..., trade_len - 1]
+        trade_step = self.trade_calendar.get_trade_step()
+        trade_start_time, trade_end_time = self.trade_calendar.get_step_time(trade_step)
+        pred_start_time, pred_end_time = self.trade_calendar.get_step_time(trade_step, shift=1)
+        pred_score = self.signal.get_signal(start_time=pred_start_time, end_time=pred_end_time)
+        if pred_score is None:
+            return TradeDecisionWO([], self)
+        current_temp = copy.deepcopy(self.trade_position)
+        assert isinstance(current_temp, Position)  # Avoid InfPosition
+
+        target_weight_position = self.generate_target_weight_position(
+            score=pred_score, current=current_temp, trade_start_time=trade_start_time, trade_end_time=trade_end_time
+        )
+        order_list = self.order_generator.generate_order_list_from_target_weight_position(
+            current=current_temp,
+            trade_exchange=self.trade_exchange,
+            risk_degree=self.get_risk_degree(trade_step),
+            target_weight_position=target_weight_position,
+            pred_start_time=pred_start_time,
+            pred_end_time=pred_end_time,
+            trade_start_time=trade_start_time,
+            trade_end_time=trade_end_time,
+        )
+        return TradeDecisionWO(order_list, self)
+
+
+class EnhancedIndexingStrategy(WeightStrategyBase):
+
+    """Enhanced Indexing Strategy
+
+    Enhanced indexing combines the arts of active management and passive management,
+    with the aim of outperforming a benchmark index (e.g., S&P 500) in terms of
+    portfolio return while controlling the risk exposure (a.k.a. tracking error).
+
+    Users need to prepare their risk model data like below:
+
+    .. code-block:: text
+
+        ├── /path/to/riskmodel
+        ├──── 20210101
+        ├────── factor_exp.{csv|pkl|h5}
+        ├────── factor_cov.{csv|pkl|h5}
+        ├────── specific_risk.{csv|pkl|h5}
+        ├────── blacklist.{csv|pkl|h5}  # optional
+
+    The risk model data can be obtained from risk data provider. You can also use
+    `qlib.model.riskmodel.structured.StructuredCovEstimator` to prepare these data.
+
+    Args:
+        riskmodel_path (str): risk model path
+        name_mapping (dict): alternative file names
+    """
+
+    FACTOR_EXP_NAME = "factor_exp.pkl"
+    FACTOR_COV_NAME = "factor_cov.pkl"
+    SPECIFIC_RISK_NAME = "specific_risk.pkl"
+    BLACKLIST_NAME = "blacklist.pkl"
+
+    def __init__(
+        self,
+        *,
+        riskmodel_root,
+        market="csi500",
+        turn_limit=None,
+        name_mapping={},
+        optimizer_kwargs={},
+        verbose=False,
+        **kwargs,
+    ):
+        super().__init__(**kwargs)
+
+        self.logger = get_module_logger("EnhancedIndexingStrategy")
+
+        self.riskmodel_root = riskmodel_root
+        self.market = market
+        self.turn_limit = turn_limit
+
+        self.factor_exp_path = name_mapping.get("factor_exp", self.FACTOR_EXP_NAME)
+        self.factor_cov_path = name_mapping.get("factor_cov", self.FACTOR_COV_NAME)
+        self.specific_risk_path = name_mapping.get("specific_risk", self.SPECIFIC_RISK_NAME)
+        self.blacklist_path = name_mapping.get("blacklist", self.BLACKLIST_NAME)
+
+        self.optimizer = EnhancedIndexingOptimizer(**optimizer_kwargs)
+
+        self.verbose = verbose
+
+        self._riskdata_cache = {}
+
+    def get_risk_data(self, date):
+        if date in self._riskdata_cache:
+            return self._riskdata_cache[date]
+
+        root = self.riskmodel_root + "/" + date.strftime("%Y%m%d")
+        if not os.path.exists(root):
+            return None
+
+        factor_exp = load_dataset(root + "/" + self.factor_exp_path, index_col=[0])
+        factor_cov = load_dataset(root + "/" + self.factor_cov_path, index_col=[0])
+        specific_risk = load_dataset(root + "/" + self.specific_risk_path, index_col=[0])
+
+        if not factor_exp.index.equals(specific_risk.index):
+            # NOTE: for stocks missing specific_risk, we always assume it has the highest volatility
+            specific_risk = specific_risk.reindex(factor_exp.index, fill_value=specific_risk.max())
+
+        universe = factor_exp.index.tolist()
+
+        blacklist = []
+        if os.path.exists(root + "/" + self.blacklist_path):
+            blacklist = load_dataset(root + "/" + self.blacklist_path).index.tolist()
+
+        self._riskdata_cache[date] = factor_exp.values, factor_cov.values, specific_risk.values, universe, blacklist
+
+        return self._riskdata_cache[date]
+
+    def generate_target_weight_position(self, score, current, trade_start_time, trade_end_time):
+        trade_date = trade_start_time
+        pre_date = get_pre_trading_date(trade_date, future=True)  # previous trade date
+
+        # load risk data
+        outs = self.get_risk_data(pre_date)
+        if outs is None:
+            self.logger.warning(f"no risk data for {pre_date:%Y-%m-%d}, skip optimization")
+            return None
+        factor_exp, factor_cov, specific_risk, universe, blacklist = outs
+
+        # transform score
+        # NOTE: for stocks missing score, we always assume they have the lowest score
+        score = score.reindex(universe).fillna(score.min()).values
+
+        # get current weight
+        # NOTE: if a stock is not in universe, its current weight will be zero
+        cur_weight = current.get_stock_weight_dict(only_stock=False)
+        cur_weight = np.array([cur_weight.get(stock, 0) for stock in universe])
+        assert all(cur_weight >= 0), "current weight has negative values"
+        cur_weight = cur_weight / self.get_risk_degree(trade_date)  # sum of weight should be risk_degree
+        if cur_weight.sum() > 1 and self.verbose:
+            self.logger.warning(f"previous total holdings excess risk degree (current: {cur_weight.sum()})")
+
+        # load bench weight
+        bench_weight = D.features(
+            D.instruments("all"), [f"${self.market}_weight"], start_time=pre_date, end_time=pre_date
+        ).squeeze()
+        bench_weight.index = bench_weight.index.droplevel(level="datetime")
+        bench_weight = bench_weight.reindex(universe).fillna(0).values
+
+        # whether stock tradable
+        # NOTE: currently we use last day volume to check whether tradable
+        tradable = D.features(D.instruments("all"), ["$volume"], start_time=pre_date, end_time=pre_date).squeeze()
+        tradable.index = tradable.index.droplevel(level="datetime")
+        tradable = tradable.reindex(universe).gt(0).values
+        mask_force_hold = ~tradable
+
+        # mask force sell
+        mask_force_sell = np.array([stock in blacklist for stock in universe], dtype=bool)
+
+        # optimize
+        weight = self.optimizer(
+            r=score,
+            F=factor_exp,
+            cov_b=factor_cov,
+            var_u=specific_risk**2,
+            w0=cur_weight,
+            wb=bench_weight,
+            mfh=mask_force_hold,
+            mfs=mask_force_sell,
+        )
+
+        target_weight_position = {stock: weight for stock, weight in zip(universe, weight) if weight > 0}
+
+        if self.verbose:
+            self.logger.info("trade date: {:%Y-%m-%d}".format(trade_date))
+            self.logger.info("number of holding stocks: {}".format(len(target_weight_position)))
+            self.logger.info("total holding weight: {:.6f}".format(weight.sum()))
+
+        return target_weight_position
```

## qlib/contrib/strategy/optimizer/__init__.py

 * *Ordering differences only*

```diff
@@ -1,9 +1,9 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-from .base import BaseOptimizer
-from .optimizer import PortfolioOptimizer
-from .enhanced_indexing import EnhancedIndexingOptimizer
-
-
-__all__ = ["BaseOptimizer", "PortfolioOptimizer", "EnhancedIndexingOptimizer"]
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+from .base import BaseOptimizer
+from .optimizer import PortfolioOptimizer
+from .enhanced_indexing import EnhancedIndexingOptimizer
+
+
+__all__ = ["BaseOptimizer", "PortfolioOptimizer", "EnhancedIndexingOptimizer"]
```

## qlib/contrib/strategy/optimizer/base.py

 * *Ordering differences only*

```diff
@@ -1,12 +1,12 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-import abc
-
-
-class BaseOptimizer(abc.ABC):
-    """Construct portfolio with a optimization related method"""
-
-    @abc.abstractmethod
-    def __call__(self, *args, **kwargs) -> object:
-        """Generate a optimized portfolio allocation"""
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+import abc
+
+
+class BaseOptimizer(abc.ABC):
+    """Construct portfolio with a optimization related method"""
+
+    @abc.abstractmethod
+    def __call__(self, *args, **kwargs) -> object:
+        """Generate a optimized portfolio allocation"""
```

## qlib/contrib/strategy/optimizer/enhanced_indexing.py

 * *Ordering differences only*

```diff
@@ -1,202 +1,202 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-import numpy as np
-import cvxpy as cp
-
-from typing import Union, Optional, Dict, Any, List
-
-from qlib.log import get_module_logger
-from .base import BaseOptimizer
-
-
-logger = get_module_logger("EnhancedIndexingOptimizer")
-
-
-class EnhancedIndexingOptimizer(BaseOptimizer):
-    """
-    Portfolio Optimizer for Enhanced Indexing
-
-    Notations:
-        w0: current holding weights
-        wb: benchmark weight
-        r: expected return
-        F: factor exposure
-        cov_b: factor covariance
-        var_u: residual variance (diagonal)
-        lamb: risk aversion parameter
-        delta: total turnover limit
-        b_dev: benchmark deviation limit
-        f_dev: factor deviation limit
-
-    Also denote:
-        d = w - wb: benchmark deviation
-        v = d @ F: factor deviation
-
-    The optimization problem for enhanced indexing:
-        max_w  d @ r - lamb * (v @ cov_b @ v + var_u @ d**2)
-        s.t.   w >= 0
-               sum(w) == 1
-               sum(|w - w0|) <= delta
-               d >= -b_dev
-               d <= b_dev
-               v >= -f_dev
-               v <= f_dev
-    """
-
-    def __init__(
-        self,
-        lamb: float = 1,
-        delta: Optional[float] = 0.2,
-        b_dev: Optional[float] = 0.01,
-        f_dev: Optional[Union[List[float], np.ndarray]] = None,
-        scale_return: bool = True,
-        epsilon: float = 5e-5,
-        solver_kwargs: Optional[Dict[str, Any]] = {},
-    ):
-        """
-        Args:
-            lamb (float): risk aversion parameter (larger `lamb` means more focus on risk)
-            delta (float): total turnover limit
-            b_dev (float): benchmark deviation limit
-            f_dev (list): factor deviation limit
-            scale_return (bool): whether scale return to match estimated volatility
-            epsilon (float): minimum weight
-            solver_kwargs (dict): kwargs for cvxpy solver
-        """
-
-        assert lamb >= 0, "risk aversion parameter `lamb` should be positive"
-        self.lamb = lamb
-
-        assert delta >= 0, "turnover limit `delta` should be positive"
-        self.delta = delta
-
-        assert b_dev is None or b_dev >= 0, "benchmark deviation limit `b_dev` should be positive"
-        self.b_dev = b_dev
-
-        if isinstance(f_dev, float):
-            assert f_dev >= 0, "factor deviation limit `f_dev` should be positive"
-        elif f_dev is not None:
-            f_dev = np.array(f_dev)
-            assert all(f_dev >= 0), "factor deviation limit `f_dev` should be positive"
-        self.f_dev = f_dev
-
-        self.scale_return = scale_return
-        self.epsilon = epsilon
-        self.solver_kwargs = solver_kwargs
-
-    def __call__(
-        self,
-        r: np.ndarray,
-        F: np.ndarray,
-        cov_b: np.ndarray,
-        var_u: np.ndarray,
-        w0: np.ndarray,
-        wb: np.ndarray,
-        mfh: Optional[np.ndarray] = None,
-        mfs: Optional[np.ndarray] = None,
-    ) -> np.ndarray:
-        """
-        Args:
-            r (np.ndarray): expected returns
-            F (np.ndarray): factor exposure
-            cov_b (np.ndarray): factor covariance
-            var_u (np.ndarray): residual variance
-            w0 (np.ndarray): current holding weights
-            wb (np.ndarray): benchmark weights
-            mfh (np.ndarray): mask force holding
-            mfs (np.ndarray): mask force selling
-
-        Returns:
-            np.ndarray: optimized portfolio allocation
-        """
-        # scale return to match volatility
-        if self.scale_return:
-            r = r / r.std()
-            r *= np.sqrt(np.mean(np.diag(F @ cov_b @ F.T) + var_u))
-
-        # target weight
-        w = cp.Variable(len(r), nonneg=True)
-        w.value = wb  # for warm start
-
-        # precompute exposure
-        d = w - wb  # benchmark exposure
-        v = d @ F  # factor exposure
-
-        # objective
-        ret = d @ r  # excess return
-        risk = cp.quad_form(v, cov_b) + var_u @ (d**2)  # tracking error
-        obj = cp.Maximize(ret - self.lamb * risk)
-
-        # weight bounds
-        lb = np.zeros_like(wb)
-        ub = np.ones_like(wb)
-
-        # bench bounds
-        if self.b_dev is not None:
-            lb = np.maximum(lb, wb - self.b_dev)
-            ub = np.minimum(ub, wb + self.b_dev)
-
-        # force holding
-        if mfh is not None:
-            lb[mfh] = w0[mfh]
-            ub[mfh] = w0[mfh]
-
-        # force selling
-        # NOTE: this will override mfh
-        if mfs is not None:
-            lb[mfs] = 0
-            ub[mfs] = 0
-
-        # constraints
-        # TODO: currently we assume fullly invest in the stocks,
-        # in the future we should support holding cash as an asset
-        cons = [cp.sum(w) == 1, w >= lb, w <= ub]
-
-        # factor deviation
-        if self.f_dev is not None:
-            cons.extend([v >= -self.f_dev, v <= self.f_dev])  # pylint: disable=E1130
-
-        # total turnover constraint
-        t_cons = []
-        if self.delta is not None:
-            if w0 is not None and w0.sum() > 0:
-                t_cons.extend([cp.norm(w - w0, 1) <= self.delta])
-
-        # optimize
-        # trial 1: use all constraints
-        success = False
-        try:
-            prob = cp.Problem(obj, cons + t_cons)
-            prob.solve(solver=cp.ECOS, warm_start=True, **self.solver_kwargs)
-            assert prob.status == "optimal"
-            success = True
-        except Exception as e:
-            logger.warning(f"trial 1 failed {e} (status: {prob.status})")
-
-        # trial 2: remove turnover constraint
-        if not success and len(t_cons):
-            logger.info("try removing turnover constraint as the last optimization failed")
-            try:
-                w.value = wb
-                prob = cp.Problem(obj, cons)
-                prob.solve(solver=cp.ECOS, warm_start=True, **self.solver_kwargs)
-                assert prob.status in ["optimal", "optimal_inaccurate"]
-                success = True
-            except Exception as e:
-                logger.warning(f"trial 2 failed {e} (status: {prob.status})")
-
-        # return current weight if not success
-        if not success:
-            logger.warning("optimization failed, will return current holding weight")
-            return w0
-
-        if prob.status == "optimal_inaccurate":
-            logger.warning(f"the optimization is inaccurate")
-
-        # remove small weight
-        w = np.asarray(w.value)
-        w[w < self.epsilon] = 0
-        w /= w.sum()
-
-        return w
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+import numpy as np
+import cvxpy as cp
+
+from typing import Union, Optional, Dict, Any, List
+
+from qlib.log import get_module_logger
+from .base import BaseOptimizer
+
+
+logger = get_module_logger("EnhancedIndexingOptimizer")
+
+
+class EnhancedIndexingOptimizer(BaseOptimizer):
+    """
+    Portfolio Optimizer for Enhanced Indexing
+
+    Notations:
+        w0: current holding weights
+        wb: benchmark weight
+        r: expected return
+        F: factor exposure
+        cov_b: factor covariance
+        var_u: residual variance (diagonal)
+        lamb: risk aversion parameter
+        delta: total turnover limit
+        b_dev: benchmark deviation limit
+        f_dev: factor deviation limit
+
+    Also denote:
+        d = w - wb: benchmark deviation
+        v = d @ F: factor deviation
+
+    The optimization problem for enhanced indexing:
+        max_w  d @ r - lamb * (v @ cov_b @ v + var_u @ d**2)
+        s.t.   w >= 0
+               sum(w) == 1
+               sum(|w - w0|) <= delta
+               d >= -b_dev
+               d <= b_dev
+               v >= -f_dev
+               v <= f_dev
+    """
+
+    def __init__(
+        self,
+        lamb: float = 1,
+        delta: Optional[float] = 0.2,
+        b_dev: Optional[float] = 0.01,
+        f_dev: Optional[Union[List[float], np.ndarray]] = None,
+        scale_return: bool = True,
+        epsilon: float = 5e-5,
+        solver_kwargs: Optional[Dict[str, Any]] = {},
+    ):
+        """
+        Args:
+            lamb (float): risk aversion parameter (larger `lamb` means more focus on risk)
+            delta (float): total turnover limit
+            b_dev (float): benchmark deviation limit
+            f_dev (list): factor deviation limit
+            scale_return (bool): whether scale return to match estimated volatility
+            epsilon (float): minimum weight
+            solver_kwargs (dict): kwargs for cvxpy solver
+        """
+
+        assert lamb >= 0, "risk aversion parameter `lamb` should be positive"
+        self.lamb = lamb
+
+        assert delta >= 0, "turnover limit `delta` should be positive"
+        self.delta = delta
+
+        assert b_dev is None or b_dev >= 0, "benchmark deviation limit `b_dev` should be positive"
+        self.b_dev = b_dev
+
+        if isinstance(f_dev, float):
+            assert f_dev >= 0, "factor deviation limit `f_dev` should be positive"
+        elif f_dev is not None:
+            f_dev = np.array(f_dev)
+            assert all(f_dev >= 0), "factor deviation limit `f_dev` should be positive"
+        self.f_dev = f_dev
+
+        self.scale_return = scale_return
+        self.epsilon = epsilon
+        self.solver_kwargs = solver_kwargs
+
+    def __call__(
+        self,
+        r: np.ndarray,
+        F: np.ndarray,
+        cov_b: np.ndarray,
+        var_u: np.ndarray,
+        w0: np.ndarray,
+        wb: np.ndarray,
+        mfh: Optional[np.ndarray] = None,
+        mfs: Optional[np.ndarray] = None,
+    ) -> np.ndarray:
+        """
+        Args:
+            r (np.ndarray): expected returns
+            F (np.ndarray): factor exposure
+            cov_b (np.ndarray): factor covariance
+            var_u (np.ndarray): residual variance
+            w0 (np.ndarray): current holding weights
+            wb (np.ndarray): benchmark weights
+            mfh (np.ndarray): mask force holding
+            mfs (np.ndarray): mask force selling
+
+        Returns:
+            np.ndarray: optimized portfolio allocation
+        """
+        # scale return to match volatility
+        if self.scale_return:
+            r = r / r.std()
+            r *= np.sqrt(np.mean(np.diag(F @ cov_b @ F.T) + var_u))
+
+        # target weight
+        w = cp.Variable(len(r), nonneg=True)
+        w.value = wb  # for warm start
+
+        # precompute exposure
+        d = w - wb  # benchmark exposure
+        v = d @ F  # factor exposure
+
+        # objective
+        ret = d @ r  # excess return
+        risk = cp.quad_form(v, cov_b) + var_u @ (d**2)  # tracking error
+        obj = cp.Maximize(ret - self.lamb * risk)
+
+        # weight bounds
+        lb = np.zeros_like(wb)
+        ub = np.ones_like(wb)
+
+        # bench bounds
+        if self.b_dev is not None:
+            lb = np.maximum(lb, wb - self.b_dev)
+            ub = np.minimum(ub, wb + self.b_dev)
+
+        # force holding
+        if mfh is not None:
+            lb[mfh] = w0[mfh]
+            ub[mfh] = w0[mfh]
+
+        # force selling
+        # NOTE: this will override mfh
+        if mfs is not None:
+            lb[mfs] = 0
+            ub[mfs] = 0
+
+        # constraints
+        # TODO: currently we assume fullly invest in the stocks,
+        # in the future we should support holding cash as an asset
+        cons = [cp.sum(w) == 1, w >= lb, w <= ub]
+
+        # factor deviation
+        if self.f_dev is not None:
+            cons.extend([v >= -self.f_dev, v <= self.f_dev])  # pylint: disable=E1130
+
+        # total turnover constraint
+        t_cons = []
+        if self.delta is not None:
+            if w0 is not None and w0.sum() > 0:
+                t_cons.extend([cp.norm(w - w0, 1) <= self.delta])
+
+        # optimize
+        # trial 1: use all constraints
+        success = False
+        try:
+            prob = cp.Problem(obj, cons + t_cons)
+            prob.solve(solver=cp.ECOS, warm_start=True, **self.solver_kwargs)
+            assert prob.status == "optimal"
+            success = True
+        except Exception as e:
+            logger.warning(f"trial 1 failed {e} (status: {prob.status})")
+
+        # trial 2: remove turnover constraint
+        if not success and len(t_cons):
+            logger.info("try removing turnover constraint as the last optimization failed")
+            try:
+                w.value = wb
+                prob = cp.Problem(obj, cons)
+                prob.solve(solver=cp.ECOS, warm_start=True, **self.solver_kwargs)
+                assert prob.status in ["optimal", "optimal_inaccurate"]
+                success = True
+            except Exception as e:
+                logger.warning(f"trial 2 failed {e} (status: {prob.status})")
+
+        # return current weight if not success
+        if not success:
+            logger.warning("optimization failed, will return current holding weight")
+            return w0
+
+        if prob.status == "optimal_inaccurate":
+            logger.warning(f"the optimization is inaccurate")
+
+        # remove small weight
+        w = np.asarray(w.value)
+        w[w < self.epsilon] = 0
+        w /= w.sum()
+
+        return w
```

## qlib/contrib/strategy/optimizer/optimizer.py

```diff
@@ -1,266 +1,265 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-
-import warnings
-import numpy as np
-import pandas as pd
-import scipy.optimize as so
-from typing import Optional, Union, Callable, List
-
-from .base import BaseOptimizer
-
-
-class PortfolioOptimizer(BaseOptimizer):
-    """Portfolio Optimizer
-
-    The following optimization algorithms are supported:
-        - `gmv`: Global Minimum Variance Portfolio
-        - `mvo`: Mean Variance Optimized Portfolio
-        - `rp`: Risk Parity
-        - `inv`: Inverse Volatility
-
-    Note:
-        This optimizer always assumes full investment and no-shorting.
-    """
-
-    OPT_GMV = "gmv"
-    OPT_MVO = "mvo"
-    OPT_RP = "rp"
-    OPT_INV = "inv"
-
-    def __init__(
-        self,
-        method: str = "inv",
-        lamb: float = 0,
-        delta: float = 0,
-        alpha: float = 0.0,
-        scale_return: bool = True,
-        tol: float = 1e-8,
-    ):
-        """
-        Args:
-            method (str): portfolio optimization method
-            lamb (float): risk aversion parameter (larger `lamb` means more focus on return)
-            delta (float): turnover rate limit
-            alpha (float): l2 norm regularizer
-            scale_return (bool): if to scale alpha to match the volatility of the covariance matrix
-            tol (float): tolerance for optimization termination
-        """
-        assert method in [self.OPT_GMV, self.OPT_MVO, self.OPT_RP, self.OPT_INV], f"method `{method}` is not supported"
-        self.method = method
-
-        assert lamb >= 0, f"risk aversion parameter `lamb` should be positive"
-        self.lamb = lamb
-
-        assert delta >= 0, f"turnover limit `delta` should be positive"
-        self.delta = delta
-
-        assert alpha >= 0, f"l2 norm regularizer `alpha` should be positive"
-        self.alpha = alpha
-
-        self.tol = tol
-        self.scale_return = scale_return
-
-    def __call__(
-        self,
-        S: Union[np.ndarray, pd.DataFrame],
-        r: Optional[Union[np.ndarray, pd.Series]] = None,
-        w0: Optional[Union[np.ndarray, pd.Series]] = None,
-    ) -> Union[np.ndarray, pd.Series]:
-        """
-        Args:
-            S (np.ndarray or pd.DataFrame): covariance matrix
-            r (np.ndarray or pd.Series): expected return
-            w0 (np.ndarray or pd.Series): initial weights (for turnover control)
-
-        Returns:
-            np.ndarray or pd.Series: optimized portfolio allocation
-        """
-        # transform dataframe into array
-        index = None
-        if isinstance(S, pd.DataFrame):
-            index = S.index
-            S = S.values
-
-        # transform return
-        if r is not None:
-            assert len(r) == len(S), "`r` has mismatched shape"
-            if isinstance(r, pd.Series):
-                assert r.index.equals(index), "`r` has mismatched index"
-                r = r.values
-
-        # transform initial weights
-        if w0 is not None:
-            assert len(w0) == len(S), "`w0` has mismatched shape"
-            if isinstance(w0, pd.Series):
-                assert w0.index.equals(index), "`w0` has mismatched index"
-                w0 = w0.values
-
-        # scale return to match volatility
-        if r is not None and self.scale_return:
-            r = r / r.std()
-            r *= np.sqrt(np.mean(np.diag(S)))
-
-        # optimize
-        w = self._optimize(S, r, w0)
-
-        # restore index if needed
-        if index is not None:
-            w = pd.Series(w, index=index)
-
-        return w
-
-    def _optimize(self, S: np.ndarray, r: Optional[np.ndarray] = None, w0: Optional[np.ndarray] = None) -> np.ndarray:
-
-        # inverse volatility
-        if self.method == self.OPT_INV:
-            if r is not None:
-                warnings.warn("`r` is set but will not be used for `inv` portfolio")
-            if w0 is not None:
-                warnings.warn("`w0` is set but will not be used for `inv` portfolio")
-            return self._optimize_inv(S)
-
-        # global minimum variance
-        if self.method == self.OPT_GMV:
-            if r is not None:
-                warnings.warn("`r` is set but will not be used for `gmv` portfolio")
-            return self._optimize_gmv(S, w0)
-
-        # mean-variance
-        if self.method == self.OPT_MVO:
-            return self._optimize_mvo(S, r, w0)
-
-        # risk parity
-        if self.method == self.OPT_RP:
-            if r is not None:
-                warnings.warn("`r` is set but will not be used for `rp` portfolio")
-            return self._optimize_rp(S, w0)
-
-    def _optimize_inv(self, S: np.ndarray) -> np.ndarray:
-        """Inverse volatility"""
-        vola = np.diag(S) ** 0.5
-        w = 1 / vola
-        w /= w.sum()
-        return w
-
-    def _optimize_gmv(self, S: np.ndarray, w0: Optional[np.ndarray] = None) -> np.ndarray:
-        """optimize global minimum variance portfolio
-
-        This method solves the following optimization problem
-            min_w w' S w
-            s.t. w >= 0, sum(w) == 1
-        where `S` is the covariance matrix.
-        """
-        return self._solve(len(S), self._get_objective_gmv(S), *self._get_constrains(w0))
-
-    def _optimize_mvo(
-        self, S: np.ndarray, r: Optional[np.ndarray] = None, w0: Optional[np.ndarray] = None
-    ) -> np.ndarray:
-        """optimize mean-variance portfolio
-
-        This method solves the following optimization problem
-            min_w   - w' r + lamb * w' S w
-            s.t.   w >= 0, sum(w) == 1
-        where `S` is the covariance matrix, `u` is the expected returns,
-        and `lamb` is the risk aversion parameter.
-        """
-        return self._solve(len(S), self._get_objective_mvo(S, r), *self._get_constrains(w0))
-
-    def _optimize_rp(self, S: np.ndarray, w0: Optional[np.ndarray] = None) -> np.ndarray:
-        """optimize risk parity portfolio
-
-        This method solves the following optimization problem
-            min_w sum_i [w_i - (w' S w) / ((S w)_i * N)]**2
-            s.t. w >= 0, sum(w) == 1
-        where `S` is the covariance matrix and `N` is the number of stocks.
-        """
-        return self._solve(len(S), self._get_objective_rp(S), *self._get_constrains(w0))
-
-    def _get_objective_gmv(self, S: np.ndarray) -> Callable:
-        """global minimum variance optimization objective
-
-        Optimization objective
-            min_w w' S w
-        """
-
-        def func(x):
-            return x @ S @ x
-
-        return func
-
-    def _get_objective_mvo(self, S: np.ndarray, r: np.ndarray = None) -> Callable:
-        """mean-variance optimization objective
-
-        Optimization objective
-            min_w - w' r + lamb * w' S w
-        """
-
-        def func(x):
-            risk = x @ S @ x
-            ret = x @ r
-            return -ret + self.lamb * risk
-
-        return func
-
-    def _get_objective_rp(self, S: np.ndarray) -> Callable:
-        """risk-parity optimization objective
-
-        Optimization objective
-            min_w sum_i [w_i - (w' S w) / ((S w)_i * N)]**2
-        """
-
-        def func(x):
-            N = len(x)
-            Sx = S @ x
-            xSx = x @ Sx
-            return np.sum((x - xSx / Sx / N) ** 2)
-
-        return func
-
-    def _get_constrains(self, w0: Optional[np.ndarray] = None):
-        """optimization constraints
-
-        Defines the following constraints:
-            - no shorting and leverage: 0 <= w <= 1
-            - full investment: sum(w) == 1
-            - turnover constraint: |w - w0| <= delta
-        """
-
-        # no shorting and leverage
-        bounds = so.Bounds(0.0, 1.0)
-
-        # full investment constraint
-        cons = [{"type": "eq", "fun": lambda x: np.sum(x) - 1}]  # == 0
-
-        # turnover constraint
-        if w0 is not None:
-            cons.append({"type": "ineq", "fun": lambda x: self.delta - np.sum(np.abs(x - w0))})  # >= 0
-
-        return bounds, cons
-
-    def _solve(self, n: int, obj: Callable, bounds: so.Bounds, cons: List) -> np.ndarray:
-        """solve optimization
-
-        Args:
-            n (int): number of parameters
-            obj (callable): optimization objective
-            bounds (Bounds): bounds of parameters
-            cons (list): optimization constraints
-        """
-        # add l2 regularization
-        wrapped_obj = obj
-        if self.alpha > 0:
-
-            def opt_obj(x):
-                return obj(x) + self.alpha * np.sum(np.square(x))
-
-            wrapped_obj = opt_obj
-
-        # solve
-        x0 = np.ones(n) / n  # init results
-        sol = so.minimize(wrapped_obj, x0, bounds=bounds, constraints=cons, tol=self.tol)
-        if not sol.success:
-            warnings.warn(f"optimization not success ({sol.status})")
-
-        return sol.x
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+
+import warnings
+import numpy as np
+import pandas as pd
+import scipy.optimize as so
+from typing import Optional, Union, Callable, List
+
+from .base import BaseOptimizer
+
+
+class PortfolioOptimizer(BaseOptimizer):
+    """Portfolio Optimizer
+
+    The following optimization algorithms are supported:
+        - `gmv`: Global Minimum Variance Portfolio
+        - `mvo`: Mean Variance Optimized Portfolio
+        - `rp`: Risk Parity
+        - `inv`: Inverse Volatility
+
+    Note:
+        This optimizer always assumes full investment and no-shorting.
+    """
+
+    OPT_GMV = "gmv"
+    OPT_MVO = "mvo"
+    OPT_RP = "rp"
+    OPT_INV = "inv"
+
+    def __init__(
+        self,
+        method: str = "inv",
+        lamb: float = 0,
+        delta: float = 0,
+        alpha: float = 0.0,
+        scale_return: bool = True,
+        tol: float = 1e-8,
+    ):
+        """
+        Args:
+            method (str): portfolio optimization method
+            lamb (float): risk aversion parameter (larger `lamb` means more focus on return)
+            delta (float): turnover rate limit
+            alpha (float): l2 norm regularizer
+            scale_return (bool): if to scale alpha to match the volatility of the covariance matrix
+            tol (float): tolerance for optimization termination
+        """
+        assert method in [self.OPT_GMV, self.OPT_MVO, self.OPT_RP, self.OPT_INV], f"method `{method}` is not supported"
+        self.method = method
+
+        assert lamb >= 0, f"risk aversion parameter `lamb` should be positive"
+        self.lamb = lamb
+
+        assert delta >= 0, f"turnover limit `delta` should be positive"
+        self.delta = delta
+
+        assert alpha >= 0, f"l2 norm regularizer `alpha` should be positive"
+        self.alpha = alpha
+
+        self.tol = tol
+        self.scale_return = scale_return
+
+    def __call__(
+        self,
+        S: Union[np.ndarray, pd.DataFrame],
+        r: Optional[Union[np.ndarray, pd.Series]] = None,
+        w0: Optional[Union[np.ndarray, pd.Series]] = None,
+    ) -> Union[np.ndarray, pd.Series]:
+        """
+        Args:
+            S (np.ndarray or pd.DataFrame): covariance matrix
+            r (np.ndarray or pd.Series): expected return
+            w0 (np.ndarray or pd.Series): initial weights (for turnover control)
+
+        Returns:
+            np.ndarray or pd.Series: optimized portfolio allocation
+        """
+        # transform dataframe into array
+        index = None
+        if isinstance(S, pd.DataFrame):
+            index = S.index
+            S = S.values
+
+        # transform return
+        if r is not None:
+            assert len(r) == len(S), "`r` has mismatched shape"
+            if isinstance(r, pd.Series):
+                assert r.index.equals(index), "`r` has mismatched index"
+                r = r.values
+
+        # transform initial weights
+        if w0 is not None:
+            assert len(w0) == len(S), "`w0` has mismatched shape"
+            if isinstance(w0, pd.Series):
+                assert w0.index.equals(index), "`w0` has mismatched index"
+                w0 = w0.values
+
+        # scale return to match volatility
+        if r is not None and self.scale_return:
+            r = r / r.std()
+            r *= np.sqrt(np.mean(np.diag(S)))
+
+        # optimize
+        w = self._optimize(S, r, w0)
+
+        # restore index if needed
+        if index is not None:
+            w = pd.Series(w, index=index)
+
+        return w
+
+    def _optimize(self, S: np.ndarray, r: Optional[np.ndarray] = None, w0: Optional[np.ndarray] = None) -> np.ndarray:
+        # inverse volatility
+        if self.method == self.OPT_INV:
+            if r is not None:
+                warnings.warn("`r` is set but will not be used for `inv` portfolio")
+            if w0 is not None:
+                warnings.warn("`w0` is set but will not be used for `inv` portfolio")
+            return self._optimize_inv(S)
+
+        # global minimum variance
+        if self.method == self.OPT_GMV:
+            if r is not None:
+                warnings.warn("`r` is set but will not be used for `gmv` portfolio")
+            return self._optimize_gmv(S, w0)
+
+        # mean-variance
+        if self.method == self.OPT_MVO:
+            return self._optimize_mvo(S, r, w0)
+
+        # risk parity
+        if self.method == self.OPT_RP:
+            if r is not None:
+                warnings.warn("`r` is set but will not be used for `rp` portfolio")
+            return self._optimize_rp(S, w0)
+
+    def _optimize_inv(self, S: np.ndarray) -> np.ndarray:
+        """Inverse volatility"""
+        vola = np.diag(S) ** 0.5
+        w = 1 / vola
+        w /= w.sum()
+        return w
+
+    def _optimize_gmv(self, S: np.ndarray, w0: Optional[np.ndarray] = None) -> np.ndarray:
+        """optimize global minimum variance portfolio
+
+        This method solves the following optimization problem
+            min_w w' S w
+            s.t. w >= 0, sum(w) == 1
+        where `S` is the covariance matrix.
+        """
+        return self._solve(len(S), self._get_objective_gmv(S), *self._get_constrains(w0))
+
+    def _optimize_mvo(
+        self, S: np.ndarray, r: Optional[np.ndarray] = None, w0: Optional[np.ndarray] = None
+    ) -> np.ndarray:
+        """optimize mean-variance portfolio
+
+        This method solves the following optimization problem
+            min_w   - w' r + lamb * w' S w
+            s.t.   w >= 0, sum(w) == 1
+        where `S` is the covariance matrix, `u` is the expected returns,
+        and `lamb` is the risk aversion parameter.
+        """
+        return self._solve(len(S), self._get_objective_mvo(S, r), *self._get_constrains(w0))
+
+    def _optimize_rp(self, S: np.ndarray, w0: Optional[np.ndarray] = None) -> np.ndarray:
+        """optimize risk parity portfolio
+
+        This method solves the following optimization problem
+            min_w sum_i [w_i - (w' S w) / ((S w)_i * N)]**2
+            s.t. w >= 0, sum(w) == 1
+        where `S` is the covariance matrix and `N` is the number of stocks.
+        """
+        return self._solve(len(S), self._get_objective_rp(S), *self._get_constrains(w0))
+
+    def _get_objective_gmv(self, S: np.ndarray) -> Callable:
+        """global minimum variance optimization objective
+
+        Optimization objective
+            min_w w' S w
+        """
+
+        def func(x):
+            return x @ S @ x
+
+        return func
+
+    def _get_objective_mvo(self, S: np.ndarray, r: np.ndarray = None) -> Callable:
+        """mean-variance optimization objective
+
+        Optimization objective
+            min_w - w' r + lamb * w' S w
+        """
+
+        def func(x):
+            risk = x @ S @ x
+            ret = x @ r
+            return -ret + self.lamb * risk
+
+        return func
+
+    def _get_objective_rp(self, S: np.ndarray) -> Callable:
+        """risk-parity optimization objective
+
+        Optimization objective
+            min_w sum_i [w_i - (w' S w) / ((S w)_i * N)]**2
+        """
+
+        def func(x):
+            N = len(x)
+            Sx = S @ x
+            xSx = x @ Sx
+            return np.sum((x - xSx / Sx / N) ** 2)
+
+        return func
+
+    def _get_constrains(self, w0: Optional[np.ndarray] = None):
+        """optimization constraints
+
+        Defines the following constraints:
+            - no shorting and leverage: 0 <= w <= 1
+            - full investment: sum(w) == 1
+            - turnover constraint: |w - w0| <= delta
+        """
+
+        # no shorting and leverage
+        bounds = so.Bounds(0.0, 1.0)
+
+        # full investment constraint
+        cons = [{"type": "eq", "fun": lambda x: np.sum(x) - 1}]  # == 0
+
+        # turnover constraint
+        if w0 is not None:
+            cons.append({"type": "ineq", "fun": lambda x: self.delta - np.sum(np.abs(x - w0))})  # >= 0
+
+        return bounds, cons
+
+    def _solve(self, n: int, obj: Callable, bounds: so.Bounds, cons: List) -> np.ndarray:
+        """solve optimization
+
+        Args:
+            n (int): number of parameters
+            obj (callable): optimization objective
+            bounds (Bounds): bounds of parameters
+            cons (list): optimization constraints
+        """
+        # add l2 regularization
+        wrapped_obj = obj
+        if self.alpha > 0:
+
+            def opt_obj(x):
+                return obj(x) + self.alpha * np.sum(np.square(x))
+
+            wrapped_obj = opt_obj
+
+        # solve
+        x0 = np.ones(n) / n  # init results
+        sol = so.minimize(wrapped_obj, x0, bounds=bounds, constraints=cons, tol=self.tol)
+        if not sol.success:
+            warnings.warn(f"optimization not success ({sol.status})")
+
+        return sol.x
```

## qlib/contrib/tuner/__init__.py

 * *Ordering differences only*

```diff
@@ -1,2 +1,2 @@
-# pylint: skip-file
-# flake8: noqa
+# pylint: skip-file
+# flake8: noqa
```

## qlib/contrib/tuner/config.py

```diff
@@ -1,91 +1,89 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-# pylint: skip-file
-# flake8: noqa
-
-import yaml
-import copy
-import os
-
-
-class TunerConfigManager:
-    def __init__(self, config_path):
-
-        if not config_path:
-            raise ValueError("Config path is invalid.")
-        self.config_path = config_path
-
-        with open(config_path) as fp:
-            config = yaml.safe_load(fp)
-        self.config = copy.deepcopy(config)
-
-        self.pipeline_ex_config = PipelineExperimentConfig(config.get("experiment", dict()), self)
-        self.pipeline_config = config.get("tuner_pipeline", list())
-        self.optim_config = OptimizationConfig(config.get("optimization_criteria", dict()), self)
-
-        self.time_config = config.get("time_period", dict())
-        self.data_config = config.get("data", dict())
-        self.backtest_config = config.get("backtest", dict())
-        self.qlib_client_config = config.get("qlib_client", dict())
-
-
-class PipelineExperimentConfig:
-    def __init__(self, config, TUNER_CONFIG_MANAGER):
-        """
-        :param config:  The config dict for tuner experiment
-        :param TUNER_CONFIG_MANAGER:   The tuner config manager
-        """
-        self.name = config.get("name", "tuner_experiment")
-        # The dir of the config
-        self.global_dir = config.get("dir", os.path.dirname(TUNER_CONFIG_MANAGER.config_path))
-        # The dir of the result of tuner experiment
-        self.tuner_ex_dir = config.get("tuner_ex_dir", os.path.join(self.global_dir, self.name))
-        if not os.path.exists(self.tuner_ex_dir):
-            os.makedirs(self.tuner_ex_dir)
-        # The dir of the results of all estimator experiments
-        self.estimator_ex_dir = config.get("estimator_ex_dir", os.path.join(self.tuner_ex_dir, "estimator_experiment"))
-        if not os.path.exists(self.estimator_ex_dir):
-            os.makedirs(self.estimator_ex_dir)
-        # Get the tuner type
-        self.tuner_module_path = config.get("tuner_module_path", "qlib.contrib.tuner.tuner")
-        self.tuner_class = config.get("tuner_class", "QLibTuner")
-        # Save the tuner experiment for further view
-        tuner_ex_config_path = os.path.join(self.tuner_ex_dir, "tuner_config.yaml")
-        with open(tuner_ex_config_path, "w") as fp:
-            yaml.dump(TUNER_CONFIG_MANAGER.config, fp)
-
-
-class OptimizationConfig:
-    def __init__(self, config, TUNER_CONFIG_MANAGER):
-
-        self.report_type = config.get("report_type", "pred_long")
-        if self.report_type not in [
-            "pred_long",
-            "pred_long_short",
-            "pred_short",
-            "excess_return_without_cost",
-            "excess_return_with_cost",
-            "model",
-        ]:
-            raise ValueError(
-                "report_type should be one of pred_long, pred_long_short, pred_short, excess_return_without_cost, excess_return_with_cost and model"
-            )
-
-        self.report_factor = config.get("report_factor", "information_ratio")
-        if self.report_factor not in [
-            "annualized_return",
-            "information_ratio",
-            "max_drawdown",
-            "mean",
-            "std",
-            "model_score",
-            "model_pearsonr",
-        ]:
-            raise ValueError(
-                "report_factor should be one of annualized_return, information_ratio, max_drawdown, mean, std, model_pearsonr and model_score"
-            )
-
-        self.optim_type = config.get("optim_type", "max")
-        if self.optim_type not in ["min", "max", "correlation"]:
-            raise ValueError("optim_type should be min, max or correlation")
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+# pylint: skip-file
+# flake8: noqa
+
+import yaml
+import copy
+import os
+
+
+class TunerConfigManager:
+    def __init__(self, config_path):
+        if not config_path:
+            raise ValueError("Config path is invalid.")
+        self.config_path = config_path
+
+        with open(config_path) as fp:
+            config = yaml.safe_load(fp)
+        self.config = copy.deepcopy(config)
+
+        self.pipeline_ex_config = PipelineExperimentConfig(config.get("experiment", dict()), self)
+        self.pipeline_config = config.get("tuner_pipeline", list())
+        self.optim_config = OptimizationConfig(config.get("optimization_criteria", dict()), self)
+
+        self.time_config = config.get("time_period", dict())
+        self.data_config = config.get("data", dict())
+        self.backtest_config = config.get("backtest", dict())
+        self.qlib_client_config = config.get("qlib_client", dict())
+
+
+class PipelineExperimentConfig:
+    def __init__(self, config, TUNER_CONFIG_MANAGER):
+        """
+        :param config:  The config dict for tuner experiment
+        :param TUNER_CONFIG_MANAGER:   The tuner config manager
+        """
+        self.name = config.get("name", "tuner_experiment")
+        # The dir of the config
+        self.global_dir = config.get("dir", os.path.dirname(TUNER_CONFIG_MANAGER.config_path))
+        # The dir of the result of tuner experiment
+        self.tuner_ex_dir = config.get("tuner_ex_dir", os.path.join(self.global_dir, self.name))
+        if not os.path.exists(self.tuner_ex_dir):
+            os.makedirs(self.tuner_ex_dir)
+        # The dir of the results of all estimator experiments
+        self.estimator_ex_dir = config.get("estimator_ex_dir", os.path.join(self.tuner_ex_dir, "estimator_experiment"))
+        if not os.path.exists(self.estimator_ex_dir):
+            os.makedirs(self.estimator_ex_dir)
+        # Get the tuner type
+        self.tuner_module_path = config.get("tuner_module_path", "qlib.contrib.tuner.tuner")
+        self.tuner_class = config.get("tuner_class", "QLibTuner")
+        # Save the tuner experiment for further view
+        tuner_ex_config_path = os.path.join(self.tuner_ex_dir, "tuner_config.yaml")
+        with open(tuner_ex_config_path, "w") as fp:
+            yaml.dump(TUNER_CONFIG_MANAGER.config, fp)
+
+
+class OptimizationConfig:
+    def __init__(self, config, TUNER_CONFIG_MANAGER):
+        self.report_type = config.get("report_type", "pred_long")
+        if self.report_type not in [
+            "pred_long",
+            "pred_long_short",
+            "pred_short",
+            "excess_return_without_cost",
+            "excess_return_with_cost",
+            "model",
+        ]:
+            raise ValueError(
+                "report_type should be one of pred_long, pred_long_short, pred_short, excess_return_without_cost, excess_return_with_cost and model"
+            )
+
+        self.report_factor = config.get("report_factor", "information_ratio")
+        if self.report_factor not in [
+            "annualized_return",
+            "information_ratio",
+            "max_drawdown",
+            "mean",
+            "std",
+            "model_score",
+            "model_pearsonr",
+        ]:
+            raise ValueError(
+                "report_factor should be one of annualized_return, information_ratio, max_drawdown, mean, std, model_pearsonr and model_score"
+            )
+
+        self.optim_type = config.get("optim_type", "max")
+        if self.optim_type not in ["min", "max", "correlation"]:
+            raise ValueError("optim_type should be min, max or correlation")
```

## qlib/contrib/tuner/launcher.py

 * *Ordering differences only*

```diff
@@ -1,37 +1,37 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-# pylint: skip-file
-# flake8: noqa
-
-# coding=utf-8
-
-import argparse
-import importlib
-import os
-import yaml
-
-from .config import TunerConfigManager
-
-
-args_parser = argparse.ArgumentParser(prog="tuner")
-args_parser.add_argument(
-    "-c",
-    "--config_path",
-    required=True,
-    type=str,
-    help="config path indicates where to load yaml config.",
-)
-
-args = args_parser.parse_args()
-
-TUNER_CONFIG_MANAGER = TunerConfigManager(args.config_path)
-
-
-def run():
-    # 1. Get pipeline class.
-    tuner_pipeline_class = getattr(importlib.import_module(".pipeline", package="qlib.contrib.tuner"), "Pipeline")
-    # 2. Init tuner pipeline.
-    tuner_pipeline = tuner_pipeline_class(TUNER_CONFIG_MANAGER)
-    # 3. Begin to tune
-    tuner_pipeline.run()
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+# pylint: skip-file
+# flake8: noqa
+
+# coding=utf-8
+
+import argparse
+import importlib
+import os
+import yaml
+
+from .config import TunerConfigManager
+
+
+args_parser = argparse.ArgumentParser(prog="tuner")
+args_parser.add_argument(
+    "-c",
+    "--config_path",
+    required=True,
+    type=str,
+    help="config path indicates where to load yaml config.",
+)
+
+args = args_parser.parse_args()
+
+TUNER_CONFIG_MANAGER = TunerConfigManager(args.config_path)
+
+
+def run():
+    # 1. Get pipeline class.
+    tuner_pipeline_class = getattr(importlib.import_module(".pipeline", package="qlib.contrib.tuner"), "Pipeline")
+    # 2. Init tuner pipeline.
+    tuner_pipeline = tuner_pipeline_class(TUNER_CONFIG_MANAGER)
+    # 3. Begin to tune
+    tuner_pipeline.run()
```

## qlib/contrib/tuner/pipeline.py

```diff
@@ -1,89 +1,85 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-# pylint: skip-file
-# flake8: noqa
-
-import os
-import json
-import logging
-import importlib
-from abc import abstractmethod
-
-from ...log import get_module_logger, TimeInspector
-from ...utils import get_module_by_module_path
-
-
-class Pipeline:
-
-    GLOBAL_BEST_PARAMS_NAME = "global_best_params.json"
-
-    def __init__(self, tuner_config_manager):
-
-        self.logger = get_module_logger("Pipeline", sh_level=logging.INFO)
-
-        self.tuner_config_manager = tuner_config_manager
-
-        self.pipeline_ex_config = tuner_config_manager.pipeline_ex_config
-        self.optim_config = tuner_config_manager.optim_config
-        self.time_config = tuner_config_manager.time_config
-        self.pipeline_config = tuner_config_manager.pipeline_config
-        self.data_config = tuner_config_manager.data_config
-        self.backtest_config = tuner_config_manager.backtest_config
-        self.qlib_client_config = tuner_config_manager.qlib_client_config
-
-        self.global_best_res = None
-        self.global_best_params = None
-        self.best_tuner_index = None
-
-    def run(self):
-
-        TimeInspector.set_time_mark()
-        for tuner_index, tuner_config in enumerate(self.pipeline_config):
-            tuner = self.init_tuner(tuner_index, tuner_config)
-            tuner.tune()
-            if self.global_best_res is None or self.global_best_res > tuner.best_res:
-                self.global_best_res = tuner.best_res
-                self.global_best_params = tuner.best_params
-                self.best_tuner_index = tuner_index
-        TimeInspector.log_cost_time("Finished tuner pipeline.")
-
-        self.save_tuner_exp_info()
-
-    def init_tuner(self, tuner_index, tuner_config):
-        """
-        Implement this method to build the tuner by config
-        return: tuner
-        """
-        # 1. Add experiment config in tuner_config
-        tuner_config["experiment"] = {
-            "name": "estimator_experiment_{}".format(tuner_index),
-            "id": tuner_index,
-            "dir": self.pipeline_ex_config.estimator_ex_dir,
-            "observer_type": "file_storage",
-        }
-        tuner_config["qlib_client"] = self.qlib_client_config
-        # 2. Add data config in tuner_config
-        tuner_config["data"] = self.data_config
-        # 3. Add backtest config in tuner_config
-        tuner_config["backtest"] = self.backtest_config
-        # 4. Update trainer in tuner_config
-        tuner_config["trainer"].update({"args": self.time_config})
-
-        # 5. Import Tuner class
-        tuner_module = get_module_by_module_path(self.pipeline_ex_config.tuner_module_path)
-        tuner_class = getattr(tuner_module, self.pipeline_ex_config.tuner_class)
-        # 6. Return the specific tuner
-        return tuner_class(tuner_config, self.optim_config)
-
-    def save_tuner_exp_info(self):
-
-        TimeInspector.set_time_mark()
-        save_path = os.path.join(self.pipeline_ex_config.tuner_ex_dir, Pipeline.GLOBAL_BEST_PARAMS_NAME)
-        with open(save_path, "w") as fp:
-            json.dump(self.global_best_params, fp)
-        TimeInspector.log_cost_time("Finished save global best tuner parameters.")
-
-        self.logger.info("Best Tuner id: {}.".format(self.best_tuner_index))
-        self.logger.info("Global best parameters: {}.".format(self.global_best_params))
-        self.logger.info("You can check the best parameters at {}.".format(save_path))
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+# pylint: skip-file
+# flake8: noqa
+
+import os
+import json
+import logging
+import importlib
+from abc import abstractmethod
+
+from ...log import get_module_logger, TimeInspector
+from ...utils import get_module_by_module_path
+
+
+class Pipeline:
+    GLOBAL_BEST_PARAMS_NAME = "global_best_params.json"
+
+    def __init__(self, tuner_config_manager):
+        self.logger = get_module_logger("Pipeline", sh_level=logging.INFO)
+
+        self.tuner_config_manager = tuner_config_manager
+
+        self.pipeline_ex_config = tuner_config_manager.pipeline_ex_config
+        self.optim_config = tuner_config_manager.optim_config
+        self.time_config = tuner_config_manager.time_config
+        self.pipeline_config = tuner_config_manager.pipeline_config
+        self.data_config = tuner_config_manager.data_config
+        self.backtest_config = tuner_config_manager.backtest_config
+        self.qlib_client_config = tuner_config_manager.qlib_client_config
+
+        self.global_best_res = None
+        self.global_best_params = None
+        self.best_tuner_index = None
+
+    def run(self):
+        TimeInspector.set_time_mark()
+        for tuner_index, tuner_config in enumerate(self.pipeline_config):
+            tuner = self.init_tuner(tuner_index, tuner_config)
+            tuner.tune()
+            if self.global_best_res is None or self.global_best_res > tuner.best_res:
+                self.global_best_res = tuner.best_res
+                self.global_best_params = tuner.best_params
+                self.best_tuner_index = tuner_index
+        TimeInspector.log_cost_time("Finished tuner pipeline.")
+
+        self.save_tuner_exp_info()
+
+    def init_tuner(self, tuner_index, tuner_config):
+        """
+        Implement this method to build the tuner by config
+        return: tuner
+        """
+        # 1. Add experiment config in tuner_config
+        tuner_config["experiment"] = {
+            "name": "estimator_experiment_{}".format(tuner_index),
+            "id": tuner_index,
+            "dir": self.pipeline_ex_config.estimator_ex_dir,
+            "observer_type": "file_storage",
+        }
+        tuner_config["qlib_client"] = self.qlib_client_config
+        # 2. Add data config in tuner_config
+        tuner_config["data"] = self.data_config
+        # 3. Add backtest config in tuner_config
+        tuner_config["backtest"] = self.backtest_config
+        # 4. Update trainer in tuner_config
+        tuner_config["trainer"].update({"args": self.time_config})
+
+        # 5. Import Tuner class
+        tuner_module = get_module_by_module_path(self.pipeline_ex_config.tuner_module_path)
+        tuner_class = getattr(tuner_module, self.pipeline_ex_config.tuner_class)
+        # 6. Return the specific tuner
+        return tuner_class(tuner_config, self.optim_config)
+
+    def save_tuner_exp_info(self):
+        TimeInspector.set_time_mark()
+        save_path = os.path.join(self.pipeline_ex_config.tuner_ex_dir, Pipeline.GLOBAL_BEST_PARAMS_NAME)
+        with open(save_path, "w") as fp:
+            json.dump(self.global_best_params, fp)
+        TimeInspector.log_cost_time("Finished save global best tuner parameters.")
+
+        self.logger.info("Best Tuner id: {}.".format(self.best_tuner_index))
+        self.logger.info("Global best parameters: {}.".format(self.global_best_params))
+        self.logger.info("You can check the best parameters at {}.".format(save_path))
```

## qlib/contrib/tuner/space.py

 * *Ordering differences only*

```diff
@@ -1,20 +1,20 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-# pylint: skip-file
-# flake8: noqa
-
-from hyperopt import hp
-
-
-TopkAmountStrategySpace = {
-    "topk": hp.choice("topk", [30, 35, 40]),
-    "buffer_margin": hp.choice("buffer_margin", [200, 250, 300]),
-}
-
-QLibDataLabelSpace = {
-    "labels": hp.choice(
-        "labels",
-        [["Ref($vwap, -2)/Ref($vwap, -1) - 1"], ["Ref($close, -5)/$close - 1"]],
-    )
-}
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+# pylint: skip-file
+# flake8: noqa
+
+from hyperopt import hp
+
+
+TopkAmountStrategySpace = {
+    "topk": hp.choice("topk", [30, 35, 40]),
+    "buffer_margin": hp.choice("buffer_margin", [200, 250, 300]),
+}
+
+QLibDataLabelSpace = {
+    "labels": hp.choice(
+        "labels",
+        [["Ref($vwap, -2)/Ref($vwap, -1) - 1"], ["Ref($close, -5)/$close - 1"]],
+    )
+}
```

## qlib/contrib/tuner/tuner.py

```diff
@@ -1,222 +1,215 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-# pylint: skip-file
-# flake8: noqa
-
-import os
-import yaml
-import json
-import copy
-import pickle
-import logging
-import importlib
-import subprocess
-import pandas as pd
-import numpy as np
-
-from abc import abstractmethod
-
-from ...log import get_module_logger, TimeInspector
-from hyperopt import fmin, tpe
-from hyperopt import STATUS_OK, STATUS_FAIL
-
-
-class Tuner:
-    def __init__(self, tuner_config, optim_config):
-
-        self.logger = get_module_logger("Tuner", sh_level=logging.INFO)
-
-        self.tuner_config = tuner_config
-        self.optim_config = optim_config
-
-        self.max_evals = self.tuner_config.get("max_evals", 10)
-        self.ex_dir = os.path.join(
-            self.tuner_config["experiment"]["dir"],
-            self.tuner_config["experiment"]["name"],
-        )
-
-        self.best_params = None
-        self.best_res = None
-
-        self.space = self.setup_space()
-
-    def tune(self):
-
-        TimeInspector.set_time_mark()
-        fmin(
-            fn=self.objective,
-            space=self.space,
-            algo=tpe.suggest,
-            max_evals=self.max_evals,
-            show_progressbar=False,
-        )
-        self.logger.info("Local best params: {} ".format(self.best_params))
-        TimeInspector.log_cost_time(
-            "Finished searching best parameters in Tuner {}.".format(self.tuner_config["experiment"]["id"])
-        )
-
-        self.save_local_best_params()
-
-    @abstractmethod
-    def objective(self, params):
-        """
-        Implement this method to give an optimization factor using parameters in space.
-        :return: {'loss': a factor for optimization, float type,
-                  'status': the status of this evaluation step, STATUS_OK or STATUS_FAIL}.
-        """
-        pass
-
-    @abstractmethod
-    def setup_space(self):
-        """
-        Implement this method to setup the searching space of tuner.
-        :return: searching space, dict type.
-        """
-        pass
-
-    @abstractmethod
-    def save_local_best_params(self):
-        """
-        Implement this method to save the best parameters of this tuner.
-        """
-        pass
-
-
-class QLibTuner(Tuner):
-
-    ESTIMATOR_CONFIG_NAME = "estimator_config.yaml"
-    EXP_INFO_NAME = "exp_info.json"
-    EXP_RESULT_DIR = "sacred/{}"
-    EXP_RESULT_NAME = "analysis.pkl"
-    LOCAL_BEST_PARAMS_NAME = "local_best_params.json"
-
-    def objective(self, params):
-
-        # 1. Setup an config for a specific estimator process
-        estimator_path = self.setup_estimator_config(params)
-        self.logger.info("Searching params: {} ".format(params))
-
-        # 2. Use subprocess to do the estimator program, this process will wait until subprocess finish
-        sub_fails = subprocess.call("estimator -c {}".format(estimator_path), shell=True)
-        if sub_fails:
-            # If this subprocess failed, ignore this evaluation step
-            self.logger.info("Estimator experiment failed when using this searching parameters")
-            return {"loss": np.nan, "status": STATUS_FAIL}
-
-        # 3. Fetch the result of subprocess, and check whether the result is Nan
-        res = self.fetch_result()
-        if np.isnan(res):
-            status = STATUS_FAIL
-        else:
-            status = STATUS_OK
-
-        # 4. Save the best score and params
-        if self.best_res is None or self.best_res > res:
-            self.best_res = res
-            self.best_params = params
-
-        # 5. Return the result as optim objective
-        return {"loss": res, "status": status}
-
-    def fetch_result(self):
-
-        # 1. Get experiment information
-        exp_info_path = os.path.join(self.ex_dir, QLibTuner.EXP_INFO_NAME)
-        with open(exp_info_path) as fp:
-            exp_info = json.load(fp)
-        estimator_ex_id = exp_info["id"]
-
-        # 2. Return model result if needed
-        if self.optim_config.report_type == "model":
-            if self.optim_config.report_factor == "model_score":
-                # if estimator experiment is multi-label training, user need to process the scores by himself
-                # Default method is return the average score
-                return np.mean(exp_info["performance"]["model_score"])
-            elif self.optim_config.report_factor == "model_pearsonr":
-                # pearsonr is a correlation coefficient, 1 is the best
-                return np.abs(exp_info["performance"]["model_pearsonr"] - 1)
-
-        # 3. Get backtest results
-        exp_result_dir = os.path.join(self.ex_dir, QLibTuner.EXP_RESULT_DIR.format(estimator_ex_id))
-        exp_result_path = os.path.join(exp_result_dir, QLibTuner.EXP_RESULT_NAME)
-        with open(exp_result_path, "rb") as fp:
-            analysis_df = pickle.load(fp)
-
-        # 4. Get the backtest factor which user want to optimize, if user want to maximize the factor, then reverse the result
-        res = analysis_df.loc[self.optim_config.report_type].loc[self.optim_config.report_factor]
-        # res = res.values[0] if self.optim_config.optim_type == 'min' else -res.values[0]
-        if self.optim_config == "min":
-            return res.values[0]
-        elif self.optim_config == "max":
-            return -res.values[0]
-        else:
-            # self.optim_config == 'correlation'
-            return np.abs(res.values[0] - 1)
-
-    def setup_estimator_config(self, params):
-
-        estimator_config = copy.deepcopy(self.tuner_config)
-        estimator_config["model"].update({"args": params["model_space"]})
-        estimator_config["strategy"].update({"args": params["strategy_space"]})
-        if params.get("data_label_space", None) is not None:
-            estimator_config["data"]["args"].update(params["data_label_space"])
-
-        estimator_path = os.path.join(
-            self.tuner_config["experiment"].get("dir", "../"),
-            QLibTuner.ESTIMATOR_CONFIG_NAME,
-        )
-
-        with open(estimator_path, "w") as fp:
-            yaml.dump(estimator_config, fp)
-
-        return estimator_path
-
-    def setup_space(self):
-        # 1. Setup model space
-        model_space_name = self.tuner_config["model"].get("space", None)
-        if model_space_name is None:
-            raise ValueError("Please give the search space of model.")
-        model_space = getattr(
-            importlib.import_module(".space", package="qlib.contrib.tuner"),
-            model_space_name,
-        )
-
-        # 2. Setup strategy space
-        strategy_space_name = self.tuner_config["strategy"].get("space", None)
-        if strategy_space_name is None:
-            raise ValueError("Please give the search space of strategy.")
-        strategy_space = getattr(
-            importlib.import_module(".space", package="qlib.contrib.tuner"),
-            strategy_space_name,
-        )
-
-        # 3. Setup data label space if given
-        if self.tuner_config.get("data_label", None) is not None:
-            data_label_space_name = self.tuner_config["data_label"].get("space", None)
-            if data_label_space_name is not None:
-                data_label_space = getattr(
-                    importlib.import_module(".space", package="qlib.contrib.tuner"),
-                    data_label_space_name,
-                )
-        else:
-            data_label_space_name = None
-
-        # 4. Combine the searching space
-        space = dict()
-        space.update({"model_space": model_space})
-        space.update({"strategy_space": strategy_space})
-        if data_label_space_name is not None:
-            space.update({"data_label_space": data_label_space})
-
-        return space
-
-    def save_local_best_params(self):
-
-        TimeInspector.set_time_mark()
-        local_best_params_path = os.path.join(self.ex_dir, QLibTuner.LOCAL_BEST_PARAMS_NAME)
-        with open(local_best_params_path, "w") as fp:
-            json.dump(self.best_params, fp)
-        TimeInspector.log_cost_time(
-            "Finished saving local best tuner parameters to: {} .".format(local_best_params_path)
-        )
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+# pylint: skip-file
+# flake8: noqa
+
+import os
+import yaml
+import json
+import copy
+import pickle
+import logging
+import importlib
+import subprocess
+import pandas as pd
+import numpy as np
+
+from abc import abstractmethod
+
+from ...log import get_module_logger, TimeInspector
+from hyperopt import fmin, tpe
+from hyperopt import STATUS_OK, STATUS_FAIL
+
+
+class Tuner:
+    def __init__(self, tuner_config, optim_config):
+        self.logger = get_module_logger("Tuner", sh_level=logging.INFO)
+
+        self.tuner_config = tuner_config
+        self.optim_config = optim_config
+
+        self.max_evals = self.tuner_config.get("max_evals", 10)
+        self.ex_dir = os.path.join(
+            self.tuner_config["experiment"]["dir"],
+            self.tuner_config["experiment"]["name"],
+        )
+
+        self.best_params = None
+        self.best_res = None
+
+        self.space = self.setup_space()
+
+    def tune(self):
+        TimeInspector.set_time_mark()
+        fmin(
+            fn=self.objective,
+            space=self.space,
+            algo=tpe.suggest,
+            max_evals=self.max_evals,
+            show_progressbar=False,
+        )
+        self.logger.info("Local best params: {} ".format(self.best_params))
+        TimeInspector.log_cost_time(
+            "Finished searching best parameters in Tuner {}.".format(self.tuner_config["experiment"]["id"])
+        )
+
+        self.save_local_best_params()
+
+    @abstractmethod
+    def objective(self, params):
+        """
+        Implement this method to give an optimization factor using parameters in space.
+        :return: {'loss': a factor for optimization, float type,
+                  'status': the status of this evaluation step, STATUS_OK or STATUS_FAIL}.
+        """
+        pass
+
+    @abstractmethod
+    def setup_space(self):
+        """
+        Implement this method to setup the searching space of tuner.
+        :return: searching space, dict type.
+        """
+        pass
+
+    @abstractmethod
+    def save_local_best_params(self):
+        """
+        Implement this method to save the best parameters of this tuner.
+        """
+        pass
+
+
+class QLibTuner(Tuner):
+    ESTIMATOR_CONFIG_NAME = "estimator_config.yaml"
+    EXP_INFO_NAME = "exp_info.json"
+    EXP_RESULT_DIR = "sacred/{}"
+    EXP_RESULT_NAME = "analysis.pkl"
+    LOCAL_BEST_PARAMS_NAME = "local_best_params.json"
+
+    def objective(self, params):
+        # 1. Setup an config for a specific estimator process
+        estimator_path = self.setup_estimator_config(params)
+        self.logger.info("Searching params: {} ".format(params))
+
+        # 2. Use subprocess to do the estimator program, this process will wait until subprocess finish
+        sub_fails = subprocess.call("estimator -c {}".format(estimator_path), shell=True)
+        if sub_fails:
+            # If this subprocess failed, ignore this evaluation step
+            self.logger.info("Estimator experiment failed when using this searching parameters")
+            return {"loss": np.nan, "status": STATUS_FAIL}
+
+        # 3. Fetch the result of subprocess, and check whether the result is Nan
+        res = self.fetch_result()
+        if np.isnan(res):
+            status = STATUS_FAIL
+        else:
+            status = STATUS_OK
+
+        # 4. Save the best score and params
+        if self.best_res is None or self.best_res > res:
+            self.best_res = res
+            self.best_params = params
+
+        # 5. Return the result as optim objective
+        return {"loss": res, "status": status}
+
+    def fetch_result(self):
+        # 1. Get experiment information
+        exp_info_path = os.path.join(self.ex_dir, QLibTuner.EXP_INFO_NAME)
+        with open(exp_info_path) as fp:
+            exp_info = json.load(fp)
+        estimator_ex_id = exp_info["id"]
+
+        # 2. Return model result if needed
+        if self.optim_config.report_type == "model":
+            if self.optim_config.report_factor == "model_score":
+                # if estimator experiment is multi-label training, user need to process the scores by himself
+                # Default method is return the average score
+                return np.mean(exp_info["performance"]["model_score"])
+            elif self.optim_config.report_factor == "model_pearsonr":
+                # pearsonr is a correlation coefficient, 1 is the best
+                return np.abs(exp_info["performance"]["model_pearsonr"] - 1)
+
+        # 3. Get backtest results
+        exp_result_dir = os.path.join(self.ex_dir, QLibTuner.EXP_RESULT_DIR.format(estimator_ex_id))
+        exp_result_path = os.path.join(exp_result_dir, QLibTuner.EXP_RESULT_NAME)
+        with open(exp_result_path, "rb") as fp:
+            analysis_df = pickle.load(fp)
+
+        # 4. Get the backtest factor which user want to optimize, if user want to maximize the factor, then reverse the result
+        res = analysis_df.loc[self.optim_config.report_type].loc[self.optim_config.report_factor]
+        # res = res.values[0] if self.optim_config.optim_type == 'min' else -res.values[0]
+        if self.optim_config == "min":
+            return res.values[0]
+        elif self.optim_config == "max":
+            return -res.values[0]
+        else:
+            # self.optim_config == 'correlation'
+            return np.abs(res.values[0] - 1)
+
+    def setup_estimator_config(self, params):
+        estimator_config = copy.deepcopy(self.tuner_config)
+        estimator_config["model"].update({"args": params["model_space"]})
+        estimator_config["strategy"].update({"args": params["strategy_space"]})
+        if params.get("data_label_space", None) is not None:
+            estimator_config["data"]["args"].update(params["data_label_space"])
+
+        estimator_path = os.path.join(
+            self.tuner_config["experiment"].get("dir", "../"),
+            QLibTuner.ESTIMATOR_CONFIG_NAME,
+        )
+
+        with open(estimator_path, "w") as fp:
+            yaml.dump(estimator_config, fp)
+
+        return estimator_path
+
+    def setup_space(self):
+        # 1. Setup model space
+        model_space_name = self.tuner_config["model"].get("space", None)
+        if model_space_name is None:
+            raise ValueError("Please give the search space of model.")
+        model_space = getattr(
+            importlib.import_module(".space", package="qlib.contrib.tuner"),
+            model_space_name,
+        )
+
+        # 2. Setup strategy space
+        strategy_space_name = self.tuner_config["strategy"].get("space", None)
+        if strategy_space_name is None:
+            raise ValueError("Please give the search space of strategy.")
+        strategy_space = getattr(
+            importlib.import_module(".space", package="qlib.contrib.tuner"),
+            strategy_space_name,
+        )
+
+        # 3. Setup data label space if given
+        if self.tuner_config.get("data_label", None) is not None:
+            data_label_space_name = self.tuner_config["data_label"].get("space", None)
+            if data_label_space_name is not None:
+                data_label_space = getattr(
+                    importlib.import_module(".space", package="qlib.contrib.tuner"),
+                    data_label_space_name,
+                )
+        else:
+            data_label_space_name = None
+
+        # 4. Combine the searching space
+        space = dict()
+        space.update({"model_space": model_space})
+        space.update({"strategy_space": strategy_space})
+        if data_label_space_name is not None:
+            space.update({"data_label_space": data_label_space})
+
+        return space
+
+    def save_local_best_params(self):
+        TimeInspector.set_time_mark()
+        local_best_params_path = os.path.join(self.ex_dir, QLibTuner.LOCAL_BEST_PARAMS_NAME)
+        with open(local_best_params_path, "w") as fp:
+            json.dump(self.best_params, fp)
+        TimeInspector.log_cost_time(
+            "Finished saving local best tuner parameters to: {} .".format(local_best_params_path)
+        )
```

## qlib/contrib/workflow/__init__.py

 * *Ordering differences only*

```diff
@@ -1,7 +1,7 @@
-#  Copyright (c) Microsoft Corporation.
-#  Licensed under the MIT License.
-from .record_temp import MultiSegRecord
-from .record_temp import SignalMseRecord
-
-
-__all__ = ["MultiSegRecord", "SignalMseRecord"]
+#  Copyright (c) Microsoft Corporation.
+#  Licensed under the MIT License.
+from .record_temp import MultiSegRecord
+from .record_temp import SignalMseRecord
+
+
+__all__ = ["MultiSegRecord", "SignalMseRecord"]
```

## qlib/contrib/workflow/record_temp.py

 * *Ordering differences only*

```diff
@@ -1,86 +1,86 @@
-#  Copyright (c) Microsoft Corporation.
-#  Licensed under the MIT License.
-
-import logging
-import pandas as pd
-import numpy as np
-from sklearn.metrics import mean_squared_error
-from typing import Dict, Text, Any
-
-from ...contrib.eva.alpha import calc_ic
-from ...workflow.record_temp import RecordTemp
-from ...workflow.record_temp import SignalRecord
-from ...data import dataset as qlib_dataset
-from ...log import get_module_logger
-
-logger = get_module_logger("workflow", logging.INFO)
-
-
-class MultiSegRecord(RecordTemp):
-    """
-    This is the multiple segments signal record class that generates the signal prediction.
-    This class inherits the ``RecordTemp`` class.
-    """
-
-    def __init__(self, model, dataset, recorder=None):
-        super().__init__(recorder=recorder)
-        if not isinstance(dataset, qlib_dataset.DatasetH):
-            raise ValueError("The type of dataset is not DatasetH instead of {:}".format(type(dataset)))
-        self.model = model
-        self.dataset = dataset
-
-    def generate(self, segments: Dict[Text, Any], save: bool = False):
-        for key, segment in segments.items():
-            predics = self.model.predict(self.dataset, segment)
-            if isinstance(predics, pd.Series):
-                predics = predics.to_frame("score")
-            labels = self.dataset.prepare(
-                segments=segment, col_set="label", data_key=qlib_dataset.handler.DataHandlerLP.DK_R
-            )
-            # Compute the IC and Rank IC
-            ic, ric = calc_ic(predics.iloc[:, 0], labels.iloc[:, 0])
-            results = {"all-IC": ic, "mean-IC": ic.mean(), "all-Rank-IC": ric, "mean-Rank-IC": ric.mean()}
-            logger.info("--- Results for {:} ({:}) ---".format(key, segment))
-            ic_x100, ric_x100 = ic * 100, ric * 100
-            logger.info("IC: {:.4f}%".format(ic_x100.mean()))
-            logger.info("ICIR: {:.4f}%".format(ic_x100.mean() / ic_x100.std()))
-            logger.info("Rank IC: {:.4f}%".format(ric_x100.mean()))
-            logger.info("Rank ICIR: {:.4f}%".format(ric_x100.mean() / ric_x100.std()))
-
-            if save:
-                save_name = "results-{:}.pkl".format(key)
-                self.save(**{save_name: results})
-                logger.info(
-                    "The record '{:}' has been saved as the artifact of the Experiment {:}".format(
-                        save_name, self.recorder.experiment_id
-                    )
-                )
-
-
-class SignalMseRecord(RecordTemp):
-    """
-    This is the Signal MSE Record class that computes the mean squared error (MSE).
-    This class inherits the ``SignalMseRecord`` class.
-    """
-
-    artifact_path = "sig_analysis"
-    depend_cls = SignalRecord
-
-    def __init__(self, recorder, **kwargs):
-        super().__init__(recorder=recorder, **kwargs)
-
-    def generate(self):
-        self.check()
-
-        pred = self.load("pred.pkl")
-        label = self.load("label.pkl")
-        masks = ~np.isnan(label.values)
-        mse = mean_squared_error(pred.values[masks], label[masks])
-        metrics = {"MSE": mse, "RMSE": np.sqrt(mse)}
-        objects = {"mse.pkl": mse, "rmse.pkl": np.sqrt(mse)}
-        self.recorder.log_metrics(**metrics)
-        self.save(**objects)
-        logger.info("The evaluation results in SignalMseRecord is {:}".format(metrics))
-
-    def list(self):
-        return ["mse.pkl", "rmse.pkl"]
+#  Copyright (c) Microsoft Corporation.
+#  Licensed under the MIT License.
+
+import logging
+import pandas as pd
+import numpy as np
+from sklearn.metrics import mean_squared_error
+from typing import Dict, Text, Any
+
+from ...contrib.eva.alpha import calc_ic
+from ...workflow.record_temp import RecordTemp
+from ...workflow.record_temp import SignalRecord
+from ...data import dataset as qlib_dataset
+from ...log import get_module_logger
+
+logger = get_module_logger("workflow", logging.INFO)
+
+
+class MultiSegRecord(RecordTemp):
+    """
+    This is the multiple segments signal record class that generates the signal prediction.
+    This class inherits the ``RecordTemp`` class.
+    """
+
+    def __init__(self, model, dataset, recorder=None):
+        super().__init__(recorder=recorder)
+        if not isinstance(dataset, qlib_dataset.DatasetH):
+            raise ValueError("The type of dataset is not DatasetH instead of {:}".format(type(dataset)))
+        self.model = model
+        self.dataset = dataset
+
+    def generate(self, segments: Dict[Text, Any], save: bool = False):
+        for key, segment in segments.items():
+            predics = self.model.predict(self.dataset, segment)
+            if isinstance(predics, pd.Series):
+                predics = predics.to_frame("score")
+            labels = self.dataset.prepare(
+                segments=segment, col_set="label", data_key=qlib_dataset.handler.DataHandlerLP.DK_R
+            )
+            # Compute the IC and Rank IC
+            ic, ric = calc_ic(predics.iloc[:, 0], labels.iloc[:, 0])
+            results = {"all-IC": ic, "mean-IC": ic.mean(), "all-Rank-IC": ric, "mean-Rank-IC": ric.mean()}
+            logger.info("--- Results for {:} ({:}) ---".format(key, segment))
+            ic_x100, ric_x100 = ic * 100, ric * 100
+            logger.info("IC: {:.4f}%".format(ic_x100.mean()))
+            logger.info("ICIR: {:.4f}%".format(ic_x100.mean() / ic_x100.std()))
+            logger.info("Rank IC: {:.4f}%".format(ric_x100.mean()))
+            logger.info("Rank ICIR: {:.4f}%".format(ric_x100.mean() / ric_x100.std()))
+
+            if save:
+                save_name = "results-{:}.pkl".format(key)
+                self.save(**{save_name: results})
+                logger.info(
+                    "The record '{:}' has been saved as the artifact of the Experiment {:}".format(
+                        save_name, self.recorder.experiment_id
+                    )
+                )
+
+
+class SignalMseRecord(RecordTemp):
+    """
+    This is the Signal MSE Record class that computes the mean squared error (MSE).
+    This class inherits the ``SignalMseRecord`` class.
+    """
+
+    artifact_path = "sig_analysis"
+    depend_cls = SignalRecord
+
+    def __init__(self, recorder, **kwargs):
+        super().__init__(recorder=recorder, **kwargs)
+
+    def generate(self):
+        self.check()
+
+        pred = self.load("pred.pkl")
+        label = self.load("label.pkl")
+        masks = ~np.isnan(label.values)
+        mse = mean_squared_error(pred.values[masks], label[masks])
+        metrics = {"MSE": mse, "RMSE": np.sqrt(mse)}
+        objects = {"mse.pkl": mse, "rmse.pkl": np.sqrt(mse)}
+        self.recorder.log_metrics(**metrics)
+        self.save(**objects)
+        logger.info("The evaluation results in SignalMseRecord is {:}".format(metrics))
+
+    def list(self):
+        return ["mse.pkl", "rmse.pkl"]
```

## qlib/data/__init__.py

 * *Ordering differences only*

```diff
@@ -1,66 +1,66 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-
-from __future__ import division
-from __future__ import print_function
-
-from .data import (
-    D,
-    CalendarProvider,
-    InstrumentProvider,
-    FeatureProvider,
-    ExpressionProvider,
-    DatasetProvider,
-    LocalCalendarProvider,
-    LocalInstrumentProvider,
-    LocalFeatureProvider,
-    LocalPITProvider,
-    LocalExpressionProvider,
-    LocalDatasetProvider,
-    ClientCalendarProvider,
-    ClientInstrumentProvider,
-    ClientDatasetProvider,
-    BaseProvider,
-    LocalProvider,
-    ClientProvider,
-)
-
-from .cache import (
-    ExpressionCache,
-    DatasetCache,
-    DiskExpressionCache,
-    DiskDatasetCache,
-    SimpleDatasetCache,
-    DatasetURICache,
-    MemoryCalendarCache,
-)
-
-
-__all__ = [
-    "D",
-    "CalendarProvider",
-    "InstrumentProvider",
-    "FeatureProvider",
-    "ExpressionProvider",
-    "DatasetProvider",
-    "LocalCalendarProvider",
-    "LocalInstrumentProvider",
-    "LocalFeatureProvider",
-    "LocalPITProvider",
-    "LocalExpressionProvider",
-    "LocalDatasetProvider",
-    "ClientCalendarProvider",
-    "ClientInstrumentProvider",
-    "ClientDatasetProvider",
-    "BaseProvider",
-    "LocalProvider",
-    "ClientProvider",
-    "ExpressionCache",
-    "DatasetCache",
-    "DiskExpressionCache",
-    "DiskDatasetCache",
-    "SimpleDatasetCache",
-    "DatasetURICache",
-    "MemoryCalendarCache",
-]
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+
+from __future__ import division
+from __future__ import print_function
+
+from .data import (
+    D,
+    CalendarProvider,
+    InstrumentProvider,
+    FeatureProvider,
+    ExpressionProvider,
+    DatasetProvider,
+    LocalCalendarProvider,
+    LocalInstrumentProvider,
+    LocalFeatureProvider,
+    LocalPITProvider,
+    LocalExpressionProvider,
+    LocalDatasetProvider,
+    ClientCalendarProvider,
+    ClientInstrumentProvider,
+    ClientDatasetProvider,
+    BaseProvider,
+    LocalProvider,
+    ClientProvider,
+)
+
+from .cache import (
+    ExpressionCache,
+    DatasetCache,
+    DiskExpressionCache,
+    DiskDatasetCache,
+    SimpleDatasetCache,
+    DatasetURICache,
+    MemoryCalendarCache,
+)
+
+
+__all__ = [
+    "D",
+    "CalendarProvider",
+    "InstrumentProvider",
+    "FeatureProvider",
+    "ExpressionProvider",
+    "DatasetProvider",
+    "LocalCalendarProvider",
+    "LocalInstrumentProvider",
+    "LocalFeatureProvider",
+    "LocalPITProvider",
+    "LocalExpressionProvider",
+    "LocalDatasetProvider",
+    "ClientCalendarProvider",
+    "ClientInstrumentProvider",
+    "ClientDatasetProvider",
+    "BaseProvider",
+    "LocalProvider",
+    "ClientProvider",
+    "ExpressionCache",
+    "DatasetCache",
+    "DiskExpressionCache",
+    "DiskDatasetCache",
+    "SimpleDatasetCache",
+    "DatasetURICache",
+    "MemoryCalendarCache",
+]
```

## qlib/data/base.py

 * *Ordering differences only*

```diff
@@ -1,281 +1,281 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-
-from __future__ import division
-from __future__ import print_function
-
-import abc
-import pandas as pd
-from ..log import get_module_logger
-
-
-class Expression(abc.ABC):
-    """
-    Expression base class
-
-    Expression is designed to handle the calculation of data with the format below
-    data with two dimension for each instrument,
-
-    - feature
-    - time:  it  could be observation time or period time.
-
-        - period time is designed for Point-in-time database.  For example, the period time maybe 2014Q4, its value can observed for multiple times(different value may be observed at different time due to amendment).
-    """
-
-    def __str__(self):
-        return type(self).__name__
-
-    def __repr__(self):
-        return str(self)
-
-    def __gt__(self, other):
-        from .ops import Gt  # pylint: disable=C0415
-
-        return Gt(self, other)
-
-    def __ge__(self, other):
-        from .ops import Ge  # pylint: disable=C0415
-
-        return Ge(self, other)
-
-    def __lt__(self, other):
-        from .ops import Lt  # pylint: disable=C0415
-
-        return Lt(self, other)
-
-    def __le__(self, other):
-        from .ops import Le  # pylint: disable=C0415
-
-        return Le(self, other)
-
-    def __eq__(self, other):
-        from .ops import Eq  # pylint: disable=C0415
-
-        return Eq(self, other)
-
-    def __ne__(self, other):
-        from .ops import Ne  # pylint: disable=C0415
-
-        return Ne(self, other)
-
-    def __add__(self, other):
-        from .ops import Add  # pylint: disable=C0415
-
-        return Add(self, other)
-
-    def __radd__(self, other):
-        from .ops import Add  # pylint: disable=C0415
-
-        return Add(other, self)
-
-    def __sub__(self, other):
-        from .ops import Sub  # pylint: disable=C0415
-
-        return Sub(self, other)
-
-    def __rsub__(self, other):
-        from .ops import Sub  # pylint: disable=C0415
-
-        return Sub(other, self)
-
-    def __mul__(self, other):
-        from .ops import Mul  # pylint: disable=C0415
-
-        return Mul(self, other)
-
-    def __rmul__(self, other):
-        from .ops import Mul  # pylint: disable=C0415
-
-        return Mul(self, other)
-
-    def __div__(self, other):
-        from .ops import Div  # pylint: disable=C0415
-
-        return Div(self, other)
-
-    def __rdiv__(self, other):
-        from .ops import Div  # pylint: disable=C0415
-
-        return Div(other, self)
-
-    def __truediv__(self, other):
-        from .ops import Div  # pylint: disable=C0415
-
-        return Div(self, other)
-
-    def __rtruediv__(self, other):
-        from .ops import Div  # pylint: disable=C0415
-
-        return Div(other, self)
-
-    def __pow__(self, other):
-        from .ops import Power  # pylint: disable=C0415
-
-        return Power(self, other)
-
-    def __rpow__(self, other):
-        from .ops import Power  # pylint: disable=C0415
-
-        return Power(other, self)
-
-    def __and__(self, other):
-        from .ops import And  # pylint: disable=C0415
-
-        return And(self, other)
-
-    def __rand__(self, other):
-        from .ops import And  # pylint: disable=C0415
-
-        return And(other, self)
-
-    def __or__(self, other):
-        from .ops import Or  # pylint: disable=C0415
-
-        return Or(self, other)
-
-    def __ror__(self, other):
-        from .ops import Or  # pylint: disable=C0415
-
-        return Or(other, self)
-
-    def load(self, instrument, start_index, end_index, *args):
-        """load  feature
-        This function is responsible for loading feature/expression based on the expression engine.
-
-        The concrete implementation will be separated into two parts:
-
-        1) caching data, handle errors.
-
-            - This part is shared by all the expressions and implemented in Expression
-        2) processing and calculating data based on the specific expression.
-
-            - This part is different in each expression and implemented in each expression
-
-        Expression Engine is shared by different data.
-        Different data will have different extra information for `args`.
-
-        Parameters
-        ----------
-        instrument : str
-            instrument code.
-        start_index : str
-            feature start index [in calendar].
-        end_index : str
-            feature end  index  [in calendar].
-
-        *args may contain following information:
-        1) if it is used in basic expression engine data, it contains following arguments
-            freq: str
-                feature frequency.
-
-        2) if is used in PIT data, it contains following arguments
-            cur_pit:
-                it is designed for the point-in-time data.
-            period: int
-                This is used for query specific period.
-                The period is represented with int in Qlib. (e.g. 202001 may represent the first quarter in 2020)
-
-        Returns
-        ----------
-        pd.Series
-            feature series: The index of the series is the calendar index
-        """
-        from .cache import H  # pylint: disable=C0415
-
-        # cache
-        cache_key = str(self), instrument, start_index, end_index, *args
-        if cache_key in H["f"]:
-            return H["f"][cache_key]
-        if start_index is not None and end_index is not None and start_index > end_index:
-            raise ValueError("Invalid index range: {} {}".format(start_index, end_index))
-        try:
-            series = self._load_internal(instrument, start_index, end_index, *args)
-        except Exception as e:
-            get_module_logger("data").debug(
-                f"Loading data error: instrument={instrument}, expression={str(self)}, "
-                f"start_index={start_index}, end_index={end_index}, args={args}. "
-                f"error info: {str(e)}"
-            )
-            raise
-        series.name = str(self)
-        H["f"][cache_key] = series
-        return series
-
-    @abc.abstractmethod
-    def _load_internal(self, instrument, start_index, end_index, *args) -> pd.Series:
-        raise NotImplementedError("This function must be implemented in your newly defined feature")
-
-    @abc.abstractmethod
-    def get_longest_back_rolling(self):
-        """Get the longest length of historical data the feature has accessed
-
-        This is designed for getting the needed range of the data to calculate
-        the features in specific range at first.  However, situations like
-        Ref(Ref($close, -1), 1) can not be handled rightly.
-
-        So this will only used for detecting the length of historical data needed.
-        """
-        # TODO: forward operator like Ref($close, -1) is not supported yet.
-        raise NotImplementedError("This function must be implemented in your newly defined feature")
-
-    @abc.abstractmethod
-    def get_extended_window_size(self):
-        """get_extend_window_size
-
-        For to calculate this Operator in range[start_index, end_index]
-        We have to get the *leaf feature* in
-        range[start_index - lft_etd, end_index + rght_etd].
-
-        Returns
-        ----------
-        (int, int)
-            lft_etd, rght_etd
-        """
-        raise NotImplementedError("This function must be implemented in your newly defined feature")
-
-
-class Feature(Expression):
-    """Static Expression
-
-    This kind of feature will load data from provider
-    """
-
-    def __init__(self, name=None):
-        if name:
-            self._name = name
-        else:
-            self._name = type(self).__name__
-
-    def __str__(self):
-        return "$" + self._name
-
-    def _load_internal(self, instrument, start_index, end_index, freq):
-        # load
-        from .data import FeatureD  # pylint: disable=C0415
-
-        return FeatureD.feature(instrument, str(self), start_index, end_index, freq)
-
-    def get_longest_back_rolling(self):
-        return 0
-
-    def get_extended_window_size(self):
-        return 0, 0
-
-
-class PFeature(Feature):
-    def __str__(self):
-        return "$$" + self._name
-
-    def _load_internal(self, instrument, start_index, end_index, cur_time, period=None):
-        from .data import PITD  # pylint: disable=C0415
-
-        return PITD.period_feature(instrument, str(self), start_index, end_index, cur_time, period)
-
-
-class ExpressionOps(Expression):
-    """Operator Expression
-
-    This kind of feature will use operator for feature
-    construction on the fly.
-    """
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+
+from __future__ import division
+from __future__ import print_function
+
+import abc
+import pandas as pd
+from ..log import get_module_logger
+
+
+class Expression(abc.ABC):
+    """
+    Expression base class
+
+    Expression is designed to handle the calculation of data with the format below
+    data with two dimension for each instrument,
+
+    - feature
+    - time:  it  could be observation time or period time.
+
+        - period time is designed for Point-in-time database.  For example, the period time maybe 2014Q4, its value can observed for multiple times(different value may be observed at different time due to amendment).
+    """
+
+    def __str__(self):
+        return type(self).__name__
+
+    def __repr__(self):
+        return str(self)
+
+    def __gt__(self, other):
+        from .ops import Gt  # pylint: disable=C0415
+
+        return Gt(self, other)
+
+    def __ge__(self, other):
+        from .ops import Ge  # pylint: disable=C0415
+
+        return Ge(self, other)
+
+    def __lt__(self, other):
+        from .ops import Lt  # pylint: disable=C0415
+
+        return Lt(self, other)
+
+    def __le__(self, other):
+        from .ops import Le  # pylint: disable=C0415
+
+        return Le(self, other)
+
+    def __eq__(self, other):
+        from .ops import Eq  # pylint: disable=C0415
+
+        return Eq(self, other)
+
+    def __ne__(self, other):
+        from .ops import Ne  # pylint: disable=C0415
+
+        return Ne(self, other)
+
+    def __add__(self, other):
+        from .ops import Add  # pylint: disable=C0415
+
+        return Add(self, other)
+
+    def __radd__(self, other):
+        from .ops import Add  # pylint: disable=C0415
+
+        return Add(other, self)
+
+    def __sub__(self, other):
+        from .ops import Sub  # pylint: disable=C0415
+
+        return Sub(self, other)
+
+    def __rsub__(self, other):
+        from .ops import Sub  # pylint: disable=C0415
+
+        return Sub(other, self)
+
+    def __mul__(self, other):
+        from .ops import Mul  # pylint: disable=C0415
+
+        return Mul(self, other)
+
+    def __rmul__(self, other):
+        from .ops import Mul  # pylint: disable=C0415
+
+        return Mul(self, other)
+
+    def __div__(self, other):
+        from .ops import Div  # pylint: disable=C0415
+
+        return Div(self, other)
+
+    def __rdiv__(self, other):
+        from .ops import Div  # pylint: disable=C0415
+
+        return Div(other, self)
+
+    def __truediv__(self, other):
+        from .ops import Div  # pylint: disable=C0415
+
+        return Div(self, other)
+
+    def __rtruediv__(self, other):
+        from .ops import Div  # pylint: disable=C0415
+
+        return Div(other, self)
+
+    def __pow__(self, other):
+        from .ops import Power  # pylint: disable=C0415
+
+        return Power(self, other)
+
+    def __rpow__(self, other):
+        from .ops import Power  # pylint: disable=C0415
+
+        return Power(other, self)
+
+    def __and__(self, other):
+        from .ops import And  # pylint: disable=C0415
+
+        return And(self, other)
+
+    def __rand__(self, other):
+        from .ops import And  # pylint: disable=C0415
+
+        return And(other, self)
+
+    def __or__(self, other):
+        from .ops import Or  # pylint: disable=C0415
+
+        return Or(self, other)
+
+    def __ror__(self, other):
+        from .ops import Or  # pylint: disable=C0415
+
+        return Or(other, self)
+
+    def load(self, instrument, start_index, end_index, *args):
+        """load  feature
+        This function is responsible for loading feature/expression based on the expression engine.
+
+        The concrete implementation will be separated into two parts:
+
+        1) caching data, handle errors.
+
+            - This part is shared by all the expressions and implemented in Expression
+        2) processing and calculating data based on the specific expression.
+
+            - This part is different in each expression and implemented in each expression
+
+        Expression Engine is shared by different data.
+        Different data will have different extra information for `args`.
+
+        Parameters
+        ----------
+        instrument : str
+            instrument code.
+        start_index : str
+            feature start index [in calendar].
+        end_index : str
+            feature end  index  [in calendar].
+
+        *args may contain following information:
+        1) if it is used in basic expression engine data, it contains following arguments
+            freq: str
+                feature frequency.
+
+        2) if is used in PIT data, it contains following arguments
+            cur_pit:
+                it is designed for the point-in-time data.
+            period: int
+                This is used for query specific period.
+                The period is represented with int in Qlib. (e.g. 202001 may represent the first quarter in 2020)
+
+        Returns
+        ----------
+        pd.Series
+            feature series: The index of the series is the calendar index
+        """
+        from .cache import H  # pylint: disable=C0415
+
+        # cache
+        cache_key = str(self), instrument, start_index, end_index, *args
+        if cache_key in H["f"]:
+            return H["f"][cache_key]
+        if start_index is not None and end_index is not None and start_index > end_index:
+            raise ValueError("Invalid index range: {} {}".format(start_index, end_index))
+        try:
+            series = self._load_internal(instrument, start_index, end_index, *args)
+        except Exception as e:
+            get_module_logger("data").debug(
+                f"Loading data error: instrument={instrument}, expression={str(self)}, "
+                f"start_index={start_index}, end_index={end_index}, args={args}. "
+                f"error info: {str(e)}"
+            )
+            raise
+        series.name = str(self)
+        H["f"][cache_key] = series
+        return series
+
+    @abc.abstractmethod
+    def _load_internal(self, instrument, start_index, end_index, *args) -> pd.Series:
+        raise NotImplementedError("This function must be implemented in your newly defined feature")
+
+    @abc.abstractmethod
+    def get_longest_back_rolling(self):
+        """Get the longest length of historical data the feature has accessed
+
+        This is designed for getting the needed range of the data to calculate
+        the features in specific range at first.  However, situations like
+        Ref(Ref($close, -1), 1) can not be handled rightly.
+
+        So this will only used for detecting the length of historical data needed.
+        """
+        # TODO: forward operator like Ref($close, -1) is not supported yet.
+        raise NotImplementedError("This function must be implemented in your newly defined feature")
+
+    @abc.abstractmethod
+    def get_extended_window_size(self):
+        """get_extend_window_size
+
+        For to calculate this Operator in range[start_index, end_index]
+        We have to get the *leaf feature* in
+        range[start_index - lft_etd, end_index + rght_etd].
+
+        Returns
+        ----------
+        (int, int)
+            lft_etd, rght_etd
+        """
+        raise NotImplementedError("This function must be implemented in your newly defined feature")
+
+
+class Feature(Expression):
+    """Static Expression
+
+    This kind of feature will load data from provider
+    """
+
+    def __init__(self, name=None):
+        if name:
+            self._name = name
+        else:
+            self._name = type(self).__name__
+
+    def __str__(self):
+        return "$" + self._name
+
+    def _load_internal(self, instrument, start_index, end_index, freq):
+        # load
+        from .data import FeatureD  # pylint: disable=C0415
+
+        return FeatureD.feature(instrument, str(self), start_index, end_index, freq)
+
+    def get_longest_back_rolling(self):
+        return 0
+
+    def get_extended_window_size(self):
+        return 0, 0
+
+
+class PFeature(Feature):
+    def __str__(self):
+        return "$$" + self._name
+
+    def _load_internal(self, instrument, start_index, end_index, cur_time, period=None):
+        from .data import PITD  # pylint: disable=C0415
+
+        return PITD.period_feature(instrument, str(self), start_index, end_index, cur_time, period)
+
+
+class ExpressionOps(Expression):
+    """Operator Expression
+
+    This kind of feature will use operator for feature
+    construction on the fly.
+    """
```

## qlib/data/cache.py

```diff
@@ -1,1203 +1,1198 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-
-from __future__ import division
-from __future__ import print_function
-
-import os
-import sys
-import stat
-import time
-import pickle
-import traceback
-import redis_lock
-import contextlib
-import abc
-from pathlib import Path
-import numpy as np
-import pandas as pd
-from typing import Union, Iterable
-from collections import OrderedDict
-
-from ..config import C
-from ..utils import (
-    hash_args,
-    get_redis_connection,
-    read_bin,
-    parse_field,
-    remove_fields_space,
-    normalize_cache_fields,
-    normalize_cache_instruments,
-)
-
-from ..log import get_module_logger
-from .base import Feature
-from .ops import Operators  # pylint: disable=W0611  # noqa: F401
-
-
-class QlibCacheException(RuntimeError):
-    pass
-
-
-class MemCacheUnit(abc.ABC):
-    """Memory Cache Unit."""
-
-    def __init__(self, *args, **kwargs):
-        self.size_limit = kwargs.pop("size_limit", 0)
-        self._size = 0
-        self.od = OrderedDict()
-
-    def __setitem__(self, key, value):
-        # TODO: thread safe?__setitem__ failure might cause inconsistent size?
-
-        # precalculate the size after od.__setitem__
-        self._adjust_size(key, value)
-
-        self.od.__setitem__(key, value)
-
-        # move the key to end,make it latest
-        self.od.move_to_end(key)
-
-        if self.limited:
-            # pop the oldest items beyond size limit
-            while self._size > self.size_limit:
-                self.popitem(last=False)
-
-    def __getitem__(self, key):
-        v = self.od.__getitem__(key)
-        self.od.move_to_end(key)
-        return v
-
-    def __contains__(self, key):
-        return key in self.od
-
-    def __len__(self):
-        return self.od.__len__()
-
-    def __repr__(self):
-        return f"{self.__class__.__name__}<size_limit:{self.size_limit if self.limited else 'no limit'} total_size:{self._size}>\n{self.od.__repr__()}"
-
-    def set_limit_size(self, limit):
-        self.size_limit = limit
-
-    @property
-    def limited(self):
-        """whether memory cache is limited"""
-        return self.size_limit > 0
-
-    @property
-    def total_size(self):
-        return self._size
-
-    def clear(self):
-        self._size = 0
-        self.od.clear()
-
-    def popitem(self, last=True):
-        k, v = self.od.popitem(last=last)
-        self._size -= self._get_value_size(v)
-
-        return k, v
-
-    def pop(self, key):
-        v = self.od.pop(key)
-        self._size -= self._get_value_size(v)
-
-        return v
-
-    def _adjust_size(self, key, value):
-        if key in self.od:
-            self._size -= self._get_value_size(self.od[key])
-
-        self._size += self._get_value_size(value)
-
-    @abc.abstractmethod
-    def _get_value_size(self, value):
-        raise NotImplementedError
-
-
-class MemCacheLengthUnit(MemCacheUnit):
-    def __init__(self, size_limit=0):
-        super().__init__(size_limit=size_limit)
-
-    def _get_value_size(self, value):
-        return 1
-
-
-class MemCacheSizeofUnit(MemCacheUnit):
-    def __init__(self, size_limit=0):
-        super().__init__(size_limit=size_limit)
-
-    def _get_value_size(self, value):
-        return sys.getsizeof(value)
-
-
-class MemCache:
-    """Memory cache."""
-
-    def __init__(self, mem_cache_size_limit=None, limit_type="length"):
-        """
-
-        Parameters
-        ----------
-        mem_cache_size_limit:
-            cache max size.
-        limit_type:
-            length or sizeof; length(call fun: len), size(call fun: sys.getsizeof).
-        """
-
-        size_limit = C.mem_cache_size_limit if mem_cache_size_limit is None else mem_cache_size_limit
-        limit_type = C.mem_cache_limit_type if limit_type is None else limit_type
-
-        if limit_type == "length":
-            klass = MemCacheLengthUnit
-        elif limit_type == "sizeof":
-            klass = MemCacheSizeofUnit
-        else:
-            raise ValueError(f"limit_type must be length or sizeof, your limit_type is {limit_type}")
-
-        self.__calendar_mem_cache = klass(size_limit)
-        self.__instrument_mem_cache = klass(size_limit)
-        self.__feature_mem_cache = klass(size_limit)
-
-    def __getitem__(self, key):
-        if key == "c":
-            return self.__calendar_mem_cache
-        elif key == "i":
-            return self.__instrument_mem_cache
-        elif key == "f":
-            return self.__feature_mem_cache
-        else:
-            raise KeyError("Unknown memcache unit")
-
-    def clear(self):
-        self.__calendar_mem_cache.clear()
-        self.__instrument_mem_cache.clear()
-        self.__feature_mem_cache.clear()
-
-
-class MemCacheExpire:
-    CACHE_EXPIRE = C.mem_cache_expire
-
-    @staticmethod
-    def set_cache(mem_cache, key, value):
-        """set cache
-
-        :param mem_cache: MemCache attribute('c'/'i'/'f').
-        :param key: cache key.
-        :param value: cache value.
-        """
-        mem_cache[key] = value, time.time()
-
-    @staticmethod
-    def get_cache(mem_cache, key):
-        """get mem cache
-
-        :param mem_cache: MemCache attribute('c'/'i'/'f').
-        :param key: cache key.
-        :return: cache value; if cache not exist, return None.
-        """
-        value = None
-        expire = False
-        if key in mem_cache:
-            value, latest_time = mem_cache[key]
-            expire = (time.time() - latest_time) > MemCacheExpire.CACHE_EXPIRE
-        return value, expire
-
-
-class CacheUtils:
-    LOCK_ID = "QLIB"
-
-    @staticmethod
-    def organize_meta_file():
-        pass
-
-    @staticmethod
-    def reset_lock():
-        r = get_redis_connection()
-        redis_lock.reset_all(r)
-
-    @staticmethod
-    def visit(cache_path: Union[str, Path]):
-        # FIXME: Because read_lock was canceled when reading the cache, multiple processes may have read and write exceptions here
-        try:
-            cache_path = Path(cache_path)
-            meta_path = cache_path.with_suffix(".meta")
-            with meta_path.open("rb") as f:
-                d = pickle.load(f)
-            with meta_path.open("wb") as f:
-                try:
-                    d["meta"]["last_visit"] = str(time.time())
-                    d["meta"]["visits"] = d["meta"]["visits"] + 1
-                except KeyError as key_e:
-                    raise KeyError("Unknown meta keyword") from key_e
-                pickle.dump(d, f, protocol=C.dump_protocol_version)
-        except Exception as e:
-            get_module_logger("CacheUtils").warning(f"visit {cache_path} cache error: {e}")
-
-    @staticmethod
-    def acquire(lock, lock_name):
-        try:
-            lock.acquire()
-        except redis_lock.AlreadyAcquired as lock_acquired:
-            raise QlibCacheException(
-                f"""It sees the key(lock:{repr(lock_name)[1:-1]}-wlock) of the redis lock has existed in your redis db now.
-                    You can use the following command to clear your redis keys and rerun your commands:
-                    $ redis-cli
-                    > select {C.redis_task_db}
-                    > del "lock:{repr(lock_name)[1:-1]}-wlock"
-                    > quit
-                    If the issue is not resolved, use "keys *" to find if multiple keys exist. If so, try using "flushall" to clear all the keys.
-                """
-            ) from lock_acquired
-
-    @staticmethod
-    @contextlib.contextmanager
-    def reader_lock(redis_t, lock_name: str):
-        current_cache_rlock = redis_lock.Lock(redis_t, f"{lock_name}-rlock")
-        current_cache_wlock = redis_lock.Lock(redis_t, f"{lock_name}-wlock")
-        lock_reader = f"{lock_name}-reader"
-        # make sure only one reader is entering
-        current_cache_rlock.acquire(timeout=60)
-        try:
-            current_cache_readers = redis_t.get(lock_reader)
-            if current_cache_readers is None or int(current_cache_readers) == 0:
-                CacheUtils.acquire(current_cache_wlock, lock_name)
-            redis_t.incr(lock_reader)
-        finally:
-            current_cache_rlock.release()
-        try:
-            yield
-        finally:
-            # make sure only one reader is leaving
-            current_cache_rlock.acquire(timeout=60)
-            try:
-                redis_t.decr(lock_reader)
-                if int(redis_t.get(lock_reader)) == 0:
-                    redis_t.delete(lock_reader)
-                    current_cache_wlock.reset()
-            finally:
-                current_cache_rlock.release()
-
-    @staticmethod
-    @contextlib.contextmanager
-    def writer_lock(redis_t, lock_name):
-        current_cache_wlock = redis_lock.Lock(redis_t, f"{lock_name}-wlock", id=CacheUtils.LOCK_ID)
-        CacheUtils.acquire(current_cache_wlock, lock_name)
-        try:
-            yield
-        finally:
-            current_cache_wlock.release()
-
-
-class BaseProviderCache:
-    """Provider cache base class"""
-
-    def __init__(self, provider):
-        self.provider = provider
-        self.logger = get_module_logger(self.__class__.__name__)
-
-    def __getattr__(self, attr):
-        return getattr(self.provider, attr)
-
-    @staticmethod
-    def check_cache_exists(cache_path: Union[str, Path], suffix_list: Iterable = (".index", ".meta")) -> bool:
-        cache_path = Path(cache_path)
-        for p in [cache_path] + [cache_path.with_suffix(_s) for _s in suffix_list]:
-            if not p.exists():
-                return False
-        return True
-
-    @staticmethod
-    def clear_cache(cache_path: Union[str, Path]):
-        for p in [
-            cache_path,
-            cache_path.with_suffix(".meta"),
-            cache_path.with_suffix(".index"),
-        ]:
-            if p.exists():
-                p.unlink()
-
-    @staticmethod
-    def get_cache_dir(dir_name: str, freq: str = None) -> Path:
-        cache_dir = Path(C.dpm.get_data_uri(freq)).joinpath(dir_name)
-        cache_dir.mkdir(parents=True, exist_ok=True)
-        return cache_dir
-
-
-class ExpressionCache(BaseProviderCache):
-    """Expression cache mechanism base class.
-
-    This class is used to wrap expression provider with self-defined expression cache mechanism.
-
-    .. note:: Override the `_uri` and `_expression` method to create your own expression cache mechanism.
-    """
-
-    def expression(self, instrument, field, start_time, end_time, freq):
-        """Get expression data.
-
-        .. note:: Same interface as `expression` method in expression provider
-        """
-        try:
-            return self._expression(instrument, field, start_time, end_time, freq)
-        except NotImplementedError:
-            return self.provider.expression(instrument, field, start_time, end_time, freq)
-
-    def _uri(self, instrument, field, start_time, end_time, freq):
-        """Get expression cache file uri.
-
-        Override this method to define how to get expression cache file uri corresponding to users' own cache mechanism.
-        """
-        raise NotImplementedError("Implement this function to match your own cache mechanism")
-
-    def _expression(self, instrument, field, start_time, end_time, freq):
-        """Get expression data using cache.
-
-        Override this method to define how to get expression data corresponding to users' own cache mechanism.
-        """
-        raise NotImplementedError("Implement this method if you want to use expression cache")
-
-    def update(self, cache_uri: Union[str, Path], freq: str = "day"):
-        """Update expression cache to latest calendar.
-
-        Override this method to define how to update expression cache corresponding to users' own cache mechanism.
-
-        Parameters
-        ----------
-        cache_uri : str or Path
-            the complete uri of expression cache file (include dir path).
-        freq : str
-
-        Returns
-        -------
-        int
-            0(successful update)/ 1(no need to update)/ 2(update failure).
-        """
-        raise NotImplementedError("Implement this method if you want to make expression cache up to date")
-
-
-class DatasetCache(BaseProviderCache):
-    """Dataset cache mechanism base class.
-
-    This class is used to wrap dataset provider with self-defined dataset cache mechanism.
-
-    .. note:: Override the `_uri` and `_dataset` method to create your own dataset cache mechanism.
-    """
-
-    HDF_KEY = "df"
-
-    def dataset(
-        self, instruments, fields, start_time=None, end_time=None, freq="day", disk_cache=1, inst_processors=[]
-    ):
-        """Get feature dataset.
-
-        .. note:: Same interface as `dataset` method in dataset provider
-
-        .. note:: The server use redis_lock to make sure
-            read-write conflicts will not be triggered
-            but client readers are not considered.
-        """
-        if disk_cache == 0:
-            # skip cache
-            return self.provider.dataset(
-                instruments, fields, start_time, end_time, freq, inst_processors=inst_processors
-            )
-        else:
-            # use and replace cache
-            try:
-                return self._dataset(
-                    instruments, fields, start_time, end_time, freq, disk_cache, inst_processors=inst_processors
-                )
-            except NotImplementedError:
-                return self.provider.dataset(
-                    instruments, fields, start_time, end_time, freq, inst_processors=inst_processors
-                )
-
-    def _uri(self, instruments, fields, start_time, end_time, freq, **kwargs):
-        """Get dataset cache file uri.
-
-        Override this method to define how to get dataset cache file uri corresponding to users' own cache mechanism.
-        """
-        raise NotImplementedError("Implement this function to match your own cache mechanism")
-
-    def _dataset(
-        self, instruments, fields, start_time=None, end_time=None, freq="day", disk_cache=1, inst_processors=[]
-    ):
-        """Get feature dataset using cache.
-
-        Override this method to define how to get feature dataset corresponding to users' own cache mechanism.
-        """
-        raise NotImplementedError("Implement this method if you want to use dataset feature cache")
-
-    def _dataset_uri(
-        self, instruments, fields, start_time=None, end_time=None, freq="day", disk_cache=1, inst_processors=[]
-    ):
-        """Get a uri of feature dataset using cache.
-        specially:
-            disk_cache=1 means using data set cache and return the uri of cache file.
-            disk_cache=0 means client knows the path of expression cache,
-                         server checks if the cache exists(if not, generate it), and client loads data by itself.
-        Override this method to define how to get feature dataset uri corresponding to users' own cache mechanism.
-        """
-        raise NotImplementedError(
-            "Implement this method if you want to use dataset feature cache as a cache file for client"
-        )
-
-    def update(self, cache_uri: Union[str, Path], freq: str = "day"):
-        """Update dataset cache to latest calendar.
-
-        Override this method to define how to update dataset cache corresponding to users' own cache mechanism.
-
-        Parameters
-        ----------
-        cache_uri : str or Path
-            the complete uri of dataset cache file (include dir path).
-        freq : str
-
-        Returns
-        -------
-        int
-            0(successful update)/ 1(no need to update)/ 2(update failure)
-        """
-        raise NotImplementedError("Implement this method if you want to make expression cache up to date")
-
-    @staticmethod
-    def cache_to_origin_data(data, fields):
-        """cache data to origin data
-
-        :param data: pd.DataFrame, cache data.
-        :param fields: feature fields.
-        :return: pd.DataFrame.
-        """
-        not_space_fields = remove_fields_space(fields)
-        data = data.loc[:, not_space_fields]
-        # set features fields
-        data.columns = [str(i) for i in fields]
-        return data
-
-    @staticmethod
-    def normalize_uri_args(instruments, fields, freq):
-        """normalize uri args"""
-        instruments = normalize_cache_instruments(instruments)
-        fields = normalize_cache_fields(fields)
-        freq = freq.lower()
-
-        return instruments, fields, freq
-
-
-class DiskExpressionCache(ExpressionCache):
-    """Prepared cache mechanism for server."""
-
-    def __init__(self, provider, **kwargs):
-        super(DiskExpressionCache, self).__init__(provider)
-        self.r = get_redis_connection()
-        # remote==True means client is using this module, writing behaviour will not be allowed.
-        self.remote = kwargs.get("remote", False)
-
-    def get_cache_dir(self, freq: str = None) -> Path:
-        return super(DiskExpressionCache, self).get_cache_dir(C.features_cache_dir_name, freq)
-
-    def _uri(self, instrument, field, start_time, end_time, freq):
-        field = remove_fields_space(field)
-        instrument = str(instrument).lower()
-        return hash_args(instrument, field, freq)
-
-    def _expression(self, instrument, field, start_time=None, end_time=None, freq="day"):
-        _cache_uri = self._uri(instrument=instrument, field=field, start_time=None, end_time=None, freq=freq)
-        _instrument_dir = self.get_cache_dir(freq).joinpath(instrument.lower())
-        cache_path = _instrument_dir.joinpath(_cache_uri)
-        # get calendar
-        from .data import Cal  # pylint: disable=C0415
-
-        _calendar = Cal.calendar(freq=freq)
-
-        _, _, start_index, end_index = Cal.locate_index(start_time, end_time, freq, future=False)
-
-        if self.check_cache_exists(cache_path, suffix_list=[".meta"]):
-            """
-            In most cases, we do not need reader_lock.
-            Because updating data is a small probability event compare to reading data.
-
-            """
-            # FIXME: Removing the reader lock may result in conflicts.
-            # with CacheUtils.reader_lock(self.r, 'expression-%s' % _cache_uri):
-
-            # modify expression cache meta file
-            try:
-                # FIXME: Multiple readers may result in error visit number
-                if not self.remote:
-                    CacheUtils.visit(cache_path)
-                series = read_bin(cache_path, start_index, end_index)
-                return series
-            except Exception:
-                series = None
-                self.logger.error("reading %s file error : %s" % (cache_path, traceback.format_exc()))
-            return series
-        else:
-            # normalize field
-            field = remove_fields_space(field)
-            # cache unavailable, generate the cache
-            _instrument_dir.mkdir(parents=True, exist_ok=True)
-            if not isinstance(eval(parse_field(field)), Feature):
-                # When the expression is not a raw feature
-                # generate expression cache if the feature is not a Feature
-                # instance
-                series = self.provider.expression(instrument, field, _calendar[0], _calendar[-1], freq)
-                if not series.empty:
-                    # This expression is empty, we don't generate any cache for it.
-                    with CacheUtils.writer_lock(self.r, f"{str(C.dpm.get_data_uri(freq))}:expression-{_cache_uri}"):
-                        self.gen_expression_cache(
-                            expression_data=series,
-                            cache_path=cache_path,
-                            instrument=instrument,
-                            field=field,
-                            freq=freq,
-                            last_update=str(_calendar[-1]),
-                        )
-                    return series.loc[start_index:end_index]
-                else:
-                    return series
-            else:
-                # If the expression is a raw feature(such as $close, $open)
-                return self.provider.expression(instrument, field, start_time, end_time, freq)
-
-    def gen_expression_cache(self, expression_data, cache_path, instrument, field, freq, last_update):
-        """use bin file to save like feature-data."""
-        # Make sure the cache runs right when the directory is deleted
-        # while running
-        meta = {
-            "info": {"instrument": instrument, "field": field, "freq": freq, "last_update": last_update},
-            "meta": {"last_visit": time.time(), "visits": 1},
-        }
-        self.logger.debug(f"generating expression cache: {meta}")
-        self.clear_cache(cache_path)
-        meta_path = cache_path.with_suffix(".meta")
-
-        with meta_path.open("wb") as f:
-            pickle.dump(meta, f, protocol=C.dump_protocol_version)
-        meta_path.chmod(stat.S_IRWXU | stat.S_IRGRP | stat.S_IROTH)
-        df = expression_data.to_frame()
-
-        r = np.hstack([df.index[0], expression_data]).astype("<f")
-        r.tofile(str(cache_path))
-
-    def update(self, sid, cache_uri, freq: str = "day"):
-
-        cp_cache_uri = self.get_cache_dir(freq).joinpath(sid).joinpath(cache_uri)
-        meta_path = cp_cache_uri.with_suffix(".meta")
-        if not self.check_cache_exists(cp_cache_uri, suffix_list=[".meta"]):
-            self.logger.info(f"The cache {cp_cache_uri} has corrupted. It will be removed")
-            self.clear_cache(cp_cache_uri)
-            return 2
-
-        with CacheUtils.writer_lock(self.r, f"{str(C.dpm.get_data_uri())}:expression-{cache_uri}"):
-            with meta_path.open("rb") as f:
-                d = pickle.load(f)
-            instrument = d["info"]["instrument"]
-            field = d["info"]["field"]
-            freq = d["info"]["freq"]
-            last_update_time = d["info"]["last_update"]
-
-            # get newest calendar
-            from .data import Cal, ExpressionD  # pylint: disable=C0415
-
-            whole_calendar = Cal.calendar(start_time=None, end_time=None, freq=freq)
-            # calendar since last updated.
-            new_calendar = Cal.calendar(start_time=last_update_time, end_time=None, freq=freq)
-
-            # get append data
-            if len(new_calendar) <= 1:
-                # Including last updated calendar, we only get 1 item.
-                # No future updating is needed.
-                return 1
-            else:
-                # get the data needed after the historical data are removed.
-                # The start index of new data
-                current_index = len(whole_calendar) - len(new_calendar) + 1
-
-                # The existing data length
-                size_bytes = os.path.getsize(cp_cache_uri)
-                ele_size = np.dtype("<f").itemsize
-                assert size_bytes % ele_size == 0
-                ele_n = size_bytes // ele_size - 1
-
-                expr = ExpressionD.get_expression_instance(field)
-                lft_etd, rght_etd = expr.get_extended_window_size()
-                # The expression used the future data after rght_etd days.
-                # So the last rght_etd data should be removed.
-                # There are most `ele_n` period of data can be remove
-                remove_n = min(rght_etd, ele_n)
-                assert new_calendar[1] == whole_calendar[current_index]
-                data = self.provider.expression(
-                    instrument, field, whole_calendar[current_index - remove_n], new_calendar[-1], freq
-                )
-                with open(cp_cache_uri, "ab") as f:
-                    data = np.array(data).astype("<f")
-                    # Remove the last bits
-                    f.truncate(size_bytes - ele_size * remove_n)
-                    f.write(data)
-                # update meta file
-                d["info"]["last_update"] = str(new_calendar[-1])
-                with meta_path.open("wb") as f:
-                    pickle.dump(d, f, protocol=C.dump_protocol_version)
-        return 0
-
-
-class DiskDatasetCache(DatasetCache):
-    """Prepared cache mechanism for server."""
-
-    def __init__(self, provider, **kwargs):
-        super(DiskDatasetCache, self).__init__(provider)
-        self.r = get_redis_connection()
-        self.remote = kwargs.get("remote", False)
-
-    @staticmethod
-    def _uri(instruments, fields, start_time, end_time, freq, disk_cache=1, inst_processors=[], **kwargs):
-        return hash_args(*DatasetCache.normalize_uri_args(instruments, fields, freq), disk_cache, inst_processors)
-
-    def get_cache_dir(self, freq: str = None) -> Path:
-        return super(DiskDatasetCache, self).get_cache_dir(C.dataset_cache_dir_name, freq)
-
-    @classmethod
-    def read_data_from_cache(cls, cache_path: Union[str, Path], start_time, end_time, fields):
-        """read_cache_from
-
-        This function can read data from the disk cache dataset
-
-        :param cache_path:
-        :param start_time:
-        :param end_time:
-        :param fields: The fields order of the dataset cache is sorted. So rearrange the columns to make it consistent.
-        :return:
-        """
-
-        im = DiskDatasetCache.IndexManager(cache_path)
-        index_data = im.get_index(start_time, end_time)
-        if index_data.shape[0] > 0:
-            start, stop = (
-                index_data["start"].iloc[0].item(),
-                index_data["end"].iloc[-1].item(),
-            )
-        else:
-            start = stop = 0
-
-        with pd.HDFStore(cache_path, mode="r") as store:
-            if "/{}".format(im.KEY) in store.keys():
-                df = store.select(key=im.KEY, start=start, stop=stop)
-                df = df.swaplevel("datetime", "instrument").sort_index()
-                # read cache and need to replace not-space fields to field
-                df = cls.cache_to_origin_data(df, fields)
-
-            else:
-                df = pd.DataFrame(columns=fields)
-        return df
-
-    def _dataset(
-        self, instruments, fields, start_time=None, end_time=None, freq="day", disk_cache=0, inst_processors=[]
-    ):
-
-        if disk_cache == 0:
-            # In this case, data_set cache is configured but will not be used.
-            return self.provider.dataset(
-                instruments, fields, start_time, end_time, freq, inst_processors=inst_processors
-            )
-        # FIXME: The cache after resample, when read again and intercepted with end_time, results in incomplete data date
-        if inst_processors:
-            raise ValueError(
-                f"{self.__class__.__name__} does not support inst_processor. "
-                f"Please use `D.features(disk_cache=0)` or `qlib.init(dataset_cache=None)`"
-            )
-        _cache_uri = self._uri(
-            instruments=instruments,
-            fields=fields,
-            start_time=None,
-            end_time=None,
-            freq=freq,
-            disk_cache=disk_cache,
-            inst_processors=inst_processors,
-        )
-
-        cache_path = self.get_cache_dir(freq).joinpath(_cache_uri)
-
-        features = pd.DataFrame()
-        gen_flag = False
-
-        if self.check_cache_exists(cache_path):
-            if disk_cache == 1:
-                # use cache
-                with CacheUtils.reader_lock(self.r, f"{str(C.dpm.get_data_uri(freq))}:dataset-{_cache_uri}"):
-                    CacheUtils.visit(cache_path)
-                    features = self.read_data_from_cache(cache_path, start_time, end_time, fields)
-            elif disk_cache == 2:
-                gen_flag = True
-        else:
-            gen_flag = True
-
-        if gen_flag:
-            # cache unavailable, generate the cache
-            with CacheUtils.writer_lock(self.r, f"{str(C.dpm.get_data_uri(freq))}:dataset-{_cache_uri}"):
-                features = self.gen_dataset_cache(
-                    cache_path=cache_path,
-                    instruments=instruments,
-                    fields=fields,
-                    freq=freq,
-                    inst_processors=inst_processors,
-                )
-            if not features.empty:
-                features = features.sort_index().loc(axis=0)[:, start_time:end_time]
-        return features
-
-    def _dataset_uri(
-        self, instruments, fields, start_time=None, end_time=None, freq="day", disk_cache=0, inst_processors=[]
-    ):
-        if disk_cache == 0:
-            # In this case, server only checks the expression cache.
-            # The client will load the cache data by itself.
-            from .data import LocalDatasetProvider  # pylint: disable=C0415
-
-            LocalDatasetProvider.multi_cache_walker(instruments, fields, start_time, end_time, freq)
-            return ""
-        # FIXME: The cache after resample, when read again and intercepted with end_time, results in incomplete data date
-        if inst_processors:
-            raise ValueError(
-                f"{self.__class__.__name__} does not support inst_processor. "
-                f"Please use `D.features(disk_cache=0)` or `qlib.init(dataset_cache=None)`"
-            )
-        _cache_uri = self._uri(
-            instruments=instruments,
-            fields=fields,
-            start_time=None,
-            end_time=None,
-            freq=freq,
-            disk_cache=disk_cache,
-            inst_processors=inst_processors,
-        )
-        cache_path = self.get_cache_dir(freq).joinpath(_cache_uri)
-
-        if self.check_cache_exists(cache_path):
-            self.logger.debug(f"The cache dataset has already existed {cache_path}. Return the uri directly")
-            with CacheUtils.reader_lock(self.r, f"{str(C.dpm.get_data_uri(freq))}:dataset-{_cache_uri}"):
-                CacheUtils.visit(cache_path)
-            return _cache_uri
-        else:
-            # cache unavailable, generate the cache
-            with CacheUtils.writer_lock(self.r, f"{str(C.dpm.get_data_uri(freq))}:dataset-{_cache_uri}"):
-                self.gen_dataset_cache(
-                    cache_path=cache_path,
-                    instruments=instruments,
-                    fields=fields,
-                    freq=freq,
-                    inst_processors=inst_processors,
-                )
-            return _cache_uri
-
-    class IndexManager:
-        """
-        The lock is not considered in the class. Please consider the lock outside the code.
-        This class is the proxy of the disk data.
-        """
-
-        KEY = "df"
-
-        def __init__(self, cache_path: Union[str, Path]):
-
-            self.index_path = cache_path.with_suffix(".index")
-            self._data = None
-            self.logger = get_module_logger(self.__class__.__name__)
-
-        def get_index(self, start_time=None, end_time=None):
-            # TODO: fast read index from the disk.
-            if self._data is None:
-                self.sync_from_disk()
-            return self._data.loc[start_time:end_time].copy()
-
-        def sync_to_disk(self):
-            if self._data is None:
-                raise ValueError("No data to sync to disk.")
-            self._data.sort_index(inplace=True)
-            self._data.to_hdf(self.index_path, key=self.KEY, mode="w", format="table")
-            # The index should be readable for all users
-            self.index_path.chmod(stat.S_IRWXU | stat.S_IRGRP | stat.S_IROTH)
-
-        def sync_from_disk(self):
-            # The file will not be closed directly if we read_hdf from the disk directly
-            with pd.HDFStore(self.index_path, mode="r") as store:
-                if "/{}".format(self.KEY) in store.keys():
-                    self._data = pd.read_hdf(store, key=self.KEY)
-                else:
-                    self._data = pd.DataFrame()
-
-        def update(self, data, sync=True):
-            self._data = data.astype(np.int32).copy()
-            if sync:
-                self.sync_to_disk()
-
-        def append_index(self, data, to_disk=True):
-            data = data.astype(np.int32).copy()
-            data.sort_index(inplace=True)
-            self._data = pd.concat([self._data, data])
-            if to_disk:
-                with pd.HDFStore(self.index_path) as store:
-                    store.append(self.KEY, data, append=True)
-
-        @staticmethod
-        def build_index_from_data(data, start_index=0):
-            if data.empty:
-                return pd.DataFrame()
-            line_data = data.groupby("datetime").size()
-            line_data.sort_index(inplace=True)
-            index_end = line_data.cumsum()
-            index_start = index_end.shift(1, fill_value=0)
-
-            index_data = pd.DataFrame()
-            index_data["start"] = index_start
-            index_data["end"] = index_end
-            index_data += start_index
-            return index_data
-
-    def gen_dataset_cache(self, cache_path: Union[str, Path], instruments, fields, freq, inst_processors=[]):
-        """gen_dataset_cache
-
-        .. note:: This function does not consider the cache read write lock. Please
-            acquire the lock outside this function
-
-        The format the cache contains 3 parts(followed by typical filename).
-
-        - index : cache/d41366901e25de3ec47297f12e2ba11d.index
-
-            - The content of the file may be in following format(pandas.Series)
-
-                .. code-block:: python
-
-                                        start end
-                    1999-11-10 00:00:00     0   1
-                    1999-11-11 00:00:00     1   2
-                    1999-11-12 00:00:00     2   3
-                    ...
-
-                .. note:: The start is closed. The end is open!!!!!
-
-            - Each line contains two element <start_index, end_index> with a timestamp as its index.
-            - It indicates the `start_index` (included) and `end_index` (excluded) of the data for `timestamp`
-
-        - meta data: cache/d41366901e25de3ec47297f12e2ba11d.meta
-
-        - data     : cache/d41366901e25de3ec47297f12e2ba11d
-
-            - This is a hdf file sorted by datetime
-
-        :param cache_path:  The path to store the cache.
-        :param instruments:  The instruments to store the cache.
-        :param fields:  The fields to store the cache.
-        :param freq:  The freq to store the cache.
-        :param inst_processors:  Instrument processors.
-
-        :return type pd.DataFrame; The fields of the returned DataFrame are consistent with the parameters of the function.
-        """
-        # get calendar
-        from .data import Cal  # pylint: disable=C0415
-
-        cache_path = Path(cache_path)
-        _calendar = Cal.calendar(freq=freq)
-        self.logger.debug(f"Generating dataset cache {cache_path}")
-        # Make sure the cache runs right when the directory is deleted
-        # while running
-        self.clear_cache(cache_path)
-
-        features = self.provider.dataset(
-            instruments, fields, _calendar[0], _calendar[-1], freq, inst_processors=inst_processors
-        )
-
-        if features.empty:
-            return features
-
-        # swap index and sorted
-        features = features.swaplevel("instrument", "datetime").sort_index()
-
-        # write cache data
-        with pd.HDFStore(str(cache_path.with_suffix(".data"))) as store:
-            cache_to_orig_map = dict(zip(remove_fields_space(features.columns), features.columns))
-            orig_to_cache_map = dict(zip(features.columns, remove_fields_space(features.columns)))
-            cache_features = features[list(cache_to_orig_map.values())].rename(columns=orig_to_cache_map)
-            # cache columns
-            cache_columns = sorted(cache_features.columns)
-            cache_features = cache_features.loc[:, cache_columns]
-            cache_features = cache_features.loc[:, ~cache_features.columns.duplicated()]
-            store.append(DatasetCache.HDF_KEY, cache_features, append=False)
-        # write meta file
-        meta = {
-            "info": {
-                "instruments": instruments,
-                "fields": list(cache_features.columns),
-                "freq": freq,
-                "last_update": str(_calendar[-1]),  # The last_update to store the cache
-                "inst_processors": inst_processors,  # The last_update to store the cache
-            },
-            "meta": {"last_visit": time.time(), "visits": 1},
-        }
-        with cache_path.with_suffix(".meta").open("wb") as f:
-            pickle.dump(meta, f, protocol=C.dump_protocol_version)
-        cache_path.with_suffix(".meta").chmod(stat.S_IRWXU | stat.S_IRGRP | stat.S_IROTH)
-        # write index file
-        im = DiskDatasetCache.IndexManager(cache_path)
-        index_data = im.build_index_from_data(features)
-        im.update(index_data)
-
-        # rename the file after the cache has been generated
-        # this doesn't work well on windows, but our server won't use windows
-        # temporarily
-        cache_path.with_suffix(".data").rename(cache_path)
-        # the fields of the cached features are converted to the original fields
-        return features.swaplevel("datetime", "instrument")
-
-    def update(self, cache_uri, freq: str = "day"):
-        cp_cache_uri = self.get_cache_dir(freq).joinpath(cache_uri)
-        meta_path = cp_cache_uri.with_suffix(".meta")
-        if not self.check_cache_exists(cp_cache_uri):
-            self.logger.info(f"The cache {cp_cache_uri} has corrupted. It will be removed")
-            self.clear_cache(cp_cache_uri)
-            return 2
-
-        im = DiskDatasetCache.IndexManager(cp_cache_uri)
-        with CacheUtils.writer_lock(self.r, f"{str(C.dpm.get_data_uri())}:dataset-{cache_uri}"):
-            with meta_path.open("rb") as f:
-                d = pickle.load(f)
-            instruments = d["info"]["instruments"]
-            fields = d["info"]["fields"]
-            freq = d["info"]["freq"]
-            last_update_time = d["info"]["last_update"]
-            inst_processors = d["info"].get("inst_processors", [])
-            index_data = im.get_index()
-
-            self.logger.debug("Updating dataset: {}".format(d))
-            from .data import Inst  # pylint: disable=C0415
-
-            if Inst.get_inst_type(instruments) == Inst.DICT:
-                self.logger.info(f"The file {cache_uri} has dict cache. Skip updating")
-                return 1
-
-            # get newest calendar
-            from .data import Cal  # pylint: disable=C0415
-
-            whole_calendar = Cal.calendar(start_time=None, end_time=None, freq=freq)
-            # The calendar since last updated
-            new_calendar = Cal.calendar(start_time=last_update_time, end_time=None, freq=freq)
-
-            # get append data
-            if len(new_calendar) <= 1:
-                # Including last updated calendar, we only get 1 item.
-                # No future updating is needed.
-                return 1
-            else:
-                # get the data needed after the historical data are removed.
-                # The start index of new data
-                current_index = len(whole_calendar) - len(new_calendar) + 1
-
-                # To avoid recursive import
-                from .data import ExpressionD  # pylint: disable=C0415
-
-                # The existing data length
-                lft_etd = rght_etd = 0
-                for field in fields:
-                    expr = ExpressionD.get_expression_instance(field)
-                    l, r = expr.get_extended_window_size()
-                    lft_etd = max(lft_etd, l)
-                    rght_etd = max(rght_etd, r)
-                # remove the period that should be updated.
-                if index_data.empty:
-                    # We don't have any data for such dataset. Nothing to remove
-                    rm_n_period = rm_lines = 0
-                else:
-                    rm_n_period = min(rght_etd, index_data.shape[0])
-                    rm_lines = (
-                        (index_data["end"] - index_data["start"])
-                        .loc[whole_calendar[current_index - rm_n_period] :]
-                        .sum()
-                        .item()
-                    )
-
-                data = self.provider.dataset(
-                    instruments,
-                    fields,
-                    whole_calendar[current_index - rm_n_period],
-                    new_calendar[-1],
-                    freq,
-                    inst_processors=inst_processors,
-                )
-
-                if not data.empty:
-                    data.reset_index(inplace=True)
-                    data.set_index(["datetime", "instrument"], inplace=True)
-                    data.sort_index(inplace=True)
-                else:
-                    return 0  # No data to update cache
-
-                store = pd.HDFStore(cp_cache_uri)
-                # FIXME:
-                # Because the feature cache are stored as .bin file.
-                # So the series read from features are all float32.
-                # However, the first dataset cache is calculated based on the
-                # raw data. So the data type may be float64.
-                # Different data type will result in failure of appending data
-                if "/{}".format(DatasetCache.HDF_KEY) in store.keys():
-                    schema = store.select(DatasetCache.HDF_KEY, start=0, stop=0)
-                    for col, dtype in schema.dtypes.items():
-                        data[col] = data[col].astype(dtype)
-                if rm_lines > 0:
-                    store.remove(key=im.KEY, start=-rm_lines)
-                store.append(DatasetCache.HDF_KEY, data)
-                store.close()
-
-                # update index file
-                new_index_data = im.build_index_from_data(
-                    data.loc(axis=0)[whole_calendar[current_index] :, :],
-                    start_index=0 if index_data.empty else index_data["end"].iloc[-1],
-                )
-                im.append_index(new_index_data)
-
-                # update meta file
-                d["info"]["last_update"] = str(new_calendar[-1])
-                with meta_path.open("wb") as f:
-                    pickle.dump(d, f, protocol=C.dump_protocol_version)
-                return 0
-
-
-class SimpleDatasetCache(DatasetCache):
-    """Simple dataset cache that can be used locally or on client."""
-
-    def __init__(self, provider):
-        super(SimpleDatasetCache, self).__init__(provider)
-        try:
-            self.local_cache_path: Path = Path(C["local_cache_path"]).expanduser().resolve()
-        except (KeyError, TypeError):
-            self.logger.error("Assign a local_cache_path in config if you want to use this cache mechanism")
-            raise
-        self.logger.info(
-            f"DatasetCache directory: {self.local_cache_path}, "
-            f"modify the cache directory via the local_cache_path in the config"
-        )
-
-    def _uri(self, instruments, fields, start_time, end_time, freq, disk_cache=1, inst_processors=[], **kwargs):
-        instruments, fields, freq = self.normalize_uri_args(instruments, fields, freq)
-        return hash_args(
-            instruments, fields, start_time, end_time, freq, disk_cache, str(self.local_cache_path), inst_processors
-        )
-
-    def _dataset(
-        self, instruments, fields, start_time=None, end_time=None, freq="day", disk_cache=1, inst_processors=[]
-    ):
-        if disk_cache == 0:
-            # In this case, data_set cache is configured but will not be used.
-            return self.provider.dataset(instruments, fields, start_time, end_time, freq)
-        self.local_cache_path.mkdir(exist_ok=True, parents=True)
-        cache_file = self.local_cache_path.joinpath(
-            self._uri(
-                instruments, fields, start_time, end_time, freq, disk_cache=disk_cache, inst_processors=inst_processors
-            )
-        )
-        gen_flag = False
-
-        if cache_file.exists():
-            if disk_cache == 1:
-                # use cache
-                df = pd.read_pickle(cache_file)
-                return self.cache_to_origin_data(df, fields)
-            elif disk_cache == 2:
-                # replace cache
-                gen_flag = True
-        else:
-            gen_flag = True
-
-        if gen_flag:
-            data = self.provider.dataset(
-                instruments, normalize_cache_fields(fields), start_time, end_time, freq, inst_processors=inst_processors
-            )
-            data.to_pickle(cache_file)
-            return self.cache_to_origin_data(data, fields)
-
-
-class DatasetURICache(DatasetCache):
-    """Prepared cache mechanism for server."""
-
-    def _uri(self, instruments, fields, start_time, end_time, freq, disk_cache=1, inst_processors=[], **kwargs):
-        return hash_args(*self.normalize_uri_args(instruments, fields, freq), disk_cache, inst_processors)
-
-    def dataset(
-        self, instruments, fields, start_time=None, end_time=None, freq="day", disk_cache=0, inst_processors=[]
-    ):
-
-        if "local" in C.dataset_provider.lower():
-            # use LocalDatasetProvider
-            return self.provider.dataset(
-                instruments, fields, start_time, end_time, freq, inst_processors=inst_processors
-            )
-
-        if disk_cache == 0:
-            # do not use data_set cache, load data from remote expression cache directly
-            return self.provider.dataset(
-                instruments,
-                fields,
-                start_time,
-                end_time,
-                freq,
-                disk_cache,
-                return_uri=False,
-                inst_processors=inst_processors,
-            )
-        # FIXME: The cache after resample, when read again and intercepted with end_time, results in incomplete data date
-        if inst_processors:
-            raise ValueError(
-                f"{self.__class__.__name__} does not support inst_processor. "
-                f"Please use `D.features(disk_cache=0)` or `qlib.init(dataset_cache=None)`"
-            )
-        # use ClientDatasetProvider
-        feature_uri = self._uri(
-            instruments, fields, None, None, freq, disk_cache=disk_cache, inst_processors=inst_processors
-        )
-        value, expire = MemCacheExpire.get_cache(H["f"], feature_uri)
-        mnt_feature_uri = C.dpm.get_data_uri(freq).joinpath(C.dataset_cache_dir_name).joinpath(feature_uri)
-        if value is None or expire or not mnt_feature_uri.exists():
-            df, uri = self.provider.dataset(
-                instruments,
-                fields,
-                start_time,
-                end_time,
-                freq,
-                disk_cache,
-                return_uri=True,
-                inst_processors=inst_processors,
-            )
-            # cache uri
-            MemCacheExpire.set_cache(H["f"], uri, uri)
-            # cache DataFrame
-            # HZ['f'][uri] = df.copy()
-            get_module_logger("cache").debug(f"get feature from {C.dataset_provider}")
-        else:
-            df = DiskDatasetCache.read_data_from_cache(mnt_feature_uri, start_time, end_time, fields)
-            get_module_logger("cache").debug("get feature from uri cache")
-
-        return df
-
-
-class CalendarCache(BaseProviderCache):
-    pass
-
-
-class MemoryCalendarCache(CalendarCache):
-    def calendar(self, start_time=None, end_time=None, freq="day", future=False):
-        uri = self._uri(start_time, end_time, freq, future)
-        result, expire = MemCacheExpire.get_cache(H["c"], uri)
-        if result is None or expire:
-
-            result = self.provider.calendar(start_time, end_time, freq, future)
-            MemCacheExpire.set_cache(H["c"], uri, result)
-
-            get_module_logger("data").debug(f"get calendar from {C.calendar_provider}")
-        else:
-            get_module_logger("data").debug("get calendar from local cache")
-
-        return result
-
-
-H = MemCache()
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+
+from __future__ import division
+from __future__ import print_function
+
+import os
+import sys
+import stat
+import time
+import pickle
+import traceback
+import redis_lock
+import contextlib
+import abc
+from pathlib import Path
+import numpy as np
+import pandas as pd
+from typing import Union, Iterable
+from collections import OrderedDict
+
+from ..config import C
+from ..utils import (
+    hash_args,
+    get_redis_connection,
+    read_bin,
+    parse_field,
+    remove_fields_space,
+    normalize_cache_fields,
+    normalize_cache_instruments,
+)
+
+from ..log import get_module_logger
+from .base import Feature
+from .ops import Operators  # pylint: disable=W0611  # noqa: F401
+
+
+class QlibCacheException(RuntimeError):
+    pass
+
+
+class MemCacheUnit(abc.ABC):
+    """Memory Cache Unit."""
+
+    def __init__(self, *args, **kwargs):
+        self.size_limit = kwargs.pop("size_limit", 0)
+        self._size = 0
+        self.od = OrderedDict()
+
+    def __setitem__(self, key, value):
+        # TODO: thread safe?__setitem__ failure might cause inconsistent size?
+
+        # precalculate the size after od.__setitem__
+        self._adjust_size(key, value)
+
+        self.od.__setitem__(key, value)
+
+        # move the key to end,make it latest
+        self.od.move_to_end(key)
+
+        if self.limited:
+            # pop the oldest items beyond size limit
+            while self._size > self.size_limit:
+                self.popitem(last=False)
+
+    def __getitem__(self, key):
+        v = self.od.__getitem__(key)
+        self.od.move_to_end(key)
+        return v
+
+    def __contains__(self, key):
+        return key in self.od
+
+    def __len__(self):
+        return self.od.__len__()
+
+    def __repr__(self):
+        return f"{self.__class__.__name__}<size_limit:{self.size_limit if self.limited else 'no limit'} total_size:{self._size}>\n{self.od.__repr__()}"
+
+    def set_limit_size(self, limit):
+        self.size_limit = limit
+
+    @property
+    def limited(self):
+        """whether memory cache is limited"""
+        return self.size_limit > 0
+
+    @property
+    def total_size(self):
+        return self._size
+
+    def clear(self):
+        self._size = 0
+        self.od.clear()
+
+    def popitem(self, last=True):
+        k, v = self.od.popitem(last=last)
+        self._size -= self._get_value_size(v)
+
+        return k, v
+
+    def pop(self, key):
+        v = self.od.pop(key)
+        self._size -= self._get_value_size(v)
+
+        return v
+
+    def _adjust_size(self, key, value):
+        if key in self.od:
+            self._size -= self._get_value_size(self.od[key])
+
+        self._size += self._get_value_size(value)
+
+    @abc.abstractmethod
+    def _get_value_size(self, value):
+        raise NotImplementedError
+
+
+class MemCacheLengthUnit(MemCacheUnit):
+    def __init__(self, size_limit=0):
+        super().__init__(size_limit=size_limit)
+
+    def _get_value_size(self, value):
+        return 1
+
+
+class MemCacheSizeofUnit(MemCacheUnit):
+    def __init__(self, size_limit=0):
+        super().__init__(size_limit=size_limit)
+
+    def _get_value_size(self, value):
+        return sys.getsizeof(value)
+
+
+class MemCache:
+    """Memory cache."""
+
+    def __init__(self, mem_cache_size_limit=None, limit_type="length"):
+        """
+
+        Parameters
+        ----------
+        mem_cache_size_limit:
+            cache max size.
+        limit_type:
+            length or sizeof; length(call fun: len), size(call fun: sys.getsizeof).
+        """
+
+        size_limit = C.mem_cache_size_limit if mem_cache_size_limit is None else mem_cache_size_limit
+        limit_type = C.mem_cache_limit_type if limit_type is None else limit_type
+
+        if limit_type == "length":
+            klass = MemCacheLengthUnit
+        elif limit_type == "sizeof":
+            klass = MemCacheSizeofUnit
+        else:
+            raise ValueError(f"limit_type must be length or sizeof, your limit_type is {limit_type}")
+
+        self.__calendar_mem_cache = klass(size_limit)
+        self.__instrument_mem_cache = klass(size_limit)
+        self.__feature_mem_cache = klass(size_limit)
+
+    def __getitem__(self, key):
+        if key == "c":
+            return self.__calendar_mem_cache
+        elif key == "i":
+            return self.__instrument_mem_cache
+        elif key == "f":
+            return self.__feature_mem_cache
+        else:
+            raise KeyError("Unknown memcache unit")
+
+    def clear(self):
+        self.__calendar_mem_cache.clear()
+        self.__instrument_mem_cache.clear()
+        self.__feature_mem_cache.clear()
+
+
+class MemCacheExpire:
+    CACHE_EXPIRE = C.mem_cache_expire
+
+    @staticmethod
+    def set_cache(mem_cache, key, value):
+        """set cache
+
+        :param mem_cache: MemCache attribute('c'/'i'/'f').
+        :param key: cache key.
+        :param value: cache value.
+        """
+        mem_cache[key] = value, time.time()
+
+    @staticmethod
+    def get_cache(mem_cache, key):
+        """get mem cache
+
+        :param mem_cache: MemCache attribute('c'/'i'/'f').
+        :param key: cache key.
+        :return: cache value; if cache not exist, return None.
+        """
+        value = None
+        expire = False
+        if key in mem_cache:
+            value, latest_time = mem_cache[key]
+            expire = (time.time() - latest_time) > MemCacheExpire.CACHE_EXPIRE
+        return value, expire
+
+
+class CacheUtils:
+    LOCK_ID = "QLIB"
+
+    @staticmethod
+    def organize_meta_file():
+        pass
+
+    @staticmethod
+    def reset_lock():
+        r = get_redis_connection()
+        redis_lock.reset_all(r)
+
+    @staticmethod
+    def visit(cache_path: Union[str, Path]):
+        # FIXME: Because read_lock was canceled when reading the cache, multiple processes may have read and write exceptions here
+        try:
+            cache_path = Path(cache_path)
+            meta_path = cache_path.with_suffix(".meta")
+            with meta_path.open("rb") as f:
+                d = pickle.load(f)
+            with meta_path.open("wb") as f:
+                try:
+                    d["meta"]["last_visit"] = str(time.time())
+                    d["meta"]["visits"] = d["meta"]["visits"] + 1
+                except KeyError as key_e:
+                    raise KeyError("Unknown meta keyword") from key_e
+                pickle.dump(d, f, protocol=C.dump_protocol_version)
+        except Exception as e:
+            get_module_logger("CacheUtils").warning(f"visit {cache_path} cache error: {e}")
+
+    @staticmethod
+    def acquire(lock, lock_name):
+        try:
+            lock.acquire()
+        except redis_lock.AlreadyAcquired as lock_acquired:
+            raise QlibCacheException(
+                f"""It sees the key(lock:{repr(lock_name)[1:-1]}-wlock) of the redis lock has existed in your redis db now.
+                    You can use the following command to clear your redis keys and rerun your commands:
+                    $ redis-cli
+                    > select {C.redis_task_db}
+                    > del "lock:{repr(lock_name)[1:-1]}-wlock"
+                    > quit
+                    If the issue is not resolved, use "keys *" to find if multiple keys exist. If so, try using "flushall" to clear all the keys.
+                """
+            ) from lock_acquired
+
+    @staticmethod
+    @contextlib.contextmanager
+    def reader_lock(redis_t, lock_name: str):
+        current_cache_rlock = redis_lock.Lock(redis_t, f"{lock_name}-rlock")
+        current_cache_wlock = redis_lock.Lock(redis_t, f"{lock_name}-wlock")
+        lock_reader = f"{lock_name}-reader"
+        # make sure only one reader is entering
+        current_cache_rlock.acquire(timeout=60)
+        try:
+            current_cache_readers = redis_t.get(lock_reader)
+            if current_cache_readers is None or int(current_cache_readers) == 0:
+                CacheUtils.acquire(current_cache_wlock, lock_name)
+            redis_t.incr(lock_reader)
+        finally:
+            current_cache_rlock.release()
+        try:
+            yield
+        finally:
+            # make sure only one reader is leaving
+            current_cache_rlock.acquire(timeout=60)
+            try:
+                redis_t.decr(lock_reader)
+                if int(redis_t.get(lock_reader)) == 0:
+                    redis_t.delete(lock_reader)
+                    current_cache_wlock.reset()
+            finally:
+                current_cache_rlock.release()
+
+    @staticmethod
+    @contextlib.contextmanager
+    def writer_lock(redis_t, lock_name):
+        current_cache_wlock = redis_lock.Lock(redis_t, f"{lock_name}-wlock", id=CacheUtils.LOCK_ID)
+        CacheUtils.acquire(current_cache_wlock, lock_name)
+        try:
+            yield
+        finally:
+            current_cache_wlock.release()
+
+
+class BaseProviderCache:
+    """Provider cache base class"""
+
+    def __init__(self, provider):
+        self.provider = provider
+        self.logger = get_module_logger(self.__class__.__name__)
+
+    def __getattr__(self, attr):
+        return getattr(self.provider, attr)
+
+    @staticmethod
+    def check_cache_exists(cache_path: Union[str, Path], suffix_list: Iterable = (".index", ".meta")) -> bool:
+        cache_path = Path(cache_path)
+        for p in [cache_path] + [cache_path.with_suffix(_s) for _s in suffix_list]:
+            if not p.exists():
+                return False
+        return True
+
+    @staticmethod
+    def clear_cache(cache_path: Union[str, Path]):
+        for p in [
+            cache_path,
+            cache_path.with_suffix(".meta"),
+            cache_path.with_suffix(".index"),
+        ]:
+            if p.exists():
+                p.unlink()
+
+    @staticmethod
+    def get_cache_dir(dir_name: str, freq: str = None) -> Path:
+        cache_dir = Path(C.dpm.get_data_uri(freq)).joinpath(dir_name)
+        cache_dir.mkdir(parents=True, exist_ok=True)
+        return cache_dir
+
+
+class ExpressionCache(BaseProviderCache):
+    """Expression cache mechanism base class.
+
+    This class is used to wrap expression provider with self-defined expression cache mechanism.
+
+    .. note:: Override the `_uri` and `_expression` method to create your own expression cache mechanism.
+    """
+
+    def expression(self, instrument, field, start_time, end_time, freq):
+        """Get expression data.
+
+        .. note:: Same interface as `expression` method in expression provider
+        """
+        try:
+            return self._expression(instrument, field, start_time, end_time, freq)
+        except NotImplementedError:
+            return self.provider.expression(instrument, field, start_time, end_time, freq)
+
+    def _uri(self, instrument, field, start_time, end_time, freq):
+        """Get expression cache file uri.
+
+        Override this method to define how to get expression cache file uri corresponding to users' own cache mechanism.
+        """
+        raise NotImplementedError("Implement this function to match your own cache mechanism")
+
+    def _expression(self, instrument, field, start_time, end_time, freq):
+        """Get expression data using cache.
+
+        Override this method to define how to get expression data corresponding to users' own cache mechanism.
+        """
+        raise NotImplementedError("Implement this method if you want to use expression cache")
+
+    def update(self, cache_uri: Union[str, Path], freq: str = "day"):
+        """Update expression cache to latest calendar.
+
+        Override this method to define how to update expression cache corresponding to users' own cache mechanism.
+
+        Parameters
+        ----------
+        cache_uri : str or Path
+            the complete uri of expression cache file (include dir path).
+        freq : str
+
+        Returns
+        -------
+        int
+            0(successful update)/ 1(no need to update)/ 2(update failure).
+        """
+        raise NotImplementedError("Implement this method if you want to make expression cache up to date")
+
+
+class DatasetCache(BaseProviderCache):
+    """Dataset cache mechanism base class.
+
+    This class is used to wrap dataset provider with self-defined dataset cache mechanism.
+
+    .. note:: Override the `_uri` and `_dataset` method to create your own dataset cache mechanism.
+    """
+
+    HDF_KEY = "df"
+
+    def dataset(
+        self, instruments, fields, start_time=None, end_time=None, freq="day", disk_cache=1, inst_processors=[]
+    ):
+        """Get feature dataset.
+
+        .. note:: Same interface as `dataset` method in dataset provider
+
+        .. note:: The server use redis_lock to make sure
+            read-write conflicts will not be triggered
+            but client readers are not considered.
+        """
+        if disk_cache == 0:
+            # skip cache
+            return self.provider.dataset(
+                instruments, fields, start_time, end_time, freq, inst_processors=inst_processors
+            )
+        else:
+            # use and replace cache
+            try:
+                return self._dataset(
+                    instruments, fields, start_time, end_time, freq, disk_cache, inst_processors=inst_processors
+                )
+            except NotImplementedError:
+                return self.provider.dataset(
+                    instruments, fields, start_time, end_time, freq, inst_processors=inst_processors
+                )
+
+    def _uri(self, instruments, fields, start_time, end_time, freq, **kwargs):
+        """Get dataset cache file uri.
+
+        Override this method to define how to get dataset cache file uri corresponding to users' own cache mechanism.
+        """
+        raise NotImplementedError("Implement this function to match your own cache mechanism")
+
+    def _dataset(
+        self, instruments, fields, start_time=None, end_time=None, freq="day", disk_cache=1, inst_processors=[]
+    ):
+        """Get feature dataset using cache.
+
+        Override this method to define how to get feature dataset corresponding to users' own cache mechanism.
+        """
+        raise NotImplementedError("Implement this method if you want to use dataset feature cache")
+
+    def _dataset_uri(
+        self, instruments, fields, start_time=None, end_time=None, freq="day", disk_cache=1, inst_processors=[]
+    ):
+        """Get a uri of feature dataset using cache.
+        specially:
+            disk_cache=1 means using data set cache and return the uri of cache file.
+            disk_cache=0 means client knows the path of expression cache,
+                         server checks if the cache exists(if not, generate it), and client loads data by itself.
+        Override this method to define how to get feature dataset uri corresponding to users' own cache mechanism.
+        """
+        raise NotImplementedError(
+            "Implement this method if you want to use dataset feature cache as a cache file for client"
+        )
+
+    def update(self, cache_uri: Union[str, Path], freq: str = "day"):
+        """Update dataset cache to latest calendar.
+
+        Override this method to define how to update dataset cache corresponding to users' own cache mechanism.
+
+        Parameters
+        ----------
+        cache_uri : str or Path
+            the complete uri of dataset cache file (include dir path).
+        freq : str
+
+        Returns
+        -------
+        int
+            0(successful update)/ 1(no need to update)/ 2(update failure)
+        """
+        raise NotImplementedError("Implement this method if you want to make expression cache up to date")
+
+    @staticmethod
+    def cache_to_origin_data(data, fields):
+        """cache data to origin data
+
+        :param data: pd.DataFrame, cache data.
+        :param fields: feature fields.
+        :return: pd.DataFrame.
+        """
+        not_space_fields = remove_fields_space(fields)
+        data = data.loc[:, not_space_fields]
+        # set features fields
+        data.columns = [str(i) for i in fields]
+        return data
+
+    @staticmethod
+    def normalize_uri_args(instruments, fields, freq):
+        """normalize uri args"""
+        instruments = normalize_cache_instruments(instruments)
+        fields = normalize_cache_fields(fields)
+        freq = freq.lower()
+
+        return instruments, fields, freq
+
+
+class DiskExpressionCache(ExpressionCache):
+    """Prepared cache mechanism for server."""
+
+    def __init__(self, provider, **kwargs):
+        super(DiskExpressionCache, self).__init__(provider)
+        self.r = get_redis_connection()
+        # remote==True means client is using this module, writing behaviour will not be allowed.
+        self.remote = kwargs.get("remote", False)
+
+    def get_cache_dir(self, freq: str = None) -> Path:
+        return super(DiskExpressionCache, self).get_cache_dir(C.features_cache_dir_name, freq)
+
+    def _uri(self, instrument, field, start_time, end_time, freq):
+        field = remove_fields_space(field)
+        instrument = str(instrument).lower()
+        return hash_args(instrument, field, freq)
+
+    def _expression(self, instrument, field, start_time=None, end_time=None, freq="day"):
+        _cache_uri = self._uri(instrument=instrument, field=field, start_time=None, end_time=None, freq=freq)
+        _instrument_dir = self.get_cache_dir(freq).joinpath(instrument.lower())
+        cache_path = _instrument_dir.joinpath(_cache_uri)
+        # get calendar
+        from .data import Cal  # pylint: disable=C0415
+
+        _calendar = Cal.calendar(freq=freq)
+
+        _, _, start_index, end_index = Cal.locate_index(start_time, end_time, freq, future=False)
+
+        if self.check_cache_exists(cache_path, suffix_list=[".meta"]):
+            """
+            In most cases, we do not need reader_lock.
+            Because updating data is a small probability event compare to reading data.
+
+            """
+            # FIXME: Removing the reader lock may result in conflicts.
+            # with CacheUtils.reader_lock(self.r, 'expression-%s' % _cache_uri):
+
+            # modify expression cache meta file
+            try:
+                # FIXME: Multiple readers may result in error visit number
+                if not self.remote:
+                    CacheUtils.visit(cache_path)
+                series = read_bin(cache_path, start_index, end_index)
+                return series
+            except Exception:
+                series = None
+                self.logger.error("reading %s file error : %s" % (cache_path, traceback.format_exc()))
+            return series
+        else:
+            # normalize field
+            field = remove_fields_space(field)
+            # cache unavailable, generate the cache
+            _instrument_dir.mkdir(parents=True, exist_ok=True)
+            if not isinstance(eval(parse_field(field)), Feature):
+                # When the expression is not a raw feature
+                # generate expression cache if the feature is not a Feature
+                # instance
+                series = self.provider.expression(instrument, field, _calendar[0], _calendar[-1], freq)
+                if not series.empty:
+                    # This expression is empty, we don't generate any cache for it.
+                    with CacheUtils.writer_lock(self.r, f"{str(C.dpm.get_data_uri(freq))}:expression-{_cache_uri}"):
+                        self.gen_expression_cache(
+                            expression_data=series,
+                            cache_path=cache_path,
+                            instrument=instrument,
+                            field=field,
+                            freq=freq,
+                            last_update=str(_calendar[-1]),
+                        )
+                    return series.loc[start_index:end_index]
+                else:
+                    return series
+            else:
+                # If the expression is a raw feature(such as $close, $open)
+                return self.provider.expression(instrument, field, start_time, end_time, freq)
+
+    def gen_expression_cache(self, expression_data, cache_path, instrument, field, freq, last_update):
+        """use bin file to save like feature-data."""
+        # Make sure the cache runs right when the directory is deleted
+        # while running
+        meta = {
+            "info": {"instrument": instrument, "field": field, "freq": freq, "last_update": last_update},
+            "meta": {"last_visit": time.time(), "visits": 1},
+        }
+        self.logger.debug(f"generating expression cache: {meta}")
+        self.clear_cache(cache_path)
+        meta_path = cache_path.with_suffix(".meta")
+
+        with meta_path.open("wb") as f:
+            pickle.dump(meta, f, protocol=C.dump_protocol_version)
+        meta_path.chmod(stat.S_IRWXU | stat.S_IRGRP | stat.S_IROTH)
+        df = expression_data.to_frame()
+
+        r = np.hstack([df.index[0], expression_data]).astype("<f")
+        r.tofile(str(cache_path))
+
+    def update(self, sid, cache_uri, freq: str = "day"):
+        cp_cache_uri = self.get_cache_dir(freq).joinpath(sid).joinpath(cache_uri)
+        meta_path = cp_cache_uri.with_suffix(".meta")
+        if not self.check_cache_exists(cp_cache_uri, suffix_list=[".meta"]):
+            self.logger.info(f"The cache {cp_cache_uri} has corrupted. It will be removed")
+            self.clear_cache(cp_cache_uri)
+            return 2
+
+        with CacheUtils.writer_lock(self.r, f"{str(C.dpm.get_data_uri())}:expression-{cache_uri}"):
+            with meta_path.open("rb") as f:
+                d = pickle.load(f)
+            instrument = d["info"]["instrument"]
+            field = d["info"]["field"]
+            freq = d["info"]["freq"]
+            last_update_time = d["info"]["last_update"]
+
+            # get newest calendar
+            from .data import Cal, ExpressionD  # pylint: disable=C0415
+
+            whole_calendar = Cal.calendar(start_time=None, end_time=None, freq=freq)
+            # calendar since last updated.
+            new_calendar = Cal.calendar(start_time=last_update_time, end_time=None, freq=freq)
+
+            # get append data
+            if len(new_calendar) <= 1:
+                # Including last updated calendar, we only get 1 item.
+                # No future updating is needed.
+                return 1
+            else:
+                # get the data needed after the historical data are removed.
+                # The start index of new data
+                current_index = len(whole_calendar) - len(new_calendar) + 1
+
+                # The existing data length
+                size_bytes = os.path.getsize(cp_cache_uri)
+                ele_size = np.dtype("<f").itemsize
+                assert size_bytes % ele_size == 0
+                ele_n = size_bytes // ele_size - 1
+
+                expr = ExpressionD.get_expression_instance(field)
+                lft_etd, rght_etd = expr.get_extended_window_size()
+                # The expression used the future data after rght_etd days.
+                # So the last rght_etd data should be removed.
+                # There are most `ele_n` period of data can be remove
+                remove_n = min(rght_etd, ele_n)
+                assert new_calendar[1] == whole_calendar[current_index]
+                data = self.provider.expression(
+                    instrument, field, whole_calendar[current_index - remove_n], new_calendar[-1], freq
+                )
+                with open(cp_cache_uri, "ab") as f:
+                    data = np.array(data).astype("<f")
+                    # Remove the last bits
+                    f.truncate(size_bytes - ele_size * remove_n)
+                    f.write(data)
+                # update meta file
+                d["info"]["last_update"] = str(new_calendar[-1])
+                with meta_path.open("wb") as f:
+                    pickle.dump(d, f, protocol=C.dump_protocol_version)
+        return 0
+
+
+class DiskDatasetCache(DatasetCache):
+    """Prepared cache mechanism for server."""
+
+    def __init__(self, provider, **kwargs):
+        super(DiskDatasetCache, self).__init__(provider)
+        self.r = get_redis_connection()
+        self.remote = kwargs.get("remote", False)
+
+    @staticmethod
+    def _uri(instruments, fields, start_time, end_time, freq, disk_cache=1, inst_processors=[], **kwargs):
+        return hash_args(*DatasetCache.normalize_uri_args(instruments, fields, freq), disk_cache, inst_processors)
+
+    def get_cache_dir(self, freq: str = None) -> Path:
+        return super(DiskDatasetCache, self).get_cache_dir(C.dataset_cache_dir_name, freq)
+
+    @classmethod
+    def read_data_from_cache(cls, cache_path: Union[str, Path], start_time, end_time, fields):
+        """read_cache_from
+
+        This function can read data from the disk cache dataset
+
+        :param cache_path:
+        :param start_time:
+        :param end_time:
+        :param fields: The fields order of the dataset cache is sorted. So rearrange the columns to make it consistent.
+        :return:
+        """
+
+        im = DiskDatasetCache.IndexManager(cache_path)
+        index_data = im.get_index(start_time, end_time)
+        if index_data.shape[0] > 0:
+            start, stop = (
+                index_data["start"].iloc[0].item(),
+                index_data["end"].iloc[-1].item(),
+            )
+        else:
+            start = stop = 0
+
+        with pd.HDFStore(cache_path, mode="r") as store:
+            if "/{}".format(im.KEY) in store.keys():
+                df = store.select(key=im.KEY, start=start, stop=stop)
+                df = df.swaplevel("datetime", "instrument").sort_index()
+                # read cache and need to replace not-space fields to field
+                df = cls.cache_to_origin_data(df, fields)
+
+            else:
+                df = pd.DataFrame(columns=fields)
+        return df
+
+    def _dataset(
+        self, instruments, fields, start_time=None, end_time=None, freq="day", disk_cache=0, inst_processors=[]
+    ):
+        if disk_cache == 0:
+            # In this case, data_set cache is configured but will not be used.
+            return self.provider.dataset(
+                instruments, fields, start_time, end_time, freq, inst_processors=inst_processors
+            )
+        # FIXME: The cache after resample, when read again and intercepted with end_time, results in incomplete data date
+        if inst_processors:
+            raise ValueError(
+                f"{self.__class__.__name__} does not support inst_processor. "
+                f"Please use `D.features(disk_cache=0)` or `qlib.init(dataset_cache=None)`"
+            )
+        _cache_uri = self._uri(
+            instruments=instruments,
+            fields=fields,
+            start_time=None,
+            end_time=None,
+            freq=freq,
+            disk_cache=disk_cache,
+            inst_processors=inst_processors,
+        )
+
+        cache_path = self.get_cache_dir(freq).joinpath(_cache_uri)
+
+        features = pd.DataFrame()
+        gen_flag = False
+
+        if self.check_cache_exists(cache_path):
+            if disk_cache == 1:
+                # use cache
+                with CacheUtils.reader_lock(self.r, f"{str(C.dpm.get_data_uri(freq))}:dataset-{_cache_uri}"):
+                    CacheUtils.visit(cache_path)
+                    features = self.read_data_from_cache(cache_path, start_time, end_time, fields)
+            elif disk_cache == 2:
+                gen_flag = True
+        else:
+            gen_flag = True
+
+        if gen_flag:
+            # cache unavailable, generate the cache
+            with CacheUtils.writer_lock(self.r, f"{str(C.dpm.get_data_uri(freq))}:dataset-{_cache_uri}"):
+                features = self.gen_dataset_cache(
+                    cache_path=cache_path,
+                    instruments=instruments,
+                    fields=fields,
+                    freq=freq,
+                    inst_processors=inst_processors,
+                )
+            if not features.empty:
+                features = features.sort_index().loc(axis=0)[:, start_time:end_time]
+        return features
+
+    def _dataset_uri(
+        self, instruments, fields, start_time=None, end_time=None, freq="day", disk_cache=0, inst_processors=[]
+    ):
+        if disk_cache == 0:
+            # In this case, server only checks the expression cache.
+            # The client will load the cache data by itself.
+            from .data import LocalDatasetProvider  # pylint: disable=C0415
+
+            LocalDatasetProvider.multi_cache_walker(instruments, fields, start_time, end_time, freq)
+            return ""
+        # FIXME: The cache after resample, when read again and intercepted with end_time, results in incomplete data date
+        if inst_processors:
+            raise ValueError(
+                f"{self.__class__.__name__} does not support inst_processor. "
+                f"Please use `D.features(disk_cache=0)` or `qlib.init(dataset_cache=None)`"
+            )
+        _cache_uri = self._uri(
+            instruments=instruments,
+            fields=fields,
+            start_time=None,
+            end_time=None,
+            freq=freq,
+            disk_cache=disk_cache,
+            inst_processors=inst_processors,
+        )
+        cache_path = self.get_cache_dir(freq).joinpath(_cache_uri)
+
+        if self.check_cache_exists(cache_path):
+            self.logger.debug(f"The cache dataset has already existed {cache_path}. Return the uri directly")
+            with CacheUtils.reader_lock(self.r, f"{str(C.dpm.get_data_uri(freq))}:dataset-{_cache_uri}"):
+                CacheUtils.visit(cache_path)
+            return _cache_uri
+        else:
+            # cache unavailable, generate the cache
+            with CacheUtils.writer_lock(self.r, f"{str(C.dpm.get_data_uri(freq))}:dataset-{_cache_uri}"):
+                self.gen_dataset_cache(
+                    cache_path=cache_path,
+                    instruments=instruments,
+                    fields=fields,
+                    freq=freq,
+                    inst_processors=inst_processors,
+                )
+            return _cache_uri
+
+    class IndexManager:
+        """
+        The lock is not considered in the class. Please consider the lock outside the code.
+        This class is the proxy of the disk data.
+        """
+
+        KEY = "df"
+
+        def __init__(self, cache_path: Union[str, Path]):
+            self.index_path = cache_path.with_suffix(".index")
+            self._data = None
+            self.logger = get_module_logger(self.__class__.__name__)
+
+        def get_index(self, start_time=None, end_time=None):
+            # TODO: fast read index from the disk.
+            if self._data is None:
+                self.sync_from_disk()
+            return self._data.loc[start_time:end_time].copy()
+
+        def sync_to_disk(self):
+            if self._data is None:
+                raise ValueError("No data to sync to disk.")
+            self._data.sort_index(inplace=True)
+            self._data.to_hdf(self.index_path, key=self.KEY, mode="w", format="table")
+            # The index should be readable for all users
+            self.index_path.chmod(stat.S_IRWXU | stat.S_IRGRP | stat.S_IROTH)
+
+        def sync_from_disk(self):
+            # The file will not be closed directly if we read_hdf from the disk directly
+            with pd.HDFStore(self.index_path, mode="r") as store:
+                if "/{}".format(self.KEY) in store.keys():
+                    self._data = pd.read_hdf(store, key=self.KEY)
+                else:
+                    self._data = pd.DataFrame()
+
+        def update(self, data, sync=True):
+            self._data = data.astype(np.int32).copy()
+            if sync:
+                self.sync_to_disk()
+
+        def append_index(self, data, to_disk=True):
+            data = data.astype(np.int32).copy()
+            data.sort_index(inplace=True)
+            self._data = pd.concat([self._data, data])
+            if to_disk:
+                with pd.HDFStore(self.index_path) as store:
+                    store.append(self.KEY, data, append=True)
+
+        @staticmethod
+        def build_index_from_data(data, start_index=0):
+            if data.empty:
+                return pd.DataFrame()
+            line_data = data.groupby("datetime").size()
+            line_data.sort_index(inplace=True)
+            index_end = line_data.cumsum()
+            index_start = index_end.shift(1, fill_value=0)
+
+            index_data = pd.DataFrame()
+            index_data["start"] = index_start
+            index_data["end"] = index_end
+            index_data += start_index
+            return index_data
+
+    def gen_dataset_cache(self, cache_path: Union[str, Path], instruments, fields, freq, inst_processors=[]):
+        """gen_dataset_cache
+
+        .. note:: This function does not consider the cache read write lock. Please
+            acquire the lock outside this function
+
+        The format the cache contains 3 parts(followed by typical filename).
+
+        - index : cache/d41366901e25de3ec47297f12e2ba11d.index
+
+            - The content of the file may be in following format(pandas.Series)
+
+                .. code-block:: python
+
+                                        start end
+                    1999-11-10 00:00:00     0   1
+                    1999-11-11 00:00:00     1   2
+                    1999-11-12 00:00:00     2   3
+                    ...
+
+                .. note:: The start is closed. The end is open!!!!!
+
+            - Each line contains two element <start_index, end_index> with a timestamp as its index.
+            - It indicates the `start_index` (included) and `end_index` (excluded) of the data for `timestamp`
+
+        - meta data: cache/d41366901e25de3ec47297f12e2ba11d.meta
+
+        - data     : cache/d41366901e25de3ec47297f12e2ba11d
+
+            - This is a hdf file sorted by datetime
+
+        :param cache_path:  The path to store the cache.
+        :param instruments:  The instruments to store the cache.
+        :param fields:  The fields to store the cache.
+        :param freq:  The freq to store the cache.
+        :param inst_processors:  Instrument processors.
+
+        :return type pd.DataFrame; The fields of the returned DataFrame are consistent with the parameters of the function.
+        """
+        # get calendar
+        from .data import Cal  # pylint: disable=C0415
+
+        cache_path = Path(cache_path)
+        _calendar = Cal.calendar(freq=freq)
+        self.logger.debug(f"Generating dataset cache {cache_path}")
+        # Make sure the cache runs right when the directory is deleted
+        # while running
+        self.clear_cache(cache_path)
+
+        features = self.provider.dataset(
+            instruments, fields, _calendar[0], _calendar[-1], freq, inst_processors=inst_processors
+        )
+
+        if features.empty:
+            return features
+
+        # swap index and sorted
+        features = features.swaplevel("instrument", "datetime").sort_index()
+
+        # write cache data
+        with pd.HDFStore(str(cache_path.with_suffix(".data"))) as store:
+            cache_to_orig_map = dict(zip(remove_fields_space(features.columns), features.columns))
+            orig_to_cache_map = dict(zip(features.columns, remove_fields_space(features.columns)))
+            cache_features = features[list(cache_to_orig_map.values())].rename(columns=orig_to_cache_map)
+            # cache columns
+            cache_columns = sorted(cache_features.columns)
+            cache_features = cache_features.loc[:, cache_columns]
+            cache_features = cache_features.loc[:, ~cache_features.columns.duplicated()]
+            store.append(DatasetCache.HDF_KEY, cache_features, append=False)
+        # write meta file
+        meta = {
+            "info": {
+                "instruments": instruments,
+                "fields": list(cache_features.columns),
+                "freq": freq,
+                "last_update": str(_calendar[-1]),  # The last_update to store the cache
+                "inst_processors": inst_processors,  # The last_update to store the cache
+            },
+            "meta": {"last_visit": time.time(), "visits": 1},
+        }
+        with cache_path.with_suffix(".meta").open("wb") as f:
+            pickle.dump(meta, f, protocol=C.dump_protocol_version)
+        cache_path.with_suffix(".meta").chmod(stat.S_IRWXU | stat.S_IRGRP | stat.S_IROTH)
+        # write index file
+        im = DiskDatasetCache.IndexManager(cache_path)
+        index_data = im.build_index_from_data(features)
+        im.update(index_data)
+
+        # rename the file after the cache has been generated
+        # this doesn't work well on windows, but our server won't use windows
+        # temporarily
+        cache_path.with_suffix(".data").rename(cache_path)
+        # the fields of the cached features are converted to the original fields
+        return features.swaplevel("datetime", "instrument")
+
+    def update(self, cache_uri, freq: str = "day"):
+        cp_cache_uri = self.get_cache_dir(freq).joinpath(cache_uri)
+        meta_path = cp_cache_uri.with_suffix(".meta")
+        if not self.check_cache_exists(cp_cache_uri):
+            self.logger.info(f"The cache {cp_cache_uri} has corrupted. It will be removed")
+            self.clear_cache(cp_cache_uri)
+            return 2
+
+        im = DiskDatasetCache.IndexManager(cp_cache_uri)
+        with CacheUtils.writer_lock(self.r, f"{str(C.dpm.get_data_uri())}:dataset-{cache_uri}"):
+            with meta_path.open("rb") as f:
+                d = pickle.load(f)
+            instruments = d["info"]["instruments"]
+            fields = d["info"]["fields"]
+            freq = d["info"]["freq"]
+            last_update_time = d["info"]["last_update"]
+            inst_processors = d["info"].get("inst_processors", [])
+            index_data = im.get_index()
+
+            self.logger.debug("Updating dataset: {}".format(d))
+            from .data import Inst  # pylint: disable=C0415
+
+            if Inst.get_inst_type(instruments) == Inst.DICT:
+                self.logger.info(f"The file {cache_uri} has dict cache. Skip updating")
+                return 1
+
+            # get newest calendar
+            from .data import Cal  # pylint: disable=C0415
+
+            whole_calendar = Cal.calendar(start_time=None, end_time=None, freq=freq)
+            # The calendar since last updated
+            new_calendar = Cal.calendar(start_time=last_update_time, end_time=None, freq=freq)
+
+            # get append data
+            if len(new_calendar) <= 1:
+                # Including last updated calendar, we only get 1 item.
+                # No future updating is needed.
+                return 1
+            else:
+                # get the data needed after the historical data are removed.
+                # The start index of new data
+                current_index = len(whole_calendar) - len(new_calendar) + 1
+
+                # To avoid recursive import
+                from .data import ExpressionD  # pylint: disable=C0415
+
+                # The existing data length
+                lft_etd = rght_etd = 0
+                for field in fields:
+                    expr = ExpressionD.get_expression_instance(field)
+                    l, r = expr.get_extended_window_size()
+                    lft_etd = max(lft_etd, l)
+                    rght_etd = max(rght_etd, r)
+                # remove the period that should be updated.
+                if index_data.empty:
+                    # We don't have any data for such dataset. Nothing to remove
+                    rm_n_period = rm_lines = 0
+                else:
+                    rm_n_period = min(rght_etd, index_data.shape[0])
+                    rm_lines = (
+                        (index_data["end"] - index_data["start"])
+                        .loc[whole_calendar[current_index - rm_n_period] :]
+                        .sum()
+                        .item()
+                    )
+
+                data = self.provider.dataset(
+                    instruments,
+                    fields,
+                    whole_calendar[current_index - rm_n_period],
+                    new_calendar[-1],
+                    freq,
+                    inst_processors=inst_processors,
+                )
+
+                if not data.empty:
+                    data.reset_index(inplace=True)
+                    data.set_index(["datetime", "instrument"], inplace=True)
+                    data.sort_index(inplace=True)
+                else:
+                    return 0  # No data to update cache
+
+                store = pd.HDFStore(cp_cache_uri)
+                # FIXME:
+                # Because the feature cache are stored as .bin file.
+                # So the series read from features are all float32.
+                # However, the first dataset cache is calculated based on the
+                # raw data. So the data type may be float64.
+                # Different data type will result in failure of appending data
+                if "/{}".format(DatasetCache.HDF_KEY) in store.keys():
+                    schema = store.select(DatasetCache.HDF_KEY, start=0, stop=0)
+                    for col, dtype in schema.dtypes.items():
+                        data[col] = data[col].astype(dtype)
+                if rm_lines > 0:
+                    store.remove(key=im.KEY, start=-rm_lines)
+                store.append(DatasetCache.HDF_KEY, data)
+                store.close()
+
+                # update index file
+                new_index_data = im.build_index_from_data(
+                    data.loc(axis=0)[whole_calendar[current_index] :, :],
+                    start_index=0 if index_data.empty else index_data["end"].iloc[-1],
+                )
+                im.append_index(new_index_data)
+
+                # update meta file
+                d["info"]["last_update"] = str(new_calendar[-1])
+                with meta_path.open("wb") as f:
+                    pickle.dump(d, f, protocol=C.dump_protocol_version)
+                return 0
+
+
+class SimpleDatasetCache(DatasetCache):
+    """Simple dataset cache that can be used locally or on client."""
+
+    def __init__(self, provider):
+        super(SimpleDatasetCache, self).__init__(provider)
+        try:
+            self.local_cache_path: Path = Path(C["local_cache_path"]).expanduser().resolve()
+        except (KeyError, TypeError):
+            self.logger.error("Assign a local_cache_path in config if you want to use this cache mechanism")
+            raise
+        self.logger.info(
+            f"DatasetCache directory: {self.local_cache_path}, "
+            f"modify the cache directory via the local_cache_path in the config"
+        )
+
+    def _uri(self, instruments, fields, start_time, end_time, freq, disk_cache=1, inst_processors=[], **kwargs):
+        instruments, fields, freq = self.normalize_uri_args(instruments, fields, freq)
+        return hash_args(
+            instruments, fields, start_time, end_time, freq, disk_cache, str(self.local_cache_path), inst_processors
+        )
+
+    def _dataset(
+        self, instruments, fields, start_time=None, end_time=None, freq="day", disk_cache=1, inst_processors=[]
+    ):
+        if disk_cache == 0:
+            # In this case, data_set cache is configured but will not be used.
+            return self.provider.dataset(instruments, fields, start_time, end_time, freq)
+        self.local_cache_path.mkdir(exist_ok=True, parents=True)
+        cache_file = self.local_cache_path.joinpath(
+            self._uri(
+                instruments, fields, start_time, end_time, freq, disk_cache=disk_cache, inst_processors=inst_processors
+            )
+        )
+        gen_flag = False
+
+        if cache_file.exists():
+            if disk_cache == 1:
+                # use cache
+                df = pd.read_pickle(cache_file)
+                return self.cache_to_origin_data(df, fields)
+            elif disk_cache == 2:
+                # replace cache
+                gen_flag = True
+        else:
+            gen_flag = True
+
+        if gen_flag:
+            data = self.provider.dataset(
+                instruments, normalize_cache_fields(fields), start_time, end_time, freq, inst_processors=inst_processors
+            )
+            data.to_pickle(cache_file)
+            return self.cache_to_origin_data(data, fields)
+
+
+class DatasetURICache(DatasetCache):
+    """Prepared cache mechanism for server."""
+
+    def _uri(self, instruments, fields, start_time, end_time, freq, disk_cache=1, inst_processors=[], **kwargs):
+        return hash_args(*self.normalize_uri_args(instruments, fields, freq), disk_cache, inst_processors)
+
+    def dataset(
+        self, instruments, fields, start_time=None, end_time=None, freq="day", disk_cache=0, inst_processors=[]
+    ):
+        if "local" in C.dataset_provider.lower():
+            # use LocalDatasetProvider
+            return self.provider.dataset(
+                instruments, fields, start_time, end_time, freq, inst_processors=inst_processors
+            )
+
+        if disk_cache == 0:
+            # do not use data_set cache, load data from remote expression cache directly
+            return self.provider.dataset(
+                instruments,
+                fields,
+                start_time,
+                end_time,
+                freq,
+                disk_cache,
+                return_uri=False,
+                inst_processors=inst_processors,
+            )
+        # FIXME: The cache after resample, when read again and intercepted with end_time, results in incomplete data date
+        if inst_processors:
+            raise ValueError(
+                f"{self.__class__.__name__} does not support inst_processor. "
+                f"Please use `D.features(disk_cache=0)` or `qlib.init(dataset_cache=None)`"
+            )
+        # use ClientDatasetProvider
+        feature_uri = self._uri(
+            instruments, fields, None, None, freq, disk_cache=disk_cache, inst_processors=inst_processors
+        )
+        value, expire = MemCacheExpire.get_cache(H["f"], feature_uri)
+        mnt_feature_uri = C.dpm.get_data_uri(freq).joinpath(C.dataset_cache_dir_name).joinpath(feature_uri)
+        if value is None or expire or not mnt_feature_uri.exists():
+            df, uri = self.provider.dataset(
+                instruments,
+                fields,
+                start_time,
+                end_time,
+                freq,
+                disk_cache,
+                return_uri=True,
+                inst_processors=inst_processors,
+            )
+            # cache uri
+            MemCacheExpire.set_cache(H["f"], uri, uri)
+            # cache DataFrame
+            # HZ['f'][uri] = df.copy()
+            get_module_logger("cache").debug(f"get feature from {C.dataset_provider}")
+        else:
+            df = DiskDatasetCache.read_data_from_cache(mnt_feature_uri, start_time, end_time, fields)
+            get_module_logger("cache").debug("get feature from uri cache")
+
+        return df
+
+
+class CalendarCache(BaseProviderCache):
+    pass
+
+
+class MemoryCalendarCache(CalendarCache):
+    def calendar(self, start_time=None, end_time=None, freq="day", future=False):
+        uri = self._uri(start_time, end_time, freq, future)
+        result, expire = MemCacheExpire.get_cache(H["c"], uri)
+        if result is None or expire:
+            result = self.provider.calendar(start_time, end_time, freq, future)
+            MemCacheExpire.set_cache(H["c"], uri, result)
+
+            get_module_logger("data").debug(f"get calendar from {C.calendar_provider}")
+        else:
+            get_module_logger("data").debug("get calendar from local cache")
+
+        return result
+
+
+H = MemCache()
```

## qlib/data/client.py

 * *Ordering differences only*

```diff
@@ -1,103 +1,103 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-
-from __future__ import division
-from __future__ import print_function
-
-import socketio
-
-import qlib
-from ..config import C
-from ..log import get_module_logger
-import pickle
-
-
-class Client:
-    """A client class
-
-    Provide the connection tool functions for ClientProvider.
-    """
-
-    def __init__(self, host, port):
-        super(Client, self).__init__()
-        self.sio = socketio.Client()
-        self.server_host = host
-        self.server_port = port
-        self.logger = get_module_logger(self.__class__.__name__)
-        # bind connect/disconnect callbacks
-        self.sio.on(
-            "connect",
-            lambda: self.logger.debug("Connect to server {}".format(self.sio.connection_url)),
-        )
-        self.sio.on("disconnect", lambda: self.logger.debug("Disconnect from server!"))
-
-    def connect_server(self):
-        """Connect to server."""
-        try:
-            self.sio.connect("ws://" + self.server_host + ":" + str(self.server_port))
-        except socketio.exceptions.ConnectionError:
-            self.logger.error("Cannot connect to server - check your network or server status")
-
-    def disconnect(self):
-        """Disconnect from server."""
-        try:
-            self.sio.eio.disconnect(True)
-        except Exception as e:
-            self.logger.error("Cannot disconnect from server : %s" % e)
-
-    def send_request(self, request_type, request_content, msg_queue, msg_proc_func=None):
-        """Send a certain request to server.
-
-        Parameters
-        ----------
-        request_type : str
-            type of proposed request, 'calendar'/'instrument'/'feature'.
-        request_content : dict
-            records the information of the request.
-        msg_proc_func : func
-            the function to process the message when receiving response, should have arg `*args`.
-        msg_queue: Queue
-            The queue to pass the message after callback.
-        """
-        head_info = {"version": qlib.__version__}
-
-        def request_callback(*args):
-            """callback_wrapper
-
-            :param *args: args[0] is the response content
-            """
-            # args[0] is the response content
-            self.logger.debug("receive data and enter queue")
-            msg = dict(args[0])
-            if msg["detailed_info"] is not None:
-                if msg["status"] != 0:
-                    self.logger.error(msg["detailed_info"])
-                else:
-                    self.logger.info(msg["detailed_info"])
-            if msg["status"] != 0:
-                ex = ValueError(f"Bad response(status=={msg['status']}), detailed info: {msg['detailed_info']}")
-                msg_queue.put(ex)
-            else:
-                if msg_proc_func is not None:
-                    try:
-                        ret = msg_proc_func(msg["result"])
-                    except Exception as e:
-                        self.logger.exception("Error when processing message.")
-                        ret = e
-                else:
-                    ret = msg["result"]
-                msg_queue.put(ret)
-            self.disconnect()
-            self.logger.debug("disconnected")
-
-        self.logger.debug("try connecting")
-        self.connect_server()
-        self.logger.debug("connected")
-        # The pickle is for passing some parameters with special type(such as
-        # pd.Timestamp)
-        request_content = {"head": head_info, "body": pickle.dumps(request_content, protocol=C.dump_protocol_version)}
-        self.sio.on(request_type + "_response", request_callback)
-        self.logger.debug("try sending")
-        self.sio.emit(request_type + "_request", request_content)
-        self.sio.wait()
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+
+from __future__ import division
+from __future__ import print_function
+
+import socketio
+
+import qlib
+from ..config import C
+from ..log import get_module_logger
+import pickle
+
+
+class Client:
+    """A client class
+
+    Provide the connection tool functions for ClientProvider.
+    """
+
+    def __init__(self, host, port):
+        super(Client, self).__init__()
+        self.sio = socketio.Client()
+        self.server_host = host
+        self.server_port = port
+        self.logger = get_module_logger(self.__class__.__name__)
+        # bind connect/disconnect callbacks
+        self.sio.on(
+            "connect",
+            lambda: self.logger.debug("Connect to server {}".format(self.sio.connection_url)),
+        )
+        self.sio.on("disconnect", lambda: self.logger.debug("Disconnect from server!"))
+
+    def connect_server(self):
+        """Connect to server."""
+        try:
+            self.sio.connect("ws://" + self.server_host + ":" + str(self.server_port))
+        except socketio.exceptions.ConnectionError:
+            self.logger.error("Cannot connect to server - check your network or server status")
+
+    def disconnect(self):
+        """Disconnect from server."""
+        try:
+            self.sio.eio.disconnect(True)
+        except Exception as e:
+            self.logger.error("Cannot disconnect from server : %s" % e)
+
+    def send_request(self, request_type, request_content, msg_queue, msg_proc_func=None):
+        """Send a certain request to server.
+
+        Parameters
+        ----------
+        request_type : str
+            type of proposed request, 'calendar'/'instrument'/'feature'.
+        request_content : dict
+            records the information of the request.
+        msg_proc_func : func
+            the function to process the message when receiving response, should have arg `*args`.
+        msg_queue: Queue
+            The queue to pass the message after callback.
+        """
+        head_info = {"version": qlib.__version__}
+
+        def request_callback(*args):
+            """callback_wrapper
+
+            :param *args: args[0] is the response content
+            """
+            # args[0] is the response content
+            self.logger.debug("receive data and enter queue")
+            msg = dict(args[0])
+            if msg["detailed_info"] is not None:
+                if msg["status"] != 0:
+                    self.logger.error(msg["detailed_info"])
+                else:
+                    self.logger.info(msg["detailed_info"])
+            if msg["status"] != 0:
+                ex = ValueError(f"Bad response(status=={msg['status']}), detailed info: {msg['detailed_info']}")
+                msg_queue.put(ex)
+            else:
+                if msg_proc_func is not None:
+                    try:
+                        ret = msg_proc_func(msg["result"])
+                    except Exception as e:
+                        self.logger.exception("Error when processing message.")
+                        ret = e
+                else:
+                    ret = msg["result"]
+                msg_queue.put(ret)
+            self.disconnect()
+            self.logger.debug("disconnected")
+
+        self.logger.debug("try connecting")
+        self.connect_server()
+        self.logger.debug("connected")
+        # The pickle is for passing some parameters with special type(such as
+        # pd.Timestamp)
+        request_content = {"head": head_info, "body": pickle.dumps(request_content, protocol=C.dump_protocol_version)}
+        self.sio.on(request_type + "_response", request_callback)
+        self.logger.debug("try sending")
+        self.sio.emit(request_type + "_request", request_content)
+        self.sio.wait()
```

## qlib/data/data.py

```diff
@@ -1,1334 +1,1333 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-
-from __future__ import division
-from __future__ import print_function
-
-import re
-import abc
-import copy
-import queue
-import bisect
-import numpy as np
-import pandas as pd
-from typing import List, Union, Optional
-
-# For supporting multiprocessing in outer code, joblib is used
-from joblib import delayed
-
-from .cache import H
-from ..config import C
-from .inst_processor import InstProcessor
-
-from ..log import get_module_logger
-from .cache import DiskDatasetCache
-from ..utils import (
-    Wrapper,
-    init_instance_by_config,
-    register_wrapper,
-    get_module_by_module_path,
-    parse_field,
-    hash_args,
-    normalize_cache_fields,
-    code_to_fname,
-    time_to_slc_point,
-    read_period_data,
-    get_period_list,
-)
-from ..utils.paral import ParallelExt
-from .ops import Operators  # pylint: disable=W0611  # noqa: F401
-
-
-class ProviderBackendMixin:
-    """
-    This helper class tries to make the provider based on storage backend more convenient
-    It is not necessary to inherent this class if that provider don't rely on the backend storage
-    """
-
-    def get_default_backend(self):
-        backend = {}
-        provider_name: str = re.findall("[A-Z][^A-Z]*", self.__class__.__name__)[-2]
-        # set default storage class
-        backend.setdefault("class", f"File{provider_name}Storage")
-        # set default storage module
-        backend.setdefault("module_path", "qlib.data.storage.file_storage")
-        return backend
-
-    def backend_obj(self, **kwargs):
-        backend = self.backend if self.backend else self.get_default_backend()
-        backend = copy.deepcopy(backend)
-        backend.setdefault("kwargs", {}).update(**kwargs)
-        return init_instance_by_config(backend)
-
-
-class CalendarProvider(abc.ABC):
-    """Calendar provider base class
-
-    Provide calendar data.
-    """
-
-    def calendar(self, start_time=None, end_time=None, freq="day", future=False):
-        """Get calendar of certain market in given time range.
-
-        Parameters
-        ----------
-        start_time : str
-            start of the time range.
-        end_time : str
-            end of the time range.
-        freq : str
-            time frequency, available: year/quarter/month/week/day.
-        future : bool
-            whether including future trading day.
-
-        Returns
-        ----------
-        list
-            calendar list
-        """
-        _calendar, _calendar_index = self._get_calendar(freq, future)
-        if start_time == "None":
-            start_time = None
-        if end_time == "None":
-            end_time = None
-        # strip
-        if start_time:
-            start_time = pd.Timestamp(start_time)
-            if start_time > _calendar[-1]:
-                return np.array([])
-        else:
-            start_time = _calendar[0]
-        if end_time:
-            end_time = pd.Timestamp(end_time)
-            if end_time < _calendar[0]:
-                return np.array([])
-        else:
-            end_time = _calendar[-1]
-        _, _, si, ei = self.locate_index(start_time, end_time, freq, future)
-        return _calendar[si : ei + 1]
-
-    def locate_index(
-        self, start_time: Union[pd.Timestamp, str], end_time: Union[pd.Timestamp, str], freq: str, future: bool = False
-    ):
-        """Locate the start time index and end time index in a calendar under certain frequency.
-
-        Parameters
-        ----------
-        start_time : pd.Timestamp
-            start of the time range.
-        end_time : pd.Timestamp
-            end of the time range.
-        freq : str
-            time frequency, available: year/quarter/month/week/day.
-        future : bool
-            whether including future trading day.
-
-        Returns
-        -------
-        pd.Timestamp
-            the real start time.
-        pd.Timestamp
-            the real end time.
-        int
-            the index of start time.
-        int
-            the index of end time.
-        """
-        start_time = pd.Timestamp(start_time)
-        end_time = pd.Timestamp(end_time)
-        calendar, calendar_index = self._get_calendar(freq=freq, future=future)
-        if start_time not in calendar_index:
-            try:
-                start_time = calendar[bisect.bisect_left(calendar, start_time)]
-            except IndexError as index_e:
-                raise IndexError(
-                    "`start_time` uses a future date, if you want to get future trading days, you can use: `future=True`"
-                ) from index_e
-        start_index = calendar_index[start_time]
-        if end_time not in calendar_index:
-            end_time = calendar[bisect.bisect_right(calendar, end_time) - 1]
-        end_index = calendar_index[end_time]
-        return start_time, end_time, start_index, end_index
-
-    def _get_calendar(self, freq, future):
-        """Load calendar using memcache.
-
-        Parameters
-        ----------
-        freq : str
-            frequency of read calendar file.
-        future : bool
-            whether including future trading day.
-
-        Returns
-        -------
-        list
-            list of timestamps.
-        dict
-            dict composed by timestamp as key and index as value for fast search.
-        """
-        flag = f"{freq}_future_{future}"
-        if flag not in H["c"]:
-            _calendar = np.array(self.load_calendar(freq, future))
-            _calendar_index = {x: i for i, x in enumerate(_calendar)}  # for fast search
-            H["c"][flag] = _calendar, _calendar_index
-        return H["c"][flag]
-
-    def _uri(self, start_time, end_time, freq, future=False):
-        """Get the uri of calendar generation task."""
-        return hash_args(start_time, end_time, freq, future)
-
-    def load_calendar(self, freq, future):
-        """Load original calendar timestamp from file.
-
-        Parameters
-        ----------
-        freq : str
-            frequency of read calendar file.
-        future: bool
-
-        Returns
-        ----------
-        list
-            list of timestamps
-        """
-        raise NotImplementedError("Subclass of CalendarProvider must implement `load_calendar` method")
-
-
-class InstrumentProvider(abc.ABC):
-    """Instrument provider base class
-
-    Provide instrument data.
-    """
-
-    @staticmethod
-    def instruments(market: Union[List, str] = "all", filter_pipe: Union[List, None] = None):
-        """Get the general config dictionary for a base market adding several dynamic filters.
-
-        Parameters
-        ----------
-        market : Union[List, str]
-            str:
-                market/industry/index shortname, e.g. all/sse/szse/sse50/csi300/csi500.
-            list:
-                ["ID1", "ID2"]. A list of stocks
-        filter_pipe : list
-            the list of dynamic filters.
-
-        Returns
-        ----------
-        dict: if isinstance(market, str)
-            dict of stockpool config.
-
-            {`market` => base market name, `filter_pipe` => list of filters}
-
-            example :
-
-            .. code-block::
-
-                {'market': 'csi500',
-                'filter_pipe': [{'filter_type': 'ExpressionDFilter',
-                'rule_expression': '$open<40',
-                'filter_start_time': None,
-                'filter_end_time': None,
-                'keep': False},
-                {'filter_type': 'NameDFilter',
-                'name_rule_re': 'SH[0-9]{4}55',
-                'filter_start_time': None,
-                'filter_end_time': None}]}
-
-        list: if isinstance(market, list)
-            just return the original list directly.
-            NOTE: this will make the instruments compatible with more cases. The user code will be simpler.
-        """
-        if isinstance(market, list):
-            return market
-        from .filter import SeriesDFilter  # pylint: disable=C0415
-
-        if filter_pipe is None:
-            filter_pipe = []
-        config = {"market": market, "filter_pipe": []}
-        # the order of the filters will affect the result, so we need to keep
-        # the order
-        for filter_t in filter_pipe:
-            if isinstance(filter_t, dict):
-                _config = filter_t
-            elif isinstance(filter_t, SeriesDFilter):
-                _config = filter_t.to_config()
-            else:
-                raise TypeError(
-                    f"Unsupported filter types: {type(filter_t)}! Filter only supports dict or isinstance(filter, SeriesDFilter)"
-                )
-            config["filter_pipe"].append(_config)
-        return config
-
-    @abc.abstractmethod
-    def list_instruments(self, instruments, start_time=None, end_time=None, freq="day", as_list=False):
-        """List the instruments based on a certain stockpool config.
-
-        Parameters
-        ----------
-        instruments : dict
-            stockpool config.
-        start_time : str
-            start of the time range.
-        end_time : str
-            end of the time range.
-        as_list : bool
-            return instruments as list or dict.
-
-        Returns
-        -------
-        dict or list
-            instruments list or dictionary with time spans
-        """
-        raise NotImplementedError("Subclass of InstrumentProvider must implement `list_instruments` method")
-
-    def _uri(self, instruments, start_time=None, end_time=None, freq="day", as_list=False):
-        return hash_args(instruments, start_time, end_time, freq, as_list)
-
-    # instruments type
-    LIST = "LIST"
-    DICT = "DICT"
-    CONF = "CONF"
-
-    @classmethod
-    def get_inst_type(cls, inst):
-        if "market" in inst:
-            return cls.CONF
-        if isinstance(inst, dict):
-            return cls.DICT
-        if isinstance(inst, (list, tuple, pd.Index, np.ndarray)):
-            return cls.LIST
-        raise ValueError(f"Unknown instrument type {inst}")
-
-
-class FeatureProvider(abc.ABC):
-    """Feature provider class
-
-    Provide feature data.
-    """
-
-    @abc.abstractmethod
-    def feature(self, instrument, field, start_time, end_time, freq):
-        """Get feature data.
-
-        Parameters
-        ----------
-        instrument : str
-            a certain instrument.
-        field : str
-            a certain field of feature.
-        start_time : str
-            start of the time range.
-        end_time : str
-            end of the time range.
-        freq : str
-            time frequency, available: year/quarter/month/week/day.
-
-        Returns
-        -------
-        pd.Series
-            data of a certain feature
-        """
-        raise NotImplementedError("Subclass of FeatureProvider must implement `feature` method")
-
-
-class PITProvider(abc.ABC):
-    @abc.abstractmethod
-    def period_feature(
-        self,
-        instrument,
-        field,
-        start_index: int,
-        end_index: int,
-        cur_time: pd.Timestamp,
-        period: Optional[int] = None,
-    ) -> pd.Series:
-        """
-        get the historical periods data series between `start_index` and `end_index`
-
-        Parameters
-        ----------
-        start_index: int
-            start_index is a relative index to the latest period to cur_time
-
-        end_index: int
-            end_index is a relative index to the latest period to cur_time
-            in most cases, the start_index and end_index will be a non-positive values
-            For example, start_index == -3 end_index == 0 and current period index is cur_idx,
-            then the data between [start_index + cur_idx, end_index + cur_idx] will be retrieved.
-
-        period: int
-            This is used for query specific period.
-            The period is represented with int in Qlib. (e.g. 202001 may represent the first quarter in 2020)
-            NOTE: `period`  will override `start_index` and `end_index`
-
-        Returns
-        -------
-        pd.Series
-            The index will be integers to indicate the periods of the data
-            An typical examples will be
-            TODO
-
-        Raises
-        ------
-        FileNotFoundError
-            This exception will be raised if the queried data do not exist.
-        """
-        raise NotImplementedError(f"Please implement the `period_feature` method")
-
-
-class ExpressionProvider(abc.ABC):
-    """Expression provider class
-
-    Provide Expression data.
-    """
-
-    def __init__(self):
-        self.expression_instance_cache = {}
-
-    def get_expression_instance(self, field):
-        try:
-            if field in self.expression_instance_cache:
-                expression = self.expression_instance_cache[field]
-            else:
-                expression = eval(parse_field(field))
-                self.expression_instance_cache[field] = expression
-        except NameError as e:
-            get_module_logger("data").exception(
-                "ERROR: field [%s] contains invalid operator/variable [%s]" % (str(field), str(e).split()[1])
-            )
-            raise
-        except SyntaxError:
-            get_module_logger("data").exception("ERROR: field [%s] contains invalid syntax" % str(field))
-            raise
-        return expression
-
-    @abc.abstractmethod
-    def expression(self, instrument, field, start_time=None, end_time=None, freq="day") -> pd.Series:
-        """Get Expression data.
-
-        The responsibility of `expression`
-        - parse the `field` and `load` the according data.
-        - When loading the data, it should handle the time dependency of the data. `get_expression_instance` is commonly used in this method
-
-        Parameters
-        ----------
-        instrument : str
-            a certain instrument.
-        field : str
-            a certain field of feature.
-        start_time : str
-            start of the time range.
-        end_time : str
-            end of the time range.
-        freq : str
-            time frequency, available: year/quarter/month/week/day.
-
-        Returns
-        -------
-        pd.Series
-            data of a certain expression
-
-            The data has two types of format
-
-            1) expression with datetime index
-
-            2) expression with integer index
-
-                - because the datetime is not as good as
-        """
-        raise NotImplementedError("Subclass of ExpressionProvider must implement `Expression` method")
-
-
-class DatasetProvider(abc.ABC):
-    """Dataset provider class
-
-    Provide Dataset data.
-    """
-
-    @abc.abstractmethod
-    def dataset(self, instruments, fields, start_time=None, end_time=None, freq="day", inst_processors=[]):
-        """Get dataset data.
-
-        Parameters
-        ----------
-        instruments : list or dict
-            list/dict of instruments or dict of stockpool config.
-        fields : list
-            list of feature instances.
-        start_time : str
-            start of the time range.
-        end_time : str
-            end of the time range.
-        freq : str
-            time frequency.
-        inst_processors:  Iterable[Union[dict, InstProcessor]]
-            the operations performed on each instrument
-
-        Returns
-        ----------
-        pd.DataFrame
-            a pandas dataframe with <instrument, datetime> index.
-        """
-        raise NotImplementedError("Subclass of DatasetProvider must implement `Dataset` method")
-
-    def _uri(
-        self,
-        instruments,
-        fields,
-        start_time=None,
-        end_time=None,
-        freq="day",
-        disk_cache=1,
-        inst_processors=[],
-        **kwargs,
-    ):
-        """Get task uri, used when generating rabbitmq task in qlib_server
-
-        Parameters
-        ----------
-        instruments : list or dict
-            list/dict of instruments or dict of stockpool config.
-        fields : list
-            list of feature instances.
-        start_time : str
-            start of the time range.
-        end_time : str
-            end of the time range.
-        freq : str
-            time frequency.
-        disk_cache : int
-            whether to skip(0)/use(1)/replace(2) disk_cache.
-
-        """
-        # TODO: qlib-server support inst_processors
-        return DiskDatasetCache._uri(instruments, fields, start_time, end_time, freq, disk_cache, inst_processors)
-
-    @staticmethod
-    def get_instruments_d(instruments, freq):
-        """
-        Parse different types of input instruments to output instruments_d
-        Wrong format of input instruments will lead to exception.
-
-        """
-        if isinstance(instruments, dict):
-            if "market" in instruments:
-                # dict of stockpool config
-                instruments_d = Inst.list_instruments(instruments=instruments, freq=freq, as_list=False)
-            else:
-                # dict of instruments and timestamp
-                instruments_d = instruments
-        elif isinstance(instruments, (list, tuple, pd.Index, np.ndarray)):
-            # list or tuple of a group of instruments
-            instruments_d = list(instruments)
-        else:
-            raise ValueError("Unsupported input type for param `instrument`")
-        return instruments_d
-
-    @staticmethod
-    def get_column_names(fields):
-        """
-        Get column names from input fields
-
-        """
-        if len(fields) == 0:
-            raise ValueError("fields cannot be empty")
-        fields = fields.copy()
-        column_names = [str(f) for f in fields]
-        return column_names
-
-    @staticmethod
-    def parse_fields(fields):
-        # parse and check the input fields
-        return [ExpressionD.get_expression_instance(f) for f in fields]
-
-    @staticmethod
-    def dataset_processor(instruments_d, column_names, start_time, end_time, freq, inst_processors=[]):
-        """
-        Load and process the data, return the data set.
-        - default using multi-kernel method.
-
-        """
-        normalize_column_names = normalize_cache_fields(column_names)
-        # One process for one task, so that the memory will be freed quicker.
-        workers = max(min(C.get_kernels(freq), len(instruments_d)), 1)
-
-        # create iterator
-        if isinstance(instruments_d, dict):
-            it = instruments_d.items()
-        else:
-            it = zip(instruments_d, [None] * len(instruments_d))
-
-        inst_l = []
-        task_l = []
-        for inst, spans in it:
-            inst_l.append(inst)
-            task_l.append(
-                delayed(DatasetProvider.inst_calculator)(
-                    inst, start_time, end_time, freq, normalize_column_names, spans, C, inst_processors
-                )
-            )
-
-        data = dict(
-            zip(
-                inst_l,
-                ParallelExt(n_jobs=workers, backend=C.joblib_backend, maxtasksperchild=C.maxtasksperchild)(task_l),
-            )
-        )
-
-        new_data = dict()
-        for inst in sorted(data.keys()):
-            if len(data[inst]) > 0:
-                # NOTE: Python version >= 3.6; in versions after python3.6, dict will always guarantee the insertion order
-                new_data[inst] = data[inst]
-
-        if len(new_data) > 0:
-            data = pd.concat(new_data, names=["instrument"], sort=False)
-            data = DiskDatasetCache.cache_to_origin_data(data, column_names)
-        else:
-            data = pd.DataFrame(
-                index=pd.MultiIndex.from_arrays([[], []], names=("instrument", "datetime")),
-                columns=column_names,
-                dtype=np.float32,
-            )
-
-        return data
-
-    @staticmethod
-    def inst_calculator(inst, start_time, end_time, freq, column_names, spans=None, g_config=None, inst_processors=[]):
-        """
-        Calculate the expressions for **one** instrument, return a df result.
-        If the expression has been calculated before, load from cache.
-
-        return value: A data frame with index 'datetime' and other data columns.
-
-        """
-        # FIXME: Windows OS or MacOS using spawn: https://docs.python.org/3.8/library/multiprocessing.html?highlight=spawn#contexts-and-start-methods
-        # NOTE: This place is compatible with windows, windows multi-process is spawn
-        C.register_from_C(g_config)
-
-        obj = dict()
-        for field in column_names:
-            #  The client does not have expression provider, the data will be loaded from cache using static method.
-            obj[field] = ExpressionD.expression(inst, field, start_time, end_time, freq)
-
-        data = pd.DataFrame(obj)
-        if not data.empty and not np.issubdtype(data.index.dtype, np.dtype("M")):
-            # If the underlaying provides the data not in datatime formmat, we'll convert it into datetime format
-            _calendar = Cal.calendar(freq=freq)
-            data.index = _calendar[data.index.values.astype(int)]
-        data.index.names = ["datetime"]
-
-        if not data.empty and spans is not None:
-            mask = np.zeros(len(data), dtype=bool)
-            for begin, end in spans:
-                mask |= (data.index >= begin) & (data.index <= end)
-            data = data[mask]
-
-        for _processor in inst_processors:
-            if _processor:
-                _processor_obj = init_instance_by_config(_processor, accept_types=InstProcessor)
-                data = _processor_obj(data, instrument=inst)
-        return data
-
-
-class LocalCalendarProvider(CalendarProvider, ProviderBackendMixin):
-    """Local calendar data provider class
-
-    Provide calendar data from local data source.
-    """
-
-    def __init__(self, remote=False, backend={}):
-        super().__init__()
-        self.remote = remote
-        self.backend = backend
-
-    def load_calendar(self, freq, future):
-        """Load original calendar timestamp from file.
-
-        Parameters
-        ----------
-        freq : str
-            frequency of read calendar file.
-        future: bool
-        Returns
-        ----------
-        list
-            list of timestamps
-        """
-        try:
-            backend_obj = self.backend_obj(freq=freq, future=future).data
-        except ValueError:
-            if future:
-                get_module_logger("data").warning(
-                    f"load calendar error: freq={freq}, future={future}; return current calendar!"
-                )
-                get_module_logger("data").warning(
-                    "You can get future calendar by referring to the following document: https://github.com/microsoft/qlib/blob/main/scripts/data_collector/contrib/README.md"
-                )
-                backend_obj = self.backend_obj(freq=freq, future=False).data
-            else:
-                raise
-
-        return [pd.Timestamp(x) for x in backend_obj]
-
-
-class LocalInstrumentProvider(InstrumentProvider, ProviderBackendMixin):
-    """Local instrument data provider class
-
-    Provide instrument data from local data source.
-    """
-
-    def __init__(self, backend={}) -> None:
-        super().__init__()
-        self.backend = backend
-
-    def _load_instruments(self, market, freq):
-        return self.backend_obj(market=market, freq=freq).data
-
-    def list_instruments(self, instruments, start_time=None, end_time=None, freq="day", as_list=False):
-        market = instruments["market"]
-        if market in H["i"]:
-            _instruments = H["i"][market]
-        else:
-            _instruments = self._load_instruments(market, freq=freq)
-            H["i"][market] = _instruments
-        # strip
-        # use calendar boundary
-        cal = Cal.calendar(freq=freq)
-        start_time = pd.Timestamp(start_time or cal[0])
-        end_time = pd.Timestamp(end_time or cal[-1])
-        _instruments_filtered = {
-            inst: list(
-                filter(
-                    lambda x: x[0] <= x[1],
-                    [(max(start_time, pd.Timestamp(x[0])), min(end_time, pd.Timestamp(x[1]))) for x in spans],
-                )
-            )
-            for inst, spans in _instruments.items()
-        }
-        _instruments_filtered = {key: value for key, value in _instruments_filtered.items() if value}
-        # filter
-        filter_pipe = instruments["filter_pipe"]
-        for filter_config in filter_pipe:
-            from . import filter as F  # pylint: disable=C0415
-
-            filter_t = getattr(F, filter_config["filter_type"]).from_config(filter_config)
-            _instruments_filtered = filter_t(_instruments_filtered, start_time, end_time, freq)
-        # as list
-        if as_list:
-            return list(_instruments_filtered)
-        return _instruments_filtered
-
-
-class LocalFeatureProvider(FeatureProvider, ProviderBackendMixin):
-    """Local feature data provider class
-
-    Provide feature data from local data source.
-    """
-
-    def __init__(self, remote=False, backend={}):
-        super().__init__()
-        self.remote = remote
-        self.backend = backend
-
-    def feature(self, instrument, field, start_index, end_index, freq):
-        # validate
-        field = str(field)[1:]
-        instrument = code_to_fname(instrument)
-        return self.backend_obj(instrument=instrument, field=field, freq=freq)[start_index : end_index + 1]
-
-
-class LocalPITProvider(PITProvider):
-    # TODO: Add PIT backend file storage
-    # NOTE: This class is not multi-threading-safe!!!!
-
-    def period_feature(self, instrument, field, start_index, end_index, cur_time, period=None):
-        if not isinstance(cur_time, pd.Timestamp):
-            raise ValueError(
-                f"Expected pd.Timestamp for `cur_time`, got '{cur_time}'. Advices: you can't query PIT data directly(e.g. '$$roewa_q'), you must use `P` operator to convert data to each day (e.g. 'P($$roewa_q)')"
-            )
-
-        assert end_index <= 0  # PIT don't support querying future data
-
-        DATA_RECORDS = [
-            ("date", C.pit_record_type["date"]),
-            ("period", C.pit_record_type["period"]),
-            ("value", C.pit_record_type["value"]),
-            ("_next", C.pit_record_type["index"]),
-        ]
-        VALUE_DTYPE = C.pit_record_type["value"]
-
-        field = str(field).lower()[2:]
-        instrument = code_to_fname(instrument)
-
-        # {For acceleration
-        # start_index, end_index, cur_index = kwargs["info"]
-        # if cur_index == start_index:
-        #     if not hasattr(self, "all_fields"):
-        #         self.all_fields = []
-        #     self.all_fields.append(field)
-        #     if not hasattr(self, "period_index"):
-        #         self.period_index = {}
-        #     if field not in self.period_index:
-        #         self.period_index[field] = {}
-        # For acceleration}
-
-        if not field.endswith("_q") and not field.endswith("_a"):
-            raise ValueError("period field must ends with '_q' or '_a'")
-        quarterly = field.endswith("_q")
-        index_path = C.dpm.get_data_uri() / "financial" / instrument.lower() / f"{field}.index"
-        data_path = C.dpm.get_data_uri() / "financial" / instrument.lower() / f"{field}.data"
-        if not (index_path.exists() and data_path.exists()):
-            raise FileNotFoundError("No file is found.")
-        # NOTE: The most significant performance loss is here.
-        # Does the acceleration that makes the program complicated really matters?
-        # - It makes parameters of the interface complicate
-        # - It does not performance in the optimal way (places all the pieces together, we may achieve higher performance)
-        #    - If we design it carefully, we can go through for only once to get the historical evolution of the data.
-        # So I decide to deprecated previous implementation and keep the logic of the program simple
-        # Instead, I'll add a cache for the index file.
-        data = np.fromfile(data_path, dtype=DATA_RECORDS)
-
-        # find all revision periods before `cur_time`
-        cur_time_int = int(cur_time.year) * 10000 + int(cur_time.month) * 100 + int(cur_time.day)
-        loc = np.searchsorted(data["date"], cur_time_int, side="right")
-        if loc <= 0:
-            return pd.Series(dtype=C.pit_record_type["value"])
-        last_period = data["period"][:loc].max()  # return the latest quarter
-        first_period = data["period"][:loc].min()
-        period_list = get_period_list(first_period, last_period, quarterly)
-        if period is not None:
-            # NOTE: `period` has higher priority than `start_index` & `end_index`
-            if period not in period_list:
-                return pd.Series(dtype=C.pit_record_type["value"])
-            else:
-                period_list = [period]
-        else:
-            period_list = period_list[max(0, len(period_list) + start_index - 1) : len(period_list) + end_index]
-        value = np.full((len(period_list),), np.nan, dtype=VALUE_DTYPE)
-        for i, p in enumerate(period_list):
-            # last_period_index = self.period_index[field].get(period)  # For acceleration
-            value[i], now_period_index = read_period_data(
-                index_path, data_path, p, cur_time_int, quarterly  # , last_period_index  # For acceleration
-            )
-            # self.period_index[field].update({period: now_period_index})  # For acceleration
-        # NOTE: the index is period_list; So it may result in unexpected values(e.g. nan)
-        # when calculation between different features and only part of its financial indicator is published
-        series = pd.Series(value, index=period_list, dtype=VALUE_DTYPE)
-
-        # {For acceleration
-        # if cur_index == end_index:
-        #     self.all_fields.remove(field)
-        #     if not len(self.all_fields):
-        #         del self.all_fields
-        #         del self.period_index
-        # For acceleration}
-
-        return series
-
-
-class LocalExpressionProvider(ExpressionProvider):
-    """Local expression data provider class
-
-    Provide expression data from local data source.
-    """
-
-    def __init__(self, time2idx=True):
-        super().__init__()
-        self.time2idx = time2idx
-
-    def expression(self, instrument, field, start_time=None, end_time=None, freq="day"):
-        expression = self.get_expression_instance(field)
-        start_time = time_to_slc_point(start_time)
-        end_time = time_to_slc_point(end_time)
-
-        # Two kinds of queries are supported
-        # - Index-based expression: this may save a lot of memory because the datetime index is not saved on the disk
-        # - Data with datetime index expression: this will make it more convenient to integrating with some existing databases
-        if self.time2idx:
-            _, _, start_index, end_index = Cal.locate_index(start_time, end_time, freq=freq, future=False)
-            lft_etd, rght_etd = expression.get_extended_window_size()
-            query_start, query_end = max(0, start_index - lft_etd), end_index + rght_etd
-        else:
-            start_index, end_index = query_start, query_end = start_time, end_time
-
-        try:
-            series = expression.load(instrument, query_start, query_end, freq)
-        except Exception as e:
-            get_module_logger("data").debug(
-                f"Loading expression error: "
-                f"instrument={instrument}, field=({field}), start_time={start_time}, end_time={end_time}, freq={freq}. "
-                f"error info: {str(e)}"
-            )
-            raise
-        # Ensure that each column type is consistent
-        # FIXME:
-        # 1) The stock data is currently float. If there is other types of data, this part needs to be re-implemented.
-        # 2) The precision should be configurable
-        try:
-            series = series.astype(np.float32)
-        except ValueError:
-            pass
-        except TypeError:
-            pass
-        if not series.empty:
-            series = series.loc[start_index:end_index]
-        return series
-
-
-class LocalDatasetProvider(DatasetProvider):
-    """Local dataset data provider class
-
-    Provide dataset data from local data source.
-    """
-
-    def __init__(self, align_time: bool = True):
-        """
-        Parameters
-        ----------
-        align_time : bool
-            Will we align the time to calendar
-            the frequency is flexible in some dataset and can't be aligned.
-            For the data with fixed frequency with a shared calendar, the align data to the calendar will provides following benefits
-
-            - Align queries to the same parameters, so the cache can be shared.
-        """
-        super().__init__()
-        self.align_time = align_time
-
-    def dataset(
-        self,
-        instruments,
-        fields,
-        start_time=None,
-        end_time=None,
-        freq="day",
-        inst_processors=[],
-    ):
-        instruments_d = self.get_instruments_d(instruments, freq)
-        column_names = self.get_column_names(fields)
-        if self.align_time:
-            # NOTE: if the frequency is a fixed value.
-            # align the data to fixed calendar point
-            cal = Cal.calendar(start_time, end_time, freq)
-            if len(cal) == 0:
-                return pd.DataFrame(
-                    index=pd.MultiIndex.from_arrays([[], []], names=("instrument", "datetime")), columns=column_names
-                )
-            start_time = cal[0]
-            end_time = cal[-1]
-        data = self.dataset_processor(
-            instruments_d, column_names, start_time, end_time, freq, inst_processors=inst_processors
-        )
-
-        return data
-
-    @staticmethod
-    def multi_cache_walker(instruments, fields, start_time=None, end_time=None, freq="day"):
-        """
-        This method is used to prepare the expression cache for the client.
-        Then the client will load the data from expression cache by itself.
-
-        """
-        instruments_d = DatasetProvider.get_instruments_d(instruments, freq)
-        column_names = DatasetProvider.get_column_names(fields)
-        cal = Cal.calendar(start_time, end_time, freq)
-        if len(cal) == 0:
-            return
-        start_time = cal[0]
-        end_time = cal[-1]
-        workers = max(min(C.kernels, len(instruments_d)), 1)
-
-        ParallelExt(n_jobs=workers, backend=C.joblib_backend, maxtasksperchild=C.maxtasksperchild)(
-            delayed(LocalDatasetProvider.cache_walker)(inst, start_time, end_time, freq, column_names)
-            for inst in instruments_d
-        )
-
-    @staticmethod
-    def cache_walker(inst, start_time, end_time, freq, column_names):
-        """
-        If the expressions of one instrument haven't been calculated before,
-        calculate it and write it into expression cache.
-
-        """
-        for field in column_names:
-            ExpressionD.expression(inst, field, start_time, end_time, freq)
-
-
-class ClientCalendarProvider(CalendarProvider):
-    """Client calendar data provider class
-
-    Provide calendar data by requesting data from server as a client.
-    """
-
-    def __init__(self):
-        self.conn = None
-        self.queue = queue.Queue()
-
-    def set_conn(self, conn):
-        self.conn = conn
-
-    def calendar(self, start_time=None, end_time=None, freq="day", future=False):
-        self.conn.send_request(
-            request_type="calendar",
-            request_content={"start_time": str(start_time), "end_time": str(end_time), "freq": freq, "future": future},
-            msg_queue=self.queue,
-            msg_proc_func=lambda response_content: [pd.Timestamp(c) for c in response_content],
-        )
-        result = self.queue.get(timeout=C["timeout"])
-        return result
-
-
-class ClientInstrumentProvider(InstrumentProvider):
-    """Client instrument data provider class
-
-    Provide instrument data by requesting data from server as a client.
-    """
-
-    def __init__(self):
-        self.conn = None
-        self.queue = queue.Queue()
-
-    def set_conn(self, conn):
-        self.conn = conn
-
-    def list_instruments(self, instruments, start_time=None, end_time=None, freq="day", as_list=False):
-        def inst_msg_proc_func(response_content):
-            if isinstance(response_content, dict):
-                instrument = {
-                    i: [(pd.Timestamp(s), pd.Timestamp(e)) for s, e in t] for i, t in response_content.items()
-                }
-            else:
-                instrument = response_content
-            return instrument
-
-        self.conn.send_request(
-            request_type="instrument",
-            request_content={
-                "instruments": instruments,
-                "start_time": str(start_time),
-                "end_time": str(end_time),
-                "freq": freq,
-                "as_list": as_list,
-            },
-            msg_queue=self.queue,
-            msg_proc_func=inst_msg_proc_func,
-        )
-        result = self.queue.get(timeout=C["timeout"])
-        if isinstance(result, Exception):
-            raise result
-        get_module_logger("data").debug("get result")
-        return result
-
-
-class ClientDatasetProvider(DatasetProvider):
-    """Client dataset data provider class
-
-    Provide dataset data by requesting data from server as a client.
-    """
-
-    def __init__(self):
-        self.conn = None
-
-    def set_conn(self, conn):
-        self.conn = conn
-        self.queue = queue.Queue()
-
-    def dataset(
-        self,
-        instruments,
-        fields,
-        start_time=None,
-        end_time=None,
-        freq="day",
-        disk_cache=0,
-        return_uri=False,
-        inst_processors=[],
-    ):
-        if Inst.get_inst_type(instruments) == Inst.DICT:
-            get_module_logger("data").warning(
-                "Getting features from a dict of instruments is not recommended because the features will not be "
-                "cached! "
-                "The dict of instruments will be cleaned every day."
-            )
-
-        if disk_cache == 0:
-            """
-            Call the server to generate the expression cache.
-            Then load the data from the expression cache directly.
-            - default using multi-kernel method.
-
-            """
-            self.conn.send_request(
-                request_type="feature",
-                request_content={
-                    "instruments": instruments,
-                    "fields": fields,
-                    "start_time": start_time,
-                    "end_time": end_time,
-                    "freq": freq,
-                    "disk_cache": 0,
-                },
-                msg_queue=self.queue,
-            )
-            feature_uri = self.queue.get(timeout=C["timeout"])
-            if isinstance(feature_uri, Exception):
-                raise feature_uri
-            else:
-                instruments_d = self.get_instruments_d(instruments, freq)
-                column_names = self.get_column_names(fields)
-                cal = Cal.calendar(start_time, end_time, freq)
-                if len(cal) == 0:
-                    return pd.DataFrame(
-                        index=pd.MultiIndex.from_arrays([[], []], names=("instrument", "datetime")),
-                        columns=column_names,
-                    )
-                start_time = cal[0]
-                end_time = cal[-1]
-
-                data = self.dataset_processor(instruments_d, column_names, start_time, end_time, freq, inst_processors)
-                if return_uri:
-                    return data, feature_uri
-                else:
-                    return data
-        else:
-
-            """
-            Call the server to generate the data-set cache, get the uri of the cache file.
-            Then load the data from the file on NFS directly.
-            - using single-process implementation.
-
-            """
-            # TODO: support inst_processors, need to change the code of qlib-server at the same time
-            # FIXME: The cache after resample, when read again and intercepted with end_time, results in incomplete data date
-            if inst_processors:
-                raise ValueError(
-                    f"{self.__class__.__name__} does not support inst_processor. "
-                    f"Please use `D.features(disk_cache=0)` or `qlib.init(dataset_cache=None)`"
-                )
-            self.conn.send_request(
-                request_type="feature",
-                request_content={
-                    "instruments": instruments,
-                    "fields": fields,
-                    "start_time": start_time,
-                    "end_time": end_time,
-                    "freq": freq,
-                    "disk_cache": 1,
-                },
-                msg_queue=self.queue,
-            )
-            # - Done in callback
-            feature_uri = self.queue.get(timeout=C["timeout"])
-            if isinstance(feature_uri, Exception):
-                raise feature_uri
-            get_module_logger("data").debug("get result")
-            try:
-                # pre-mound nfs, used for demo
-                mnt_feature_uri = C.dpm.get_data_uri(freq).joinpath(C.dataset_cache_dir_name, feature_uri)
-                df = DiskDatasetCache.read_data_from_cache(mnt_feature_uri, start_time, end_time, fields)
-                get_module_logger("data").debug("finish slicing data")
-                if return_uri:
-                    return df, feature_uri
-                return df
-            except AttributeError as attribute_e:
-                raise IOError("Unable to fetch instruments from remote server!") from attribute_e
-
-
-class BaseProvider:
-    """Local provider class
-    It is a set of interface that allow users to access data.
-    Because PITD is not exposed publicly to users, so it is not included in the interface.
-
-    To keep compatible with old qlib provider.
-    """
-
-    def calendar(self, start_time=None, end_time=None, freq="day", future=False):
-        return Cal.calendar(start_time, end_time, freq, future=future)
-
-    def instruments(self, market="all", filter_pipe=None, start_time=None, end_time=None):
-        if start_time is not None or end_time is not None:
-            get_module_logger("Provider").warning(
-                "The instruments corresponds to a stock pool. "
-                "Parameters `start_time` and `end_time` does not take effect now."
-            )
-        return InstrumentProvider.instruments(market, filter_pipe)
-
-    def list_instruments(self, instruments, start_time=None, end_time=None, freq="day", as_list=False):
-        return Inst.list_instruments(instruments, start_time, end_time, freq, as_list)
-
-    def features(
-        self,
-        instruments,
-        fields,
-        start_time=None,
-        end_time=None,
-        freq="day",
-        disk_cache=None,
-        inst_processors=[],
-    ):
-        """
-        Parameters
-        ----------
-        disk_cache : int
-            whether to skip(0)/use(1)/replace(2) disk_cache
-
-
-        This function will try to use cache method which has a keyword `disk_cache`,
-        and will use provider method if a type error is raised because the DatasetD instance
-        is a provider class.
-        """
-        disk_cache = C.default_disk_cache if disk_cache is None else disk_cache
-        fields = list(fields)  # In case of tuple.
-        try:
-            return DatasetD.dataset(
-                instruments, fields, start_time, end_time, freq, disk_cache, inst_processors=inst_processors
-            )
-        except TypeError:
-            return DatasetD.dataset(instruments, fields, start_time, end_time, freq, inst_processors=inst_processors)
-
-
-class LocalProvider(BaseProvider):
-    def _uri(self, type, **kwargs):
-        """_uri
-        The server hope to get the uri of the request. The uri will be decided
-        by the dataprovider. For ex, different cache layer has different uri.
-
-        :param type: The type of resource for the uri
-        :param **kwargs:
-        """
-        if type == "calendar":
-            return Cal._uri(**kwargs)
-        elif type == "instrument":
-            return Inst._uri(**kwargs)
-        elif type == "feature":
-            return DatasetD._uri(**kwargs)
-
-    def features_uri(self, instruments, fields, start_time, end_time, freq, disk_cache=1):
-        """features_uri
-
-        Return the uri of the generated cache of features/dataset
-
-        :param disk_cache:
-        :param instruments:
-        :param fields:
-        :param start_time:
-        :param end_time:
-        :param freq:
-        """
-        return DatasetD._dataset_uri(instruments, fields, start_time, end_time, freq, disk_cache)
-
-
-class ClientProvider(BaseProvider):
-    """Client Provider
-
-    Requesting data from server as a client. Can propose requests:
-
-        - Calendar : Directly respond a list of calendars
-        - Instruments (without filter): Directly respond a list/dict of instruments
-        - Instruments (with filters):  Respond a list/dict of instruments
-        - Features : Respond a cache uri
-
-    The general workflow is described as follows:
-    When the user use client provider to propose a request, the client provider will connect the server and send the request. The client will start to wait for the response. The response will be made instantly indicating whether the cache is available. The waiting procedure will terminate only when the client get the response saying `feature_available` is true.
-    `BUG` : Everytime we make request for certain data we need to connect to the server, wait for the response and disconnect from it. We can't make a sequence of requests within one connection. You can refer to https://python-socketio.readthedocs.io/en/latest/client.html for documentation of python-socketIO client.
-    """
-
-    def __init__(self):
-        def is_instance_of_provider(instance: object, cls: type):
-            if isinstance(instance, Wrapper):
-                p = getattr(instance, "_provider", None)
-
-                return False if p is None else isinstance(p, cls)
-
-            return isinstance(instance, cls)
-
-        from .client import Client  # pylint: disable=C0415
-
-        self.client = Client(C.flask_server, C.flask_port)
-        self.logger = get_module_logger(self.__class__.__name__)
-        if is_instance_of_provider(Cal, ClientCalendarProvider):
-            Cal.set_conn(self.client)
-        if is_instance_of_provider(Inst, ClientInstrumentProvider):
-            Inst.set_conn(self.client)
-        if hasattr(DatasetD, "provider"):
-            DatasetD.provider.set_conn(self.client)
-        else:
-            DatasetD.set_conn(self.client)
-
-
-import sys
-
-if sys.version_info >= (3, 9):
-    from typing import Annotated
-
-    CalendarProviderWrapper = Annotated[CalendarProvider, Wrapper]
-    InstrumentProviderWrapper = Annotated[InstrumentProvider, Wrapper]
-    FeatureProviderWrapper = Annotated[FeatureProvider, Wrapper]
-    PITProviderWrapper = Annotated[PITProvider, Wrapper]
-    ExpressionProviderWrapper = Annotated[ExpressionProvider, Wrapper]
-    DatasetProviderWrapper = Annotated[DatasetProvider, Wrapper]
-    BaseProviderWrapper = Annotated[BaseProvider, Wrapper]
-else:
-    CalendarProviderWrapper = CalendarProvider
-    InstrumentProviderWrapper = InstrumentProvider
-    FeatureProviderWrapper = FeatureProvider
-    PITProviderWrapper = PITProvider
-    ExpressionProviderWrapper = ExpressionProvider
-    DatasetProviderWrapper = DatasetProvider
-    BaseProviderWrapper = BaseProvider
-
-Cal: CalendarProviderWrapper = Wrapper()
-Inst: InstrumentProviderWrapper = Wrapper()
-FeatureD: FeatureProviderWrapper = Wrapper()
-PITD: PITProviderWrapper = Wrapper()
-ExpressionD: ExpressionProviderWrapper = Wrapper()
-DatasetD: DatasetProviderWrapper = Wrapper()
-D: BaseProviderWrapper = Wrapper()
-
-
-def register_all_wrappers(C):
-    """register_all_wrappers"""
-    logger = get_module_logger("data")
-    module = get_module_by_module_path("qlib.data")
-
-    _calendar_provider = init_instance_by_config(C.calendar_provider, module)
-    if getattr(C, "calendar_cache", None) is not None:
-        _calendar_provider = init_instance_by_config(C.calendar_cache, module, provide=_calendar_provider)
-    register_wrapper(Cal, _calendar_provider, "qlib.data")
-    logger.debug(f"registering Cal {C.calendar_provider}-{C.calendar_cache}")
-
-    _instrument_provider = init_instance_by_config(C.instrument_provider, module)
-    register_wrapper(Inst, _instrument_provider, "qlib.data")
-    logger.debug(f"registering Inst {C.instrument_provider}")
-
-    if getattr(C, "feature_provider", None) is not None:
-        feature_provider = init_instance_by_config(C.feature_provider, module)
-        register_wrapper(FeatureD, feature_provider, "qlib.data")
-        logger.debug(f"registering FeatureD {C.feature_provider}")
-
-    if getattr(C, "pit_provider", None) is not None:
-        pit_provider = init_instance_by_config(C.pit_provider, module)
-        register_wrapper(PITD, pit_provider, "qlib.data")
-        logger.debug(f"registering PITD {C.pit_provider}")
-
-    if getattr(C, "expression_provider", None) is not None:
-        # This provider is unnecessary in client provider
-        _eprovider = init_instance_by_config(C.expression_provider, module)
-        if getattr(C, "expression_cache", None) is not None:
-            _eprovider = init_instance_by_config(C.expression_cache, module, provider=_eprovider)
-        register_wrapper(ExpressionD, _eprovider, "qlib.data")
-        logger.debug(f"registering ExpressionD {C.expression_provider}-{C.expression_cache}")
-
-    _dprovider = init_instance_by_config(C.dataset_provider, module)
-    if getattr(C, "dataset_cache", None) is not None:
-        _dprovider = init_instance_by_config(C.dataset_cache, module, provider=_dprovider)
-    register_wrapper(DatasetD, _dprovider, "qlib.data")
-    logger.debug(f"registering DatasetD {C.dataset_provider}-{C.dataset_cache}")
-
-    register_wrapper(D, C.provider, "qlib.data")
-    logger.debug(f"registering D {C.provider}")
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+
+from __future__ import division
+from __future__ import print_function
+
+import re
+import abc
+import copy
+import queue
+import bisect
+import numpy as np
+import pandas as pd
+from typing import List, Union, Optional
+
+# For supporting multiprocessing in outer code, joblib is used
+from joblib import delayed
+
+from .cache import H
+from ..config import C
+from .inst_processor import InstProcessor
+
+from ..log import get_module_logger
+from .cache import DiskDatasetCache
+from ..utils import (
+    Wrapper,
+    init_instance_by_config,
+    register_wrapper,
+    get_module_by_module_path,
+    parse_field,
+    hash_args,
+    normalize_cache_fields,
+    code_to_fname,
+    time_to_slc_point,
+    read_period_data,
+    get_period_list,
+)
+from ..utils.paral import ParallelExt
+from .ops import Operators  # pylint: disable=W0611  # noqa: F401
+
+
+class ProviderBackendMixin:
+    """
+    This helper class tries to make the provider based on storage backend more convenient
+    It is not necessary to inherent this class if that provider don't rely on the backend storage
+    """
+
+    def get_default_backend(self):
+        backend = {}
+        provider_name: str = re.findall("[A-Z][^A-Z]*", self.__class__.__name__)[-2]
+        # set default storage class
+        backend.setdefault("class", f"File{provider_name}Storage")
+        # set default storage module
+        backend.setdefault("module_path", "qlib.data.storage.file_storage")
+        return backend
+
+    def backend_obj(self, **kwargs):
+        backend = self.backend if self.backend else self.get_default_backend()
+        backend = copy.deepcopy(backend)
+        backend.setdefault("kwargs", {}).update(**kwargs)
+        return init_instance_by_config(backend)
+
+
+class CalendarProvider(abc.ABC):
+    """Calendar provider base class
+
+    Provide calendar data.
+    """
+
+    def calendar(self, start_time=None, end_time=None, freq="day", future=False):
+        """Get calendar of certain market in given time range.
+
+        Parameters
+        ----------
+        start_time : str
+            start of the time range.
+        end_time : str
+            end of the time range.
+        freq : str
+            time frequency, available: year/quarter/month/week/day.
+        future : bool
+            whether including future trading day.
+
+        Returns
+        ----------
+        list
+            calendar list
+        """
+        _calendar, _calendar_index = self._get_calendar(freq, future)
+        if start_time == "None":
+            start_time = None
+        if end_time == "None":
+            end_time = None
+        # strip
+        if start_time:
+            start_time = pd.Timestamp(start_time)
+            if start_time > _calendar[-1]:
+                return np.array([])
+        else:
+            start_time = _calendar[0]
+        if end_time:
+            end_time = pd.Timestamp(end_time)
+            if end_time < _calendar[0]:
+                return np.array([])
+        else:
+            end_time = _calendar[-1]
+        _, _, si, ei = self.locate_index(start_time, end_time, freq, future)
+        return _calendar[si : ei + 1]
+
+    def locate_index(
+        self, start_time: Union[pd.Timestamp, str], end_time: Union[pd.Timestamp, str], freq: str, future: bool = False
+    ):
+        """Locate the start time index and end time index in a calendar under certain frequency.
+
+        Parameters
+        ----------
+        start_time : pd.Timestamp
+            start of the time range.
+        end_time : pd.Timestamp
+            end of the time range.
+        freq : str
+            time frequency, available: year/quarter/month/week/day.
+        future : bool
+            whether including future trading day.
+
+        Returns
+        -------
+        pd.Timestamp
+            the real start time.
+        pd.Timestamp
+            the real end time.
+        int
+            the index of start time.
+        int
+            the index of end time.
+        """
+        start_time = pd.Timestamp(start_time)
+        end_time = pd.Timestamp(end_time)
+        calendar, calendar_index = self._get_calendar(freq=freq, future=future)
+        if start_time not in calendar_index:
+            try:
+                start_time = calendar[bisect.bisect_left(calendar, start_time)]
+            except IndexError as index_e:
+                raise IndexError(
+                    "`start_time` uses a future date, if you want to get future trading days, you can use: `future=True`"
+                ) from index_e
+        start_index = calendar_index[start_time]
+        if end_time not in calendar_index:
+            end_time = calendar[bisect.bisect_right(calendar, end_time) - 1]
+        end_index = calendar_index[end_time]
+        return start_time, end_time, start_index, end_index
+
+    def _get_calendar(self, freq, future):
+        """Load calendar using memcache.
+
+        Parameters
+        ----------
+        freq : str
+            frequency of read calendar file.
+        future : bool
+            whether including future trading day.
+
+        Returns
+        -------
+        list
+            list of timestamps.
+        dict
+            dict composed by timestamp as key and index as value for fast search.
+        """
+        flag = f"{freq}_future_{future}"
+        if flag not in H["c"]:
+            _calendar = np.array(self.load_calendar(freq, future))
+            _calendar_index = {x: i for i, x in enumerate(_calendar)}  # for fast search
+            H["c"][flag] = _calendar, _calendar_index
+        return H["c"][flag]
+
+    def _uri(self, start_time, end_time, freq, future=False):
+        """Get the uri of calendar generation task."""
+        return hash_args(start_time, end_time, freq, future)
+
+    def load_calendar(self, freq, future):
+        """Load original calendar timestamp from file.
+
+        Parameters
+        ----------
+        freq : str
+            frequency of read calendar file.
+        future: bool
+
+        Returns
+        ----------
+        list
+            list of timestamps
+        """
+        raise NotImplementedError("Subclass of CalendarProvider must implement `load_calendar` method")
+
+
+class InstrumentProvider(abc.ABC):
+    """Instrument provider base class
+
+    Provide instrument data.
+    """
+
+    @staticmethod
+    def instruments(market: Union[List, str] = "all", filter_pipe: Union[List, None] = None):
+        """Get the general config dictionary for a base market adding several dynamic filters.
+
+        Parameters
+        ----------
+        market : Union[List, str]
+            str:
+                market/industry/index shortname, e.g. all/sse/szse/sse50/csi300/csi500.
+            list:
+                ["ID1", "ID2"]. A list of stocks
+        filter_pipe : list
+            the list of dynamic filters.
+
+        Returns
+        ----------
+        dict: if isinstance(market, str)
+            dict of stockpool config.
+
+            {`market` => base market name, `filter_pipe` => list of filters}
+
+            example :
+
+            .. code-block::
+
+                {'market': 'csi500',
+                'filter_pipe': [{'filter_type': 'ExpressionDFilter',
+                'rule_expression': '$open<40',
+                'filter_start_time': None,
+                'filter_end_time': None,
+                'keep': False},
+                {'filter_type': 'NameDFilter',
+                'name_rule_re': 'SH[0-9]{4}55',
+                'filter_start_time': None,
+                'filter_end_time': None}]}
+
+        list: if isinstance(market, list)
+            just return the original list directly.
+            NOTE: this will make the instruments compatible with more cases. The user code will be simpler.
+        """
+        if isinstance(market, list):
+            return market
+        from .filter import SeriesDFilter  # pylint: disable=C0415
+
+        if filter_pipe is None:
+            filter_pipe = []
+        config = {"market": market, "filter_pipe": []}
+        # the order of the filters will affect the result, so we need to keep
+        # the order
+        for filter_t in filter_pipe:
+            if isinstance(filter_t, dict):
+                _config = filter_t
+            elif isinstance(filter_t, SeriesDFilter):
+                _config = filter_t.to_config()
+            else:
+                raise TypeError(
+                    f"Unsupported filter types: {type(filter_t)}! Filter only supports dict or isinstance(filter, SeriesDFilter)"
+                )
+            config["filter_pipe"].append(_config)
+        return config
+
+    @abc.abstractmethod
+    def list_instruments(self, instruments, start_time=None, end_time=None, freq="day", as_list=False):
+        """List the instruments based on a certain stockpool config.
+
+        Parameters
+        ----------
+        instruments : dict
+            stockpool config.
+        start_time : str
+            start of the time range.
+        end_time : str
+            end of the time range.
+        as_list : bool
+            return instruments as list or dict.
+
+        Returns
+        -------
+        dict or list
+            instruments list or dictionary with time spans
+        """
+        raise NotImplementedError("Subclass of InstrumentProvider must implement `list_instruments` method")
+
+    def _uri(self, instruments, start_time=None, end_time=None, freq="day", as_list=False):
+        return hash_args(instruments, start_time, end_time, freq, as_list)
+
+    # instruments type
+    LIST = "LIST"
+    DICT = "DICT"
+    CONF = "CONF"
+
+    @classmethod
+    def get_inst_type(cls, inst):
+        if "market" in inst:
+            return cls.CONF
+        if isinstance(inst, dict):
+            return cls.DICT
+        if isinstance(inst, (list, tuple, pd.Index, np.ndarray)):
+            return cls.LIST
+        raise ValueError(f"Unknown instrument type {inst}")
+
+
+class FeatureProvider(abc.ABC):
+    """Feature provider class
+
+    Provide feature data.
+    """
+
+    @abc.abstractmethod
+    def feature(self, instrument, field, start_time, end_time, freq):
+        """Get feature data.
+
+        Parameters
+        ----------
+        instrument : str
+            a certain instrument.
+        field : str
+            a certain field of feature.
+        start_time : str
+            start of the time range.
+        end_time : str
+            end of the time range.
+        freq : str
+            time frequency, available: year/quarter/month/week/day.
+
+        Returns
+        -------
+        pd.Series
+            data of a certain feature
+        """
+        raise NotImplementedError("Subclass of FeatureProvider must implement `feature` method")
+
+
+class PITProvider(abc.ABC):
+    @abc.abstractmethod
+    def period_feature(
+        self,
+        instrument,
+        field,
+        start_index: int,
+        end_index: int,
+        cur_time: pd.Timestamp,
+        period: Optional[int] = None,
+    ) -> pd.Series:
+        """
+        get the historical periods data series between `start_index` and `end_index`
+
+        Parameters
+        ----------
+        start_index: int
+            start_index is a relative index to the latest period to cur_time
+
+        end_index: int
+            end_index is a relative index to the latest period to cur_time
+            in most cases, the start_index and end_index will be a non-positive values
+            For example, start_index == -3 end_index == 0 and current period index is cur_idx,
+            then the data between [start_index + cur_idx, end_index + cur_idx] will be retrieved.
+
+        period: int
+            This is used for query specific period.
+            The period is represented with int in Qlib. (e.g. 202001 may represent the first quarter in 2020)
+            NOTE: `period`  will override `start_index` and `end_index`
+
+        Returns
+        -------
+        pd.Series
+            The index will be integers to indicate the periods of the data
+            An typical examples will be
+            TODO
+
+        Raises
+        ------
+        FileNotFoundError
+            This exception will be raised if the queried data do not exist.
+        """
+        raise NotImplementedError(f"Please implement the `period_feature` method")
+
+
+class ExpressionProvider(abc.ABC):
+    """Expression provider class
+
+    Provide Expression data.
+    """
+
+    def __init__(self):
+        self.expression_instance_cache = {}
+
+    def get_expression_instance(self, field):
+        try:
+            if field in self.expression_instance_cache:
+                expression = self.expression_instance_cache[field]
+            else:
+                expression = eval(parse_field(field))
+                self.expression_instance_cache[field] = expression
+        except NameError as e:
+            get_module_logger("data").exception(
+                "ERROR: field [%s] contains invalid operator/variable [%s]" % (str(field), str(e).split()[1])
+            )
+            raise
+        except SyntaxError:
+            get_module_logger("data").exception("ERROR: field [%s] contains invalid syntax" % str(field))
+            raise
+        return expression
+
+    @abc.abstractmethod
+    def expression(self, instrument, field, start_time=None, end_time=None, freq="day") -> pd.Series:
+        """Get Expression data.
+
+        The responsibility of `expression`
+        - parse the `field` and `load` the according data.
+        - When loading the data, it should handle the time dependency of the data. `get_expression_instance` is commonly used in this method
+
+        Parameters
+        ----------
+        instrument : str
+            a certain instrument.
+        field : str
+            a certain field of feature.
+        start_time : str
+            start of the time range.
+        end_time : str
+            end of the time range.
+        freq : str
+            time frequency, available: year/quarter/month/week/day.
+
+        Returns
+        -------
+        pd.Series
+            data of a certain expression
+
+            The data has two types of format
+
+            1) expression with datetime index
+
+            2) expression with integer index
+
+                - because the datetime is not as good as
+        """
+        raise NotImplementedError("Subclass of ExpressionProvider must implement `Expression` method")
+
+
+class DatasetProvider(abc.ABC):
+    """Dataset provider class
+
+    Provide Dataset data.
+    """
+
+    @abc.abstractmethod
+    def dataset(self, instruments, fields, start_time=None, end_time=None, freq="day", inst_processors=[]):
+        """Get dataset data.
+
+        Parameters
+        ----------
+        instruments : list or dict
+            list/dict of instruments or dict of stockpool config.
+        fields : list
+            list of feature instances.
+        start_time : str
+            start of the time range.
+        end_time : str
+            end of the time range.
+        freq : str
+            time frequency.
+        inst_processors:  Iterable[Union[dict, InstProcessor]]
+            the operations performed on each instrument
+
+        Returns
+        ----------
+        pd.DataFrame
+            a pandas dataframe with <instrument, datetime> index.
+        """
+        raise NotImplementedError("Subclass of DatasetProvider must implement `Dataset` method")
+
+    def _uri(
+        self,
+        instruments,
+        fields,
+        start_time=None,
+        end_time=None,
+        freq="day",
+        disk_cache=1,
+        inst_processors=[],
+        **kwargs,
+    ):
+        """Get task uri, used when generating rabbitmq task in qlib_server
+
+        Parameters
+        ----------
+        instruments : list or dict
+            list/dict of instruments or dict of stockpool config.
+        fields : list
+            list of feature instances.
+        start_time : str
+            start of the time range.
+        end_time : str
+            end of the time range.
+        freq : str
+            time frequency.
+        disk_cache : int
+            whether to skip(0)/use(1)/replace(2) disk_cache.
+
+        """
+        # TODO: qlib-server support inst_processors
+        return DiskDatasetCache._uri(instruments, fields, start_time, end_time, freq, disk_cache, inst_processors)
+
+    @staticmethod
+    def get_instruments_d(instruments, freq):
+        """
+        Parse different types of input instruments to output instruments_d
+        Wrong format of input instruments will lead to exception.
+
+        """
+        if isinstance(instruments, dict):
+            if "market" in instruments:
+                # dict of stockpool config
+                instruments_d = Inst.list_instruments(instruments=instruments, freq=freq, as_list=False)
+            else:
+                # dict of instruments and timestamp
+                instruments_d = instruments
+        elif isinstance(instruments, (list, tuple, pd.Index, np.ndarray)):
+            # list or tuple of a group of instruments
+            instruments_d = list(instruments)
+        else:
+            raise ValueError("Unsupported input type for param `instrument`")
+        return instruments_d
+
+    @staticmethod
+    def get_column_names(fields):
+        """
+        Get column names from input fields
+
+        """
+        if len(fields) == 0:
+            raise ValueError("fields cannot be empty")
+        fields = fields.copy()
+        column_names = [str(f) for f in fields]
+        return column_names
+
+    @staticmethod
+    def parse_fields(fields):
+        # parse and check the input fields
+        return [ExpressionD.get_expression_instance(f) for f in fields]
+
+    @staticmethod
+    def dataset_processor(instruments_d, column_names, start_time, end_time, freq, inst_processors=[]):
+        """
+        Load and process the data, return the data set.
+        - default using multi-kernel method.
+
+        """
+        normalize_column_names = normalize_cache_fields(column_names)
+        # One process for one task, so that the memory will be freed quicker.
+        workers = max(min(C.get_kernels(freq), len(instruments_d)), 1)
+
+        # create iterator
+        if isinstance(instruments_d, dict):
+            it = instruments_d.items()
+        else:
+            it = zip(instruments_d, [None] * len(instruments_d))
+
+        inst_l = []
+        task_l = []
+        for inst, spans in it:
+            inst_l.append(inst)
+            task_l.append(
+                delayed(DatasetProvider.inst_calculator)(
+                    inst, start_time, end_time, freq, normalize_column_names, spans, C, inst_processors
+                )
+            )
+
+        data = dict(
+            zip(
+                inst_l,
+                ParallelExt(n_jobs=workers, backend=C.joblib_backend, maxtasksperchild=C.maxtasksperchild)(task_l),
+            )
+        )
+
+        new_data = dict()
+        for inst in sorted(data.keys()):
+            if len(data[inst]) > 0:
+                # NOTE: Python version >= 3.6; in versions after python3.6, dict will always guarantee the insertion order
+                new_data[inst] = data[inst]
+
+        if len(new_data) > 0:
+            data = pd.concat(new_data, names=["instrument"], sort=False)
+            data = DiskDatasetCache.cache_to_origin_data(data, column_names)
+        else:
+            data = pd.DataFrame(
+                index=pd.MultiIndex.from_arrays([[], []], names=("instrument", "datetime")),
+                columns=column_names,
+                dtype=np.float32,
+            )
+
+        return data
+
+    @staticmethod
+    def inst_calculator(inst, start_time, end_time, freq, column_names, spans=None, g_config=None, inst_processors=[]):
+        """
+        Calculate the expressions for **one** instrument, return a df result.
+        If the expression has been calculated before, load from cache.
+
+        return value: A data frame with index 'datetime' and other data columns.
+
+        """
+        # FIXME: Windows OS or MacOS using spawn: https://docs.python.org/3.8/library/multiprocessing.html?highlight=spawn#contexts-and-start-methods
+        # NOTE: This place is compatible with windows, windows multi-process is spawn
+        C.register_from_C(g_config)
+
+        obj = dict()
+        for field in column_names:
+            #  The client does not have expression provider, the data will be loaded from cache using static method.
+            obj[field] = ExpressionD.expression(inst, field, start_time, end_time, freq)
+
+        data = pd.DataFrame(obj)
+        if not data.empty and not np.issubdtype(data.index.dtype, np.dtype("M")):
+            # If the underlaying provides the data not in datatime formmat, we'll convert it into datetime format
+            _calendar = Cal.calendar(freq=freq)
+            data.index = _calendar[data.index.values.astype(int)]
+        data.index.names = ["datetime"]
+
+        if not data.empty and spans is not None:
+            mask = np.zeros(len(data), dtype=bool)
+            for begin, end in spans:
+                mask |= (data.index >= begin) & (data.index <= end)
+            data = data[mask]
+
+        for _processor in inst_processors:
+            if _processor:
+                _processor_obj = init_instance_by_config(_processor, accept_types=InstProcessor)
+                data = _processor_obj(data, instrument=inst)
+        return data
+
+
+class LocalCalendarProvider(CalendarProvider, ProviderBackendMixin):
+    """Local calendar data provider class
+
+    Provide calendar data from local data source.
+    """
+
+    def __init__(self, remote=False, backend={}):
+        super().__init__()
+        self.remote = remote
+        self.backend = backend
+
+    def load_calendar(self, freq, future):
+        """Load original calendar timestamp from file.
+
+        Parameters
+        ----------
+        freq : str
+            frequency of read calendar file.
+        future: bool
+        Returns
+        ----------
+        list
+            list of timestamps
+        """
+        try:
+            backend_obj = self.backend_obj(freq=freq, future=future).data
+        except ValueError:
+            if future:
+                get_module_logger("data").warning(
+                    f"load calendar error: freq={freq}, future={future}; return current calendar!"
+                )
+                get_module_logger("data").warning(
+                    "You can get future calendar by referring to the following document: https://github.com/microsoft/qlib/blob/main/scripts/data_collector/contrib/README.md"
+                )
+                backend_obj = self.backend_obj(freq=freq, future=False).data
+            else:
+                raise
+
+        return [pd.Timestamp(x) for x in backend_obj]
+
+
+class LocalInstrumentProvider(InstrumentProvider, ProviderBackendMixin):
+    """Local instrument data provider class
+
+    Provide instrument data from local data source.
+    """
+
+    def __init__(self, backend={}) -> None:
+        super().__init__()
+        self.backend = backend
+
+    def _load_instruments(self, market, freq):
+        return self.backend_obj(market=market, freq=freq).data
+
+    def list_instruments(self, instruments, start_time=None, end_time=None, freq="day", as_list=False):
+        market = instruments["market"]
+        if market in H["i"]:
+            _instruments = H["i"][market]
+        else:
+            _instruments = self._load_instruments(market, freq=freq)
+            H["i"][market] = _instruments
+        # strip
+        # use calendar boundary
+        cal = Cal.calendar(freq=freq)
+        start_time = pd.Timestamp(start_time or cal[0])
+        end_time = pd.Timestamp(end_time or cal[-1])
+        _instruments_filtered = {
+            inst: list(
+                filter(
+                    lambda x: x[0] <= x[1],
+                    [(max(start_time, pd.Timestamp(x[0])), min(end_time, pd.Timestamp(x[1]))) for x in spans],
+                )
+            )
+            for inst, spans in _instruments.items()
+        }
+        _instruments_filtered = {key: value for key, value in _instruments_filtered.items() if value}
+        # filter
+        filter_pipe = instruments["filter_pipe"]
+        for filter_config in filter_pipe:
+            from . import filter as F  # pylint: disable=C0415
+
+            filter_t = getattr(F, filter_config["filter_type"]).from_config(filter_config)
+            _instruments_filtered = filter_t(_instruments_filtered, start_time, end_time, freq)
+        # as list
+        if as_list:
+            return list(_instruments_filtered)
+        return _instruments_filtered
+
+
+class LocalFeatureProvider(FeatureProvider, ProviderBackendMixin):
+    """Local feature data provider class
+
+    Provide feature data from local data source.
+    """
+
+    def __init__(self, remote=False, backend={}):
+        super().__init__()
+        self.remote = remote
+        self.backend = backend
+
+    def feature(self, instrument, field, start_index, end_index, freq):
+        # validate
+        field = str(field)[1:]
+        instrument = code_to_fname(instrument)
+        return self.backend_obj(instrument=instrument, field=field, freq=freq)[start_index : end_index + 1]
+
+
+class LocalPITProvider(PITProvider):
+    # TODO: Add PIT backend file storage
+    # NOTE: This class is not multi-threading-safe!!!!
+
+    def period_feature(self, instrument, field, start_index, end_index, cur_time, period=None):
+        if not isinstance(cur_time, pd.Timestamp):
+            raise ValueError(
+                f"Expected pd.Timestamp for `cur_time`, got '{cur_time}'. Advices: you can't query PIT data directly(e.g. '$$roewa_q'), you must use `P` operator to convert data to each day (e.g. 'P($$roewa_q)')"
+            )
+
+        assert end_index <= 0  # PIT don't support querying future data
+
+        DATA_RECORDS = [
+            ("date", C.pit_record_type["date"]),
+            ("period", C.pit_record_type["period"]),
+            ("value", C.pit_record_type["value"]),
+            ("_next", C.pit_record_type["index"]),
+        ]
+        VALUE_DTYPE = C.pit_record_type["value"]
+
+        field = str(field).lower()[2:]
+        instrument = code_to_fname(instrument)
+
+        # {For acceleration
+        # start_index, end_index, cur_index = kwargs["info"]
+        # if cur_index == start_index:
+        #     if not hasattr(self, "all_fields"):
+        #         self.all_fields = []
+        #     self.all_fields.append(field)
+        #     if not hasattr(self, "period_index"):
+        #         self.period_index = {}
+        #     if field not in self.period_index:
+        #         self.period_index[field] = {}
+        # For acceleration}
+
+        if not field.endswith("_q") and not field.endswith("_a"):
+            raise ValueError("period field must ends with '_q' or '_a'")
+        quarterly = field.endswith("_q")
+        index_path = C.dpm.get_data_uri() / "financial" / instrument.lower() / f"{field}.index"
+        data_path = C.dpm.get_data_uri() / "financial" / instrument.lower() / f"{field}.data"
+        if not (index_path.exists() and data_path.exists()):
+            raise FileNotFoundError("No file is found.")
+        # NOTE: The most significant performance loss is here.
+        # Does the acceleration that makes the program complicated really matters?
+        # - It makes parameters of the interface complicate
+        # - It does not performance in the optimal way (places all the pieces together, we may achieve higher performance)
+        #    - If we design it carefully, we can go through for only once to get the historical evolution of the data.
+        # So I decide to deprecated previous implementation and keep the logic of the program simple
+        # Instead, I'll add a cache for the index file.
+        data = np.fromfile(data_path, dtype=DATA_RECORDS)
+
+        # find all revision periods before `cur_time`
+        cur_time_int = int(cur_time.year) * 10000 + int(cur_time.month) * 100 + int(cur_time.day)
+        loc = np.searchsorted(data["date"], cur_time_int, side="right")
+        if loc <= 0:
+            return pd.Series(dtype=C.pit_record_type["value"])
+        last_period = data["period"][:loc].max()  # return the latest quarter
+        first_period = data["period"][:loc].min()
+        period_list = get_period_list(first_period, last_period, quarterly)
+        if period is not None:
+            # NOTE: `period` has higher priority than `start_index` & `end_index`
+            if period not in period_list:
+                return pd.Series(dtype=C.pit_record_type["value"])
+            else:
+                period_list = [period]
+        else:
+            period_list = period_list[max(0, len(period_list) + start_index - 1) : len(period_list) + end_index]
+        value = np.full((len(period_list),), np.nan, dtype=VALUE_DTYPE)
+        for i, p in enumerate(period_list):
+            # last_period_index = self.period_index[field].get(period)  # For acceleration
+            value[i], now_period_index = read_period_data(
+                index_path, data_path, p, cur_time_int, quarterly  # , last_period_index  # For acceleration
+            )
+            # self.period_index[field].update({period: now_period_index})  # For acceleration
+        # NOTE: the index is period_list; So it may result in unexpected values(e.g. nan)
+        # when calculation between different features and only part of its financial indicator is published
+        series = pd.Series(value, index=period_list, dtype=VALUE_DTYPE)
+
+        # {For acceleration
+        # if cur_index == end_index:
+        #     self.all_fields.remove(field)
+        #     if not len(self.all_fields):
+        #         del self.all_fields
+        #         del self.period_index
+        # For acceleration}
+
+        return series
+
+
+class LocalExpressionProvider(ExpressionProvider):
+    """Local expression data provider class
+
+    Provide expression data from local data source.
+    """
+
+    def __init__(self, time2idx=True):
+        super().__init__()
+        self.time2idx = time2idx
+
+    def expression(self, instrument, field, start_time=None, end_time=None, freq="day"):
+        expression = self.get_expression_instance(field)
+        start_time = time_to_slc_point(start_time)
+        end_time = time_to_slc_point(end_time)
+
+        # Two kinds of queries are supported
+        # - Index-based expression: this may save a lot of memory because the datetime index is not saved on the disk
+        # - Data with datetime index expression: this will make it more convenient to integrating with some existing databases
+        if self.time2idx:
+            _, _, start_index, end_index = Cal.locate_index(start_time, end_time, freq=freq, future=False)
+            lft_etd, rght_etd = expression.get_extended_window_size()
+            query_start, query_end = max(0, start_index - lft_etd), end_index + rght_etd
+        else:
+            start_index, end_index = query_start, query_end = start_time, end_time
+
+        try:
+            series = expression.load(instrument, query_start, query_end, freq)
+        except Exception as e:
+            get_module_logger("data").debug(
+                f"Loading expression error: "
+                f"instrument={instrument}, field=({field}), start_time={start_time}, end_time={end_time}, freq={freq}. "
+                f"error info: {str(e)}"
+            )
+            raise
+        # Ensure that each column type is consistent
+        # FIXME:
+        # 1) The stock data is currently float. If there is other types of data, this part needs to be re-implemented.
+        # 2) The precision should be configurable
+        try:
+            series = series.astype(np.float32)
+        except ValueError:
+            pass
+        except TypeError:
+            pass
+        if not series.empty:
+            series = series.loc[start_index:end_index]
+        return series
+
+
+class LocalDatasetProvider(DatasetProvider):
+    """Local dataset data provider class
+
+    Provide dataset data from local data source.
+    """
+
+    def __init__(self, align_time: bool = True):
+        """
+        Parameters
+        ----------
+        align_time : bool
+            Will we align the time to calendar
+            the frequency is flexible in some dataset and can't be aligned.
+            For the data with fixed frequency with a shared calendar, the align data to the calendar will provides following benefits
+
+            - Align queries to the same parameters, so the cache can be shared.
+        """
+        super().__init__()
+        self.align_time = align_time
+
+    def dataset(
+        self,
+        instruments,
+        fields,
+        start_time=None,
+        end_time=None,
+        freq="day",
+        inst_processors=[],
+    ):
+        instruments_d = self.get_instruments_d(instruments, freq)
+        column_names = self.get_column_names(fields)
+        if self.align_time:
+            # NOTE: if the frequency is a fixed value.
+            # align the data to fixed calendar point
+            cal = Cal.calendar(start_time, end_time, freq)
+            if len(cal) == 0:
+                return pd.DataFrame(
+                    index=pd.MultiIndex.from_arrays([[], []], names=("instrument", "datetime")), columns=column_names
+                )
+            start_time = cal[0]
+            end_time = cal[-1]
+        data = self.dataset_processor(
+            instruments_d, column_names, start_time, end_time, freq, inst_processors=inst_processors
+        )
+
+        return data
+
+    @staticmethod
+    def multi_cache_walker(instruments, fields, start_time=None, end_time=None, freq="day"):
+        """
+        This method is used to prepare the expression cache for the client.
+        Then the client will load the data from expression cache by itself.
+
+        """
+        instruments_d = DatasetProvider.get_instruments_d(instruments, freq)
+        column_names = DatasetProvider.get_column_names(fields)
+        cal = Cal.calendar(start_time, end_time, freq)
+        if len(cal) == 0:
+            return
+        start_time = cal[0]
+        end_time = cal[-1]
+        workers = max(min(C.kernels, len(instruments_d)), 1)
+
+        ParallelExt(n_jobs=workers, backend=C.joblib_backend, maxtasksperchild=C.maxtasksperchild)(
+            delayed(LocalDatasetProvider.cache_walker)(inst, start_time, end_time, freq, column_names)
+            for inst in instruments_d
+        )
+
+    @staticmethod
+    def cache_walker(inst, start_time, end_time, freq, column_names):
+        """
+        If the expressions of one instrument haven't been calculated before,
+        calculate it and write it into expression cache.
+
+        """
+        for field in column_names:
+            ExpressionD.expression(inst, field, start_time, end_time, freq)
+
+
+class ClientCalendarProvider(CalendarProvider):
+    """Client calendar data provider class
+
+    Provide calendar data by requesting data from server as a client.
+    """
+
+    def __init__(self):
+        self.conn = None
+        self.queue = queue.Queue()
+
+    def set_conn(self, conn):
+        self.conn = conn
+
+    def calendar(self, start_time=None, end_time=None, freq="day", future=False):
+        self.conn.send_request(
+            request_type="calendar",
+            request_content={"start_time": str(start_time), "end_time": str(end_time), "freq": freq, "future": future},
+            msg_queue=self.queue,
+            msg_proc_func=lambda response_content: [pd.Timestamp(c) for c in response_content],
+        )
+        result = self.queue.get(timeout=C["timeout"])
+        return result
+
+
+class ClientInstrumentProvider(InstrumentProvider):
+    """Client instrument data provider class
+
+    Provide instrument data by requesting data from server as a client.
+    """
+
+    def __init__(self):
+        self.conn = None
+        self.queue = queue.Queue()
+
+    def set_conn(self, conn):
+        self.conn = conn
+
+    def list_instruments(self, instruments, start_time=None, end_time=None, freq="day", as_list=False):
+        def inst_msg_proc_func(response_content):
+            if isinstance(response_content, dict):
+                instrument = {
+                    i: [(pd.Timestamp(s), pd.Timestamp(e)) for s, e in t] for i, t in response_content.items()
+                }
+            else:
+                instrument = response_content
+            return instrument
+
+        self.conn.send_request(
+            request_type="instrument",
+            request_content={
+                "instruments": instruments,
+                "start_time": str(start_time),
+                "end_time": str(end_time),
+                "freq": freq,
+                "as_list": as_list,
+            },
+            msg_queue=self.queue,
+            msg_proc_func=inst_msg_proc_func,
+        )
+        result = self.queue.get(timeout=C["timeout"])
+        if isinstance(result, Exception):
+            raise result
+        get_module_logger("data").debug("get result")
+        return result
+
+
+class ClientDatasetProvider(DatasetProvider):
+    """Client dataset data provider class
+
+    Provide dataset data by requesting data from server as a client.
+    """
+
+    def __init__(self):
+        self.conn = None
+
+    def set_conn(self, conn):
+        self.conn = conn
+        self.queue = queue.Queue()
+
+    def dataset(
+        self,
+        instruments,
+        fields,
+        start_time=None,
+        end_time=None,
+        freq="day",
+        disk_cache=0,
+        return_uri=False,
+        inst_processors=[],
+    ):
+        if Inst.get_inst_type(instruments) == Inst.DICT:
+            get_module_logger("data").warning(
+                "Getting features from a dict of instruments is not recommended because the features will not be "
+                "cached! "
+                "The dict of instruments will be cleaned every day."
+            )
+
+        if disk_cache == 0:
+            """
+            Call the server to generate the expression cache.
+            Then load the data from the expression cache directly.
+            - default using multi-kernel method.
+
+            """
+            self.conn.send_request(
+                request_type="feature",
+                request_content={
+                    "instruments": instruments,
+                    "fields": fields,
+                    "start_time": start_time,
+                    "end_time": end_time,
+                    "freq": freq,
+                    "disk_cache": 0,
+                },
+                msg_queue=self.queue,
+            )
+            feature_uri = self.queue.get(timeout=C["timeout"])
+            if isinstance(feature_uri, Exception):
+                raise feature_uri
+            else:
+                instruments_d = self.get_instruments_d(instruments, freq)
+                column_names = self.get_column_names(fields)
+                cal = Cal.calendar(start_time, end_time, freq)
+                if len(cal) == 0:
+                    return pd.DataFrame(
+                        index=pd.MultiIndex.from_arrays([[], []], names=("instrument", "datetime")),
+                        columns=column_names,
+                    )
+                start_time = cal[0]
+                end_time = cal[-1]
+
+                data = self.dataset_processor(instruments_d, column_names, start_time, end_time, freq, inst_processors)
+                if return_uri:
+                    return data, feature_uri
+                else:
+                    return data
+        else:
+            """
+            Call the server to generate the data-set cache, get the uri of the cache file.
+            Then load the data from the file on NFS directly.
+            - using single-process implementation.
+
+            """
+            # TODO: support inst_processors, need to change the code of qlib-server at the same time
+            # FIXME: The cache after resample, when read again and intercepted with end_time, results in incomplete data date
+            if inst_processors:
+                raise ValueError(
+                    f"{self.__class__.__name__} does not support inst_processor. "
+                    f"Please use `D.features(disk_cache=0)` or `qlib.init(dataset_cache=None)`"
+                )
+            self.conn.send_request(
+                request_type="feature",
+                request_content={
+                    "instruments": instruments,
+                    "fields": fields,
+                    "start_time": start_time,
+                    "end_time": end_time,
+                    "freq": freq,
+                    "disk_cache": 1,
+                },
+                msg_queue=self.queue,
+            )
+            # - Done in callback
+            feature_uri = self.queue.get(timeout=C["timeout"])
+            if isinstance(feature_uri, Exception):
+                raise feature_uri
+            get_module_logger("data").debug("get result")
+            try:
+                # pre-mound nfs, used for demo
+                mnt_feature_uri = C.dpm.get_data_uri(freq).joinpath(C.dataset_cache_dir_name, feature_uri)
+                df = DiskDatasetCache.read_data_from_cache(mnt_feature_uri, start_time, end_time, fields)
+                get_module_logger("data").debug("finish slicing data")
+                if return_uri:
+                    return df, feature_uri
+                return df
+            except AttributeError as attribute_e:
+                raise IOError("Unable to fetch instruments from remote server!") from attribute_e
+
+
+class BaseProvider:
+    """Local provider class
+    It is a set of interface that allow users to access data.
+    Because PITD is not exposed publicly to users, so it is not included in the interface.
+
+    To keep compatible with old qlib provider.
+    """
+
+    def calendar(self, start_time=None, end_time=None, freq="day", future=False):
+        return Cal.calendar(start_time, end_time, freq, future=future)
+
+    def instruments(self, market="all", filter_pipe=None, start_time=None, end_time=None):
+        if start_time is not None or end_time is not None:
+            get_module_logger("Provider").warning(
+                "The instruments corresponds to a stock pool. "
+                "Parameters `start_time` and `end_time` does not take effect now."
+            )
+        return InstrumentProvider.instruments(market, filter_pipe)
+
+    def list_instruments(self, instruments, start_time=None, end_time=None, freq="day", as_list=False):
+        return Inst.list_instruments(instruments, start_time, end_time, freq, as_list)
+
+    def features(
+        self,
+        instruments,
+        fields,
+        start_time=None,
+        end_time=None,
+        freq="day",
+        disk_cache=None,
+        inst_processors=[],
+    ):
+        """
+        Parameters
+        ----------
+        disk_cache : int
+            whether to skip(0)/use(1)/replace(2) disk_cache
+
+
+        This function will try to use cache method which has a keyword `disk_cache`,
+        and will use provider method if a type error is raised because the DatasetD instance
+        is a provider class.
+        """
+        disk_cache = C.default_disk_cache if disk_cache is None else disk_cache
+        fields = list(fields)  # In case of tuple.
+        try:
+            return DatasetD.dataset(
+                instruments, fields, start_time, end_time, freq, disk_cache, inst_processors=inst_processors
+            )
+        except TypeError:
+            return DatasetD.dataset(instruments, fields, start_time, end_time, freq, inst_processors=inst_processors)
+
+
+class LocalProvider(BaseProvider):
+    def _uri(self, type, **kwargs):
+        """_uri
+        The server hope to get the uri of the request. The uri will be decided
+        by the dataprovider. For ex, different cache layer has different uri.
+
+        :param type: The type of resource for the uri
+        :param **kwargs:
+        """
+        if type == "calendar":
+            return Cal._uri(**kwargs)
+        elif type == "instrument":
+            return Inst._uri(**kwargs)
+        elif type == "feature":
+            return DatasetD._uri(**kwargs)
+
+    def features_uri(self, instruments, fields, start_time, end_time, freq, disk_cache=1):
+        """features_uri
+
+        Return the uri of the generated cache of features/dataset
+
+        :param disk_cache:
+        :param instruments:
+        :param fields:
+        :param start_time:
+        :param end_time:
+        :param freq:
+        """
+        return DatasetD._dataset_uri(instruments, fields, start_time, end_time, freq, disk_cache)
+
+
+class ClientProvider(BaseProvider):
+    """Client Provider
+
+    Requesting data from server as a client. Can propose requests:
+
+        - Calendar : Directly respond a list of calendars
+        - Instruments (without filter): Directly respond a list/dict of instruments
+        - Instruments (with filters):  Respond a list/dict of instruments
+        - Features : Respond a cache uri
+
+    The general workflow is described as follows:
+    When the user use client provider to propose a request, the client provider will connect the server and send the request. The client will start to wait for the response. The response will be made instantly indicating whether the cache is available. The waiting procedure will terminate only when the client get the response saying `feature_available` is true.
+    `BUG` : Everytime we make request for certain data we need to connect to the server, wait for the response and disconnect from it. We can't make a sequence of requests within one connection. You can refer to https://python-socketio.readthedocs.io/en/latest/client.html for documentation of python-socketIO client.
+    """
+
+    def __init__(self):
+        def is_instance_of_provider(instance: object, cls: type):
+            if isinstance(instance, Wrapper):
+                p = getattr(instance, "_provider", None)
+
+                return False if p is None else isinstance(p, cls)
+
+            return isinstance(instance, cls)
+
+        from .client import Client  # pylint: disable=C0415
+
+        self.client = Client(C.flask_server, C.flask_port)
+        self.logger = get_module_logger(self.__class__.__name__)
+        if is_instance_of_provider(Cal, ClientCalendarProvider):
+            Cal.set_conn(self.client)
+        if is_instance_of_provider(Inst, ClientInstrumentProvider):
+            Inst.set_conn(self.client)
+        if hasattr(DatasetD, "provider"):
+            DatasetD.provider.set_conn(self.client)
+        else:
+            DatasetD.set_conn(self.client)
+
+
+import sys
+
+if sys.version_info >= (3, 9):
+    from typing import Annotated
+
+    CalendarProviderWrapper = Annotated[CalendarProvider, Wrapper]
+    InstrumentProviderWrapper = Annotated[InstrumentProvider, Wrapper]
+    FeatureProviderWrapper = Annotated[FeatureProvider, Wrapper]
+    PITProviderWrapper = Annotated[PITProvider, Wrapper]
+    ExpressionProviderWrapper = Annotated[ExpressionProvider, Wrapper]
+    DatasetProviderWrapper = Annotated[DatasetProvider, Wrapper]
+    BaseProviderWrapper = Annotated[BaseProvider, Wrapper]
+else:
+    CalendarProviderWrapper = CalendarProvider
+    InstrumentProviderWrapper = InstrumentProvider
+    FeatureProviderWrapper = FeatureProvider
+    PITProviderWrapper = PITProvider
+    ExpressionProviderWrapper = ExpressionProvider
+    DatasetProviderWrapper = DatasetProvider
+    BaseProviderWrapper = BaseProvider
+
+Cal: CalendarProviderWrapper = Wrapper()
+Inst: InstrumentProviderWrapper = Wrapper()
+FeatureD: FeatureProviderWrapper = Wrapper()
+PITD: PITProviderWrapper = Wrapper()
+ExpressionD: ExpressionProviderWrapper = Wrapper()
+DatasetD: DatasetProviderWrapper = Wrapper()
+D: BaseProviderWrapper = Wrapper()
+
+
+def register_all_wrappers(C):
+    """register_all_wrappers"""
+    logger = get_module_logger("data")
+    module = get_module_by_module_path("qlib.data")
+
+    _calendar_provider = init_instance_by_config(C.calendar_provider, module)
+    if getattr(C, "calendar_cache", None) is not None:
+        _calendar_provider = init_instance_by_config(C.calendar_cache, module, provide=_calendar_provider)
+    register_wrapper(Cal, _calendar_provider, "qlib.data")
+    logger.debug(f"registering Cal {C.calendar_provider}-{C.calendar_cache}")
+
+    _instrument_provider = init_instance_by_config(C.instrument_provider, module)
+    register_wrapper(Inst, _instrument_provider, "qlib.data")
+    logger.debug(f"registering Inst {C.instrument_provider}")
+
+    if getattr(C, "feature_provider", None) is not None:
+        feature_provider = init_instance_by_config(C.feature_provider, module)
+        register_wrapper(FeatureD, feature_provider, "qlib.data")
+        logger.debug(f"registering FeatureD {C.feature_provider}")
+
+    if getattr(C, "pit_provider", None) is not None:
+        pit_provider = init_instance_by_config(C.pit_provider, module)
+        register_wrapper(PITD, pit_provider, "qlib.data")
+        logger.debug(f"registering PITD {C.pit_provider}")
+
+    if getattr(C, "expression_provider", None) is not None:
+        # This provider is unnecessary in client provider
+        _eprovider = init_instance_by_config(C.expression_provider, module)
+        if getattr(C, "expression_cache", None) is not None:
+            _eprovider = init_instance_by_config(C.expression_cache, module, provider=_eprovider)
+        register_wrapper(ExpressionD, _eprovider, "qlib.data")
+        logger.debug(f"registering ExpressionD {C.expression_provider}-{C.expression_cache}")
+
+    _dprovider = init_instance_by_config(C.dataset_provider, module)
+    if getattr(C, "dataset_cache", None) is not None:
+        _dprovider = init_instance_by_config(C.dataset_cache, module, provider=_dprovider)
+    register_wrapper(DatasetD, _dprovider, "qlib.data")
+    logger.debug(f"registering DatasetD {C.dataset_provider}-{C.dataset_cache}")
+
+    register_wrapper(D, C.provider, "qlib.data")
+    logger.debug(f"registering D {C.provider}")
```

## qlib/data/filter.py

 * *Ordering differences only*

```diff
@@ -1,374 +1,374 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-from __future__ import print_function
-from abc import abstractmethod
-
-import re
-import pandas as pd
-import numpy as np
-import abc
-
-from .data import Cal, DatasetD
-
-
-class BaseDFilter(abc.ABC):
-    """Dynamic Instruments Filter Abstract class
-
-    Users can override this class to construct their own filter
-
-    Override __init__ to input filter regulations
-
-    Override filter_main to use the regulations to filter instruments
-    """
-
-    def __init__(self):
-        pass
-
-    @staticmethod
-    def from_config(config):
-        """Construct an instance from config dict.
-
-        Parameters
-        ----------
-        config : dict
-            dict of config parameters.
-        """
-        raise NotImplementedError("Subclass of BaseDFilter must reimplement `from_config` method")
-
-    @abstractmethod
-    def to_config(self):
-        """Construct an instance from config dict.
-
-        Returns
-        ----------
-        dict
-            return the dict of config parameters.
-        """
-        raise NotImplementedError("Subclass of BaseDFilter must reimplement `to_config` method")
-
-
-class SeriesDFilter(BaseDFilter):
-    """Dynamic Instruments Filter Abstract class to filter a series of certain features
-
-    Filters should provide parameters:
-
-    - filter start time
-    - filter end time
-    - filter rule
-
-    Override __init__ to assign a certain rule to filter the series.
-
-    Override _getFilterSeries to use the rule to filter the series and get a dict of {inst => series}, or override filter_main for more advanced series filter rule
-    """
-
-    def __init__(self, fstart_time=None, fend_time=None, keep=False):
-        """Init function for filter base class.
-            Filter a set of instruments based on a certain rule within a certain period assigned by fstart_time and fend_time.
-
-        Parameters
-        ----------
-        fstart_time: str
-            the time for the filter rule to start filter the instruments.
-        fend_time: str
-            the time for the filter rule to stop filter the instruments.
-        keep: bool
-            whether to keep the instruments of which features don't exist in the filter time span.
-        """
-        super(SeriesDFilter, self).__init__()
-        self.filter_start_time = pd.Timestamp(fstart_time) if fstart_time else None
-        self.filter_end_time = pd.Timestamp(fend_time) if fend_time else None
-        self.keep = keep
-
-    def _getTimeBound(self, instruments):
-        """Get time bound for all instruments.
-
-        Parameters
-        ----------
-        instruments: dict
-            the dict of instruments in the form {instrument_name => list of timestamp tuple}.
-
-        Returns
-        ----------
-        pd.Timestamp, pd.Timestamp
-            the lower time bound and upper time bound of all the instruments.
-        """
-        trange = Cal.calendar(freq=self.filter_freq)
-        ubound, lbound = trange[0], trange[-1]
-        for _, timestamp in instruments.items():
-            if timestamp:
-                lbound = timestamp[0][0] if timestamp[0][0] < lbound else lbound
-                ubound = timestamp[-1][-1] if timestamp[-1][-1] > ubound else ubound
-        return lbound, ubound
-
-    def _toSeries(self, time_range, target_timestamp):
-        """Convert the target timestamp to a pandas series of bool value within a time range.
-            Make the time inside the target_timestamp range TRUE, others FALSE.
-
-        Parameters
-        ----------
-        time_range : D.calendar
-            the time range of the instruments.
-        target_timestamp : list
-            the list of tuple (timestamp, timestamp).
-
-        Returns
-        ----------
-        pd.Series
-            the series of bool value for an instrument.
-        """
-        # Construct a whole dict of {date => bool}
-        timestamp_series = {timestamp: False for timestamp in time_range}
-        # Convert to pd.Series
-        timestamp_series = pd.Series(timestamp_series)
-        # Fill the date within target_timestamp with TRUE
-        for start, end in target_timestamp:
-            timestamp_series[Cal.calendar(start_time=start, end_time=end, freq=self.filter_freq)] = True
-        return timestamp_series
-
-    def _filterSeries(self, timestamp_series, filter_series):
-        """Filter the timestamp series with filter series by using element-wise AND operation of the two series.
-
-        Parameters
-        ----------
-        timestamp_series : pd.Series
-            the series of bool value indicating existing time.
-        filter_series : pd.Series
-            the series of bool value indicating filter feature.
-
-        Returns
-        ----------
-        pd.Series
-            the series of bool value indicating whether the date satisfies the filter condition and exists in target timestamp.
-        """
-        fstart, fend = list(filter_series.keys())[0], list(filter_series.keys())[-1]
-        filter_series = filter_series.astype("bool")  # Make sure the filter_series is boolean
-        timestamp_series[fstart:fend] = timestamp_series[fstart:fend] & filter_series
-        return timestamp_series
-
-    def _toTimestamp(self, timestamp_series):
-        """Convert the timestamp series to a list of tuple (timestamp, timestamp) indicating a continuous range of TRUE.
-
-        Parameters
-        ----------
-        timestamp_series: pd.Series
-            the series of bool value after being filtered.
-
-        Returns
-        ----------
-        list
-            the list of tuple (timestamp, timestamp).
-        """
-        # sort the timestamp_series according to the timestamps
-        timestamp_series.sort_index()
-        timestamp = []
-        _lbool = None
-        _ltime = None
-        for _ts, _bool in timestamp_series.items():
-            # there is likely to be NAN when the filter series don't have the
-            # bool value, so we just change the NAN into False
-            if _bool == np.nan:
-                _bool = False
-            if _lbool is None:
-                _cur_start = _ts
-                _lbool = _bool
-                _ltime = _ts
-                continue
-            if (_lbool, _bool) == (True, False):
-                if _cur_start:
-                    timestamp.append((_cur_start, _ltime))
-            elif (_lbool, _bool) == (False, True):
-                _cur_start = _ts
-            _lbool = _bool
-            _ltime = _ts
-        if _lbool:
-            timestamp.append((_cur_start, _ltime))
-        return timestamp
-
-    def __call__(self, instruments, start_time=None, end_time=None, freq="day"):
-        """Call this filter to get filtered instruments list"""
-        self.filter_freq = freq
-        return self.filter_main(instruments, start_time, end_time)
-
-    @abstractmethod
-    def _getFilterSeries(self, instruments, fstart, fend):
-        """Get filter series based on the rules assigned during the initialization and the input time range.
-
-        Parameters
-        ----------
-        instruments : dict
-            the dict of instruments to be filtered.
-        fstart : pd.Timestamp
-            start time of filter.
-        fend : pd.Timestamp
-            end time of filter.
-
-        .. note:: fstart/fend indicates the intersection of instruments start/end time and filter start/end time.
-
-        Returns
-        ----------
-        pd.Dataframe
-            a series of {pd.Timestamp => bool}.
-        """
-        raise NotImplementedError("Subclass of SeriesDFilter must reimplement `getFilterSeries` method")
-
-    def filter_main(self, instruments, start_time=None, end_time=None):
-        """Implement this method to filter the instruments.
-
-        Parameters
-        ----------
-        instruments: dict
-            input instruments to be filtered.
-        start_time: str
-            start of the time range.
-        end_time: str
-            end of the time range.
-
-        Returns
-        ----------
-        dict
-            filtered instruments, same structure as input instruments.
-        """
-        lbound, ubound = self._getTimeBound(instruments)
-        start_time = pd.Timestamp(start_time or lbound)
-        end_time = pd.Timestamp(end_time or ubound)
-        _instruments_filtered = {}
-        _all_calendar = Cal.calendar(start_time=start_time, end_time=end_time, freq=self.filter_freq)
-        _filter_calendar = Cal.calendar(
-            start_time=self.filter_start_time and max(self.filter_start_time, _all_calendar[0]) or _all_calendar[0],
-            end_time=self.filter_end_time and min(self.filter_end_time, _all_calendar[-1]) or _all_calendar[-1],
-            freq=self.filter_freq,
-        )
-        _all_filter_series = self._getFilterSeries(instruments, _filter_calendar[0], _filter_calendar[-1])
-        for inst, timestamp in instruments.items():
-            # Construct a whole map of date
-            _timestamp_series = self._toSeries(_all_calendar, timestamp)
-            # Get filter series
-            if inst in _all_filter_series:
-                _filter_series = _all_filter_series[inst]
-            else:
-                if self.keep:
-                    _filter_series = pd.Series({timestamp: True for timestamp in _filter_calendar})
-                else:
-                    _filter_series = pd.Series({timestamp: False for timestamp in _filter_calendar})
-            # Calculate bool value within the range of filter
-            _timestamp_series = self._filterSeries(_timestamp_series, _filter_series)
-            # Reform the map to (start_timestamp, end_timestamp) format
-            _timestamp = self._toTimestamp(_timestamp_series)
-            # Remove empty timestamp
-            if _timestamp:
-                _instruments_filtered[inst] = _timestamp
-        return _instruments_filtered
-
-
-class NameDFilter(SeriesDFilter):
-    """Name dynamic instrument filter
-
-    Filter the instruments based on a regulated name format.
-
-    A name rule regular expression is required.
-    """
-
-    def __init__(self, name_rule_re, fstart_time=None, fend_time=None):
-        """Init function for name filter class
-
-        Parameters
-        ----------
-        name_rule_re: str
-            regular expression for the name rule.
-        """
-        super(NameDFilter, self).__init__(fstart_time, fend_time)
-        self.name_rule_re = name_rule_re
-
-    def _getFilterSeries(self, instruments, fstart, fend):
-        all_filter_series = {}
-        filter_calendar = Cal.calendar(start_time=fstart, end_time=fend, freq=self.filter_freq)
-        for inst, timestamp in instruments.items():
-            if re.match(self.name_rule_re, inst):
-                _filter_series = pd.Series({timestamp: True for timestamp in filter_calendar})
-            else:
-                _filter_series = pd.Series({timestamp: False for timestamp in filter_calendar})
-            all_filter_series[inst] = _filter_series
-        return all_filter_series
-
-    @staticmethod
-    def from_config(config):
-        return NameDFilter(
-            name_rule_re=config["name_rule_re"],
-            fstart_time=config["filter_start_time"],
-            fend_time=config["filter_end_time"],
-        )
-
-    def to_config(self):
-        return {
-            "filter_type": "NameDFilter",
-            "name_rule_re": self.name_rule_re,
-            "filter_start_time": str(self.filter_start_time) if self.filter_start_time else self.filter_start_time,
-            "filter_end_time": str(self.filter_end_time) if self.filter_end_time else self.filter_end_time,
-        }
-
-
-class ExpressionDFilter(SeriesDFilter):
-    """Expression dynamic instrument filter
-
-    Filter the instruments based on a certain expression.
-
-    An expression rule indicating a certain feature field is required.
-
-    Examples
-    ----------
-    - *basic features filter* : rule_expression = '$close/$open>5'
-    - *cross-sectional features filter* : rule_expression = '$rank($close)<10'
-    - *time-sequence features filter* : rule_expression = '$Ref($close, 3)>100'
-    """
-
-    def __init__(self, rule_expression, fstart_time=None, fend_time=None, keep=False):
-        """Init function for expression filter class
-
-        Parameters
-        ----------
-        fstart_time: str
-            filter the feature starting from this time.
-        fend_time: str
-            filter the feature ending by this time.
-        rule_expression: str
-            an input expression for the rule.
-        """
-        super(ExpressionDFilter, self).__init__(fstart_time, fend_time, keep=keep)
-        self.rule_expression = rule_expression
-
-    def _getFilterSeries(self, instruments, fstart, fend):
-        # do not use dataset cache
-        try:
-            _features = DatasetD.dataset(
-                instruments,
-                [self.rule_expression],
-                fstart,
-                fend,
-                freq=self.filter_freq,
-                disk_cache=0,
-            )
-        except TypeError:
-            # use LocalDatasetProvider
-            _features = DatasetD.dataset(instruments, [self.rule_expression], fstart, fend, freq=self.filter_freq)
-        rule_expression_field_name = list(_features.keys())[0]
-        all_filter_series = _features[rule_expression_field_name]
-        return all_filter_series
-
-    @staticmethod
-    def from_config(config):
-        return ExpressionDFilter(
-            rule_expression=config["rule_expression"],
-            fstart_time=config["filter_start_time"],
-            fend_time=config["filter_end_time"],
-            keep=config["keep"],
-        )
-
-    def to_config(self):
-        return {
-            "filter_type": "ExpressionDFilter",
-            "rule_expression": self.rule_expression,
-            "filter_start_time": str(self.filter_start_time) if self.filter_start_time else self.filter_start_time,
-            "filter_end_time": str(self.filter_end_time) if self.filter_end_time else self.filter_end_time,
-            "keep": self.keep,
-        }
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+from __future__ import print_function
+from abc import abstractmethod
+
+import re
+import pandas as pd
+import numpy as np
+import abc
+
+from .data import Cal, DatasetD
+
+
+class BaseDFilter(abc.ABC):
+    """Dynamic Instruments Filter Abstract class
+
+    Users can override this class to construct their own filter
+
+    Override __init__ to input filter regulations
+
+    Override filter_main to use the regulations to filter instruments
+    """
+
+    def __init__(self):
+        pass
+
+    @staticmethod
+    def from_config(config):
+        """Construct an instance from config dict.
+
+        Parameters
+        ----------
+        config : dict
+            dict of config parameters.
+        """
+        raise NotImplementedError("Subclass of BaseDFilter must reimplement `from_config` method")
+
+    @abstractmethod
+    def to_config(self):
+        """Construct an instance from config dict.
+
+        Returns
+        ----------
+        dict
+            return the dict of config parameters.
+        """
+        raise NotImplementedError("Subclass of BaseDFilter must reimplement `to_config` method")
+
+
+class SeriesDFilter(BaseDFilter):
+    """Dynamic Instruments Filter Abstract class to filter a series of certain features
+
+    Filters should provide parameters:
+
+    - filter start time
+    - filter end time
+    - filter rule
+
+    Override __init__ to assign a certain rule to filter the series.
+
+    Override _getFilterSeries to use the rule to filter the series and get a dict of {inst => series}, or override filter_main for more advanced series filter rule
+    """
+
+    def __init__(self, fstart_time=None, fend_time=None, keep=False):
+        """Init function for filter base class.
+            Filter a set of instruments based on a certain rule within a certain period assigned by fstart_time and fend_time.
+
+        Parameters
+        ----------
+        fstart_time: str
+            the time for the filter rule to start filter the instruments.
+        fend_time: str
+            the time for the filter rule to stop filter the instruments.
+        keep: bool
+            whether to keep the instruments of which features don't exist in the filter time span.
+        """
+        super(SeriesDFilter, self).__init__()
+        self.filter_start_time = pd.Timestamp(fstart_time) if fstart_time else None
+        self.filter_end_time = pd.Timestamp(fend_time) if fend_time else None
+        self.keep = keep
+
+    def _getTimeBound(self, instruments):
+        """Get time bound for all instruments.
+
+        Parameters
+        ----------
+        instruments: dict
+            the dict of instruments in the form {instrument_name => list of timestamp tuple}.
+
+        Returns
+        ----------
+        pd.Timestamp, pd.Timestamp
+            the lower time bound and upper time bound of all the instruments.
+        """
+        trange = Cal.calendar(freq=self.filter_freq)
+        ubound, lbound = trange[0], trange[-1]
+        for _, timestamp in instruments.items():
+            if timestamp:
+                lbound = timestamp[0][0] if timestamp[0][0] < lbound else lbound
+                ubound = timestamp[-1][-1] if timestamp[-1][-1] > ubound else ubound
+        return lbound, ubound
+
+    def _toSeries(self, time_range, target_timestamp):
+        """Convert the target timestamp to a pandas series of bool value within a time range.
+            Make the time inside the target_timestamp range TRUE, others FALSE.
+
+        Parameters
+        ----------
+        time_range : D.calendar
+            the time range of the instruments.
+        target_timestamp : list
+            the list of tuple (timestamp, timestamp).
+
+        Returns
+        ----------
+        pd.Series
+            the series of bool value for an instrument.
+        """
+        # Construct a whole dict of {date => bool}
+        timestamp_series = {timestamp: False for timestamp in time_range}
+        # Convert to pd.Series
+        timestamp_series = pd.Series(timestamp_series)
+        # Fill the date within target_timestamp with TRUE
+        for start, end in target_timestamp:
+            timestamp_series[Cal.calendar(start_time=start, end_time=end, freq=self.filter_freq)] = True
+        return timestamp_series
+
+    def _filterSeries(self, timestamp_series, filter_series):
+        """Filter the timestamp series with filter series by using element-wise AND operation of the two series.
+
+        Parameters
+        ----------
+        timestamp_series : pd.Series
+            the series of bool value indicating existing time.
+        filter_series : pd.Series
+            the series of bool value indicating filter feature.
+
+        Returns
+        ----------
+        pd.Series
+            the series of bool value indicating whether the date satisfies the filter condition and exists in target timestamp.
+        """
+        fstart, fend = list(filter_series.keys())[0], list(filter_series.keys())[-1]
+        filter_series = filter_series.astype("bool")  # Make sure the filter_series is boolean
+        timestamp_series[fstart:fend] = timestamp_series[fstart:fend] & filter_series
+        return timestamp_series
+
+    def _toTimestamp(self, timestamp_series):
+        """Convert the timestamp series to a list of tuple (timestamp, timestamp) indicating a continuous range of TRUE.
+
+        Parameters
+        ----------
+        timestamp_series: pd.Series
+            the series of bool value after being filtered.
+
+        Returns
+        ----------
+        list
+            the list of tuple (timestamp, timestamp).
+        """
+        # sort the timestamp_series according to the timestamps
+        timestamp_series.sort_index()
+        timestamp = []
+        _lbool = None
+        _ltime = None
+        for _ts, _bool in timestamp_series.items():
+            # there is likely to be NAN when the filter series don't have the
+            # bool value, so we just change the NAN into False
+            if _bool == np.nan:
+                _bool = False
+            if _lbool is None:
+                _cur_start = _ts
+                _lbool = _bool
+                _ltime = _ts
+                continue
+            if (_lbool, _bool) == (True, False):
+                if _cur_start:
+                    timestamp.append((_cur_start, _ltime))
+            elif (_lbool, _bool) == (False, True):
+                _cur_start = _ts
+            _lbool = _bool
+            _ltime = _ts
+        if _lbool:
+            timestamp.append((_cur_start, _ltime))
+        return timestamp
+
+    def __call__(self, instruments, start_time=None, end_time=None, freq="day"):
+        """Call this filter to get filtered instruments list"""
+        self.filter_freq = freq
+        return self.filter_main(instruments, start_time, end_time)
+
+    @abstractmethod
+    def _getFilterSeries(self, instruments, fstart, fend):
+        """Get filter series based on the rules assigned during the initialization and the input time range.
+
+        Parameters
+        ----------
+        instruments : dict
+            the dict of instruments to be filtered.
+        fstart : pd.Timestamp
+            start time of filter.
+        fend : pd.Timestamp
+            end time of filter.
+
+        .. note:: fstart/fend indicates the intersection of instruments start/end time and filter start/end time.
+
+        Returns
+        ----------
+        pd.Dataframe
+            a series of {pd.Timestamp => bool}.
+        """
+        raise NotImplementedError("Subclass of SeriesDFilter must reimplement `getFilterSeries` method")
+
+    def filter_main(self, instruments, start_time=None, end_time=None):
+        """Implement this method to filter the instruments.
+
+        Parameters
+        ----------
+        instruments: dict
+            input instruments to be filtered.
+        start_time: str
+            start of the time range.
+        end_time: str
+            end of the time range.
+
+        Returns
+        ----------
+        dict
+            filtered instruments, same structure as input instruments.
+        """
+        lbound, ubound = self._getTimeBound(instruments)
+        start_time = pd.Timestamp(start_time or lbound)
+        end_time = pd.Timestamp(end_time or ubound)
+        _instruments_filtered = {}
+        _all_calendar = Cal.calendar(start_time=start_time, end_time=end_time, freq=self.filter_freq)
+        _filter_calendar = Cal.calendar(
+            start_time=self.filter_start_time and max(self.filter_start_time, _all_calendar[0]) or _all_calendar[0],
+            end_time=self.filter_end_time and min(self.filter_end_time, _all_calendar[-1]) or _all_calendar[-1],
+            freq=self.filter_freq,
+        )
+        _all_filter_series = self._getFilterSeries(instruments, _filter_calendar[0], _filter_calendar[-1])
+        for inst, timestamp in instruments.items():
+            # Construct a whole map of date
+            _timestamp_series = self._toSeries(_all_calendar, timestamp)
+            # Get filter series
+            if inst in _all_filter_series:
+                _filter_series = _all_filter_series[inst]
+            else:
+                if self.keep:
+                    _filter_series = pd.Series({timestamp: True for timestamp in _filter_calendar})
+                else:
+                    _filter_series = pd.Series({timestamp: False for timestamp in _filter_calendar})
+            # Calculate bool value within the range of filter
+            _timestamp_series = self._filterSeries(_timestamp_series, _filter_series)
+            # Reform the map to (start_timestamp, end_timestamp) format
+            _timestamp = self._toTimestamp(_timestamp_series)
+            # Remove empty timestamp
+            if _timestamp:
+                _instruments_filtered[inst] = _timestamp
+        return _instruments_filtered
+
+
+class NameDFilter(SeriesDFilter):
+    """Name dynamic instrument filter
+
+    Filter the instruments based on a regulated name format.
+
+    A name rule regular expression is required.
+    """
+
+    def __init__(self, name_rule_re, fstart_time=None, fend_time=None):
+        """Init function for name filter class
+
+        Parameters
+        ----------
+        name_rule_re: str
+            regular expression for the name rule.
+        """
+        super(NameDFilter, self).__init__(fstart_time, fend_time)
+        self.name_rule_re = name_rule_re
+
+    def _getFilterSeries(self, instruments, fstart, fend):
+        all_filter_series = {}
+        filter_calendar = Cal.calendar(start_time=fstart, end_time=fend, freq=self.filter_freq)
+        for inst, timestamp in instruments.items():
+            if re.match(self.name_rule_re, inst):
+                _filter_series = pd.Series({timestamp: True for timestamp in filter_calendar})
+            else:
+                _filter_series = pd.Series({timestamp: False for timestamp in filter_calendar})
+            all_filter_series[inst] = _filter_series
+        return all_filter_series
+
+    @staticmethod
+    def from_config(config):
+        return NameDFilter(
+            name_rule_re=config["name_rule_re"],
+            fstart_time=config["filter_start_time"],
+            fend_time=config["filter_end_time"],
+        )
+
+    def to_config(self):
+        return {
+            "filter_type": "NameDFilter",
+            "name_rule_re": self.name_rule_re,
+            "filter_start_time": str(self.filter_start_time) if self.filter_start_time else self.filter_start_time,
+            "filter_end_time": str(self.filter_end_time) if self.filter_end_time else self.filter_end_time,
+        }
+
+
+class ExpressionDFilter(SeriesDFilter):
+    """Expression dynamic instrument filter
+
+    Filter the instruments based on a certain expression.
+
+    An expression rule indicating a certain feature field is required.
+
+    Examples
+    ----------
+    - *basic features filter* : rule_expression = '$close/$open>5'
+    - *cross-sectional features filter* : rule_expression = '$rank($close)<10'
+    - *time-sequence features filter* : rule_expression = '$Ref($close, 3)>100'
+    """
+
+    def __init__(self, rule_expression, fstart_time=None, fend_time=None, keep=False):
+        """Init function for expression filter class
+
+        Parameters
+        ----------
+        fstart_time: str
+            filter the feature starting from this time.
+        fend_time: str
+            filter the feature ending by this time.
+        rule_expression: str
+            an input expression for the rule.
+        """
+        super(ExpressionDFilter, self).__init__(fstart_time, fend_time, keep=keep)
+        self.rule_expression = rule_expression
+
+    def _getFilterSeries(self, instruments, fstart, fend):
+        # do not use dataset cache
+        try:
+            _features = DatasetD.dataset(
+                instruments,
+                [self.rule_expression],
+                fstart,
+                fend,
+                freq=self.filter_freq,
+                disk_cache=0,
+            )
+        except TypeError:
+            # use LocalDatasetProvider
+            _features = DatasetD.dataset(instruments, [self.rule_expression], fstart, fend, freq=self.filter_freq)
+        rule_expression_field_name = list(_features.keys())[0]
+        all_filter_series = _features[rule_expression_field_name]
+        return all_filter_series
+
+    @staticmethod
+    def from_config(config):
+        return ExpressionDFilter(
+            rule_expression=config["rule_expression"],
+            fstart_time=config["filter_start_time"],
+            fend_time=config["filter_end_time"],
+            keep=config["keep"],
+        )
+
+    def to_config(self):
+        return {
+            "filter_type": "ExpressionDFilter",
+            "rule_expression": self.rule_expression,
+            "filter_start_time": str(self.filter_start_time) if self.filter_start_time else self.filter_start_time,
+            "filter_end_time": str(self.filter_end_time) if self.filter_end_time else self.filter_end_time,
+            "keep": self.keep,
+        }
```

## qlib/data/inst_processor.py

 * *Ordering differences only*

```diff
@@ -1,22 +1,22 @@
-import abc
-import json
-import pandas as pd
-
-
-class InstProcessor:
-    @abc.abstractmethod
-    def __call__(self, df: pd.DataFrame, instrument, *args, **kwargs):
-        """
-        process the data
-
-        NOTE: **The processor could change the content of `df` inplace !!!!! **
-        User should keep a copy of data outside
-
-        Parameters
-        ----------
-        df : pd.DataFrame
-            The raw_df of handler or result from previous processor.
-        """
-
-    def __str__(self):
-        return f"{self.__class__.__name__}:{json.dumps(self.__dict__, sort_keys=True, default=str)}"
+import abc
+import json
+import pandas as pd
+
+
+class InstProcessor:
+    @abc.abstractmethod
+    def __call__(self, df: pd.DataFrame, instrument, *args, **kwargs):
+        """
+        process the data
+
+        NOTE: **The processor could change the content of `df` inplace !!!!! **
+        User should keep a copy of data outside
+
+        Parameters
+        ----------
+        df : pd.DataFrame
+            The raw_df of handler or result from previous processor.
+        """
+
+    def __str__(self):
+        return f"{self.__class__.__name__}:{json.dumps(self.__dict__, sort_keys=True, default=str)}"
```

## qlib/data/ops.py

 * *Ordering differences only*

```diff
@@ -1,1681 +1,1681 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-
-from __future__ import division
-from __future__ import print_function
-
-import numpy as np
-import pandas as pd
-
-from typing import Union, List, Type
-from scipy.stats import percentileofscore
-from .base import Expression, ExpressionOps, Feature, PFeature
-from ..log import get_module_logger
-from ..utils import get_callable_kwargs
-
-try:
-    from ._libs.rolling import rolling_slope, rolling_rsquare, rolling_resi
-    from ._libs.expanding import expanding_slope, expanding_rsquare, expanding_resi
-except ImportError:
-    print(
-        "#### Do not import qlib package in the repository directory in case of importing qlib from . without compiling #####"
-    )
-    raise
-except ValueError:
-    print("!!!!!!!! A error occurs when importing operators implemented based on Cython.!!!!!!!!")
-    print("!!!!!!!! They will be disabled. Please Upgrade your numpy to enable them     !!!!!!!!")
-    # We catch this error because some platform can't upgrade there package (e.g. Kaggle)
-    # https://www.kaggle.com/general/293387
-    # https://www.kaggle.com/product-feedback/98562
-
-
-np.seterr(invalid="ignore")
-
-
-#################### Element-Wise Operator ####################
-class ElemOperator(ExpressionOps):
-    """Element-wise Operator
-
-    Parameters
-    ----------
-    feature : Expression
-        feature instance
-
-    Returns
-    ----------
-    Expression
-        feature operation output
-    """
-
-    def __init__(self, feature):
-        self.feature = feature
-
-    def __str__(self):
-        return "{}({})".format(type(self).__name__, self.feature)
-
-    def get_longest_back_rolling(self):
-        return self.feature.get_longest_back_rolling()
-
-    def get_extended_window_size(self):
-        return self.feature.get_extended_window_size()
-
-
-class ChangeInstrument(ElemOperator):
-    """Change Instrument Operator
-    In some case, one may want to change to another instrument when calculating, for example, to
-    calculate beta of a stock with respect to a market index.
-    This would require changing the calculation of features from the stock (original instrument) to
-    the index (reference instrument)
-    Parameters
-    ----------
-    instrument: new instrument for which the downstream operations should be performed upon.
-                i.e., SH000300 (CSI300 index), or ^GPSC (SP500 index).
-
-    feature: the feature to be calculated for the new instrument.
-    Returns
-    ----------
-    Expression
-        feature operation output
-    """
-
-    def __init__(self, instrument, feature):
-        self.instrument = instrument
-        self.feature = feature
-
-    def __str__(self):
-        return "{}('{}',{})".format(type(self).__name__, self.instrument, self.feature)
-
-    def load(self, instrument, start_index, end_index, *args):
-        # the first `instrument` is ignored
-        return super().load(self.instrument, start_index, end_index, *args)
-
-    def _load_internal(self, instrument, start_index, end_index, *args):
-        return self.feature.load(instrument, start_index, end_index, *args)
-
-
-class NpElemOperator(ElemOperator):
-    """Numpy Element-wise Operator
-
-    Parameters
-    ----------
-    feature : Expression
-        feature instance
-    func : str
-        numpy feature operation method
-
-    Returns
-    ----------
-    Expression
-        feature operation output
-    """
-
-    def __init__(self, feature, func):
-        self.func = func
-        super(NpElemOperator, self).__init__(feature)
-
-    def _load_internal(self, instrument, start_index, end_index, *args):
-        series = self.feature.load(instrument, start_index, end_index, *args)
-        return getattr(np, self.func)(series)
-
-
-class Abs(NpElemOperator):
-    """Feature Absolute Value
-
-    Parameters
-    ----------
-    feature : Expression
-        feature instance
-
-    Returns
-    ----------
-    Expression
-        a feature instance with absolute output
-    """
-
-    def __init__(self, feature):
-        super(Abs, self).__init__(feature, "abs")
-
-
-class Sign(NpElemOperator):
-    """Feature Sign
-
-    Parameters
-    ----------
-    feature : Expression
-        feature instance
-
-    Returns
-    ----------
-    Expression
-        a feature instance with sign
-    """
-
-    def __init__(self, feature):
-        super(Sign, self).__init__(feature, "sign")
-
-    def _load_internal(self, instrument, start_index, end_index, *args):
-        """
-        To avoid error raised by bool type input, we transform the data into float32.
-        """
-        series = self.feature.load(instrument, start_index, end_index, *args)
-        # TODO:  More precision types should be configurable
-        series = series.astype(np.float32)
-        return getattr(np, self.func)(series)
-
-
-class Log(NpElemOperator):
-    """Feature Log
-
-    Parameters
-    ----------
-    feature : Expression
-        feature instance
-
-    Returns
-    ----------
-    Expression
-        a feature instance with log
-    """
-
-    def __init__(self, feature):
-        super(Log, self).__init__(feature, "log")
-
-
-class Mask(NpElemOperator):
-    """Feature Mask
-
-    Parameters
-    ----------
-    feature : Expression
-        feature instance
-    instrument : str
-        instrument mask
-
-    Returns
-    ----------
-    Expression
-        a feature instance with masked instrument
-    """
-
-    def __init__(self, feature, instrument):
-        super(Mask, self).__init__(feature, "mask")
-        self.instrument = instrument
-
-    def __str__(self):
-        return "{}({},{})".format(type(self).__name__, self.feature, self.instrument.lower())
-
-    def _load_internal(self, instrument, start_index, end_index, *args):
-        return self.feature.load(self.instrument, start_index, end_index, *args)
-
-
-class Not(NpElemOperator):
-    """Not Operator
-
-    Parameters
-    ----------
-    feature : Expression
-        feature instance
-
-    Returns
-    ----------
-    Feature:
-        feature elementwise not output
-    """
-
-    def __init__(self, feature):
-        super(Not, self).__init__(feature, "bitwise_not")
-
-
-#################### Pair-Wise Operator ####################
-class PairOperator(ExpressionOps):
-    """Pair-wise operator
-
-    Parameters
-    ----------
-    feature_left : Expression
-        feature instance or numeric value
-    feature_right : Expression
-        feature instance or numeric value
-
-    Returns
-    ----------
-    Feature:
-        two features' operation output
-    """
-
-    def __init__(self, feature_left, feature_right):
-        self.feature_left = feature_left
-        self.feature_right = feature_right
-
-    def __str__(self):
-        return "{}({},{})".format(type(self).__name__, self.feature_left, self.feature_right)
-
-    def get_longest_back_rolling(self):
-        if isinstance(self.feature_left, (Expression,)):
-            left_br = self.feature_left.get_longest_back_rolling()
-        else:
-            left_br = 0
-
-        if isinstance(self.feature_right, (Expression,)):
-            right_br = self.feature_right.get_longest_back_rolling()
-        else:
-            right_br = 0
-        return max(left_br, right_br)
-
-    def get_extended_window_size(self):
-        if isinstance(self.feature_left, (Expression,)):
-            ll, lr = self.feature_left.get_extended_window_size()
-        else:
-            ll, lr = 0, 0
-
-        if isinstance(self.feature_right, (Expression,)):
-            rl, rr = self.feature_right.get_extended_window_size()
-        else:
-            rl, rr = 0, 0
-        return max(ll, rl), max(lr, rr)
-
-
-class NpPairOperator(PairOperator):
-    """Numpy Pair-wise operator
-
-    Parameters
-    ----------
-    feature_left : Expression
-        feature instance or numeric value
-    feature_right : Expression
-        feature instance or numeric value
-    func : str
-        operator function
-
-    Returns
-    ----------
-    Feature:
-        two features' operation output
-    """
-
-    def __init__(self, feature_left, feature_right, func):
-        self.func = func
-        super(NpPairOperator, self).__init__(feature_left, feature_right)
-
-    def _load_internal(self, instrument, start_index, end_index, *args):
-        assert any(
-            [isinstance(self.feature_left, (Expression,)), self.feature_right, Expression]
-        ), "at least one of two inputs is Expression instance"
-        if isinstance(self.feature_left, (Expression,)):
-            series_left = self.feature_left.load(instrument, start_index, end_index, *args)
-        else:
-            series_left = self.feature_left  # numeric value
-        if isinstance(self.feature_right, (Expression,)):
-            series_right = self.feature_right.load(instrument, start_index, end_index, *args)
-        else:
-            series_right = self.feature_right
-        check_length = isinstance(series_left, (np.ndarray, pd.Series)) and isinstance(
-            series_right, (np.ndarray, pd.Series)
-        )
-        if check_length:
-            warning_info = (
-                f"Loading {instrument}: {str(self)}; np.{self.func}(series_left, series_right), "
-                f"The length of series_left and series_right is different: ({len(series_left)}, {len(series_right)}), "
-                f"series_left is {str(self.feature_left)}, series_right is {str(self.feature_right)}. Please check the data"
-            )
-        else:
-            warning_info = (
-                f"Loading {instrument}: {str(self)}; np.{self.func}(series_left, series_right), "
-                f"series_left is {str(self.feature_left)}, series_right is {str(self.feature_right)}. Please check the data"
-            )
-        try:
-            res = getattr(np, self.func)(series_left, series_right)
-        except ValueError as e:
-            get_module_logger("ops").debug(warning_info)
-            raise ValueError(f"{str(e)}. \n\t{warning_info}") from e
-        else:
-            if check_length and len(series_left) != len(series_right):
-                get_module_logger("ops").debug(warning_info)
-        return res
-
-
-class Power(NpPairOperator):
-    """Power Operator
-
-    Parameters
-    ----------
-    feature_left : Expression
-        feature instance
-    feature_right : Expression
-        feature instance
-
-    Returns
-    ----------
-    Feature:
-        The bases in feature_left raised to the exponents in feature_right
-    """
-
-    def __init__(self, feature_left, feature_right):
-        super(Power, self).__init__(feature_left, feature_right, "power")
-
-
-class Add(NpPairOperator):
-    """Add Operator
-
-    Parameters
-    ----------
-    feature_left : Expression
-        feature instance
-    feature_right : Expression
-        feature instance
-
-    Returns
-    ----------
-    Feature:
-        two features' sum
-    """
-
-    def __init__(self, feature_left, feature_right):
-        super(Add, self).__init__(feature_left, feature_right, "add")
-
-
-class Sub(NpPairOperator):
-    """Subtract Operator
-
-    Parameters
-    ----------
-    feature_left : Expression
-        feature instance
-    feature_right : Expression
-        feature instance
-
-    Returns
-    ----------
-    Feature:
-        two features' subtraction
-    """
-
-    def __init__(self, feature_left, feature_right):
-        super(Sub, self).__init__(feature_left, feature_right, "subtract")
-
-
-class Mul(NpPairOperator):
-    """Multiply Operator
-
-    Parameters
-    ----------
-    feature_left : Expression
-        feature instance
-    feature_right : Expression
-        feature instance
-
-    Returns
-    ----------
-    Feature:
-        two features' product
-    """
-
-    def __init__(self, feature_left, feature_right):
-        super(Mul, self).__init__(feature_left, feature_right, "multiply")
-
-
-class Div(NpPairOperator):
-    """Division Operator
-
-    Parameters
-    ----------
-    feature_left : Expression
-        feature instance
-    feature_right : Expression
-        feature instance
-
-    Returns
-    ----------
-    Feature:
-        two features' division
-    """
-
-    def __init__(self, feature_left, feature_right):
-        super(Div, self).__init__(feature_left, feature_right, "divide")
-
-
-class Greater(NpPairOperator):
-    """Greater Operator
-
-    Parameters
-    ----------
-    feature_left : Expression
-        feature instance
-    feature_right : Expression
-        feature instance
-
-    Returns
-    ----------
-    Feature:
-        greater elements taken from the input two features
-    """
-
-    def __init__(self, feature_left, feature_right):
-        super(Greater, self).__init__(feature_left, feature_right, "maximum")
-
-
-class Less(NpPairOperator):
-    """Less Operator
-
-    Parameters
-    ----------
-    feature_left : Expression
-        feature instance
-    feature_right : Expression
-        feature instance
-
-    Returns
-    ----------
-    Feature:
-        smaller elements taken from the input two features
-    """
-
-    def __init__(self, feature_left, feature_right):
-        super(Less, self).__init__(feature_left, feature_right, "minimum")
-
-
-class Gt(NpPairOperator):
-    """Greater Than Operator
-
-    Parameters
-    ----------
-    feature_left : Expression
-        feature instance
-    feature_right : Expression
-        feature instance
-
-    Returns
-    ----------
-    Feature:
-        bool series indicate `left > right`
-    """
-
-    def __init__(self, feature_left, feature_right):
-        super(Gt, self).__init__(feature_left, feature_right, "greater")
-
-
-class Ge(NpPairOperator):
-    """Greater Equal Than Operator
-
-    Parameters
-    ----------
-    feature_left : Expression
-        feature instance
-    feature_right : Expression
-        feature instance
-
-    Returns
-    ----------
-    Feature:
-        bool series indicate `left >= right`
-    """
-
-    def __init__(self, feature_left, feature_right):
-        super(Ge, self).__init__(feature_left, feature_right, "greater_equal")
-
-
-class Lt(NpPairOperator):
-    """Less Than Operator
-
-    Parameters
-    ----------
-    feature_left : Expression
-        feature instance
-    feature_right : Expression
-        feature instance
-
-    Returns
-    ----------
-    Feature:
-        bool series indicate `left < right`
-    """
-
-    def __init__(self, feature_left, feature_right):
-        super(Lt, self).__init__(feature_left, feature_right, "less")
-
-
-class Le(NpPairOperator):
-    """Less Equal Than Operator
-
-    Parameters
-    ----------
-    feature_left : Expression
-        feature instance
-    feature_right : Expression
-        feature instance
-
-    Returns
-    ----------
-    Feature:
-        bool series indicate `left <= right`
-    """
-
-    def __init__(self, feature_left, feature_right):
-        super(Le, self).__init__(feature_left, feature_right, "less_equal")
-
-
-class Eq(NpPairOperator):
-    """Equal Operator
-
-    Parameters
-    ----------
-    feature_left : Expression
-        feature instance
-    feature_right : Expression
-        feature instance
-
-    Returns
-    ----------
-    Feature:
-        bool series indicate `left == right`
-    """
-
-    def __init__(self, feature_left, feature_right):
-        super(Eq, self).__init__(feature_left, feature_right, "equal")
-
-
-class Ne(NpPairOperator):
-    """Not Equal Operator
-
-    Parameters
-    ----------
-    feature_left : Expression
-        feature instance
-    feature_right : Expression
-        feature instance
-
-    Returns
-    ----------
-    Feature:
-        bool series indicate `left != right`
-    """
-
-    def __init__(self, feature_left, feature_right):
-        super(Ne, self).__init__(feature_left, feature_right, "not_equal")
-
-
-class And(NpPairOperator):
-    """And Operator
-
-    Parameters
-    ----------
-    feature_left : Expression
-        feature instance
-    feature_right : Expression
-        feature instance
-
-    Returns
-    ----------
-    Feature:
-        two features' row by row & output
-    """
-
-    def __init__(self, feature_left, feature_right):
-        super(And, self).__init__(feature_left, feature_right, "bitwise_and")
-
-
-class Or(NpPairOperator):
-    """Or Operator
-
-    Parameters
-    ----------
-    feature_left : Expression
-        feature instance
-    feature_right : Expression
-        feature instance
-
-    Returns
-    ----------
-    Feature:
-        two features' row by row | outputs
-    """
-
-    def __init__(self, feature_left, feature_right):
-        super(Or, self).__init__(feature_left, feature_right, "bitwise_or")
-
-
-#################### Triple-wise Operator ####################
-class If(ExpressionOps):
-    """If Operator
-
-    Parameters
-    ----------
-    condition : Expression
-        feature instance with bool values as condition
-    feature_left : Expression
-        feature instance
-    feature_right : Expression
-        feature instance
-    """
-
-    def __init__(self, condition, feature_left, feature_right):
-        self.condition = condition
-        self.feature_left = feature_left
-        self.feature_right = feature_right
-
-    def __str__(self):
-        return "If({},{},{})".format(self.condition, self.feature_left, self.feature_right)
-
-    def _load_internal(self, instrument, start_index, end_index, *args):
-        series_cond = self.condition.load(instrument, start_index, end_index, *args)
-        if isinstance(self.feature_left, (Expression,)):
-            series_left = self.feature_left.load(instrument, start_index, end_index, *args)
-        else:
-            series_left = self.feature_left
-        if isinstance(self.feature_right, (Expression,)):
-            series_right = self.feature_right.load(instrument, start_index, end_index, *args)
-        else:
-            series_right = self.feature_right
-        series = pd.Series(np.where(series_cond, series_left, series_right), index=series_cond.index)
-        return series
-
-    def get_longest_back_rolling(self):
-        if isinstance(self.feature_left, (Expression,)):
-            left_br = self.feature_left.get_longest_back_rolling()
-        else:
-            left_br = 0
-
-        if isinstance(self.feature_right, (Expression,)):
-            right_br = self.feature_right.get_longest_back_rolling()
-        else:
-            right_br = 0
-
-        if isinstance(self.condition, (Expression,)):
-            c_br = self.condition.get_longest_back_rolling()
-        else:
-            c_br = 0
-        return max(left_br, right_br, c_br)
-
-    def get_extended_window_size(self):
-        if isinstance(self.feature_left, (Expression,)):
-            ll, lr = self.feature_left.get_extended_window_size()
-        else:
-            ll, lr = 0, 0
-
-        if isinstance(self.feature_right, (Expression,)):
-            rl, rr = self.feature_right.get_extended_window_size()
-        else:
-            rl, rr = 0, 0
-
-        if isinstance(self.condition, (Expression,)):
-            cl, cr = self.condition.get_extended_window_size()
-        else:
-            cl, cr = 0, 0
-        return max(ll, rl, cl), max(lr, rr, cr)
-
-
-#################### Rolling ####################
-# NOTE: methods like `rolling.mean` are optimized with cython,
-# and are super faster than `rolling.apply(np.mean)`
-
-
-class Rolling(ExpressionOps):
-    """Rolling Operator
-    The meaning of rolling and expanding is the same in pandas.
-    When the window is set to 0, the behaviour of the operator should follow `expanding`
-    Otherwise, it follows `rolling`
-
-    Parameters
-    ----------
-    feature : Expression
-        feature instance
-    N : int
-        rolling window size
-    func : str
-        rolling method
-
-    Returns
-    ----------
-    Expression
-        rolling outputs
-    """
-
-    def __init__(self, feature, N, func):
-        self.feature = feature
-        self.N = N
-        self.func = func
-
-    def __str__(self):
-        return "{}({},{})".format(type(self).__name__, self.feature, self.N)
-
-    def _load_internal(self, instrument, start_index, end_index, *args):
-        series = self.feature.load(instrument, start_index, end_index, *args)
-        # NOTE: remove all null check,
-        # now it's user's responsibility to decide whether use features in null days
-        # isnull = series.isnull() # NOTE: isnull = NaN, inf is not null
-        if isinstance(self.N, int) and self.N == 0:
-            series = getattr(series.expanding(min_periods=1), self.func)()
-        elif isinstance(self.N, float) and 0 < self.N < 1:
-            series = series.ewm(alpha=self.N, min_periods=1).mean()
-        else:
-            series = getattr(series.rolling(self.N, min_periods=1), self.func)()
-            # series.iloc[:self.N-1] = np.nan
-        # series[isnull] = np.nan
-        return series
-
-    def get_longest_back_rolling(self):
-        if self.N == 0:
-            return np.inf
-        if 0 < self.N < 1:
-            return int(np.log(1e-6) / np.log(1 - self.N))  # (1 - N)**window == 1e-6
-        return self.feature.get_longest_back_rolling() + self.N - 1
-
-    def get_extended_window_size(self):
-        if self.N == 0:
-            # FIXME: How to make this accurate and efficiently? Or  should we
-            # remove such support for N == 0?
-            get_module_logger(self.__class__.__name__).warning("The Rolling(ATTR, 0) will not be accurately calculated")
-            return self.feature.get_extended_window_size()
-        elif 0 < self.N < 1:
-            lft_etd, rght_etd = self.feature.get_extended_window_size()
-            size = int(np.log(1e-6) / np.log(1 - self.N))
-            lft_etd = max(lft_etd + size - 1, lft_etd)
-            return lft_etd, rght_etd
-        else:
-            lft_etd, rght_etd = self.feature.get_extended_window_size()
-            lft_etd = max(lft_etd + self.N - 1, lft_etd)
-            return lft_etd, rght_etd
-
-
-class Ref(Rolling):
-    """Feature Reference
-
-    Parameters
-    ----------
-    feature : Expression
-        feature instance
-    N : int
-        N = 0, retrieve the first data; N > 0, retrieve data of N periods ago; N < 0, future data
-
-    Returns
-    ----------
-    Expression
-        a feature instance with target reference
-    """
-
-    def __init__(self, feature, N):
-        super(Ref, self).__init__(feature, N, "ref")
-
-    def _load_internal(self, instrument, start_index, end_index, *args):
-        series = self.feature.load(instrument, start_index, end_index, *args)
-        # N = 0, return first day
-        if series.empty:
-            return series  # Pandas bug, see: https://github.com/pandas-dev/pandas/issues/21049
-        elif self.N == 0:
-            series = pd.Series(series.iloc[0], index=series.index)
-        else:
-            series = series.shift(self.N)  # copy
-        return series
-
-    def get_longest_back_rolling(self):
-        if self.N == 0:
-            return np.inf
-        return self.feature.get_longest_back_rolling() + self.N
-
-    def get_extended_window_size(self):
-        if self.N == 0:
-            get_module_logger(self.__class__.__name__).warning("The Ref(ATTR, 0) will not be accurately calculated")
-            return self.feature.get_extended_window_size()
-        else:
-            lft_etd, rght_etd = self.feature.get_extended_window_size()
-            lft_etd = max(lft_etd + self.N, lft_etd)
-            rght_etd = max(rght_etd - self.N, rght_etd)
-            return lft_etd, rght_etd
-
-
-class Mean(Rolling):
-    """Rolling Mean (MA)
-
-    Parameters
-    ----------
-    feature : Expression
-        feature instance
-    N : int
-        rolling window size
-
-    Returns
-    ----------
-    Expression
-        a feature instance with rolling average
-    """
-
-    def __init__(self, feature, N):
-        super(Mean, self).__init__(feature, N, "mean")
-
-
-class Sum(Rolling):
-    """Rolling Sum
-
-    Parameters
-    ----------
-    feature : Expression
-        feature instance
-    N : int
-        rolling window size
-
-    Returns
-    ----------
-    Expression
-        a feature instance with rolling sum
-    """
-
-    def __init__(self, feature, N):
-        super(Sum, self).__init__(feature, N, "sum")
-
-
-class Std(Rolling):
-    """Rolling Std
-
-    Parameters
-    ----------
-    feature : Expression
-        feature instance
-    N : int
-        rolling window size
-
-    Returns
-    ----------
-    Expression
-        a feature instance with rolling std
-    """
-
-    def __init__(self, feature, N):
-        super(Std, self).__init__(feature, N, "std")
-
-
-class Var(Rolling):
-    """Rolling Variance
-
-    Parameters
-    ----------
-    feature : Expression
-        feature instance
-    N : int
-        rolling window size
-
-    Returns
-    ----------
-    Expression
-        a feature instance with rolling variance
-    """
-
-    def __init__(self, feature, N):
-        super(Var, self).__init__(feature, N, "var")
-
-
-class Skew(Rolling):
-    """Rolling Skewness
-
-    Parameters
-    ----------
-    feature : Expression
-        feature instance
-    N : int
-        rolling window size
-
-    Returns
-    ----------
-    Expression
-        a feature instance with rolling skewness
-    """
-
-    def __init__(self, feature, N):
-        if N != 0 and N < 3:
-            raise ValueError("The rolling window size of Skewness operation should >= 3")
-        super(Skew, self).__init__(feature, N, "skew")
-
-
-class Kurt(Rolling):
-    """Rolling Kurtosis
-
-    Parameters
-    ----------
-    feature : Expression
-        feature instance
-    N : int
-        rolling window size
-
-    Returns
-    ----------
-    Expression
-        a feature instance with rolling kurtosis
-    """
-
-    def __init__(self, feature, N):
-        if N != 0 and N < 4:
-            raise ValueError("The rolling window size of Kurtosis operation should >= 5")
-        super(Kurt, self).__init__(feature, N, "kurt")
-
-
-class Max(Rolling):
-    """Rolling Max
-
-    Parameters
-    ----------
-    feature : Expression
-        feature instance
-    N : int
-        rolling window size
-
-    Returns
-    ----------
-    Expression
-        a feature instance with rolling max
-    """
-
-    def __init__(self, feature, N):
-        super(Max, self).__init__(feature, N, "max")
-
-
-class IdxMax(Rolling):
-    """Rolling Max Index
-
-    Parameters
-    ----------
-    feature : Expression
-        feature instance
-    N : int
-        rolling window size
-
-    Returns
-    ----------
-    Expression
-        a feature instance with rolling max index
-    """
-
-    def __init__(self, feature, N):
-        super(IdxMax, self).__init__(feature, N, "idxmax")
-
-    def _load_internal(self, instrument, start_index, end_index, *args):
-        series = self.feature.load(instrument, start_index, end_index, *args)
-        if self.N == 0:
-            series = series.expanding(min_periods=1).apply(lambda x: x.argmax() + 1, raw=True)
-        else:
-            series = series.rolling(self.N, min_periods=1).apply(lambda x: x.argmax() + 1, raw=True)
-        return series
-
-
-class Min(Rolling):
-    """Rolling Min
-
-    Parameters
-    ----------
-    feature : Expression
-        feature instance
-    N : int
-        rolling window size
-
-    Returns
-    ----------
-    Expression
-        a feature instance with rolling min
-    """
-
-    def __init__(self, feature, N):
-        super(Min, self).__init__(feature, N, "min")
-
-
-class IdxMin(Rolling):
-    """Rolling Min Index
-
-    Parameters
-    ----------
-    feature : Expression
-        feature instance
-    N : int
-        rolling window size
-
-    Returns
-    ----------
-    Expression
-        a feature instance with rolling min index
-    """
-
-    def __init__(self, feature, N):
-        super(IdxMin, self).__init__(feature, N, "idxmin")
-
-    def _load_internal(self, instrument, start_index, end_index, *args):
-        series = self.feature.load(instrument, start_index, end_index, *args)
-        if self.N == 0:
-            series = series.expanding(min_periods=1).apply(lambda x: x.argmin() + 1, raw=True)
-        else:
-            series = series.rolling(self.N, min_periods=1).apply(lambda x: x.argmin() + 1, raw=True)
-        return series
-
-
-class Quantile(Rolling):
-    """Rolling Quantile
-
-    Parameters
-    ----------
-    feature : Expression
-        feature instance
-    N : int
-        rolling window size
-
-    Returns
-    ----------
-    Expression
-        a feature instance with rolling quantile
-    """
-
-    def __init__(self, feature, N, qscore):
-        super(Quantile, self).__init__(feature, N, "quantile")
-        self.qscore = qscore
-
-    def __str__(self):
-        return "{}({},{},{})".format(type(self).__name__, self.feature, self.N, self.qscore)
-
-    def _load_internal(self, instrument, start_index, end_index, *args):
-        series = self.feature.load(instrument, start_index, end_index, *args)
-        if self.N == 0:
-            series = series.expanding(min_periods=1).quantile(self.qscore)
-        else:
-            series = series.rolling(self.N, min_periods=1).quantile(self.qscore)
-        return series
-
-
-class Med(Rolling):
-    """Rolling Median
-
-    Parameters
-    ----------
-    feature : Expression
-        feature instance
-    N : int
-        rolling window size
-
-    Returns
-    ----------
-    Expression
-        a feature instance with rolling median
-    """
-
-    def __init__(self, feature, N):
-        super(Med, self).__init__(feature, N, "median")
-
-
-class Mad(Rolling):
-    """Rolling Mean Absolute Deviation
-
-    Parameters
-    ----------
-    feature : Expression
-        feature instance
-    N : int
-        rolling window size
-
-    Returns
-    ----------
-    Expression
-        a feature instance with rolling mean absolute deviation
-    """
-
-    def __init__(self, feature, N):
-        super(Mad, self).__init__(feature, N, "mad")
-
-    def _load_internal(self, instrument, start_index, end_index, *args):
-        series = self.feature.load(instrument, start_index, end_index, *args)
-        # TODO: implement in Cython
-
-        def mad(x):
-            x1 = x[~np.isnan(x)]
-            return np.mean(np.abs(x1 - x1.mean()))
-
-        if self.N == 0:
-            series = series.expanding(min_periods=1).apply(mad, raw=True)
-        else:
-            series = series.rolling(self.N, min_periods=1).apply(mad, raw=True)
-        return series
-
-
-class Rank(Rolling):
-    """Rolling Rank (Percentile)
-
-    Parameters
-    ----------
-    feature : Expression
-        feature instance
-    N : int
-        rolling window size
-
-    Returns
-    ----------
-    Expression
-        a feature instance with rolling rank
-    """
-
-    def __init__(self, feature, N):
-        super(Rank, self).__init__(feature, N, "rank")
-
-    # for compatiblity of python 3.7, which doesn't support pandas 1.4.0+ which implements Rolling.rank
-    def _load_internal(self, instrument, start_index, end_index, *args):
-        series = self.feature.load(instrument, start_index, end_index, *args)
-
-        rolling_or_expending = series.expanding(min_periods=1) if self.N == 0 else series.rolling(self.N, min_periods=1)
-        if hasattr(rolling_or_expending, "rank"):
-            return rolling_or_expending.rank(pct=True)
-
-        def rank(x):
-            if np.isnan(x[-1]):
-                return np.nan
-            x1 = x[~np.isnan(x)]
-            if x1.shape[0] == 0:
-                return np.nan
-            return percentileofscore(x1, x1[-1]) / 100
-
-        return rolling_or_expending.apply(rank, raw=True)
-
-
-class Count(Rolling):
-    """Rolling Count
-
-    Parameters
-    ----------
-    feature : Expression
-        feature instance
-    N : int
-        rolling window size
-
-    Returns
-    ----------
-    Expression
-        a feature instance with rolling count of number of non-NaN elements
-    """
-
-    def __init__(self, feature, N):
-        super(Count, self).__init__(feature, N, "count")
-
-
-class Delta(Rolling):
-    """Rolling Delta
-
-    Parameters
-    ----------
-    feature : Expression
-        feature instance
-    N : int
-        rolling window size
-
-    Returns
-    ----------
-    Expression
-        a feature instance with end minus start in rolling window
-    """
-
-    def __init__(self, feature, N):
-        super(Delta, self).__init__(feature, N, "delta")
-
-    def _load_internal(self, instrument, start_index, end_index, *args):
-        series = self.feature.load(instrument, start_index, end_index, *args)
-        if self.N == 0:
-            series = series - series.iloc[0]
-        else:
-            series = series - series.shift(self.N)
-        return series
-
-
-# TODO:
-# support pair-wise rolling like `Slope(A, B, N)`
-class Slope(Rolling):
-    """Rolling Slope
-    This operator calculate the slope between `idx` and `feature`.
-    (e.g. [<feature_t1>, <feature_t2>, <feature_t3>] and [1, 2, 3])
-
-    Usage Example:
-    - "Slope($close, %d)/$close"
-
-    # TODO:
-    # Some users may want pair-wise rolling like `Slope(A, B, N)`
-
-    Parameters
-    ----------
-    feature : Expression
-        feature instance
-    N : int
-        rolling window size
-
-    Returns
-    ----------
-    Expression
-        a feature instance with linear regression slope of given window
-    """
-
-    def __init__(self, feature, N):
-        super(Slope, self).__init__(feature, N, "slope")
-
-    def _load_internal(self, instrument, start_index, end_index, *args):
-        series = self.feature.load(instrument, start_index, end_index, *args)
-        if self.N == 0:
-            series = pd.Series(expanding_slope(series.values), index=series.index)
-        else:
-            series = pd.Series(rolling_slope(series.values, self.N), index=series.index)
-        return series
-
-
-class Rsquare(Rolling):
-    """Rolling R-value Square
-
-    Parameters
-    ----------
-    feature : Expression
-        feature instance
-    N : int
-        rolling window size
-
-    Returns
-    ----------
-    Expression
-        a feature instance with linear regression r-value square of given window
-    """
-
-    def __init__(self, feature, N):
-        super(Rsquare, self).__init__(feature, N, "rsquare")
-
-    def _load_internal(self, instrument, start_index, end_index, *args):
-        _series = self.feature.load(instrument, start_index, end_index, *args)
-        if self.N == 0:
-            series = pd.Series(expanding_rsquare(_series.values), index=_series.index)
-        else:
-            series = pd.Series(rolling_rsquare(_series.values, self.N), index=_series.index)
-            series.loc[np.isclose(_series.rolling(self.N, min_periods=1).std(), 0, atol=2e-05)] = np.nan
-        return series
-
-
-class Resi(Rolling):
-    """Rolling Regression Residuals
-
-    Parameters
-    ----------
-    feature : Expression
-        feature instance
-    N : int
-        rolling window size
-
-    Returns
-    ----------
-    Expression
-        a feature instance with regression residuals of given window
-    """
-
-    def __init__(self, feature, N):
-        super(Resi, self).__init__(feature, N, "resi")
-
-    def _load_internal(self, instrument, start_index, end_index, *args):
-        series = self.feature.load(instrument, start_index, end_index, *args)
-        if self.N == 0:
-            series = pd.Series(expanding_resi(series.values), index=series.index)
-        else:
-            series = pd.Series(rolling_resi(series.values, self.N), index=series.index)
-        return series
-
-
-class WMA(Rolling):
-    """Rolling WMA
-
-    Parameters
-    ----------
-    feature : Expression
-        feature instance
-    N : int
-        rolling window size
-
-    Returns
-    ----------
-    Expression
-        a feature instance with weighted moving average output
-    """
-
-    def __init__(self, feature, N):
-        super(WMA, self).__init__(feature, N, "wma")
-
-    def _load_internal(self, instrument, start_index, end_index, *args):
-        series = self.feature.load(instrument, start_index, end_index, *args)
-        # TODO: implement in Cython
-
-        def weighted_mean(x):
-            w = np.arange(len(x)) + 1
-            w = w / w.sum()
-            return np.nanmean(w * x)
-
-        if self.N == 0:
-            series = series.expanding(min_periods=1).apply(weighted_mean, raw=True)
-        else:
-            series = series.rolling(self.N, min_periods=1).apply(weighted_mean, raw=True)
-        return series
-
-
-class EMA(Rolling):
-    """Rolling Exponential Mean (EMA)
-
-    Parameters
-    ----------
-    feature : Expression
-        feature instance
-    N : int, float
-        rolling window size
-
-    Returns
-    ----------
-    Expression
-        a feature instance with regression r-value square of given window
-    """
-
-    def __init__(self, feature, N):
-        super(EMA, self).__init__(feature, N, "ema")
-
-    def _load_internal(self, instrument, start_index, end_index, *args):
-        series = self.feature.load(instrument, start_index, end_index, *args)
-
-        def exp_weighted_mean(x):
-            a = 1 - 2 / (1 + len(x))
-            w = a ** np.arange(len(x))[::-1]
-            w /= w.sum()
-            return np.nansum(w * x)
-
-        if self.N == 0:
-            series = series.expanding(min_periods=1).apply(exp_weighted_mean, raw=True)
-        elif 0 < self.N < 1:
-            series = series.ewm(alpha=self.N, min_periods=1).mean()
-        else:
-            series = series.ewm(span=self.N, min_periods=1).mean()
-        return series
-
-
-#################### Pair-Wise Rolling ####################
-class PairRolling(ExpressionOps):
-    """Pair Rolling Operator
-
-    Parameters
-    ----------
-    feature_left : Expression
-        feature instance
-    feature_right : Expression
-        feature instance
-    N : int
-        rolling window size
-
-    Returns
-    ----------
-    Expression
-        a feature instance with rolling output of two input features
-    """
-
-    def __init__(self, feature_left, feature_right, N, func):
-        # TODO: in what case will a const be passed into `__init__` as `feature_left` or `feature_right`
-        self.feature_left = feature_left
-        self.feature_right = feature_right
-        self.N = N
-        self.func = func
-
-    def __str__(self):
-        return "{}({},{},{})".format(type(self).__name__, self.feature_left, self.feature_right, self.N)
-
-    def _load_internal(self, instrument, start_index, end_index, *args):
-        assert any(
-            [isinstance(self.feature_left, Expression), self.feature_right, Expression]
-        ), "at least one of two inputs is Expression instance"
-
-        if isinstance(self.feature_left, Expression):
-            series_left = self.feature_left.load(instrument, start_index, end_index, *args)
-        else:
-            series_left = self.feature_left  # numeric value
-        if isinstance(self.feature_right, Expression):
-            series_right = self.feature_right.load(instrument, start_index, end_index, *args)
-        else:
-            series_right = self.feature_right
-
-        if self.N == 0:
-            series = getattr(series_left.expanding(min_periods=1), self.func)(series_right)
-        else:
-            series = getattr(series_left.rolling(self.N, min_periods=1), self.func)(series_right)
-        return series
-
-    def get_longest_back_rolling(self):
-        if self.N == 0:
-            return np.inf
-        if isinstance(self.feature_left, Expression):
-            left_br = self.feature_left.get_longest_back_rolling()
-        else:
-            left_br = 0
-
-        if isinstance(self.feature_right, Expression):
-            right_br = self.feature_right.get_longest_back_rolling()
-        else:
-            right_br = 0
-        return max(left_br, right_br)
-
-    def get_extended_window_size(self):
-        if isinstance(self.feature_left, Expression):
-            ll, lr = self.feature_left.get_extended_window_size()
-        else:
-            ll, lr = 0, 0
-        if isinstance(self.feature_right, Expression):
-            rl, rr = self.feature_right.get_extended_window_size()
-        else:
-            rl, rr = 0, 0
-        if self.N == 0:
-            get_module_logger(self.__class__.__name__).warning(
-                "The PairRolling(ATTR, 0) will not be accurately calculated"
-            )
-            return -np.inf, max(lr, rr)
-        else:
-            return max(ll, rl) + self.N - 1, max(lr, rr)
-
-
-class Corr(PairRolling):
-    """Rolling Correlation
-
-    Parameters
-    ----------
-    feature_left : Expression
-        feature instance
-    feature_right : Expression
-        feature instance
-    N : int
-        rolling window size
-
-    Returns
-    ----------
-    Expression
-        a feature instance with rolling correlation of two input features
-    """
-
-    def __init__(self, feature_left, feature_right, N):
-        super(Corr, self).__init__(feature_left, feature_right, N, "corr")
-
-    def _load_internal(self, instrument, start_index, end_index, *args):
-        res: pd.Series = super(Corr, self)._load_internal(instrument, start_index, end_index, *args)
-
-        # NOTE: Load uses MemCache, so calling load again will not cause performance degradation
-        series_left = self.feature_left.load(instrument, start_index, end_index, *args)
-        series_right = self.feature_right.load(instrument, start_index, end_index, *args)
-        res.loc[
-            np.isclose(series_left.rolling(self.N, min_periods=1).std(), 0, atol=2e-05)
-            | np.isclose(series_right.rolling(self.N, min_periods=1).std(), 0, atol=2e-05)
-        ] = np.nan
-        return res
-
-
-class Cov(PairRolling):
-    """Rolling Covariance
-
-    Parameters
-    ----------
-    feature_left : Expression
-        feature instance
-    feature_right : Expression
-        feature instance
-    N : int
-        rolling window size
-
-    Returns
-    ----------
-    Expression
-        a feature instance with rolling max of two input features
-    """
-
-    def __init__(self, feature_left, feature_right, N):
-        super(Cov, self).__init__(feature_left, feature_right, N, "cov")
-
-
-#################### Operator which only support data with time index ####################
-# Convention
-# - The name of the operators in this section will start with "T"
-
-
-class TResample(ElemOperator):
-    def __init__(self, feature, freq, func):
-        """
-        Resampling the data to target frequency.
-        The resample function of pandas is used.
-
-        - the timestamp will be at the start of the time span after resample.
-
-        Parameters
-        ----------
-        feature : Expression
-            An expression for calculating the feature
-        freq : str
-            It will be passed into the resample method for resampling basedn on given frequency
-        func : method
-            The method to get the resampled values
-            Some expression are high frequently used
-        """
-        self.feature = feature
-        self.freq = freq
-        self.func = func
-
-    def __str__(self):
-        return "{}({},{})".format(type(self).__name__, self.feature, self.freq)
-
-    def _load_internal(self, instrument, start_index, end_index, *args):
-        series = self.feature.load(instrument, start_index, end_index, *args)
-
-        if series.empty:
-            return series
-        else:
-            if self.func == "sum":
-                return getattr(series.resample(self.freq), self.func)(min_count=1)
-            else:
-                return getattr(series.resample(self.freq), self.func)()
-
-
-TOpsList = [TResample]
-OpsList = [
-    ChangeInstrument,
-    Rolling,
-    Ref,
-    Max,
-    Min,
-    Sum,
-    Mean,
-    Std,
-    Var,
-    Skew,
-    Kurt,
-    Med,
-    Mad,
-    Slope,
-    Rsquare,
-    Resi,
-    Rank,
-    Quantile,
-    Count,
-    EMA,
-    WMA,
-    Corr,
-    Cov,
-    Delta,
-    Abs,
-    Sign,
-    Log,
-    Power,
-    Add,
-    Sub,
-    Mul,
-    Div,
-    Greater,
-    Less,
-    And,
-    Or,
-    Not,
-    Gt,
-    Ge,
-    Lt,
-    Le,
-    Eq,
-    Ne,
-    Mask,
-    IdxMax,
-    IdxMin,
-    If,
-    Feature,
-    PFeature,
-] + [TResample]
-
-
-class OpsWrapper:
-    """Ops Wrapper"""
-
-    def __init__(self):
-        self._ops = {}
-
-    def reset(self):
-        self._ops = {}
-
-    def register(self, ops_list: List[Union[Type[ExpressionOps], dict]]):
-        """register operator
-
-        Parameters
-        ----------
-        ops_list : List[Union[Type[ExpressionOps], dict]]
-            - if type(ops_list) is List[Type[ExpressionOps]], each element of ops_list represents the operator class, which should be the subclass of `ExpressionOps`.
-            - if type(ops_list) is List[dict], each element of ops_list represents the config of operator, which has the following format:
-
-                .. code-block:: text
-
-                    {
-                        "class": class_name,
-                        "module_path": path,
-                    }
-
-                Note: `class` should be the class name of operator, `module_path` should be a python module or path of file.
-        """
-        for _operator in ops_list:
-            if isinstance(_operator, dict):
-                _ops_class, _ = get_callable_kwargs(_operator)
-            else:
-                _ops_class = _operator
-
-            if not issubclass(_ops_class, (Expression,)):
-                raise TypeError("operator must be subclass of ExpressionOps, not {}".format(_ops_class))
-
-            if _ops_class.__name__ in self._ops:
-                get_module_logger(self.__class__.__name__).warning(
-                    "The custom operator [{}] will override the qlib default definition".format(_ops_class.__name__)
-                )
-            self._ops[_ops_class.__name__] = _ops_class
-
-    def __getattr__(self, key):
-        if key not in self._ops:
-            raise AttributeError("The operator [{0}] is not registered".format(key))
-        return self._ops[key]
-
-
-Operators = OpsWrapper()
-
-
-def register_all_ops(C):
-    """register all operator"""
-    logger = get_module_logger("ops")
-
-    from qlib.data.pit import P, PRef  # pylint: disable=C0415
-
-    Operators.reset()
-    Operators.register(OpsList + [P, PRef])
-
-    if getattr(C, "custom_ops", None) is not None:
-        Operators.register(C.custom_ops)
-        logger.debug("register custom operator {}".format(C.custom_ops))
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+
+from __future__ import division
+from __future__ import print_function
+
+import numpy as np
+import pandas as pd
+
+from typing import Union, List, Type
+from scipy.stats import percentileofscore
+from .base import Expression, ExpressionOps, Feature, PFeature
+from ..log import get_module_logger
+from ..utils import get_callable_kwargs
+
+try:
+    from ._libs.rolling import rolling_slope, rolling_rsquare, rolling_resi
+    from ._libs.expanding import expanding_slope, expanding_rsquare, expanding_resi
+except ImportError:
+    print(
+        "#### Do not import qlib package in the repository directory in case of importing qlib from . without compiling #####"
+    )
+    raise
+except ValueError:
+    print("!!!!!!!! A error occurs when importing operators implemented based on Cython.!!!!!!!!")
+    print("!!!!!!!! They will be disabled. Please Upgrade your numpy to enable them     !!!!!!!!")
+    # We catch this error because some platform can't upgrade there package (e.g. Kaggle)
+    # https://www.kaggle.com/general/293387
+    # https://www.kaggle.com/product-feedback/98562
+
+
+np.seterr(invalid="ignore")
+
+
+#################### Element-Wise Operator ####################
+class ElemOperator(ExpressionOps):
+    """Element-wise Operator
+
+    Parameters
+    ----------
+    feature : Expression
+        feature instance
+
+    Returns
+    ----------
+    Expression
+        feature operation output
+    """
+
+    def __init__(self, feature):
+        self.feature = feature
+
+    def __str__(self):
+        return "{}({})".format(type(self).__name__, self.feature)
+
+    def get_longest_back_rolling(self):
+        return self.feature.get_longest_back_rolling()
+
+    def get_extended_window_size(self):
+        return self.feature.get_extended_window_size()
+
+
+class ChangeInstrument(ElemOperator):
+    """Change Instrument Operator
+    In some case, one may want to change to another instrument when calculating, for example, to
+    calculate beta of a stock with respect to a market index.
+    This would require changing the calculation of features from the stock (original instrument) to
+    the index (reference instrument)
+    Parameters
+    ----------
+    instrument: new instrument for which the downstream operations should be performed upon.
+                i.e., SH000300 (CSI300 index), or ^GPSC (SP500 index).
+
+    feature: the feature to be calculated for the new instrument.
+    Returns
+    ----------
+    Expression
+        feature operation output
+    """
+
+    def __init__(self, instrument, feature):
+        self.instrument = instrument
+        self.feature = feature
+
+    def __str__(self):
+        return "{}('{}',{})".format(type(self).__name__, self.instrument, self.feature)
+
+    def load(self, instrument, start_index, end_index, *args):
+        # the first `instrument` is ignored
+        return super().load(self.instrument, start_index, end_index, *args)
+
+    def _load_internal(self, instrument, start_index, end_index, *args):
+        return self.feature.load(instrument, start_index, end_index, *args)
+
+
+class NpElemOperator(ElemOperator):
+    """Numpy Element-wise Operator
+
+    Parameters
+    ----------
+    feature : Expression
+        feature instance
+    func : str
+        numpy feature operation method
+
+    Returns
+    ----------
+    Expression
+        feature operation output
+    """
+
+    def __init__(self, feature, func):
+        self.func = func
+        super(NpElemOperator, self).__init__(feature)
+
+    def _load_internal(self, instrument, start_index, end_index, *args):
+        series = self.feature.load(instrument, start_index, end_index, *args)
+        return getattr(np, self.func)(series)
+
+
+class Abs(NpElemOperator):
+    """Feature Absolute Value
+
+    Parameters
+    ----------
+    feature : Expression
+        feature instance
+
+    Returns
+    ----------
+    Expression
+        a feature instance with absolute output
+    """
+
+    def __init__(self, feature):
+        super(Abs, self).__init__(feature, "abs")
+
+
+class Sign(NpElemOperator):
+    """Feature Sign
+
+    Parameters
+    ----------
+    feature : Expression
+        feature instance
+
+    Returns
+    ----------
+    Expression
+        a feature instance with sign
+    """
+
+    def __init__(self, feature):
+        super(Sign, self).__init__(feature, "sign")
+
+    def _load_internal(self, instrument, start_index, end_index, *args):
+        """
+        To avoid error raised by bool type input, we transform the data into float32.
+        """
+        series = self.feature.load(instrument, start_index, end_index, *args)
+        # TODO:  More precision types should be configurable
+        series = series.astype(np.float32)
+        return getattr(np, self.func)(series)
+
+
+class Log(NpElemOperator):
+    """Feature Log
+
+    Parameters
+    ----------
+    feature : Expression
+        feature instance
+
+    Returns
+    ----------
+    Expression
+        a feature instance with log
+    """
+
+    def __init__(self, feature):
+        super(Log, self).__init__(feature, "log")
+
+
+class Mask(NpElemOperator):
+    """Feature Mask
+
+    Parameters
+    ----------
+    feature : Expression
+        feature instance
+    instrument : str
+        instrument mask
+
+    Returns
+    ----------
+    Expression
+        a feature instance with masked instrument
+    """
+
+    def __init__(self, feature, instrument):
+        super(Mask, self).__init__(feature, "mask")
+        self.instrument = instrument
+
+    def __str__(self):
+        return "{}({},{})".format(type(self).__name__, self.feature, self.instrument.lower())
+
+    def _load_internal(self, instrument, start_index, end_index, *args):
+        return self.feature.load(self.instrument, start_index, end_index, *args)
+
+
+class Not(NpElemOperator):
+    """Not Operator
+
+    Parameters
+    ----------
+    feature : Expression
+        feature instance
+
+    Returns
+    ----------
+    Feature:
+        feature elementwise not output
+    """
+
+    def __init__(self, feature):
+        super(Not, self).__init__(feature, "bitwise_not")
+
+
+#################### Pair-Wise Operator ####################
+class PairOperator(ExpressionOps):
+    """Pair-wise operator
+
+    Parameters
+    ----------
+    feature_left : Expression
+        feature instance or numeric value
+    feature_right : Expression
+        feature instance or numeric value
+
+    Returns
+    ----------
+    Feature:
+        two features' operation output
+    """
+
+    def __init__(self, feature_left, feature_right):
+        self.feature_left = feature_left
+        self.feature_right = feature_right
+
+    def __str__(self):
+        return "{}({},{})".format(type(self).__name__, self.feature_left, self.feature_right)
+
+    def get_longest_back_rolling(self):
+        if isinstance(self.feature_left, (Expression,)):
+            left_br = self.feature_left.get_longest_back_rolling()
+        else:
+            left_br = 0
+
+        if isinstance(self.feature_right, (Expression,)):
+            right_br = self.feature_right.get_longest_back_rolling()
+        else:
+            right_br = 0
+        return max(left_br, right_br)
+
+    def get_extended_window_size(self):
+        if isinstance(self.feature_left, (Expression,)):
+            ll, lr = self.feature_left.get_extended_window_size()
+        else:
+            ll, lr = 0, 0
+
+        if isinstance(self.feature_right, (Expression,)):
+            rl, rr = self.feature_right.get_extended_window_size()
+        else:
+            rl, rr = 0, 0
+        return max(ll, rl), max(lr, rr)
+
+
+class NpPairOperator(PairOperator):
+    """Numpy Pair-wise operator
+
+    Parameters
+    ----------
+    feature_left : Expression
+        feature instance or numeric value
+    feature_right : Expression
+        feature instance or numeric value
+    func : str
+        operator function
+
+    Returns
+    ----------
+    Feature:
+        two features' operation output
+    """
+
+    def __init__(self, feature_left, feature_right, func):
+        self.func = func
+        super(NpPairOperator, self).__init__(feature_left, feature_right)
+
+    def _load_internal(self, instrument, start_index, end_index, *args):
+        assert any(
+            [isinstance(self.feature_left, (Expression,)), self.feature_right, Expression]
+        ), "at least one of two inputs is Expression instance"
+        if isinstance(self.feature_left, (Expression,)):
+            series_left = self.feature_left.load(instrument, start_index, end_index, *args)
+        else:
+            series_left = self.feature_left  # numeric value
+        if isinstance(self.feature_right, (Expression,)):
+            series_right = self.feature_right.load(instrument, start_index, end_index, *args)
+        else:
+            series_right = self.feature_right
+        check_length = isinstance(series_left, (np.ndarray, pd.Series)) and isinstance(
+            series_right, (np.ndarray, pd.Series)
+        )
+        if check_length:
+            warning_info = (
+                f"Loading {instrument}: {str(self)}; np.{self.func}(series_left, series_right), "
+                f"The length of series_left and series_right is different: ({len(series_left)}, {len(series_right)}), "
+                f"series_left is {str(self.feature_left)}, series_right is {str(self.feature_right)}. Please check the data"
+            )
+        else:
+            warning_info = (
+                f"Loading {instrument}: {str(self)}; np.{self.func}(series_left, series_right), "
+                f"series_left is {str(self.feature_left)}, series_right is {str(self.feature_right)}. Please check the data"
+            )
+        try:
+            res = getattr(np, self.func)(series_left, series_right)
+        except ValueError as e:
+            get_module_logger("ops").debug(warning_info)
+            raise ValueError(f"{str(e)}. \n\t{warning_info}") from e
+        else:
+            if check_length and len(series_left) != len(series_right):
+                get_module_logger("ops").debug(warning_info)
+        return res
+
+
+class Power(NpPairOperator):
+    """Power Operator
+
+    Parameters
+    ----------
+    feature_left : Expression
+        feature instance
+    feature_right : Expression
+        feature instance
+
+    Returns
+    ----------
+    Feature:
+        The bases in feature_left raised to the exponents in feature_right
+    """
+
+    def __init__(self, feature_left, feature_right):
+        super(Power, self).__init__(feature_left, feature_right, "power")
+
+
+class Add(NpPairOperator):
+    """Add Operator
+
+    Parameters
+    ----------
+    feature_left : Expression
+        feature instance
+    feature_right : Expression
+        feature instance
+
+    Returns
+    ----------
+    Feature:
+        two features' sum
+    """
+
+    def __init__(self, feature_left, feature_right):
+        super(Add, self).__init__(feature_left, feature_right, "add")
+
+
+class Sub(NpPairOperator):
+    """Subtract Operator
+
+    Parameters
+    ----------
+    feature_left : Expression
+        feature instance
+    feature_right : Expression
+        feature instance
+
+    Returns
+    ----------
+    Feature:
+        two features' subtraction
+    """
+
+    def __init__(self, feature_left, feature_right):
+        super(Sub, self).__init__(feature_left, feature_right, "subtract")
+
+
+class Mul(NpPairOperator):
+    """Multiply Operator
+
+    Parameters
+    ----------
+    feature_left : Expression
+        feature instance
+    feature_right : Expression
+        feature instance
+
+    Returns
+    ----------
+    Feature:
+        two features' product
+    """
+
+    def __init__(self, feature_left, feature_right):
+        super(Mul, self).__init__(feature_left, feature_right, "multiply")
+
+
+class Div(NpPairOperator):
+    """Division Operator
+
+    Parameters
+    ----------
+    feature_left : Expression
+        feature instance
+    feature_right : Expression
+        feature instance
+
+    Returns
+    ----------
+    Feature:
+        two features' division
+    """
+
+    def __init__(self, feature_left, feature_right):
+        super(Div, self).__init__(feature_left, feature_right, "divide")
+
+
+class Greater(NpPairOperator):
+    """Greater Operator
+
+    Parameters
+    ----------
+    feature_left : Expression
+        feature instance
+    feature_right : Expression
+        feature instance
+
+    Returns
+    ----------
+    Feature:
+        greater elements taken from the input two features
+    """
+
+    def __init__(self, feature_left, feature_right):
+        super(Greater, self).__init__(feature_left, feature_right, "maximum")
+
+
+class Less(NpPairOperator):
+    """Less Operator
+
+    Parameters
+    ----------
+    feature_left : Expression
+        feature instance
+    feature_right : Expression
+        feature instance
+
+    Returns
+    ----------
+    Feature:
+        smaller elements taken from the input two features
+    """
+
+    def __init__(self, feature_left, feature_right):
+        super(Less, self).__init__(feature_left, feature_right, "minimum")
+
+
+class Gt(NpPairOperator):
+    """Greater Than Operator
+
+    Parameters
+    ----------
+    feature_left : Expression
+        feature instance
+    feature_right : Expression
+        feature instance
+
+    Returns
+    ----------
+    Feature:
+        bool series indicate `left > right`
+    """
+
+    def __init__(self, feature_left, feature_right):
+        super(Gt, self).__init__(feature_left, feature_right, "greater")
+
+
+class Ge(NpPairOperator):
+    """Greater Equal Than Operator
+
+    Parameters
+    ----------
+    feature_left : Expression
+        feature instance
+    feature_right : Expression
+        feature instance
+
+    Returns
+    ----------
+    Feature:
+        bool series indicate `left >= right`
+    """
+
+    def __init__(self, feature_left, feature_right):
+        super(Ge, self).__init__(feature_left, feature_right, "greater_equal")
+
+
+class Lt(NpPairOperator):
+    """Less Than Operator
+
+    Parameters
+    ----------
+    feature_left : Expression
+        feature instance
+    feature_right : Expression
+        feature instance
+
+    Returns
+    ----------
+    Feature:
+        bool series indicate `left < right`
+    """
+
+    def __init__(self, feature_left, feature_right):
+        super(Lt, self).__init__(feature_left, feature_right, "less")
+
+
+class Le(NpPairOperator):
+    """Less Equal Than Operator
+
+    Parameters
+    ----------
+    feature_left : Expression
+        feature instance
+    feature_right : Expression
+        feature instance
+
+    Returns
+    ----------
+    Feature:
+        bool series indicate `left <= right`
+    """
+
+    def __init__(self, feature_left, feature_right):
+        super(Le, self).__init__(feature_left, feature_right, "less_equal")
+
+
+class Eq(NpPairOperator):
+    """Equal Operator
+
+    Parameters
+    ----------
+    feature_left : Expression
+        feature instance
+    feature_right : Expression
+        feature instance
+
+    Returns
+    ----------
+    Feature:
+        bool series indicate `left == right`
+    """
+
+    def __init__(self, feature_left, feature_right):
+        super(Eq, self).__init__(feature_left, feature_right, "equal")
+
+
+class Ne(NpPairOperator):
+    """Not Equal Operator
+
+    Parameters
+    ----------
+    feature_left : Expression
+        feature instance
+    feature_right : Expression
+        feature instance
+
+    Returns
+    ----------
+    Feature:
+        bool series indicate `left != right`
+    """
+
+    def __init__(self, feature_left, feature_right):
+        super(Ne, self).__init__(feature_left, feature_right, "not_equal")
+
+
+class And(NpPairOperator):
+    """And Operator
+
+    Parameters
+    ----------
+    feature_left : Expression
+        feature instance
+    feature_right : Expression
+        feature instance
+
+    Returns
+    ----------
+    Feature:
+        two features' row by row & output
+    """
+
+    def __init__(self, feature_left, feature_right):
+        super(And, self).__init__(feature_left, feature_right, "bitwise_and")
+
+
+class Or(NpPairOperator):
+    """Or Operator
+
+    Parameters
+    ----------
+    feature_left : Expression
+        feature instance
+    feature_right : Expression
+        feature instance
+
+    Returns
+    ----------
+    Feature:
+        two features' row by row | outputs
+    """
+
+    def __init__(self, feature_left, feature_right):
+        super(Or, self).__init__(feature_left, feature_right, "bitwise_or")
+
+
+#################### Triple-wise Operator ####################
+class If(ExpressionOps):
+    """If Operator
+
+    Parameters
+    ----------
+    condition : Expression
+        feature instance with bool values as condition
+    feature_left : Expression
+        feature instance
+    feature_right : Expression
+        feature instance
+    """
+
+    def __init__(self, condition, feature_left, feature_right):
+        self.condition = condition
+        self.feature_left = feature_left
+        self.feature_right = feature_right
+
+    def __str__(self):
+        return "If({},{},{})".format(self.condition, self.feature_left, self.feature_right)
+
+    def _load_internal(self, instrument, start_index, end_index, *args):
+        series_cond = self.condition.load(instrument, start_index, end_index, *args)
+        if isinstance(self.feature_left, (Expression,)):
+            series_left = self.feature_left.load(instrument, start_index, end_index, *args)
+        else:
+            series_left = self.feature_left
+        if isinstance(self.feature_right, (Expression,)):
+            series_right = self.feature_right.load(instrument, start_index, end_index, *args)
+        else:
+            series_right = self.feature_right
+        series = pd.Series(np.where(series_cond, series_left, series_right), index=series_cond.index)
+        return series
+
+    def get_longest_back_rolling(self):
+        if isinstance(self.feature_left, (Expression,)):
+            left_br = self.feature_left.get_longest_back_rolling()
+        else:
+            left_br = 0
+
+        if isinstance(self.feature_right, (Expression,)):
+            right_br = self.feature_right.get_longest_back_rolling()
+        else:
+            right_br = 0
+
+        if isinstance(self.condition, (Expression,)):
+            c_br = self.condition.get_longest_back_rolling()
+        else:
+            c_br = 0
+        return max(left_br, right_br, c_br)
+
+    def get_extended_window_size(self):
+        if isinstance(self.feature_left, (Expression,)):
+            ll, lr = self.feature_left.get_extended_window_size()
+        else:
+            ll, lr = 0, 0
+
+        if isinstance(self.feature_right, (Expression,)):
+            rl, rr = self.feature_right.get_extended_window_size()
+        else:
+            rl, rr = 0, 0
+
+        if isinstance(self.condition, (Expression,)):
+            cl, cr = self.condition.get_extended_window_size()
+        else:
+            cl, cr = 0, 0
+        return max(ll, rl, cl), max(lr, rr, cr)
+
+
+#################### Rolling ####################
+# NOTE: methods like `rolling.mean` are optimized with cython,
+# and are super faster than `rolling.apply(np.mean)`
+
+
+class Rolling(ExpressionOps):
+    """Rolling Operator
+    The meaning of rolling and expanding is the same in pandas.
+    When the window is set to 0, the behaviour of the operator should follow `expanding`
+    Otherwise, it follows `rolling`
+
+    Parameters
+    ----------
+    feature : Expression
+        feature instance
+    N : int
+        rolling window size
+    func : str
+        rolling method
+
+    Returns
+    ----------
+    Expression
+        rolling outputs
+    """
+
+    def __init__(self, feature, N, func):
+        self.feature = feature
+        self.N = N
+        self.func = func
+
+    def __str__(self):
+        return "{}({},{})".format(type(self).__name__, self.feature, self.N)
+
+    def _load_internal(self, instrument, start_index, end_index, *args):
+        series = self.feature.load(instrument, start_index, end_index, *args)
+        # NOTE: remove all null check,
+        # now it's user's responsibility to decide whether use features in null days
+        # isnull = series.isnull() # NOTE: isnull = NaN, inf is not null
+        if isinstance(self.N, int) and self.N == 0:
+            series = getattr(series.expanding(min_periods=1), self.func)()
+        elif isinstance(self.N, float) and 0 < self.N < 1:
+            series = series.ewm(alpha=self.N, min_periods=1).mean()
+        else:
+            series = getattr(series.rolling(self.N, min_periods=1), self.func)()
+            # series.iloc[:self.N-1] = np.nan
+        # series[isnull] = np.nan
+        return series
+
+    def get_longest_back_rolling(self):
+        if self.N == 0:
+            return np.inf
+        if 0 < self.N < 1:
+            return int(np.log(1e-6) / np.log(1 - self.N))  # (1 - N)**window == 1e-6
+        return self.feature.get_longest_back_rolling() + self.N - 1
+
+    def get_extended_window_size(self):
+        if self.N == 0:
+            # FIXME: How to make this accurate and efficiently? Or  should we
+            # remove such support for N == 0?
+            get_module_logger(self.__class__.__name__).warning("The Rolling(ATTR, 0) will not be accurately calculated")
+            return self.feature.get_extended_window_size()
+        elif 0 < self.N < 1:
+            lft_etd, rght_etd = self.feature.get_extended_window_size()
+            size = int(np.log(1e-6) / np.log(1 - self.N))
+            lft_etd = max(lft_etd + size - 1, lft_etd)
+            return lft_etd, rght_etd
+        else:
+            lft_etd, rght_etd = self.feature.get_extended_window_size()
+            lft_etd = max(lft_etd + self.N - 1, lft_etd)
+            return lft_etd, rght_etd
+
+
+class Ref(Rolling):
+    """Feature Reference
+
+    Parameters
+    ----------
+    feature : Expression
+        feature instance
+    N : int
+        N = 0, retrieve the first data; N > 0, retrieve data of N periods ago; N < 0, future data
+
+    Returns
+    ----------
+    Expression
+        a feature instance with target reference
+    """
+
+    def __init__(self, feature, N):
+        super(Ref, self).__init__(feature, N, "ref")
+
+    def _load_internal(self, instrument, start_index, end_index, *args):
+        series = self.feature.load(instrument, start_index, end_index, *args)
+        # N = 0, return first day
+        if series.empty:
+            return series  # Pandas bug, see: https://github.com/pandas-dev/pandas/issues/21049
+        elif self.N == 0:
+            series = pd.Series(series.iloc[0], index=series.index)
+        else:
+            series = series.shift(self.N)  # copy
+        return series
+
+    def get_longest_back_rolling(self):
+        if self.N == 0:
+            return np.inf
+        return self.feature.get_longest_back_rolling() + self.N
+
+    def get_extended_window_size(self):
+        if self.N == 0:
+            get_module_logger(self.__class__.__name__).warning("The Ref(ATTR, 0) will not be accurately calculated")
+            return self.feature.get_extended_window_size()
+        else:
+            lft_etd, rght_etd = self.feature.get_extended_window_size()
+            lft_etd = max(lft_etd + self.N, lft_etd)
+            rght_etd = max(rght_etd - self.N, rght_etd)
+            return lft_etd, rght_etd
+
+
+class Mean(Rolling):
+    """Rolling Mean (MA)
+
+    Parameters
+    ----------
+    feature : Expression
+        feature instance
+    N : int
+        rolling window size
+
+    Returns
+    ----------
+    Expression
+        a feature instance with rolling average
+    """
+
+    def __init__(self, feature, N):
+        super(Mean, self).__init__(feature, N, "mean")
+
+
+class Sum(Rolling):
+    """Rolling Sum
+
+    Parameters
+    ----------
+    feature : Expression
+        feature instance
+    N : int
+        rolling window size
+
+    Returns
+    ----------
+    Expression
+        a feature instance with rolling sum
+    """
+
+    def __init__(self, feature, N):
+        super(Sum, self).__init__(feature, N, "sum")
+
+
+class Std(Rolling):
+    """Rolling Std
+
+    Parameters
+    ----------
+    feature : Expression
+        feature instance
+    N : int
+        rolling window size
+
+    Returns
+    ----------
+    Expression
+        a feature instance with rolling std
+    """
+
+    def __init__(self, feature, N):
+        super(Std, self).__init__(feature, N, "std")
+
+
+class Var(Rolling):
+    """Rolling Variance
+
+    Parameters
+    ----------
+    feature : Expression
+        feature instance
+    N : int
+        rolling window size
+
+    Returns
+    ----------
+    Expression
+        a feature instance with rolling variance
+    """
+
+    def __init__(self, feature, N):
+        super(Var, self).__init__(feature, N, "var")
+
+
+class Skew(Rolling):
+    """Rolling Skewness
+
+    Parameters
+    ----------
+    feature : Expression
+        feature instance
+    N : int
+        rolling window size
+
+    Returns
+    ----------
+    Expression
+        a feature instance with rolling skewness
+    """
+
+    def __init__(self, feature, N):
+        if N != 0 and N < 3:
+            raise ValueError("The rolling window size of Skewness operation should >= 3")
+        super(Skew, self).__init__(feature, N, "skew")
+
+
+class Kurt(Rolling):
+    """Rolling Kurtosis
+
+    Parameters
+    ----------
+    feature : Expression
+        feature instance
+    N : int
+        rolling window size
+
+    Returns
+    ----------
+    Expression
+        a feature instance with rolling kurtosis
+    """
+
+    def __init__(self, feature, N):
+        if N != 0 and N < 4:
+            raise ValueError("The rolling window size of Kurtosis operation should >= 5")
+        super(Kurt, self).__init__(feature, N, "kurt")
+
+
+class Max(Rolling):
+    """Rolling Max
+
+    Parameters
+    ----------
+    feature : Expression
+        feature instance
+    N : int
+        rolling window size
+
+    Returns
+    ----------
+    Expression
+        a feature instance with rolling max
+    """
+
+    def __init__(self, feature, N):
+        super(Max, self).__init__(feature, N, "max")
+
+
+class IdxMax(Rolling):
+    """Rolling Max Index
+
+    Parameters
+    ----------
+    feature : Expression
+        feature instance
+    N : int
+        rolling window size
+
+    Returns
+    ----------
+    Expression
+        a feature instance with rolling max index
+    """
+
+    def __init__(self, feature, N):
+        super(IdxMax, self).__init__(feature, N, "idxmax")
+
+    def _load_internal(self, instrument, start_index, end_index, *args):
+        series = self.feature.load(instrument, start_index, end_index, *args)
+        if self.N == 0:
+            series = series.expanding(min_periods=1).apply(lambda x: x.argmax() + 1, raw=True)
+        else:
+            series = series.rolling(self.N, min_periods=1).apply(lambda x: x.argmax() + 1, raw=True)
+        return series
+
+
+class Min(Rolling):
+    """Rolling Min
+
+    Parameters
+    ----------
+    feature : Expression
+        feature instance
+    N : int
+        rolling window size
+
+    Returns
+    ----------
+    Expression
+        a feature instance with rolling min
+    """
+
+    def __init__(self, feature, N):
+        super(Min, self).__init__(feature, N, "min")
+
+
+class IdxMin(Rolling):
+    """Rolling Min Index
+
+    Parameters
+    ----------
+    feature : Expression
+        feature instance
+    N : int
+        rolling window size
+
+    Returns
+    ----------
+    Expression
+        a feature instance with rolling min index
+    """
+
+    def __init__(self, feature, N):
+        super(IdxMin, self).__init__(feature, N, "idxmin")
+
+    def _load_internal(self, instrument, start_index, end_index, *args):
+        series = self.feature.load(instrument, start_index, end_index, *args)
+        if self.N == 0:
+            series = series.expanding(min_periods=1).apply(lambda x: x.argmin() + 1, raw=True)
+        else:
+            series = series.rolling(self.N, min_periods=1).apply(lambda x: x.argmin() + 1, raw=True)
+        return series
+
+
+class Quantile(Rolling):
+    """Rolling Quantile
+
+    Parameters
+    ----------
+    feature : Expression
+        feature instance
+    N : int
+        rolling window size
+
+    Returns
+    ----------
+    Expression
+        a feature instance with rolling quantile
+    """
+
+    def __init__(self, feature, N, qscore):
+        super(Quantile, self).__init__(feature, N, "quantile")
+        self.qscore = qscore
+
+    def __str__(self):
+        return "{}({},{},{})".format(type(self).__name__, self.feature, self.N, self.qscore)
+
+    def _load_internal(self, instrument, start_index, end_index, *args):
+        series = self.feature.load(instrument, start_index, end_index, *args)
+        if self.N == 0:
+            series = series.expanding(min_periods=1).quantile(self.qscore)
+        else:
+            series = series.rolling(self.N, min_periods=1).quantile(self.qscore)
+        return series
+
+
+class Med(Rolling):
+    """Rolling Median
+
+    Parameters
+    ----------
+    feature : Expression
+        feature instance
+    N : int
+        rolling window size
+
+    Returns
+    ----------
+    Expression
+        a feature instance with rolling median
+    """
+
+    def __init__(self, feature, N):
+        super(Med, self).__init__(feature, N, "median")
+
+
+class Mad(Rolling):
+    """Rolling Mean Absolute Deviation
+
+    Parameters
+    ----------
+    feature : Expression
+        feature instance
+    N : int
+        rolling window size
+
+    Returns
+    ----------
+    Expression
+        a feature instance with rolling mean absolute deviation
+    """
+
+    def __init__(self, feature, N):
+        super(Mad, self).__init__(feature, N, "mad")
+
+    def _load_internal(self, instrument, start_index, end_index, *args):
+        series = self.feature.load(instrument, start_index, end_index, *args)
+        # TODO: implement in Cython
+
+        def mad(x):
+            x1 = x[~np.isnan(x)]
+            return np.mean(np.abs(x1 - x1.mean()))
+
+        if self.N == 0:
+            series = series.expanding(min_periods=1).apply(mad, raw=True)
+        else:
+            series = series.rolling(self.N, min_periods=1).apply(mad, raw=True)
+        return series
+
+
+class Rank(Rolling):
+    """Rolling Rank (Percentile)
+
+    Parameters
+    ----------
+    feature : Expression
+        feature instance
+    N : int
+        rolling window size
+
+    Returns
+    ----------
+    Expression
+        a feature instance with rolling rank
+    """
+
+    def __init__(self, feature, N):
+        super(Rank, self).__init__(feature, N, "rank")
+
+    # for compatiblity of python 3.7, which doesn't support pandas 1.4.0+ which implements Rolling.rank
+    def _load_internal(self, instrument, start_index, end_index, *args):
+        series = self.feature.load(instrument, start_index, end_index, *args)
+
+        rolling_or_expending = series.expanding(min_periods=1) if self.N == 0 else series.rolling(self.N, min_periods=1)
+        if hasattr(rolling_or_expending, "rank"):
+            return rolling_or_expending.rank(pct=True)
+
+        def rank(x):
+            if np.isnan(x[-1]):
+                return np.nan
+            x1 = x[~np.isnan(x)]
+            if x1.shape[0] == 0:
+                return np.nan
+            return percentileofscore(x1, x1[-1]) / 100
+
+        return rolling_or_expending.apply(rank, raw=True)
+
+
+class Count(Rolling):
+    """Rolling Count
+
+    Parameters
+    ----------
+    feature : Expression
+        feature instance
+    N : int
+        rolling window size
+
+    Returns
+    ----------
+    Expression
+        a feature instance with rolling count of number of non-NaN elements
+    """
+
+    def __init__(self, feature, N):
+        super(Count, self).__init__(feature, N, "count")
+
+
+class Delta(Rolling):
+    """Rolling Delta
+
+    Parameters
+    ----------
+    feature : Expression
+        feature instance
+    N : int
+        rolling window size
+
+    Returns
+    ----------
+    Expression
+        a feature instance with end minus start in rolling window
+    """
+
+    def __init__(self, feature, N):
+        super(Delta, self).__init__(feature, N, "delta")
+
+    def _load_internal(self, instrument, start_index, end_index, *args):
+        series = self.feature.load(instrument, start_index, end_index, *args)
+        if self.N == 0:
+            series = series - series.iloc[0]
+        else:
+            series = series - series.shift(self.N)
+        return series
+
+
+# TODO:
+# support pair-wise rolling like `Slope(A, B, N)`
+class Slope(Rolling):
+    """Rolling Slope
+    This operator calculate the slope between `idx` and `feature`.
+    (e.g. [<feature_t1>, <feature_t2>, <feature_t3>] and [1, 2, 3])
+
+    Usage Example:
+    - "Slope($close, %d)/$close"
+
+    # TODO:
+    # Some users may want pair-wise rolling like `Slope(A, B, N)`
+
+    Parameters
+    ----------
+    feature : Expression
+        feature instance
+    N : int
+        rolling window size
+
+    Returns
+    ----------
+    Expression
+        a feature instance with linear regression slope of given window
+    """
+
+    def __init__(self, feature, N):
+        super(Slope, self).__init__(feature, N, "slope")
+
+    def _load_internal(self, instrument, start_index, end_index, *args):
+        series = self.feature.load(instrument, start_index, end_index, *args)
+        if self.N == 0:
+            series = pd.Series(expanding_slope(series.values), index=series.index)
+        else:
+            series = pd.Series(rolling_slope(series.values, self.N), index=series.index)
+        return series
+
+
+class Rsquare(Rolling):
+    """Rolling R-value Square
+
+    Parameters
+    ----------
+    feature : Expression
+        feature instance
+    N : int
+        rolling window size
+
+    Returns
+    ----------
+    Expression
+        a feature instance with linear regression r-value square of given window
+    """
+
+    def __init__(self, feature, N):
+        super(Rsquare, self).__init__(feature, N, "rsquare")
+
+    def _load_internal(self, instrument, start_index, end_index, *args):
+        _series = self.feature.load(instrument, start_index, end_index, *args)
+        if self.N == 0:
+            series = pd.Series(expanding_rsquare(_series.values), index=_series.index)
+        else:
+            series = pd.Series(rolling_rsquare(_series.values, self.N), index=_series.index)
+            series.loc[np.isclose(_series.rolling(self.N, min_periods=1).std(), 0, atol=2e-05)] = np.nan
+        return series
+
+
+class Resi(Rolling):
+    """Rolling Regression Residuals
+
+    Parameters
+    ----------
+    feature : Expression
+        feature instance
+    N : int
+        rolling window size
+
+    Returns
+    ----------
+    Expression
+        a feature instance with regression residuals of given window
+    """
+
+    def __init__(self, feature, N):
+        super(Resi, self).__init__(feature, N, "resi")
+
+    def _load_internal(self, instrument, start_index, end_index, *args):
+        series = self.feature.load(instrument, start_index, end_index, *args)
+        if self.N == 0:
+            series = pd.Series(expanding_resi(series.values), index=series.index)
+        else:
+            series = pd.Series(rolling_resi(series.values, self.N), index=series.index)
+        return series
+
+
+class WMA(Rolling):
+    """Rolling WMA
+
+    Parameters
+    ----------
+    feature : Expression
+        feature instance
+    N : int
+        rolling window size
+
+    Returns
+    ----------
+    Expression
+        a feature instance with weighted moving average output
+    """
+
+    def __init__(self, feature, N):
+        super(WMA, self).__init__(feature, N, "wma")
+
+    def _load_internal(self, instrument, start_index, end_index, *args):
+        series = self.feature.load(instrument, start_index, end_index, *args)
+        # TODO: implement in Cython
+
+        def weighted_mean(x):
+            w = np.arange(len(x)) + 1
+            w = w / w.sum()
+            return np.nanmean(w * x)
+
+        if self.N == 0:
+            series = series.expanding(min_periods=1).apply(weighted_mean, raw=True)
+        else:
+            series = series.rolling(self.N, min_periods=1).apply(weighted_mean, raw=True)
+        return series
+
+
+class EMA(Rolling):
+    """Rolling Exponential Mean (EMA)
+
+    Parameters
+    ----------
+    feature : Expression
+        feature instance
+    N : int, float
+        rolling window size
+
+    Returns
+    ----------
+    Expression
+        a feature instance with regression r-value square of given window
+    """
+
+    def __init__(self, feature, N):
+        super(EMA, self).__init__(feature, N, "ema")
+
+    def _load_internal(self, instrument, start_index, end_index, *args):
+        series = self.feature.load(instrument, start_index, end_index, *args)
+
+        def exp_weighted_mean(x):
+            a = 1 - 2 / (1 + len(x))
+            w = a ** np.arange(len(x))[::-1]
+            w /= w.sum()
+            return np.nansum(w * x)
+
+        if self.N == 0:
+            series = series.expanding(min_periods=1).apply(exp_weighted_mean, raw=True)
+        elif 0 < self.N < 1:
+            series = series.ewm(alpha=self.N, min_periods=1).mean()
+        else:
+            series = series.ewm(span=self.N, min_periods=1).mean()
+        return series
+
+
+#################### Pair-Wise Rolling ####################
+class PairRolling(ExpressionOps):
+    """Pair Rolling Operator
+
+    Parameters
+    ----------
+    feature_left : Expression
+        feature instance
+    feature_right : Expression
+        feature instance
+    N : int
+        rolling window size
+
+    Returns
+    ----------
+    Expression
+        a feature instance with rolling output of two input features
+    """
+
+    def __init__(self, feature_left, feature_right, N, func):
+        # TODO: in what case will a const be passed into `__init__` as `feature_left` or `feature_right`
+        self.feature_left = feature_left
+        self.feature_right = feature_right
+        self.N = N
+        self.func = func
+
+    def __str__(self):
+        return "{}({},{},{})".format(type(self).__name__, self.feature_left, self.feature_right, self.N)
+
+    def _load_internal(self, instrument, start_index, end_index, *args):
+        assert any(
+            [isinstance(self.feature_left, Expression), self.feature_right, Expression]
+        ), "at least one of two inputs is Expression instance"
+
+        if isinstance(self.feature_left, Expression):
+            series_left = self.feature_left.load(instrument, start_index, end_index, *args)
+        else:
+            series_left = self.feature_left  # numeric value
+        if isinstance(self.feature_right, Expression):
+            series_right = self.feature_right.load(instrument, start_index, end_index, *args)
+        else:
+            series_right = self.feature_right
+
+        if self.N == 0:
+            series = getattr(series_left.expanding(min_periods=1), self.func)(series_right)
+        else:
+            series = getattr(series_left.rolling(self.N, min_periods=1), self.func)(series_right)
+        return series
+
+    def get_longest_back_rolling(self):
+        if self.N == 0:
+            return np.inf
+        if isinstance(self.feature_left, Expression):
+            left_br = self.feature_left.get_longest_back_rolling()
+        else:
+            left_br = 0
+
+        if isinstance(self.feature_right, Expression):
+            right_br = self.feature_right.get_longest_back_rolling()
+        else:
+            right_br = 0
+        return max(left_br, right_br)
+
+    def get_extended_window_size(self):
+        if isinstance(self.feature_left, Expression):
+            ll, lr = self.feature_left.get_extended_window_size()
+        else:
+            ll, lr = 0, 0
+        if isinstance(self.feature_right, Expression):
+            rl, rr = self.feature_right.get_extended_window_size()
+        else:
+            rl, rr = 0, 0
+        if self.N == 0:
+            get_module_logger(self.__class__.__name__).warning(
+                "The PairRolling(ATTR, 0) will not be accurately calculated"
+            )
+            return -np.inf, max(lr, rr)
+        else:
+            return max(ll, rl) + self.N - 1, max(lr, rr)
+
+
+class Corr(PairRolling):
+    """Rolling Correlation
+
+    Parameters
+    ----------
+    feature_left : Expression
+        feature instance
+    feature_right : Expression
+        feature instance
+    N : int
+        rolling window size
+
+    Returns
+    ----------
+    Expression
+        a feature instance with rolling correlation of two input features
+    """
+
+    def __init__(self, feature_left, feature_right, N):
+        super(Corr, self).__init__(feature_left, feature_right, N, "corr")
+
+    def _load_internal(self, instrument, start_index, end_index, *args):
+        res: pd.Series = super(Corr, self)._load_internal(instrument, start_index, end_index, *args)
+
+        # NOTE: Load uses MemCache, so calling load again will not cause performance degradation
+        series_left = self.feature_left.load(instrument, start_index, end_index, *args)
+        series_right = self.feature_right.load(instrument, start_index, end_index, *args)
+        res.loc[
+            np.isclose(series_left.rolling(self.N, min_periods=1).std(), 0, atol=2e-05)
+            | np.isclose(series_right.rolling(self.N, min_periods=1).std(), 0, atol=2e-05)
+        ] = np.nan
+        return res
+
+
+class Cov(PairRolling):
+    """Rolling Covariance
+
+    Parameters
+    ----------
+    feature_left : Expression
+        feature instance
+    feature_right : Expression
+        feature instance
+    N : int
+        rolling window size
+
+    Returns
+    ----------
+    Expression
+        a feature instance with rolling max of two input features
+    """
+
+    def __init__(self, feature_left, feature_right, N):
+        super(Cov, self).__init__(feature_left, feature_right, N, "cov")
+
+
+#################### Operator which only support data with time index ####################
+# Convention
+# - The name of the operators in this section will start with "T"
+
+
+class TResample(ElemOperator):
+    def __init__(self, feature, freq, func):
+        """
+        Resampling the data to target frequency.
+        The resample function of pandas is used.
+
+        - the timestamp will be at the start of the time span after resample.
+
+        Parameters
+        ----------
+        feature : Expression
+            An expression for calculating the feature
+        freq : str
+            It will be passed into the resample method for resampling basedn on given frequency
+        func : method
+            The method to get the resampled values
+            Some expression are high frequently used
+        """
+        self.feature = feature
+        self.freq = freq
+        self.func = func
+
+    def __str__(self):
+        return "{}({},{})".format(type(self).__name__, self.feature, self.freq)
+
+    def _load_internal(self, instrument, start_index, end_index, *args):
+        series = self.feature.load(instrument, start_index, end_index, *args)
+
+        if series.empty:
+            return series
+        else:
+            if self.func == "sum":
+                return getattr(series.resample(self.freq), self.func)(min_count=1)
+            else:
+                return getattr(series.resample(self.freq), self.func)()
+
+
+TOpsList = [TResample]
+OpsList = [
+    ChangeInstrument,
+    Rolling,
+    Ref,
+    Max,
+    Min,
+    Sum,
+    Mean,
+    Std,
+    Var,
+    Skew,
+    Kurt,
+    Med,
+    Mad,
+    Slope,
+    Rsquare,
+    Resi,
+    Rank,
+    Quantile,
+    Count,
+    EMA,
+    WMA,
+    Corr,
+    Cov,
+    Delta,
+    Abs,
+    Sign,
+    Log,
+    Power,
+    Add,
+    Sub,
+    Mul,
+    Div,
+    Greater,
+    Less,
+    And,
+    Or,
+    Not,
+    Gt,
+    Ge,
+    Lt,
+    Le,
+    Eq,
+    Ne,
+    Mask,
+    IdxMax,
+    IdxMin,
+    If,
+    Feature,
+    PFeature,
+] + [TResample]
+
+
+class OpsWrapper:
+    """Ops Wrapper"""
+
+    def __init__(self):
+        self._ops = {}
+
+    def reset(self):
+        self._ops = {}
+
+    def register(self, ops_list: List[Union[Type[ExpressionOps], dict]]):
+        """register operator
+
+        Parameters
+        ----------
+        ops_list : List[Union[Type[ExpressionOps], dict]]
+            - if type(ops_list) is List[Type[ExpressionOps]], each element of ops_list represents the operator class, which should be the subclass of `ExpressionOps`.
+            - if type(ops_list) is List[dict], each element of ops_list represents the config of operator, which has the following format:
+
+                .. code-block:: text
+
+                    {
+                        "class": class_name,
+                        "module_path": path,
+                    }
+
+                Note: `class` should be the class name of operator, `module_path` should be a python module or path of file.
+        """
+        for _operator in ops_list:
+            if isinstance(_operator, dict):
+                _ops_class, _ = get_callable_kwargs(_operator)
+            else:
+                _ops_class = _operator
+
+            if not issubclass(_ops_class, (Expression,)):
+                raise TypeError("operator must be subclass of ExpressionOps, not {}".format(_ops_class))
+
+            if _ops_class.__name__ in self._ops:
+                get_module_logger(self.__class__.__name__).warning(
+                    "The custom operator [{}] will override the qlib default definition".format(_ops_class.__name__)
+                )
+            self._ops[_ops_class.__name__] = _ops_class
+
+    def __getattr__(self, key):
+        if key not in self._ops:
+            raise AttributeError("The operator [{0}] is not registered".format(key))
+        return self._ops[key]
+
+
+Operators = OpsWrapper()
+
+
+def register_all_ops(C):
+    """register all operator"""
+    logger = get_module_logger("ops")
+
+    from qlib.data.pit import P, PRef  # pylint: disable=C0415
+
+    Operators.reset()
+    Operators.register(OpsList + [P, PRef])
+
+    if getattr(C, "custom_ops", None) is not None:
+        Operators.register(C.custom_ops)
+        logger.debug("register custom operator {}".format(C.custom_ops))
```

## qlib/data/pit.py

```diff
@@ -1,72 +1,71 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-"""
-Qlib follow the logic below to supporting point-in-time database
-
-For each stock, the format of its data is <observe_time, feature>. Expression Engine support calculation on such format of data
-
-To calculate the feature value f_t at a specific observe time t,  data with format <period_time, feature> will be used.
-For example, the average earning of last 4 quarters (period_time) on 20190719 (observe_time)
-
-The calculation of both <period_time, feature> and <observe_time, feature> data rely on expression engine. It consists of 2 phases.
-1) calculation <period_time, feature> at each observation time t and it will collasped into a point (just like a normal feature)
-2) concatenate all th collasped data, we will get data with format <observe_time, feature>.
-Qlib will use the operator `P` to perform the collapse.
-"""
-import numpy as np
-import pandas as pd
-from qlib.data.ops import ElemOperator
-from qlib.log import get_module_logger
-from .data import Cal
-
-
-class P(ElemOperator):
-    def _load_internal(self, instrument, start_index, end_index, freq):
-
-        _calendar = Cal.calendar(freq=freq)
-        resample_data = np.empty(end_index - start_index + 1, dtype="float32")
-
-        for cur_index in range(start_index, end_index + 1):
-            cur_time = _calendar[cur_index]
-            # To load expression accurately, more historical data are required
-            start_ws, end_ws = self.feature.get_extended_window_size()
-            if end_ws > 0:
-                raise ValueError(
-                    "PIT database does not support referring to future period (e.g. expressions like `Ref('$$roewa_q', -1)` are not supported"
-                )
-
-            # The calculated value will always the last element, so the end_offset is zero.
-            try:
-                s = self._load_feature(instrument, -start_ws, 0, cur_time)
-                resample_data[cur_index - start_index] = s.iloc[-1] if len(s) > 0 else np.nan
-            except FileNotFoundError:
-                get_module_logger("base").warning(f"WARN: period data not found for {str(self)}")
-                return pd.Series(dtype="float32", name=str(self))
-
-        resample_series = pd.Series(
-            resample_data, index=pd.RangeIndex(start_index, end_index + 1), dtype="float32", name=str(self)
-        )
-        return resample_series
-
-    def _load_feature(self, instrument, start_index, end_index, cur_time):
-        return self.feature.load(instrument, start_index, end_index, cur_time)
-
-    def get_longest_back_rolling(self):
-        # The period data will collapse as a normal feature. So no extending and looking back
-        return 0
-
-    def get_extended_window_size(self):
-        # The period data will collapse as a normal feature. So no extending and looking back
-        return 0, 0
-
-
-class PRef(P):
-    def __init__(self, feature, period):
-        super().__init__(feature)
-        self.period = period
-
-    def __str__(self):
-        return f"{super().__str__()}[{self.period}]"
-
-    def _load_feature(self, instrument, start_index, end_index, cur_time):
-        return self.feature.load(instrument, start_index, end_index, cur_time, self.period)
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+"""
+Qlib follow the logic below to supporting point-in-time database
+
+For each stock, the format of its data is <observe_time, feature>. Expression Engine support calculation on such format of data
+
+To calculate the feature value f_t at a specific observe time t,  data with format <period_time, feature> will be used.
+For example, the average earning of last 4 quarters (period_time) on 20190719 (observe_time)
+
+The calculation of both <period_time, feature> and <observe_time, feature> data rely on expression engine. It consists of 2 phases.
+1) calculation <period_time, feature> at each observation time t and it will collasped into a point (just like a normal feature)
+2) concatenate all th collasped data, we will get data with format <observe_time, feature>.
+Qlib will use the operator `P` to perform the collapse.
+"""
+import numpy as np
+import pandas as pd
+from qlib.data.ops import ElemOperator
+from qlib.log import get_module_logger
+from .data import Cal
+
+
+class P(ElemOperator):
+    def _load_internal(self, instrument, start_index, end_index, freq):
+        _calendar = Cal.calendar(freq=freq)
+        resample_data = np.empty(end_index - start_index + 1, dtype="float32")
+
+        for cur_index in range(start_index, end_index + 1):
+            cur_time = _calendar[cur_index]
+            # To load expression accurately, more historical data are required
+            start_ws, end_ws = self.feature.get_extended_window_size()
+            if end_ws > 0:
+                raise ValueError(
+                    "PIT database does not support referring to future period (e.g. expressions like `Ref('$$roewa_q', -1)` are not supported"
+                )
+
+            # The calculated value will always the last element, so the end_offset is zero.
+            try:
+                s = self._load_feature(instrument, -start_ws, 0, cur_time)
+                resample_data[cur_index - start_index] = s.iloc[-1] if len(s) > 0 else np.nan
+            except FileNotFoundError:
+                get_module_logger("base").warning(f"WARN: period data not found for {str(self)}")
+                return pd.Series(dtype="float32", name=str(self))
+
+        resample_series = pd.Series(
+            resample_data, index=pd.RangeIndex(start_index, end_index + 1), dtype="float32", name=str(self)
+        )
+        return resample_series
+
+    def _load_feature(self, instrument, start_index, end_index, cur_time):
+        return self.feature.load(instrument, start_index, end_index, cur_time)
+
+    def get_longest_back_rolling(self):
+        # The period data will collapse as a normal feature. So no extending and looking back
+        return 0
+
+    def get_extended_window_size(self):
+        # The period data will collapse as a normal feature. So no extending and looking back
+        return 0, 0
+
+
+class PRef(P):
+    def __init__(self, feature, period):
+        super().__init__(feature)
+        self.period = period
+
+    def __str__(self):
+        return f"{super().__str__()}[{self.period}]"
+
+    def _load_feature(self, instrument, start_index, end_index, cur_time):
+        return self.feature.load(instrument, start_index, end_index, cur_time, self.period)
```

## qlib/data/_libs/__init__.py

 * *Ordering differences only*

```diff
@@ -1,2 +1,2 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
```

## qlib/data/_libs/expanding.pyx

 * *Ordering differences only*

```diff
@@ -1,152 +1,152 @@
-# cython: profile=False
-# cython: boundscheck=False, wraparound=False, cdivision=True
-cimport cython
-cimport numpy as np
-import numpy as np
-
-from libc.math cimport sqrt, isnan, NAN
-from libcpp.vector cimport vector
-
-
-cdef class Expanding:
-    """1-D array expanding"""
-    cdef vector[double] barv
-    cdef int na_count
-    def __init__(self):
-        self.na_count = 0
-
-    cdef double update(self, double val):
-        pass
-
-
-cdef class Mean(Expanding):
-    """1-D array expanding mean"""
-    cdef double vsum
-    def __init__(self):
-        super(Mean, self).__init__()
-        self.vsum = 0
-
-    cdef double update(self, double val):
-        self.barv.push_back(val)
-        if isnan(val):
-            self.na_count += 1
-        else:
-            self.vsum += val
-        return self.vsum / (self.barv.size() - self.na_count)
-
-
-cdef class Slope(Expanding):
-    """1-D array expanding slope"""
-    cdef double x_sum
-    cdef double x2_sum
-    cdef double y_sum
-    cdef double xy_sum
-    def __init__(self):
-        super(Slope, self).__init__()
-        self.x_sum  = 0
-        self.x2_sum = 0
-        self.y_sum  = 0
-        self.xy_sum = 0
-
-    cdef double update(self, double val):
-        self.barv.push_back(val)
-        cdef size_t size = self.barv.size()
-        if isnan(val):
-            self.na_count += 1
-        else:
-            self.x_sum  += size
-            self.x2_sum += size * size
-            self.y_sum  += val
-            self.xy_sum += size * val
-        cdef int N = size - self.na_count
-        return (N*self.xy_sum - self.x_sum*self.y_sum) / \
-            (N*self.x2_sum - self.x_sum*self.x_sum)
-
-
-cdef class Resi(Expanding):
-    """1-D array expanding residuals"""
-    cdef double x_sum
-    cdef double x2_sum
-    cdef double y_sum
-    cdef double xy_sum
-    def __init__(self):
-        super(Resi, self).__init__()
-        self.x_sum  = 0
-        self.x2_sum = 0
-        self.y_sum  = 0
-        self.xy_sum = 0
-
-    cdef double update(self, double val):
-        self.barv.push_back(val)
-        cdef size_t size = self.barv.size()
-        if isnan(val):
-            self.na_count += 1
-        else:
-            self.x_sum  += size
-            self.x2_sum += size * size
-            self.y_sum  += val
-            self.xy_sum += size * val
-        cdef int N = size - self.na_count
-        slope = (N*self.xy_sum - self.x_sum*self.y_sum) / \
-                (N*self.x2_sum - self.x_sum*self.x_sum)
-        x_mean = self.x_sum / N
-        y_mean = self.y_sum / N
-        interp = y_mean - slope*x_mean
-        return val - (slope*size + interp)
-
-
-cdef class Rsquare(Expanding):
-    """1-D array expanding rsquare"""
-    cdef double x_sum
-    cdef double x2_sum
-    cdef double y_sum
-    cdef double y2_sum
-    cdef double xy_sum
-    def __init__(self):
-        super(Rsquare, self).__init__()
-        self.x_sum  = 0
-        self.x2_sum = 0
-        self.y_sum  = 0
-        self.y2_sum = 0
-        self.xy_sum = 0
-
-    cdef double update(self, double val):
-        self.barv.push_back(val)
-        cdef size_t size = self.barv.size()
-        if isnan(val):
-            self.na_count += 1
-        else:
-            self.x_sum  += size
-            self.x2_sum += size * size
-            self.y_sum  += val
-            self.y2_sum += val * val
-            self.xy_sum += size * val
-        cdef int N = size - self.na_count
-        cdef double rvalue = (N*self.xy_sum - self.x_sum*self.y_sum) / \
-            sqrt((N*self.x2_sum - self.x_sum*self.x_sum) * (N*self.y2_sum - self.y_sum*self.y_sum))
-        return rvalue * rvalue
-
-
-cdef np.ndarray[double, ndim=1] expanding(Expanding r, np.ndarray a):
-    cdef int  i
-    cdef int  N = len(a)
-    cdef np.ndarray[double, ndim=1] ret = np.empty(N)
-    for i in range(N):
-        ret[i] = r.update(a[i])
-    return ret
-
-def expanding_mean(np.ndarray a):
-    cdef Mean r = Mean()
-    return expanding(r, a)
-
-def expanding_slope(np.ndarray a):
-    cdef Slope r = Slope()
-    return expanding(r, a)
-
-def expanding_rsquare(np.ndarray a):
-    cdef Rsquare r = Rsquare()
-    return expanding(r, a)
-
-def expanding_resi(np.ndarray a):
-    cdef Resi r = Resi()
-    return expanding(r, a)
+# cython: profile=False
+# cython: boundscheck=False, wraparound=False, cdivision=True
+cimport cython
+cimport numpy as np
+import numpy as np
+
+from libc.math cimport sqrt, isnan, NAN
+from libcpp.vector cimport vector
+
+
+cdef class Expanding:
+    """1-D array expanding"""
+    cdef vector[double] barv
+    cdef int na_count
+    def __init__(self):
+        self.na_count = 0
+
+    cdef double update(self, double val):
+        pass
+
+
+cdef class Mean(Expanding):
+    """1-D array expanding mean"""
+    cdef double vsum
+    def __init__(self):
+        super(Mean, self).__init__()
+        self.vsum = 0
+
+    cdef double update(self, double val):
+        self.barv.push_back(val)
+        if isnan(val):
+            self.na_count += 1
+        else:
+            self.vsum += val
+        return self.vsum / (self.barv.size() - self.na_count)
+
+
+cdef class Slope(Expanding):
+    """1-D array expanding slope"""
+    cdef double x_sum
+    cdef double x2_sum
+    cdef double y_sum
+    cdef double xy_sum
+    def __init__(self):
+        super(Slope, self).__init__()
+        self.x_sum  = 0
+        self.x2_sum = 0
+        self.y_sum  = 0
+        self.xy_sum = 0
+
+    cdef double update(self, double val):
+        self.barv.push_back(val)
+        cdef size_t size = self.barv.size()
+        if isnan(val):
+            self.na_count += 1
+        else:
+            self.x_sum  += size
+            self.x2_sum += size * size
+            self.y_sum  += val
+            self.xy_sum += size * val
+        cdef int N = size - self.na_count
+        return (N*self.xy_sum - self.x_sum*self.y_sum) / \
+            (N*self.x2_sum - self.x_sum*self.x_sum)
+
+
+cdef class Resi(Expanding):
+    """1-D array expanding residuals"""
+    cdef double x_sum
+    cdef double x2_sum
+    cdef double y_sum
+    cdef double xy_sum
+    def __init__(self):
+        super(Resi, self).__init__()
+        self.x_sum  = 0
+        self.x2_sum = 0
+        self.y_sum  = 0
+        self.xy_sum = 0
+
+    cdef double update(self, double val):
+        self.barv.push_back(val)
+        cdef size_t size = self.barv.size()
+        if isnan(val):
+            self.na_count += 1
+        else:
+            self.x_sum  += size
+            self.x2_sum += size * size
+            self.y_sum  += val
+            self.xy_sum += size * val
+        cdef int N = size - self.na_count
+        slope = (N*self.xy_sum - self.x_sum*self.y_sum) / \
+                (N*self.x2_sum - self.x_sum*self.x_sum)
+        x_mean = self.x_sum / N
+        y_mean = self.y_sum / N
+        interp = y_mean - slope*x_mean
+        return val - (slope*size + interp)
+
+
+cdef class Rsquare(Expanding):
+    """1-D array expanding rsquare"""
+    cdef double x_sum
+    cdef double x2_sum
+    cdef double y_sum
+    cdef double y2_sum
+    cdef double xy_sum
+    def __init__(self):
+        super(Rsquare, self).__init__()
+        self.x_sum  = 0
+        self.x2_sum = 0
+        self.y_sum  = 0
+        self.y2_sum = 0
+        self.xy_sum = 0
+
+    cdef double update(self, double val):
+        self.barv.push_back(val)
+        cdef size_t size = self.barv.size()
+        if isnan(val):
+            self.na_count += 1
+        else:
+            self.x_sum  += size
+            self.x2_sum += size * size
+            self.y_sum  += val
+            self.y2_sum += val * val
+            self.xy_sum += size * val
+        cdef int N = size - self.na_count
+        cdef double rvalue = (N*self.xy_sum - self.x_sum*self.y_sum) / \
+            sqrt((N*self.x2_sum - self.x_sum*self.x_sum) * (N*self.y2_sum - self.y_sum*self.y_sum))
+        return rvalue * rvalue
+
+
+cdef np.ndarray[double, ndim=1] expanding(Expanding r, np.ndarray a):
+    cdef int  i
+    cdef int  N = len(a)
+    cdef np.ndarray[double, ndim=1] ret = np.empty(N)
+    for i in range(N):
+        ret[i] = r.update(a[i])
+    return ret
+
+def expanding_mean(np.ndarray a):
+    cdef Mean r = Mean()
+    return expanding(r, a)
+
+def expanding_slope(np.ndarray a):
+    cdef Slope r = Slope()
+    return expanding(r, a)
+
+def expanding_rsquare(np.ndarray a):
+    cdef Rsquare r = Rsquare()
+    return expanding(r, a)
+
+def expanding_resi(np.ndarray a):
+    cdef Resi r = Resi()
+    return expanding(r, a)
```

## qlib/data/_libs/rolling.pyx

 * *Ordering differences only*

```diff
@@ -1,207 +1,207 @@
-# cython: profile=False
-# cython: boundscheck=False, wraparound=False, cdivision=True
-cimport cython
-cimport numpy as np
-import numpy as np
-
-from libc.math cimport sqrt, isnan, NAN
-from libcpp.deque cimport deque
-
-
-cdef class Rolling:
-    """1-D array rolling"""
-    cdef int window
-    cdef deque[double] barv
-    cdef int na_count
-    def __init__(self, int window):
-        self.window = window
-        self.na_count = window
-        cdef int i
-        for i in range(window):
-            self.barv.push_back(NAN)
-
-    cdef double update(self, double val):
-        pass
-
-
-cdef class Mean(Rolling):
-    """1-D array rolling mean"""
-    cdef double vsum
-    def __init__(self, int window):
-        super(Mean, self).__init__(window)
-        self.vsum = 0
-        
-    cdef double update(self, double val):
-        self.barv.push_back(val)
-        if not isnan(self.barv.front()):
-            self.vsum -= self.barv.front()
-        else:
-            self.na_count -= 1
-        self.barv.pop_front()
-        if isnan(val):
-            self.na_count += 1
-            # return NAN
-        else:
-            self.vsum += val
-        return self.vsum / (self.window - self.na_count)
-
-
-cdef class Slope(Rolling):
-    """1-D array rolling slope"""
-    cdef double i_sum # can be used as i2_sum
-    cdef double x_sum
-    cdef double x2_sum
-    cdef double y_sum
-    cdef double xy_sum
-    def __init__(self, int window):
-        super(Slope, self).__init__(window)
-        self.i_sum  = 0
-        self.x_sum  = 0
-        self.x2_sum = 0
-        self.y_sum  = 0
-        self.xy_sum = 0
-
-    cdef double update(self, double val):
-        self.barv.push_back(val)
-        self.xy_sum = self.xy_sum - self.y_sum
-        self.x2_sum = self.x2_sum + self.i_sum - 2*self.x_sum
-        self.x_sum = self.x_sum - self.i_sum
-        cdef double _val
-        _val = self.barv.front()
-        if not isnan(_val):
-            self.i_sum -= 1
-            self.y_sum -= _val
-        else:
-            self.na_count -= 1
-        self.barv.pop_front()
-        if isnan(val):
-            self.na_count += 1
-            # return NAN
-        else:
-            self.i_sum  += 1
-            self.x_sum  += self.window
-            self.x2_sum += self.window * self.window
-            self.y_sum  += val
-            self.xy_sum += self.window * val
-        cdef int N = self.window - self.na_count
-        return (N*self.xy_sum - self.x_sum*self.y_sum) / \
-            (N*self.x2_sum - self.x_sum*self.x_sum)
-
-    
-cdef class Resi(Rolling):
-    """1-D array rolling residuals"""
-    cdef double i_sum # can be used as i2_sum
-    cdef double x_sum
-    cdef double x2_sum
-    cdef double y_sum
-    cdef double xy_sum
-    def __init__(self, int window):
-        super(Resi, self).__init__(window)
-        self.i_sum  = 0
-        self.x_sum  = 0
-        self.x2_sum = 0
-        self.y_sum  = 0
-        self.xy_sum = 0
-
-    cdef double update(self, double val):
-        self.barv.push_back(val)
-        self.xy_sum = self.xy_sum - self.y_sum
-        self.x2_sum = self.x2_sum + self.i_sum - 2*self.x_sum
-        self.x_sum = self.x_sum - self.i_sum
-        cdef double _val
-        _val = self.barv.front()
-        if not isnan(_val):
-            self.i_sum -= 1
-            self.y_sum -= _val
-        else:
-            self.na_count -= 1
-        self.barv.pop_front()
-        if isnan(val):
-            self.na_count += 1
-            # return NAN
-        else:
-            self.i_sum  += 1
-            self.x_sum  += self.window
-            self.x2_sum += self.window * self.window
-            self.y_sum  += val
-            self.xy_sum += self.window * val
-        cdef int N = self.window - self.na_count
-        slope = (N*self.xy_sum - self.x_sum*self.y_sum) / \
-                (N*self.x2_sum - self.x_sum*self.x_sum)
-        x_mean = self.x_sum / N
-        y_mean = self.y_sum / N
-        interp = y_mean - slope*x_mean
-        return val - (slope*self.window + interp)
-
-    
-cdef class Rsquare(Rolling):
-    """1-D array rolling rsquare"""
-    cdef double i_sum
-    cdef double x_sum
-    cdef double x2_sum
-    cdef double y_sum
-    cdef double y2_sum
-    cdef double xy_sum
-    def __init__(self, int window):
-        super(Rsquare, self).__init__(window)
-        self.i_sum  = 0
-        self.x_sum  = 0
-        self.x2_sum = 0
-        self.y_sum  = 0
-        self.y2_sum = 0
-        self.xy_sum = 0
-
-    cdef double update(self, double val):
-        self.barv.push_back(val)
-        self.xy_sum = self.xy_sum - self.y_sum
-        self.x2_sum = self.x2_sum + self.i_sum - 2*self.x_sum
-        self.x_sum = self.x_sum - self.i_sum
-        cdef double _val
-        _val = self.barv.front()
-        if not isnan(_val):
-            self.i_sum  -= 1
-            self.y_sum  -= _val
-            self.y2_sum -= _val * _val
-        else:
-            self.na_count -= 1
-        self.barv.pop_front()
-        if isnan(val):
-            self.na_count += 1
-            # return NAN
-        else:
-            self.i_sum  += 1
-            self.x_sum  += self.window
-            self.x2_sum += self.window * self.window
-            self.y_sum  += val
-            self.y2_sum += val * val
-            self.xy_sum += self.window * val
-        cdef int N = self.window - self.na_count
-        cdef double rvalue
-        rvalue = (N*self.xy_sum - self.x_sum*self.y_sum) / \
-            sqrt((N*self.x2_sum - self.x_sum*self.x_sum) * (N*self.y2_sum - self.y_sum*self.y_sum))
-        return rvalue * rvalue
-
-    
-cdef np.ndarray[double, ndim=1] rolling(Rolling r, np.ndarray a):
-    cdef int  i
-    cdef int  N = len(a)
-    cdef np.ndarray[double, ndim=1] ret = np.empty(N)
-    for i in range(N):
-        ret[i] = r.update(a[i])
-    return ret
-
-def rolling_mean(np.ndarray a, int window):
-    cdef Mean r = Mean(window)
-    return rolling(r, a)
-
-def rolling_slope(np.ndarray a, int window):
-    cdef Slope r = Slope(window)
-    return rolling(r, a)
-
-def rolling_rsquare(np.ndarray a, int window):
-    cdef Rsquare r = Rsquare(window)
-    return rolling(r, a)
-
-def rolling_resi(np.ndarray a, int window):
-    cdef Resi r = Resi(window)
-    return rolling(r, a)
+# cython: profile=False
+# cython: boundscheck=False, wraparound=False, cdivision=True
+cimport cython
+cimport numpy as np
+import numpy as np
+
+from libc.math cimport sqrt, isnan, NAN
+from libcpp.deque cimport deque
+
+
+cdef class Rolling:
+    """1-D array rolling"""
+    cdef int window
+    cdef deque[double] barv
+    cdef int na_count
+    def __init__(self, int window):
+        self.window = window
+        self.na_count = window
+        cdef int i
+        for i in range(window):
+            self.barv.push_back(NAN)
+
+    cdef double update(self, double val):
+        pass
+
+
+cdef class Mean(Rolling):
+    """1-D array rolling mean"""
+    cdef double vsum
+    def __init__(self, int window):
+        super(Mean, self).__init__(window)
+        self.vsum = 0
+        
+    cdef double update(self, double val):
+        self.barv.push_back(val)
+        if not isnan(self.barv.front()):
+            self.vsum -= self.barv.front()
+        else:
+            self.na_count -= 1
+        self.barv.pop_front()
+        if isnan(val):
+            self.na_count += 1
+            # return NAN
+        else:
+            self.vsum += val
+        return self.vsum / (self.window - self.na_count)
+
+
+cdef class Slope(Rolling):
+    """1-D array rolling slope"""
+    cdef double i_sum # can be used as i2_sum
+    cdef double x_sum
+    cdef double x2_sum
+    cdef double y_sum
+    cdef double xy_sum
+    def __init__(self, int window):
+        super(Slope, self).__init__(window)
+        self.i_sum  = 0
+        self.x_sum  = 0
+        self.x2_sum = 0
+        self.y_sum  = 0
+        self.xy_sum = 0
+
+    cdef double update(self, double val):
+        self.barv.push_back(val)
+        self.xy_sum = self.xy_sum - self.y_sum
+        self.x2_sum = self.x2_sum + self.i_sum - 2*self.x_sum
+        self.x_sum = self.x_sum - self.i_sum
+        cdef double _val
+        _val = self.barv.front()
+        if not isnan(_val):
+            self.i_sum -= 1
+            self.y_sum -= _val
+        else:
+            self.na_count -= 1
+        self.barv.pop_front()
+        if isnan(val):
+            self.na_count += 1
+            # return NAN
+        else:
+            self.i_sum  += 1
+            self.x_sum  += self.window
+            self.x2_sum += self.window * self.window
+            self.y_sum  += val
+            self.xy_sum += self.window * val
+        cdef int N = self.window - self.na_count
+        return (N*self.xy_sum - self.x_sum*self.y_sum) / \
+            (N*self.x2_sum - self.x_sum*self.x_sum)
+
+    
+cdef class Resi(Rolling):
+    """1-D array rolling residuals"""
+    cdef double i_sum # can be used as i2_sum
+    cdef double x_sum
+    cdef double x2_sum
+    cdef double y_sum
+    cdef double xy_sum
+    def __init__(self, int window):
+        super(Resi, self).__init__(window)
+        self.i_sum  = 0
+        self.x_sum  = 0
+        self.x2_sum = 0
+        self.y_sum  = 0
+        self.xy_sum = 0
+
+    cdef double update(self, double val):
+        self.barv.push_back(val)
+        self.xy_sum = self.xy_sum - self.y_sum
+        self.x2_sum = self.x2_sum + self.i_sum - 2*self.x_sum
+        self.x_sum = self.x_sum - self.i_sum
+        cdef double _val
+        _val = self.barv.front()
+        if not isnan(_val):
+            self.i_sum -= 1
+            self.y_sum -= _val
+        else:
+            self.na_count -= 1
+        self.barv.pop_front()
+        if isnan(val):
+            self.na_count += 1
+            # return NAN
+        else:
+            self.i_sum  += 1
+            self.x_sum  += self.window
+            self.x2_sum += self.window * self.window
+            self.y_sum  += val
+            self.xy_sum += self.window * val
+        cdef int N = self.window - self.na_count
+        slope = (N*self.xy_sum - self.x_sum*self.y_sum) / \
+                (N*self.x2_sum - self.x_sum*self.x_sum)
+        x_mean = self.x_sum / N
+        y_mean = self.y_sum / N
+        interp = y_mean - slope*x_mean
+        return val - (slope*self.window + interp)
+
+    
+cdef class Rsquare(Rolling):
+    """1-D array rolling rsquare"""
+    cdef double i_sum
+    cdef double x_sum
+    cdef double x2_sum
+    cdef double y_sum
+    cdef double y2_sum
+    cdef double xy_sum
+    def __init__(self, int window):
+        super(Rsquare, self).__init__(window)
+        self.i_sum  = 0
+        self.x_sum  = 0
+        self.x2_sum = 0
+        self.y_sum  = 0
+        self.y2_sum = 0
+        self.xy_sum = 0
+
+    cdef double update(self, double val):
+        self.barv.push_back(val)
+        self.xy_sum = self.xy_sum - self.y_sum
+        self.x2_sum = self.x2_sum + self.i_sum - 2*self.x_sum
+        self.x_sum = self.x_sum - self.i_sum
+        cdef double _val
+        _val = self.barv.front()
+        if not isnan(_val):
+            self.i_sum  -= 1
+            self.y_sum  -= _val
+            self.y2_sum -= _val * _val
+        else:
+            self.na_count -= 1
+        self.barv.pop_front()
+        if isnan(val):
+            self.na_count += 1
+            # return NAN
+        else:
+            self.i_sum  += 1
+            self.x_sum  += self.window
+            self.x2_sum += self.window * self.window
+            self.y_sum  += val
+            self.y2_sum += val * val
+            self.xy_sum += self.window * val
+        cdef int N = self.window - self.na_count
+        cdef double rvalue
+        rvalue = (N*self.xy_sum - self.x_sum*self.y_sum) / \
+            sqrt((N*self.x2_sum - self.x_sum*self.x_sum) * (N*self.y2_sum - self.y_sum*self.y_sum))
+        return rvalue * rvalue
+
+    
+cdef np.ndarray[double, ndim=1] rolling(Rolling r, np.ndarray a):
+    cdef int  i
+    cdef int  N = len(a)
+    cdef np.ndarray[double, ndim=1] ret = np.empty(N)
+    for i in range(N):
+        ret[i] = r.update(a[i])
+    return ret
+
+def rolling_mean(np.ndarray a, int window):
+    cdef Mean r = Mean(window)
+    return rolling(r, a)
+
+def rolling_slope(np.ndarray a, int window):
+    cdef Slope r = Slope(window)
+    return rolling(r, a)
+
+def rolling_rsquare(np.ndarray a, int window):
+    cdef Rsquare r = Rsquare(window)
+    return rolling(r, a)
+
+def rolling_resi(np.ndarray a, int window):
+    cdef Resi r = Resi(window)
+    return rolling(r, a)
```

## qlib/data/dataset/__init__.py

 * *Ordering differences only*

```diff
@@ -1,722 +1,722 @@
-from ...utils.serial import Serializable
-from typing import Callable, Union, List, Tuple, Dict, Text, Optional
-from ...utils import init_instance_by_config, np_ffill, time_to_slc_point
-from ...log import get_module_logger
-from .handler import DataHandler, DataHandlerLP
-from copy import copy, deepcopy
-from inspect import getfullargspec
-import pandas as pd
-import numpy as np
-import bisect
-from ...utils import lazy_sort_index
-from .utils import get_level_index
-
-
-class Dataset(Serializable):
-    """
-    Preparing data for model training and inferencing.
-    """
-
-    def __init__(self, **kwargs):
-        """
-        init is designed to finish following steps:
-
-        - init the sub instance and the state of the dataset(info to prepare the data)
-            - The name of essential state for preparing data should not start with '_' so that it could be serialized on disk when serializing.
-
-        - setup data
-            - The data related attributes' names should start with '_' so that it will not be saved on disk when serializing.
-
-        The data could specify the info to calculate the essential data for preparation
-        """
-        self.setup_data(**kwargs)
-        super().__init__()
-
-    def config(self, **kwargs):
-        """
-        config is designed to configure and parameters that cannot be learned from the data
-        """
-        super().config(**kwargs)
-
-    def setup_data(self, **kwargs):
-        """
-        Setup the data.
-
-        We split the setup_data function for following situation:
-
-        - User have a Dataset object with learned status on disk.
-
-        - User load the Dataset object from the disk.
-
-        - User call `setup_data` to load new data.
-
-        - User prepare data for model based on previous status.
-        """
-
-    def prepare(self, **kwargs) -> object:
-        """
-        The type of dataset depends on the model. (It could be pd.DataFrame, pytorch.DataLoader, etc.)
-        The parameters should specify the scope for the prepared data
-        The method should:
-        - process the data
-
-        - return the processed data
-
-        Returns
-        -------
-        object:
-            return the object
-        """
-
-
-class DatasetH(Dataset):
-    """
-    Dataset with Data(H)andler
-
-    User should try to put the data preprocessing functions into handler.
-    Only following data processing functions should be placed in Dataset:
-
-    - The processing is related to specific model.
-
-    - The processing is related to data split.
-    """
-
-    def __init__(
-        self,
-        handler: Union[Dict, DataHandler],
-        segments: Dict[Text, Tuple],
-        fetch_kwargs: Dict = {},
-        **kwargs,
-    ):
-        """
-        Setup the underlying data.
-
-        Parameters
-        ----------
-        handler : Union[dict, DataHandler]
-            handler could be:
-
-            - instance of `DataHandler`
-
-            - config of `DataHandler`.  Please refer to `DataHandler`
-
-        segments : dict
-            Describe the options to segment the data.
-            Here are some examples:
-
-            .. code-block::
-
-                1) 'segments': {
-                        'train': ("2008-01-01", "2014-12-31"),
-                        'valid': ("2017-01-01", "2020-08-01",),
-                        'test': ("2015-01-01", "2016-12-31",),
-                    }
-                2) 'segments': {
-                        'insample': ("2008-01-01", "2014-12-31"),
-                        'outsample': ("2017-01-01", "2020-08-01",),
-                    }
-        """
-        self.handler: DataHandler = init_instance_by_config(handler, accept_types=DataHandler)
-        self.segments = segments.copy()
-        self.fetch_kwargs = copy(fetch_kwargs)
-        super().__init__(**kwargs)
-
-    def config(self, handler_kwargs: dict = None, **kwargs):
-        """
-        Initialize the DatasetH
-
-        Parameters
-        ----------
-        handler_kwargs : dict
-            Config of DataHandler, which could include the following arguments:
-
-            - arguments of DataHandler.conf_data, such as 'instruments', 'start_time' and 'end_time'.
-
-        kwargs : dict
-            Config of DatasetH, such as
-
-            - segments : dict
-                Config of segments which is same as 'segments' in self.__init__
-
-        """
-        if handler_kwargs is not None:
-            self.handler.config(**handler_kwargs)
-        if "segments" in kwargs:
-            self.segments = deepcopy(kwargs.pop("segments"))
-        super().config(**kwargs)
-
-    def setup_data(self, handler_kwargs: dict = None, **kwargs):
-        """
-        Setup the Data
-
-        Parameters
-        ----------
-        handler_kwargs : dict
-            init arguments of DataHandler, which could include the following arguments:
-
-            - init_type : Init Type of Handler
-
-            - enable_cache : whether to enable cache
-
-        """
-        super().setup_data(**kwargs)
-        if handler_kwargs is not None:
-            self.handler.setup_data(**handler_kwargs)
-
-    def __repr__(self):
-        return "{name}(handler={handler}, segments={segments})".format(
-            name=self.__class__.__name__, handler=self.handler, segments=self.segments
-        )
-
-    def _prepare_seg(self, slc, **kwargs):
-        """
-        Give a query, retrieve the according data
-
-        Parameters
-        ----------
-        slc : please refer to the docs of `prepare`
-                NOTE: it may not be an instance of slice. It may be a segment of `segments` from `def prepare`
-        """
-        if hasattr(self, "fetch_kwargs"):
-            return self.handler.fetch(slc, **kwargs, **self.fetch_kwargs)
-        else:
-            return self.handler.fetch(slc, **kwargs)
-
-    def prepare(
-        self,
-        segments: Union[List[Text], Tuple[Text], Text, slice, pd.Index],
-        col_set=DataHandler.CS_ALL,
-        data_key=DataHandlerLP.DK_I,
-        **kwargs,
-    ) -> Union[List[pd.DataFrame], pd.DataFrame]:
-        """
-        Prepare the data for learning and inference.
-
-        Parameters
-        ----------
-        segments : Union[List[Text], Tuple[Text], Text, slice]
-            Describe the scope of the data to be prepared
-            Here are some examples:
-
-            - 'train'
-
-            - ['train', 'valid']
-
-        col_set : str
-            The col_set will be passed to self.handler when fetching data.
-            TODO: make it automatic:
-
-            - select DK_I for test data
-            - select DK_L for training data.
-        data_key : str
-            The data to fetch:  DK_*
-            Default is DK_I, which indicate fetching data for **inference**.
-
-        kwargs :
-            The parameters that kwargs may contain:
-                flt_col : str
-                    It only exists in TSDatasetH, can be used to add a column of data(True or False) to filter data.
-                    This parameter is only supported when it is an instance of TSDatasetH.
-
-        Returns
-        -------
-        Union[List[pd.DataFrame], pd.DataFrame]:
-
-        Raises
-        ------
-        NotImplementedError:
-        """
-        logger = get_module_logger("DatasetH")
-        seg_kwargs = {"col_set": col_set}
-        seg_kwargs.update(kwargs)
-        if "data_key" in getfullargspec(self.handler.fetch).args:
-            seg_kwargs["data_key"] = data_key
-        else:
-            logger.info(f"data_key[{data_key}] is ignored.")
-
-        # Conflictions may happen here
-        # - The fetched data and the segment key may both be string
-        # To resolve the confliction
-        # - The segment name will have higher priorities
-
-        # 1) Use it as segment name first
-        if isinstance(segments, str) and segments in self.segments:
-            return self._prepare_seg(self.segments[segments], **seg_kwargs)
-
-        if isinstance(segments, (list, tuple)) and all(seg in self.segments for seg in segments):
-            return [self._prepare_seg(self.segments[seg], **seg_kwargs) for seg in segments]
-
-        # 2) Use pass it directly to prepare a single seg
-        return self._prepare_seg(segments, **seg_kwargs)
-
-    # helper functions
-    @staticmethod
-    def get_min_time(segments):
-        return DatasetH._get_extrema(segments, 0, (lambda a, b: a > b))
-
-    @staticmethod
-    def get_max_time(segments):
-        return DatasetH._get_extrema(segments, 1, (lambda a, b: a < b))
-
-    @staticmethod
-    def _get_extrema(segments, idx: int, cmp: Callable, key_func=pd.Timestamp):
-        """it will act like sort and return the max value or None"""
-        candidate = None
-        for k, seg in segments.items():
-            point = seg[idx]
-            if point is None:
-                # None indicates unbounded, return directly
-                return None
-            elif candidate is None or cmp(key_func(candidate), key_func(point)):
-                candidate = point
-        return candidate
-
-
-class TSDataSampler:
-    """
-    (T)ime-(S)eries DataSampler
-    This is the result of TSDatasetH
-
-    It works like `torch.data.utils.Dataset`, it provides a very convenient interface for constructing time-series
-    dataset based on tabular data.
-    - On time step dimension, the smaller index indicates the historical data and the larger index indicates the future
-      data.
-
-    If user have further requirements for processing data, user could process them based on `TSDataSampler` or create
-    more powerful subclasses.
-
-    Known Issues:
-    - For performance issues, this Sampler will convert dataframe into arrays for better performance. This could result
-      in a different data type
-
-
-    Indices design:
-        TSDataSampler has a index mechanism to help users query time-series data efficiently.
-
-        The definition of related variables:
-            data_arr: np.ndarray
-                The original data. it will contains all the original data.
-                The querying are often for time-series of a specific stock.
-                By leveraging this data charactoristics to speed up querying, the multi-index of data_arr is rearranged in (instrument, datetime) order
-
-            data_index: pd.MultiIndex with index order <instrument, datetime>
-                it has the same shape with `idx_map`. Each elements of them are expected to be aligned.
-
-            idx_map: np.ndarray
-                It is the indexable data. It originates from data_arr, and then filtered by 1) `start` and `end`  2) `flt_data`
-                    The extra data in data_arr is useful in following cases
-                    1) creating meaningful time series data before `start` instead of padding them with zeros
-                    2) some data are excluded by `flt_data` (e.g. no <X, y> sample pair for that index). but they are still used in time-series in X
-
-                Finnally, it will look like.
-
-                array([[  0,   0],
-                       [  1,   0],
-                       [  2,   0],
-                       ...,
-                       [241, 348],
-                       [242, 348],
-                       [243, 348]], dtype=int32)
-
-                It list all indexable data(some data only used in historical time series data may not be indexabla), the values are the corresponding row and col in idx_df
-            idx_df: pd.DataFrame
-                It aims to map the <datetime, instrument> key to the original position in data_arr
-
-                For example, it may look like (NOTE: the index for a instrument time-series is continoues in memory)
-
-                    instrument SH600000 SH600008 SH600009 SH600010 SH600011 SH600015  ...
-                    datetime
-                    2017-01-03        0      242      473      717      NaN      974  ...
-                    2017-01-04        1      243      474      718      NaN      975  ...
-                    2017-01-05        2      244      475      719      NaN      976  ...
-                    2017-01-06        3      245      476      720      NaN      977  ...
-
-            With these two indices(idx_map, idx_df) and original data(data_arr), we can make the following queries fast (implemented in __getitem__)
-            (1) Get the i-th indexable sample(time-series):   (indexable sample index) -> [idx_map] -> (row col) -> [idx_df] -> (index in data_arr)
-            (2) Get the specific sample by <datetime, instrument>:  (<datetime, instrument>, i.e. <row, col>) -> [idx_df] -> (index in data_arr)
-            (3) Get the index of a time-series data:   (get the <row, col>, refer to (1), (2)) -> [idx_df] -> (all indices in data_arr for time-series)
-    """
-
-    # Please refer to the docstring of TSDataSampler for the definition of following attributes
-    data_arr: np.ndarray
-    data_index: pd.MultiIndex
-    idx_map: np.ndarray
-    idx_df: pd.DataFrame
-
-    def __init__(
-        self,
-        data: pd.DataFrame,
-        start,
-        end,
-        step_len: int,
-        fillna_type: str = "none",
-        dtype=None,
-        flt_data=None,
-    ):
-        """
-        Build a dataset which looks like torch.data.utils.Dataset.
-
-        Parameters
-        ----------
-        data : pd.DataFrame
-            The raw tabular data whose index order is <"datetime", "instrument">
-        start :
-            The indexable start time
-        end :
-            The indexable end time
-        step_len : int
-            The length of the time-series step
-        fillna_type : int
-            How will qlib handle the sample if there is on sample in a specific date.
-            none:
-                fill with np.nan
-            ffill:
-                ffill with previous sample
-            ffill+bfill:
-                ffill with previous samples first and fill with later samples second
-        flt_data : pd.Series
-            a column of data(True or False) to filter data. Its index order is <"datetime", "instrument">
-            None:
-                kepp all data
-
-        """
-        self.start = start
-        self.end = end
-        self.step_len = step_len
-        self.fillna_type = fillna_type
-        assert get_level_index(data, "datetime") == 0
-        self.data = data.swaplevel().sort_index().copy()
-        data.drop(
-            data.columns, axis=1, inplace=True
-        )  # data is useless since it's passed to a transposed one, hard code to free the memory of this dataframe to avoid three big dataframe in the memory(including: data, self.data, self.data_arr)
-
-        kwargs = {"object": self.data}
-        if dtype is not None:
-            kwargs["dtype"] = dtype
-
-        self.data_arr = np.array(**kwargs)  # Get index from numpy.array will much faster than DataFrame.values!
-        # NOTE:
-        # - append last line with full NaN for better performance in `__getitem__`
-        # - Keep the same dtype will result in a better performance
-        self.data_arr = np.append(
-            self.data_arr,
-            np.full((1, self.data_arr.shape[1]), np.nan, dtype=self.data_arr.dtype),
-            axis=0,
-        )
-        self.nan_idx = -1  # The last line is all NaN
-
-        # the data type will be changed
-        # The index of usable data is between start_idx and end_idx
-        self.idx_df, self.idx_map = self.build_index(self.data)
-        self.data_index = deepcopy(self.data.index)
-
-        if flt_data is not None:
-            if isinstance(flt_data, pd.DataFrame):
-                assert len(flt_data.columns) == 1
-                flt_data = flt_data.iloc[:, 0]
-            # NOTE: bool(np.nan) is True !!!!!!!!
-            # make sure reindex comes first. Otherwise extra NaN may appear.
-            flt_data = flt_data.swaplevel()
-            flt_data = flt_data.reindex(self.data_index).fillna(False).astype(bool)
-            self.flt_data = flt_data.values
-            self.idx_map = self.flt_idx_map(self.flt_data, self.idx_map)
-            self.data_index = self.data_index[np.where(self.flt_data)[0]]
-        self.idx_map = self.idx_map2arr(self.idx_map)
-        self.idx_map, self.data_index = self.slice_idx_map_and_data_index(
-            self.idx_map, self.idx_df, self.data_index, start, end
-        )
-
-        self.idx_arr = np.array(self.idx_df.values, dtype=np.float64)  # for better performance
-        del self.data  # save memory
-
-    @staticmethod
-    def slice_idx_map_and_data_index(
-        idx_map,
-        idx_df,
-        data_index,
-        start,
-        end,
-    ):
-        assert (
-            len(idx_map) == data_index.shape[0]
-        )  # make sure idx_map and data_index is same so index of idx_map can be used on data_index
-
-        start_row_idx, end_row_idx = idx_df.index.slice_locs(start=time_to_slc_point(start), end=time_to_slc_point(end))
-
-        time_flter_idx = (idx_map[:, 0] < end_row_idx) & (idx_map[:, 0] >= start_row_idx)
-        return idx_map[time_flter_idx], data_index[time_flter_idx]
-
-    @staticmethod
-    def idx_map2arr(idx_map):
-        # pytorch data sampler will have better memory control without large dict or list
-        # - https://github.com/pytorch/pytorch/issues/13243
-        # - https://github.com/airctic/icevision/issues/613
-        # So we convert the dict into int array.
-        # The arr_map is expected to behave the same as idx_map
-
-        dtype = np.int32
-        # set a index out of bound to indicate the none existing
-        no_existing_idx = (np.iinfo(dtype).max, np.iinfo(dtype).max)
-
-        max_idx = max(idx_map.keys())
-        arr_map = []
-        for i in range(max_idx + 1):
-            arr_map.append(idx_map.get(i, no_existing_idx))
-        arr_map = np.array(arr_map, dtype=dtype)
-        return arr_map
-
-    @staticmethod
-    def flt_idx_map(flt_data, idx_map):
-        idx = 0
-        new_idx_map = {}
-        for i, exist in enumerate(flt_data):
-            if exist:
-                new_idx_map[idx] = idx_map[i]
-                idx += 1
-        return new_idx_map
-
-    def get_index(self):
-        """
-        Get the pandas index of the data, it will be useful in following scenarios
-        - Special sampler will be used (e.g. user want to sample day by day)
-        """
-        return self.data_index.swaplevel()  # to align the order of multiple index of original data received by __init__
-
-    def config(self, **kwargs):
-        # Config the attributes
-        for k, v in kwargs.items():
-            setattr(self, k, v)
-
-    @staticmethod
-    def build_index(data: pd.DataFrame) -> Tuple[pd.DataFrame, dict]:
-        """
-        The relation of the data
-
-        Parameters
-        ----------
-        data : pd.DataFrame
-            A DataFrame with index in order <instrument, datetime>
-
-                                      RSQR5     RESI5     WVMA5    LABEL0
-            instrument datetime
-            SH600000   2017-01-03  0.016389  0.461632 -1.154788 -0.048056
-                       2017-01-04  0.884545 -0.110597 -1.059332 -0.030139
-                       2017-01-05  0.507540 -0.535493 -1.099665 -0.644983
-                       2017-01-06 -1.267771 -0.669685 -1.636733  0.295366
-                       2017-01-09  0.339346  0.074317 -0.984989  0.765540
-
-        Returns
-        -------
-        Tuple[pd.DataFrame, dict]:
-            1) the first element:  reshape the original index into a <datetime(row), instrument(column)> 2D dataframe
-                instrument SH600000 SH600008 SH600009 SH600010 SH600011 SH600015  ...
-                datetime
-                2017-01-03        0      242      473      717      NaN      974  ...
-                2017-01-04        1      243      474      718      NaN      975  ...
-                2017-01-05        2      244      475      719      NaN      976  ...
-                2017-01-06        3      245      476      720      NaN      977  ...
-            2) the second element:  {<original index>: <row, col>}
-        """
-        # object incase of pandas converting int to float
-        idx_df = pd.Series(range(data.shape[0]), index=data.index, dtype=object)
-        idx_df = lazy_sort_index(idx_df.unstack())
-        # NOTE: the correctness of `__getitem__` depends on columns sorted here
-        idx_df = lazy_sort_index(idx_df, axis=1).T
-
-        idx_map = {}
-        for i, (_, row) in enumerate(idx_df.iterrows()):
-            for j, real_idx in enumerate(row):
-                if not np.isnan(real_idx):
-                    idx_map[real_idx] = (i, j)
-        return idx_df, idx_map
-
-    @property
-    def empty(self):
-        return len(self) == 0
-
-    def _get_indices(self, row: int, col: int) -> np.array:
-        """
-        get series indices of self.data_arr from the row, col indices of self.idx_df
-
-        Parameters
-        ----------
-        row : int
-            the row in self.idx_df
-        col : int
-            the col in self.idx_df
-
-        Returns
-        -------
-        np.array:
-            The indices of data of the data
-        """
-        indices = self.idx_arr[max(row - self.step_len + 1, 0) : row + 1, col]
-
-        if len(indices) < self.step_len:
-            indices = np.concatenate([np.full((self.step_len - len(indices),), np.nan), indices])
-
-        if self.fillna_type == "ffill":
-            indices = np_ffill(indices)
-        elif self.fillna_type == "ffill+bfill":
-            indices = np_ffill(np_ffill(indices)[::-1])[::-1]
-        else:
-            assert self.fillna_type == "none"
-        return indices
-
-    def _get_row_col(self, idx) -> Tuple[int]:
-        """
-        get the col index and row index of a given sample index in self.idx_df
-
-        Parameters
-        ----------
-        idx :
-            the input of  `__getitem__`
-
-        Returns
-        -------
-        Tuple[int]:
-            the row and col index
-        """
-        # The the right row number `i` and col number `j` in idx_df
-        if isinstance(idx, (int, np.integer)):
-            real_idx = idx
-            if 0 <= real_idx < len(self.idx_map):
-                i, j = self.idx_map[real_idx]  # TODO: The performance of this line is not good
-            else:
-                raise KeyError(f"{real_idx} is out of [0, {len(self.idx_map)})")
-        elif isinstance(idx, tuple):
-            # <TSDataSampler object>["datetime", "instruments"]
-            date, inst = idx
-            date = pd.Timestamp(date)
-            i = bisect.bisect_right(self.idx_df.index, date) - 1
-            # NOTE: This relies on the idx_df columns sorted in `__init__`
-            j = bisect.bisect_left(self.idx_df.columns, inst)
-        else:
-            raise NotImplementedError(f"This type of input is not supported")
-        return i, j
-
-    def __getitem__(self, idx: Union[int, Tuple[object, str], List[int]]):
-        """
-        # We have two method to get the time-series of a sample
-        tsds is a instance of TSDataSampler
-
-        # 1) sample by int index directly
-        tsds[len(tsds) - 1]
-
-        # 2) sample by <datetime,instrument> index
-        tsds['2016-12-31', "SZ300315"]
-
-        # The return value will be similar to the data retrieved by following code
-        df.loc(axis=0)['2015-01-01':'2016-12-31', "SZ300315"].iloc[-30:]
-
-        Parameters
-        ----------
-        idx : Union[int, Tuple[object, str]]
-        """
-        # Multi-index type
-        mtit = (list, np.ndarray)
-        if isinstance(idx, mtit):
-            indices = [self._get_indices(*self._get_row_col(i)) for i in idx]
-            indices = np.concatenate(indices)
-        else:
-            indices = self._get_indices(*self._get_row_col(idx))
-
-        # 1) for better performance, use the last nan line for padding the lost date
-        # 2) In case of precision problems. We use np.float64. # TODO: I'm not sure if whether np.float64 will result in
-        # precision problems. It will not cause any problems in my tests at least
-        indices = np.nan_to_num(indices.astype(np.float64), nan=self.nan_idx).astype(int)
-
-        if (np.diff(indices) == 1).all():  # slicing instead of indexing for speeding up.
-            data = self.data_arr[indices[0] : indices[-1] + 1]
-        else:
-            data = self.data_arr[indices]
-        if isinstance(idx, mtit):
-            # if we get multiple indexes, addition dimension should be added.
-            # <sample_idx, step_idx, feature_idx>
-            data = data.reshape(-1, self.step_len, *data.shape[1:])
-        return data
-
-    def __len__(self):
-        return len(self.idx_map)
-
-
-class TSDatasetH(DatasetH):
-    """
-    (T)ime-(S)eries Dataset (H)andler
-
-
-    Convert the tabular data to Time-Series data
-
-    Requirements analysis
-
-    The typical workflow of a user to get time-series data for an sample
-    - process features
-    - slice proper data from data handler:  dimension of sample <feature, >
-    - Build relation of samples by <time, instrument> index
-        - Be able to sample times series of data <timestep, feature>
-        - It will be better if the interface is like "torch.utils.data.Dataset"
-    - User could build customized batch based on the data
-        - The dimension of a batch of data <batch_idx, feature, timestep>
-    """
-
-    DEFAULT_STEP_LEN = 30
-
-    def __init__(self, step_len=DEFAULT_STEP_LEN, **kwargs):
-        self.step_len = step_len
-        super().__init__(**kwargs)
-
-    def config(self, **kwargs):
-        if "step_len" in kwargs:
-            self.step_len = kwargs.pop("step_len")
-        super().config(**kwargs)
-
-    def setup_data(self, **kwargs):
-        super().setup_data(**kwargs)
-        # make sure the calendar is updated to latest when loading data from new config
-        cal = self.handler.fetch(col_set=self.handler.CS_RAW).index.get_level_values("datetime").unique()
-        self.cal = sorted(cal)
-
-    @staticmethod
-    def _extend_slice(slc: slice, cal: list, step_len: int) -> slice:
-        # Dataset decide how to slice data(Get more data for timeseries).
-        start, end = slc.start, slc.stop
-        start_idx = bisect.bisect_left(cal, pd.Timestamp(start))
-        pad_start_idx = max(0, start_idx - step_len)
-        pad_start = cal[pad_start_idx]
-        return slice(pad_start, end)
-
-    def _prepare_seg(self, slc: slice, **kwargs) -> TSDataSampler:
-        """
-        split the _prepare_raw_seg is to leave a hook for data preprocessing before creating processing data
-        NOTE: TSDatasetH only support slc segment on datetime !!!
-        """
-        dtype = kwargs.pop("dtype", None)
-        if not isinstance(slc, slice):
-            slc = slice(*slc)
-        start, end = slc.start, slc.stop
-        flt_col = kwargs.pop("flt_col", None)
-        # TSDatasetH will retrieve more data for complete time-series
-
-        ext_slice = self._extend_slice(slc, self.cal, self.step_len)
-        data = super()._prepare_seg(ext_slice, **kwargs)
-
-        flt_kwargs = deepcopy(kwargs)
-        if flt_col is not None:
-            flt_kwargs["col_set"] = flt_col
-            flt_data = super()._prepare_seg(ext_slice, **flt_kwargs)
-            assert len(flt_data.columns) == 1
-        else:
-            flt_data = None
-
-        tsds = TSDataSampler(
-            data=data,
-            start=start,
-            end=end,
-            step_len=self.step_len,
-            dtype=dtype,
-            flt_data=flt_data,
-        )
-        return tsds
-
-
-__all__ = ["Optional", "Dataset", "DatasetH"]
+from ...utils.serial import Serializable
+from typing import Callable, Union, List, Tuple, Dict, Text, Optional
+from ...utils import init_instance_by_config, np_ffill, time_to_slc_point
+from ...log import get_module_logger
+from .handler import DataHandler, DataHandlerLP
+from copy import copy, deepcopy
+from inspect import getfullargspec
+import pandas as pd
+import numpy as np
+import bisect
+from ...utils import lazy_sort_index
+from .utils import get_level_index
+
+
+class Dataset(Serializable):
+    """
+    Preparing data for model training and inferencing.
+    """
+
+    def __init__(self, **kwargs):
+        """
+        init is designed to finish following steps:
+
+        - init the sub instance and the state of the dataset(info to prepare the data)
+            - The name of essential state for preparing data should not start with '_' so that it could be serialized on disk when serializing.
+
+        - setup data
+            - The data related attributes' names should start with '_' so that it will not be saved on disk when serializing.
+
+        The data could specify the info to calculate the essential data for preparation
+        """
+        self.setup_data(**kwargs)
+        super().__init__()
+
+    def config(self, **kwargs):
+        """
+        config is designed to configure and parameters that cannot be learned from the data
+        """
+        super().config(**kwargs)
+
+    def setup_data(self, **kwargs):
+        """
+        Setup the data.
+
+        We split the setup_data function for following situation:
+
+        - User have a Dataset object with learned status on disk.
+
+        - User load the Dataset object from the disk.
+
+        - User call `setup_data` to load new data.
+
+        - User prepare data for model based on previous status.
+        """
+
+    def prepare(self, **kwargs) -> object:
+        """
+        The type of dataset depends on the model. (It could be pd.DataFrame, pytorch.DataLoader, etc.)
+        The parameters should specify the scope for the prepared data
+        The method should:
+        - process the data
+
+        - return the processed data
+
+        Returns
+        -------
+        object:
+            return the object
+        """
+
+
+class DatasetH(Dataset):
+    """
+    Dataset with Data(H)andler
+
+    User should try to put the data preprocessing functions into handler.
+    Only following data processing functions should be placed in Dataset:
+
+    - The processing is related to specific model.
+
+    - The processing is related to data split.
+    """
+
+    def __init__(
+        self,
+        handler: Union[Dict, DataHandler],
+        segments: Dict[Text, Tuple],
+        fetch_kwargs: Dict = {},
+        **kwargs,
+    ):
+        """
+        Setup the underlying data.
+
+        Parameters
+        ----------
+        handler : Union[dict, DataHandler]
+            handler could be:
+
+            - instance of `DataHandler`
+
+            - config of `DataHandler`.  Please refer to `DataHandler`
+
+        segments : dict
+            Describe the options to segment the data.
+            Here are some examples:
+
+            .. code-block::
+
+                1) 'segments': {
+                        'train': ("2008-01-01", "2014-12-31"),
+                        'valid': ("2017-01-01", "2020-08-01",),
+                        'test': ("2015-01-01", "2016-12-31",),
+                    }
+                2) 'segments': {
+                        'insample': ("2008-01-01", "2014-12-31"),
+                        'outsample': ("2017-01-01", "2020-08-01",),
+                    }
+        """
+        self.handler: DataHandler = init_instance_by_config(handler, accept_types=DataHandler)
+        self.segments = segments.copy()
+        self.fetch_kwargs = copy(fetch_kwargs)
+        super().__init__(**kwargs)
+
+    def config(self, handler_kwargs: dict = None, **kwargs):
+        """
+        Initialize the DatasetH
+
+        Parameters
+        ----------
+        handler_kwargs : dict
+            Config of DataHandler, which could include the following arguments:
+
+            - arguments of DataHandler.conf_data, such as 'instruments', 'start_time' and 'end_time'.
+
+        kwargs : dict
+            Config of DatasetH, such as
+
+            - segments : dict
+                Config of segments which is same as 'segments' in self.__init__
+
+        """
+        if handler_kwargs is not None:
+            self.handler.config(**handler_kwargs)
+        if "segments" in kwargs:
+            self.segments = deepcopy(kwargs.pop("segments"))
+        super().config(**kwargs)
+
+    def setup_data(self, handler_kwargs: dict = None, **kwargs):
+        """
+        Setup the Data
+
+        Parameters
+        ----------
+        handler_kwargs : dict
+            init arguments of DataHandler, which could include the following arguments:
+
+            - init_type : Init Type of Handler
+
+            - enable_cache : whether to enable cache
+
+        """
+        super().setup_data(**kwargs)
+        if handler_kwargs is not None:
+            self.handler.setup_data(**handler_kwargs)
+
+    def __repr__(self):
+        return "{name}(handler={handler}, segments={segments})".format(
+            name=self.__class__.__name__, handler=self.handler, segments=self.segments
+        )
+
+    def _prepare_seg(self, slc, **kwargs):
+        """
+        Give a query, retrieve the according data
+
+        Parameters
+        ----------
+        slc : please refer to the docs of `prepare`
+                NOTE: it may not be an instance of slice. It may be a segment of `segments` from `def prepare`
+        """
+        if hasattr(self, "fetch_kwargs"):
+            return self.handler.fetch(slc, **kwargs, **self.fetch_kwargs)
+        else:
+            return self.handler.fetch(slc, **kwargs)
+
+    def prepare(
+        self,
+        segments: Union[List[Text], Tuple[Text], Text, slice, pd.Index],
+        col_set=DataHandler.CS_ALL,
+        data_key=DataHandlerLP.DK_I,
+        **kwargs,
+    ) -> Union[List[pd.DataFrame], pd.DataFrame]:
+        """
+        Prepare the data for learning and inference.
+
+        Parameters
+        ----------
+        segments : Union[List[Text], Tuple[Text], Text, slice]
+            Describe the scope of the data to be prepared
+            Here are some examples:
+
+            - 'train'
+
+            - ['train', 'valid']
+
+        col_set : str
+            The col_set will be passed to self.handler when fetching data.
+            TODO: make it automatic:
+
+            - select DK_I for test data
+            - select DK_L for training data.
+        data_key : str
+            The data to fetch:  DK_*
+            Default is DK_I, which indicate fetching data for **inference**.
+
+        kwargs :
+            The parameters that kwargs may contain:
+                flt_col : str
+                    It only exists in TSDatasetH, can be used to add a column of data(True or False) to filter data.
+                    This parameter is only supported when it is an instance of TSDatasetH.
+
+        Returns
+        -------
+        Union[List[pd.DataFrame], pd.DataFrame]:
+
+        Raises
+        ------
+        NotImplementedError:
+        """
+        logger = get_module_logger("DatasetH")
+        seg_kwargs = {"col_set": col_set}
+        seg_kwargs.update(kwargs)
+        if "data_key" in getfullargspec(self.handler.fetch).args:
+            seg_kwargs["data_key"] = data_key
+        else:
+            logger.info(f"data_key[{data_key}] is ignored.")
+
+        # Conflictions may happen here
+        # - The fetched data and the segment key may both be string
+        # To resolve the confliction
+        # - The segment name will have higher priorities
+
+        # 1) Use it as segment name first
+        if isinstance(segments, str) and segments in self.segments:
+            return self._prepare_seg(self.segments[segments], **seg_kwargs)
+
+        if isinstance(segments, (list, tuple)) and all(seg in self.segments for seg in segments):
+            return [self._prepare_seg(self.segments[seg], **seg_kwargs) for seg in segments]
+
+        # 2) Use pass it directly to prepare a single seg
+        return self._prepare_seg(segments, **seg_kwargs)
+
+    # helper functions
+    @staticmethod
+    def get_min_time(segments):
+        return DatasetH._get_extrema(segments, 0, (lambda a, b: a > b))
+
+    @staticmethod
+    def get_max_time(segments):
+        return DatasetH._get_extrema(segments, 1, (lambda a, b: a < b))
+
+    @staticmethod
+    def _get_extrema(segments, idx: int, cmp: Callable, key_func=pd.Timestamp):
+        """it will act like sort and return the max value or None"""
+        candidate = None
+        for k, seg in segments.items():
+            point = seg[idx]
+            if point is None:
+                # None indicates unbounded, return directly
+                return None
+            elif candidate is None or cmp(key_func(candidate), key_func(point)):
+                candidate = point
+        return candidate
+
+
+class TSDataSampler:
+    """
+    (T)ime-(S)eries DataSampler
+    This is the result of TSDatasetH
+
+    It works like `torch.data.utils.Dataset`, it provides a very convenient interface for constructing time-series
+    dataset based on tabular data.
+    - On time step dimension, the smaller index indicates the historical data and the larger index indicates the future
+      data.
+
+    If user have further requirements for processing data, user could process them based on `TSDataSampler` or create
+    more powerful subclasses.
+
+    Known Issues:
+    - For performance issues, this Sampler will convert dataframe into arrays for better performance. This could result
+      in a different data type
+
+
+    Indices design:
+        TSDataSampler has a index mechanism to help users query time-series data efficiently.
+
+        The definition of related variables:
+            data_arr: np.ndarray
+                The original data. it will contains all the original data.
+                The querying are often for time-series of a specific stock.
+                By leveraging this data charactoristics to speed up querying, the multi-index of data_arr is rearranged in (instrument, datetime) order
+
+            data_index: pd.MultiIndex with index order <instrument, datetime>
+                it has the same shape with `idx_map`. Each elements of them are expected to be aligned.
+
+            idx_map: np.ndarray
+                It is the indexable data. It originates from data_arr, and then filtered by 1) `start` and `end`  2) `flt_data`
+                    The extra data in data_arr is useful in following cases
+                    1) creating meaningful time series data before `start` instead of padding them with zeros
+                    2) some data are excluded by `flt_data` (e.g. no <X, y> sample pair for that index). but they are still used in time-series in X
+
+                Finnally, it will look like.
+
+                array([[  0,   0],
+                       [  1,   0],
+                       [  2,   0],
+                       ...,
+                       [241, 348],
+                       [242, 348],
+                       [243, 348]], dtype=int32)
+
+                It list all indexable data(some data only used in historical time series data may not be indexabla), the values are the corresponding row and col in idx_df
+            idx_df: pd.DataFrame
+                It aims to map the <datetime, instrument> key to the original position in data_arr
+
+                For example, it may look like (NOTE: the index for a instrument time-series is continoues in memory)
+
+                    instrument SH600000 SH600008 SH600009 SH600010 SH600011 SH600015  ...
+                    datetime
+                    2017-01-03        0      242      473      717      NaN      974  ...
+                    2017-01-04        1      243      474      718      NaN      975  ...
+                    2017-01-05        2      244      475      719      NaN      976  ...
+                    2017-01-06        3      245      476      720      NaN      977  ...
+
+            With these two indices(idx_map, idx_df) and original data(data_arr), we can make the following queries fast (implemented in __getitem__)
+            (1) Get the i-th indexable sample(time-series):   (indexable sample index) -> [idx_map] -> (row col) -> [idx_df] -> (index in data_arr)
+            (2) Get the specific sample by <datetime, instrument>:  (<datetime, instrument>, i.e. <row, col>) -> [idx_df] -> (index in data_arr)
+            (3) Get the index of a time-series data:   (get the <row, col>, refer to (1), (2)) -> [idx_df] -> (all indices in data_arr for time-series)
+    """
+
+    # Please refer to the docstring of TSDataSampler for the definition of following attributes
+    data_arr: np.ndarray
+    data_index: pd.MultiIndex
+    idx_map: np.ndarray
+    idx_df: pd.DataFrame
+
+    def __init__(
+        self,
+        data: pd.DataFrame,
+        start,
+        end,
+        step_len: int,
+        fillna_type: str = "none",
+        dtype=None,
+        flt_data=None,
+    ):
+        """
+        Build a dataset which looks like torch.data.utils.Dataset.
+
+        Parameters
+        ----------
+        data : pd.DataFrame
+            The raw tabular data whose index order is <"datetime", "instrument">
+        start :
+            The indexable start time
+        end :
+            The indexable end time
+        step_len : int
+            The length of the time-series step
+        fillna_type : int
+            How will qlib handle the sample if there is on sample in a specific date.
+            none:
+                fill with np.nan
+            ffill:
+                ffill with previous sample
+            ffill+bfill:
+                ffill with previous samples first and fill with later samples second
+        flt_data : pd.Series
+            a column of data(True or False) to filter data. Its index order is <"datetime", "instrument">
+            None:
+                kepp all data
+
+        """
+        self.start = start
+        self.end = end
+        self.step_len = step_len
+        self.fillna_type = fillna_type
+        assert get_level_index(data, "datetime") == 0
+        self.data = data.swaplevel().sort_index().copy()
+        data.drop(
+            data.columns, axis=1, inplace=True
+        )  # data is useless since it's passed to a transposed one, hard code to free the memory of this dataframe to avoid three big dataframe in the memory(including: data, self.data, self.data_arr)
+
+        kwargs = {"object": self.data}
+        if dtype is not None:
+            kwargs["dtype"] = dtype
+
+        self.data_arr = np.array(**kwargs)  # Get index from numpy.array will much faster than DataFrame.values!
+        # NOTE:
+        # - append last line with full NaN for better performance in `__getitem__`
+        # - Keep the same dtype will result in a better performance
+        self.data_arr = np.append(
+            self.data_arr,
+            np.full((1, self.data_arr.shape[1]), np.nan, dtype=self.data_arr.dtype),
+            axis=0,
+        )
+        self.nan_idx = -1  # The last line is all NaN
+
+        # the data type will be changed
+        # The index of usable data is between start_idx and end_idx
+        self.idx_df, self.idx_map = self.build_index(self.data)
+        self.data_index = deepcopy(self.data.index)
+
+        if flt_data is not None:
+            if isinstance(flt_data, pd.DataFrame):
+                assert len(flt_data.columns) == 1
+                flt_data = flt_data.iloc[:, 0]
+            # NOTE: bool(np.nan) is True !!!!!!!!
+            # make sure reindex comes first. Otherwise extra NaN may appear.
+            flt_data = flt_data.swaplevel()
+            flt_data = flt_data.reindex(self.data_index).fillna(False).astype(bool)
+            self.flt_data = flt_data.values
+            self.idx_map = self.flt_idx_map(self.flt_data, self.idx_map)
+            self.data_index = self.data_index[np.where(self.flt_data)[0]]
+        self.idx_map = self.idx_map2arr(self.idx_map)
+        self.idx_map, self.data_index = self.slice_idx_map_and_data_index(
+            self.idx_map, self.idx_df, self.data_index, start, end
+        )
+
+        self.idx_arr = np.array(self.idx_df.values, dtype=np.float64)  # for better performance
+        del self.data  # save memory
+
+    @staticmethod
+    def slice_idx_map_and_data_index(
+        idx_map,
+        idx_df,
+        data_index,
+        start,
+        end,
+    ):
+        assert (
+            len(idx_map) == data_index.shape[0]
+        )  # make sure idx_map and data_index is same so index of idx_map can be used on data_index
+
+        start_row_idx, end_row_idx = idx_df.index.slice_locs(start=time_to_slc_point(start), end=time_to_slc_point(end))
+
+        time_flter_idx = (idx_map[:, 0] < end_row_idx) & (idx_map[:, 0] >= start_row_idx)
+        return idx_map[time_flter_idx], data_index[time_flter_idx]
+
+    @staticmethod
+    def idx_map2arr(idx_map):
+        # pytorch data sampler will have better memory control without large dict or list
+        # - https://github.com/pytorch/pytorch/issues/13243
+        # - https://github.com/airctic/icevision/issues/613
+        # So we convert the dict into int array.
+        # The arr_map is expected to behave the same as idx_map
+
+        dtype = np.int32
+        # set a index out of bound to indicate the none existing
+        no_existing_idx = (np.iinfo(dtype).max, np.iinfo(dtype).max)
+
+        max_idx = max(idx_map.keys())
+        arr_map = []
+        for i in range(max_idx + 1):
+            arr_map.append(idx_map.get(i, no_existing_idx))
+        arr_map = np.array(arr_map, dtype=dtype)
+        return arr_map
+
+    @staticmethod
+    def flt_idx_map(flt_data, idx_map):
+        idx = 0
+        new_idx_map = {}
+        for i, exist in enumerate(flt_data):
+            if exist:
+                new_idx_map[idx] = idx_map[i]
+                idx += 1
+        return new_idx_map
+
+    def get_index(self):
+        """
+        Get the pandas index of the data, it will be useful in following scenarios
+        - Special sampler will be used (e.g. user want to sample day by day)
+        """
+        return self.data_index.swaplevel()  # to align the order of multiple index of original data received by __init__
+
+    def config(self, **kwargs):
+        # Config the attributes
+        for k, v in kwargs.items():
+            setattr(self, k, v)
+
+    @staticmethod
+    def build_index(data: pd.DataFrame) -> Tuple[pd.DataFrame, dict]:
+        """
+        The relation of the data
+
+        Parameters
+        ----------
+        data : pd.DataFrame
+            A DataFrame with index in order <instrument, datetime>
+
+                                      RSQR5     RESI5     WVMA5    LABEL0
+            instrument datetime
+            SH600000   2017-01-03  0.016389  0.461632 -1.154788 -0.048056
+                       2017-01-04  0.884545 -0.110597 -1.059332 -0.030139
+                       2017-01-05  0.507540 -0.535493 -1.099665 -0.644983
+                       2017-01-06 -1.267771 -0.669685 -1.636733  0.295366
+                       2017-01-09  0.339346  0.074317 -0.984989  0.765540
+
+        Returns
+        -------
+        Tuple[pd.DataFrame, dict]:
+            1) the first element:  reshape the original index into a <datetime(row), instrument(column)> 2D dataframe
+                instrument SH600000 SH600008 SH600009 SH600010 SH600011 SH600015  ...
+                datetime
+                2017-01-03        0      242      473      717      NaN      974  ...
+                2017-01-04        1      243      474      718      NaN      975  ...
+                2017-01-05        2      244      475      719      NaN      976  ...
+                2017-01-06        3      245      476      720      NaN      977  ...
+            2) the second element:  {<original index>: <row, col>}
+        """
+        # object incase of pandas converting int to float
+        idx_df = pd.Series(range(data.shape[0]), index=data.index, dtype=object)
+        idx_df = lazy_sort_index(idx_df.unstack())
+        # NOTE: the correctness of `__getitem__` depends on columns sorted here
+        idx_df = lazy_sort_index(idx_df, axis=1).T
+
+        idx_map = {}
+        for i, (_, row) in enumerate(idx_df.iterrows()):
+            for j, real_idx in enumerate(row):
+                if not np.isnan(real_idx):
+                    idx_map[real_idx] = (i, j)
+        return idx_df, idx_map
+
+    @property
+    def empty(self):
+        return len(self) == 0
+
+    def _get_indices(self, row: int, col: int) -> np.array:
+        """
+        get series indices of self.data_arr from the row, col indices of self.idx_df
+
+        Parameters
+        ----------
+        row : int
+            the row in self.idx_df
+        col : int
+            the col in self.idx_df
+
+        Returns
+        -------
+        np.array:
+            The indices of data of the data
+        """
+        indices = self.idx_arr[max(row - self.step_len + 1, 0) : row + 1, col]
+
+        if len(indices) < self.step_len:
+            indices = np.concatenate([np.full((self.step_len - len(indices),), np.nan), indices])
+
+        if self.fillna_type == "ffill":
+            indices = np_ffill(indices)
+        elif self.fillna_type == "ffill+bfill":
+            indices = np_ffill(np_ffill(indices)[::-1])[::-1]
+        else:
+            assert self.fillna_type == "none"
+        return indices
+
+    def _get_row_col(self, idx) -> Tuple[int]:
+        """
+        get the col index and row index of a given sample index in self.idx_df
+
+        Parameters
+        ----------
+        idx :
+            the input of  `__getitem__`
+
+        Returns
+        -------
+        Tuple[int]:
+            the row and col index
+        """
+        # The the right row number `i` and col number `j` in idx_df
+        if isinstance(idx, (int, np.integer)):
+            real_idx = idx
+            if 0 <= real_idx < len(self.idx_map):
+                i, j = self.idx_map[real_idx]  # TODO: The performance of this line is not good
+            else:
+                raise KeyError(f"{real_idx} is out of [0, {len(self.idx_map)})")
+        elif isinstance(idx, tuple):
+            # <TSDataSampler object>["datetime", "instruments"]
+            date, inst = idx
+            date = pd.Timestamp(date)
+            i = bisect.bisect_right(self.idx_df.index, date) - 1
+            # NOTE: This relies on the idx_df columns sorted in `__init__`
+            j = bisect.bisect_left(self.idx_df.columns, inst)
+        else:
+            raise NotImplementedError(f"This type of input is not supported")
+        return i, j
+
+    def __getitem__(self, idx: Union[int, Tuple[object, str], List[int]]):
+        """
+        # We have two method to get the time-series of a sample
+        tsds is a instance of TSDataSampler
+
+        # 1) sample by int index directly
+        tsds[len(tsds) - 1]
+
+        # 2) sample by <datetime,instrument> index
+        tsds['2016-12-31', "SZ300315"]
+
+        # The return value will be similar to the data retrieved by following code
+        df.loc(axis=0)['2015-01-01':'2016-12-31', "SZ300315"].iloc[-30:]
+
+        Parameters
+        ----------
+        idx : Union[int, Tuple[object, str]]
+        """
+        # Multi-index type
+        mtit = (list, np.ndarray)
+        if isinstance(idx, mtit):
+            indices = [self._get_indices(*self._get_row_col(i)) for i in idx]
+            indices = np.concatenate(indices)
+        else:
+            indices = self._get_indices(*self._get_row_col(idx))
+
+        # 1) for better performance, use the last nan line for padding the lost date
+        # 2) In case of precision problems. We use np.float64. # TODO: I'm not sure if whether np.float64 will result in
+        # precision problems. It will not cause any problems in my tests at least
+        indices = np.nan_to_num(indices.astype(np.float64), nan=self.nan_idx).astype(int)
+
+        if (np.diff(indices) == 1).all():  # slicing instead of indexing for speeding up.
+            data = self.data_arr[indices[0] : indices[-1] + 1]
+        else:
+            data = self.data_arr[indices]
+        if isinstance(idx, mtit):
+            # if we get multiple indexes, addition dimension should be added.
+            # <sample_idx, step_idx, feature_idx>
+            data = data.reshape(-1, self.step_len, *data.shape[1:])
+        return data
+
+    def __len__(self):
+        return len(self.idx_map)
+
+
+class TSDatasetH(DatasetH):
+    """
+    (T)ime-(S)eries Dataset (H)andler
+
+
+    Convert the tabular data to Time-Series data
+
+    Requirements analysis
+
+    The typical workflow of a user to get time-series data for an sample
+    - process features
+    - slice proper data from data handler:  dimension of sample <feature, >
+    - Build relation of samples by <time, instrument> index
+        - Be able to sample times series of data <timestep, feature>
+        - It will be better if the interface is like "torch.utils.data.Dataset"
+    - User could build customized batch based on the data
+        - The dimension of a batch of data <batch_idx, feature, timestep>
+    """
+
+    DEFAULT_STEP_LEN = 30
+
+    def __init__(self, step_len=DEFAULT_STEP_LEN, **kwargs):
+        self.step_len = step_len
+        super().__init__(**kwargs)
+
+    def config(self, **kwargs):
+        if "step_len" in kwargs:
+            self.step_len = kwargs.pop("step_len")
+        super().config(**kwargs)
+
+    def setup_data(self, **kwargs):
+        super().setup_data(**kwargs)
+        # make sure the calendar is updated to latest when loading data from new config
+        cal = self.handler.fetch(col_set=self.handler.CS_RAW).index.get_level_values("datetime").unique()
+        self.cal = sorted(cal)
+
+    @staticmethod
+    def _extend_slice(slc: slice, cal: list, step_len: int) -> slice:
+        # Dataset decide how to slice data(Get more data for timeseries).
+        start, end = slc.start, slc.stop
+        start_idx = bisect.bisect_left(cal, pd.Timestamp(start))
+        pad_start_idx = max(0, start_idx - step_len)
+        pad_start = cal[pad_start_idx]
+        return slice(pad_start, end)
+
+    def _prepare_seg(self, slc: slice, **kwargs) -> TSDataSampler:
+        """
+        split the _prepare_raw_seg is to leave a hook for data preprocessing before creating processing data
+        NOTE: TSDatasetH only support slc segment on datetime !!!
+        """
+        dtype = kwargs.pop("dtype", None)
+        if not isinstance(slc, slice):
+            slc = slice(*slc)
+        start, end = slc.start, slc.stop
+        flt_col = kwargs.pop("flt_col", None)
+        # TSDatasetH will retrieve more data for complete time-series
+
+        ext_slice = self._extend_slice(slc, self.cal, self.step_len)
+        data = super()._prepare_seg(ext_slice, **kwargs)
+
+        flt_kwargs = deepcopy(kwargs)
+        if flt_col is not None:
+            flt_kwargs["col_set"] = flt_col
+            flt_data = super()._prepare_seg(ext_slice, **flt_kwargs)
+            assert len(flt_data.columns) == 1
+        else:
+            flt_data = None
+
+        tsds = TSDataSampler(
+            data=data,
+            start=start,
+            end=end,
+            step_len=self.step_len,
+            dtype=dtype,
+            flt_data=flt_data,
+        )
+        return tsds
+
+
+__all__ = ["Optional", "Dataset", "DatasetH"]
```

## qlib/data/dataset/handler.py

 * *Ordering differences only*

```diff
@@ -1,745 +1,745 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-# coding=utf-8
-import warnings
-from typing import Callable, Union, Tuple, List, Iterator, Optional
-
-import pandas as pd
-
-from qlib.typehint import Literal
-from ...log import get_module_logger, TimeInspector
-from ...utils import init_instance_by_config
-from ...utils.serial import Serializable
-from .utils import fetch_df_by_index, fetch_df_by_col
-from ...utils import lazy_sort_index
-from .loader import DataLoader
-
-from . import processor as processor_module
-from . import loader as data_loader_module
-
-
-# TODO: A more general handler interface which does not relies on internal pd.DataFrame is needed.
-class DataHandler(Serializable):
-    """
-    The steps to using a handler
-    1. initialized data handler  (call by `init`).
-    2. use the data.
-
-
-    The data handler try to maintain a handler with 2 level.
-    `datetime` & `instruments`.
-
-    Any order of the index level can be supported (The order will be implied in the data).
-    The order  <`datetime`, `instruments`> will be used when the dataframe index name is missed.
-
-    Example of the data:
-    The multi-index of the columns is optional.
-
-    .. code-block:: text
-
-                                feature                                                            label
-                                $close     $volume  Ref($close, 1)  Mean($close, 3)  $high-$low  LABEL0
-        datetime   instrument
-        2010-01-04 SH600000    81.807068  17145150.0       83.737389        83.016739    2.741058  0.0032
-                   SH600004    13.313329  11800983.0       13.313329        13.317701    0.183632  0.0042
-                   SH600005    37.796539  12231662.0       38.258602        37.919757    0.970325  0.0289
-
-
-    Tips for improving the performance of datahandler
-    - Fetching data with `col_set=CS_RAW` will return the raw data and may avoid pandas from copying the data when calling `loc`
-    """
-
-    _data: pd.DataFrame  # underlying data.
-
-    def __init__(
-        self,
-        instruments=None,
-        start_time=None,
-        end_time=None,
-        data_loader: Union[dict, str, DataLoader] = None,
-        init_data=True,
-        fetch_orig=True,
-    ):
-        """
-        Parameters
-        ----------
-        instruments :
-            The stock list to retrieve.
-        start_time :
-            start_time of the original data.
-        end_time :
-            end_time of the original data.
-        data_loader : Union[dict, str, DataLoader]
-            data loader to load the data.
-        init_data :
-            initialize the original data in the constructor.
-        fetch_orig : bool
-            Return the original data instead of copy if possible.
-        """
-
-        # Setup data loader
-        assert data_loader is not None  # to make start_time end_time could have None default value
-
-        # what data source to load data
-        self.data_loader = init_instance_by_config(
-            data_loader,
-            None if (isinstance(data_loader, dict) and "module_path" in data_loader) else data_loader_module,
-            accept_types=DataLoader,
-        )
-
-        # what data to be loaded from data source
-        # For IDE auto-completion.
-        self.instruments = instruments
-        self.start_time = start_time
-        self.end_time = end_time
-
-        self.fetch_orig = fetch_orig
-        if init_data:
-            with TimeInspector.logt("Init data"):
-                self.setup_data()
-        super().__init__()
-
-    def config(self, **kwargs):
-        """
-        configuration of data.
-        # what data to be loaded from data source
-
-        This method will be used when loading pickled handler from dataset.
-        The data will be initialized with different time range.
-
-        """
-        attr_list = {"instruments", "start_time", "end_time"}
-        for k, v in kwargs.items():
-            if k in attr_list:
-                setattr(self, k, v)
-
-        for attr in attr_list:
-            if attr in kwargs:
-                kwargs.pop(attr)
-
-        super().config(**kwargs)
-
-    def setup_data(self, enable_cache: bool = False):
-        """
-        Set Up the data in case of running initialization for multiple time
-
-        It is responsible for maintaining following variable
-        1) self._data
-
-        Parameters
-        ----------
-        enable_cache : bool
-            default value is false:
-
-            - if `enable_cache` == True:
-
-                the processed data will be saved on disk, and handler will load the cached data from the disk directly
-                when we call `init` next time
-        """
-        # Setup data.
-        # _data may be with multiple column index level. The outer level indicates the feature set name
-        with TimeInspector.logt("Loading data"):
-            # make sure the fetch method is based on an index-sorted pd.DataFrame
-            self._data = lazy_sort_index(self.data_loader.load(self.instruments, self.start_time, self.end_time))
-        # TODO: cache
-
-    CS_ALL = "__all"  # return all columns with single-level index column
-    CS_RAW = "__raw"  # return raw data with multi-level index column
-
-    def fetch(
-        self,
-        selector: Union[pd.Timestamp, slice, str, pd.Index] = slice(None, None),
-        level: Union[str, int] = "datetime",
-        col_set: Union[str, List[str]] = CS_ALL,
-        squeeze: bool = False,
-        proc_func: Callable = None,
-    ) -> pd.DataFrame:
-        """
-        fetch data from underlying data source
-
-        Design motivation:
-        - providing a unified interface for underlying data.
-        - Potential to make the interface more friendly.
-        - User can improve performance when fetching data in this extra layer
-
-        Parameters
-        ----------
-        selector : Union[pd.Timestamp, slice, str]
-            describe how to select data by index
-            It can be categories as following
-
-            - fetch single index
-            - fetch a range of index
-
-                - a slice range
-                - pd.Index for specific indexes
-
-            Following conflicts may occur
-
-            - Does ["20200101", "20210101"] mean selecting this slice or these two days?
-
-                - slice have higher priorities
-
-        level : Union[str, int]
-            which index level to select the data
-
-        col_set : Union[str, List[str]]
-
-            - if isinstance(col_set, str):
-
-                select a set of meaningful, pd.Index columns.(e.g. features, columns)
-
-                - if col_set == CS_RAW:
-
-                    the raw dataset will be returned.
-
-            - if isinstance(col_set, List[str]):
-
-                select several sets of meaningful columns, the returned data has multiple levels
-
-        proc_func: Callable
-
-            - Give a hook for processing data before fetching
-            - An example to explain the necessity of the hook:
-
-                - A Dataset learned some processors to process data which is related to data segmentation
-                - It will apply them every time when preparing data.
-                - The learned processor require the dataframe remains the same format when fitting and applying
-                - However the data format will change according to the parameters.
-                - So the processors should be applied to the underlayer data.
-
-        squeeze : bool
-            whether squeeze columns and index
-
-        Returns
-        -------
-        pd.DataFrame.
-        """
-        return self._fetch_data(
-            data_storage=self._data,
-            selector=selector,
-            level=level,
-            col_set=col_set,
-            squeeze=squeeze,
-            proc_func=proc_func,
-        )
-
-    def _fetch_data(
-        self,
-        data_storage,
-        selector: Union[pd.Timestamp, slice, str, pd.Index] = slice(None, None),
-        level: Union[str, int] = "datetime",
-        col_set: Union[str, List[str]] = CS_ALL,
-        squeeze: bool = False,
-        proc_func: Callable = None,
-    ):
-        # This method is extracted for sharing in subclasses
-        from .storage import BaseHandlerStorage  # pylint: disable=C0415
-
-        # Following conflicts may occur
-        # - Does [20200101", "20210101"] mean selecting this slice or these two days?
-        # To solve this issue
-        #   - slice have higher priorities (except when level is none)
-        if isinstance(selector, (tuple, list)) and level is not None:
-            # when level is None, the argument will be passed in directly
-            # we don't have to convert it into slice
-            try:
-                selector = slice(*selector)
-            except ValueError:
-                get_module_logger("DataHandlerLP").info(f"Fail to converting to query to slice. It will used directly")
-
-        if isinstance(data_storage, pd.DataFrame):
-            data_df = data_storage
-            if proc_func is not None:
-                # FIXME: fetching by time first will be more friendly to `proc_func`
-                # Copy in case of `proc_func` changing the data inplace....
-                data_df = proc_func(fetch_df_by_index(data_df, selector, level, fetch_orig=self.fetch_orig).copy())
-                data_df = fetch_df_by_col(data_df, col_set)
-            else:
-                # Fetch column  first will be more friendly to SepDataFrame
-                data_df = fetch_df_by_col(data_df, col_set)
-                data_df = fetch_df_by_index(data_df, selector, level, fetch_orig=self.fetch_orig)
-        elif isinstance(data_storage, BaseHandlerStorage):
-            if not data_storage.is_proc_func_supported():
-                if proc_func is not None:
-                    raise ValueError(f"proc_func is not supported by the storage {type(data_storage)}")
-                data_df = data_storage.fetch(
-                    selector=selector, level=level, col_set=col_set, fetch_orig=self.fetch_orig
-                )
-            else:
-                data_df = data_storage.fetch(
-                    selector=selector, level=level, col_set=col_set, fetch_orig=self.fetch_orig, proc_func=proc_func
-                )
-        else:
-            raise TypeError(f"data_storage should be pd.DataFrame|HashingStockStorage, not {type(data_storage)}")
-
-        if squeeze:
-            # squeeze columns
-            data_df = data_df.squeeze()
-            # squeeze index
-            if isinstance(selector, (str, pd.Timestamp)):
-                data_df = data_df.reset_index(level=level, drop=True)
-        return data_df
-
-    def get_cols(self, col_set=CS_ALL) -> list:
-        """
-        get the column names
-
-        Parameters
-        ----------
-        col_set : str
-            select a set of meaningful columns.(e.g. features, columns)
-
-        Returns
-        -------
-        list:
-            list of column names
-        """
-        df = self._data.head()
-        df = fetch_df_by_col(df, col_set)
-        return df.columns.to_list()
-
-    def get_range_selector(self, cur_date: Union[pd.Timestamp, str], periods: int) -> slice:
-        """
-        get range selector by number of periods
-
-        Args:
-            cur_date (pd.Timestamp or str): current date
-            periods (int): number of periods
-        """
-        trading_dates = self._data.index.unique(level="datetime")
-        cur_loc = trading_dates.get_loc(cur_date)
-        pre_loc = cur_loc - periods + 1
-        if pre_loc < 0:
-            warnings.warn("`periods` is too large. the first date will be returned.")
-            pre_loc = 0
-        ref_date = trading_dates[pre_loc]
-        return slice(ref_date, cur_date)
-
-    def get_range_iterator(
-        self, periods: int, min_periods: Optional[int] = None, **kwargs
-    ) -> Iterator[Tuple[pd.Timestamp, pd.DataFrame]]:
-        """
-        get an iterator of sliced data with given periods
-
-        Args:
-            periods (int): number of periods.
-            min_periods (int): minimum periods for sliced dataframe.
-            kwargs (dict): will be passed to `self.fetch`.
-        """
-        trading_dates = self._data.index.unique(level="datetime")
-        if min_periods is None:
-            min_periods = periods
-        for cur_date in trading_dates[min_periods:]:
-            selector = self.get_range_selector(cur_date, periods)
-            yield cur_date, self.fetch(selector, **kwargs)
-
-
-DATA_KEY_TYPE = Literal["raw", "infer", "learn"]
-
-
-class DataHandlerLP(DataHandler):
-    """
-    DataHandler with **(L)earnable (P)rocessor**
-
-    This handler will produce three pieces of data in pd.DataFrame format.
-
-    - DK_R / self._data: the raw data loaded from the loader
-    - DK_I / self._infer: the data processed for inference
-    - DK_L / self._learn: the data processed for learning model.
-
-    The motivation of using different processor workflows for learning and inference
-    Here are some examples.
-
-    - The instrument universe for learning and inference may be different.
-    - The processing of some samples may rely on label (for example, some samples hit the limit may need extra processing or be dropped).
-
-        - These processors only apply to the learning phase.
-
-    Tips for data handler
-
-    - To reduce the memory cost
-
-        - `drop_raw=True`: this will modify the data inplace on raw data;
-
-    - Please note processed data like `self._infer` or `self._learn` are concepts different from `segments` in Qlib's `Dataset` like "train" and "test"
-
-        - Processed data like `self._infer` or `self._learn` are underlying data processed with different processors
-        - `segments` in Qlib's `Dataset` like "train" and "test" are simply the time segmentations when querying data("train" are often before "test" in time-series).
-        - For example, you can query `data._infer` processed by `infer_processors` in the "train" time segmentation.
-    """
-
-    # based on `self._data`, _infer and _learn are genrated after processors
-    _infer: pd.DataFrame  # data for inference
-    _learn: pd.DataFrame  # data for learning models
-
-    # data key
-    DK_R: DATA_KEY_TYPE = "raw"
-    DK_I: DATA_KEY_TYPE = "infer"
-    DK_L: DATA_KEY_TYPE = "learn"
-    # map data_key to attribute name
-    ATTR_MAP = {DK_R: "_data", DK_I: "_infer", DK_L: "_learn"}
-
-    # process type
-    PTYPE_I = "independent"
-    # - self._infer will be processed by shared_processors + infer_processors
-    # - self._learn will be processed by shared_processors + learn_processors
-
-    # NOTE:
-    PTYPE_A = "append"
-
-    # - self._infer will be processed by shared_processors + infer_processors
-    # - self._learn will be processed by shared_processors + infer_processors + learn_processors
-    #   - (e.g. self._infer processed by learn_processors )
-
-    def __init__(
-        self,
-        instruments=None,
-        start_time=None,
-        end_time=None,
-        data_loader: Union[dict, str, DataLoader] = None,
-        infer_processors: List = [],
-        learn_processors: List = [],
-        shared_processors: List = [],
-        process_type=PTYPE_A,
-        drop_raw=False,
-        **kwargs,
-    ):
-        """
-        Parameters
-        ----------
-        infer_processors : list
-            - list of <description info> of processors to generate data for inference
-
-            - example of <description info>:
-
-            .. code-block::
-
-                1) classname & kwargs:
-                    {
-                        "class": "MinMaxNorm",
-                        "kwargs": {
-                            "fit_start_time": "20080101",
-                            "fit_end_time": "20121231"
-                        }
-                    }
-                2) Only classname:
-                    "DropnaFeature"
-                3) object instance of Processor
-
-        learn_processors : list
-            similar to infer_processors, but for generating data for learning models
-
-        process_type: str
-            PTYPE_I = 'independent'
-
-            - self._infer will be processed by infer_processors
-
-            - self._learn will be processed by learn_processors
-
-            PTYPE_A = 'append'
-
-            - self._infer will be processed by infer_processors
-
-            - self._learn will be processed by infer_processors + learn_processors
-
-              - (e.g. self._infer processed by learn_processors )
-        drop_raw: bool
-            Whether to drop the raw data
-        """
-
-        # Setup preprocessor
-        self.infer_processors = []  # for lint
-        self.learn_processors = []  # for lint
-        self.shared_processors = []  # for lint
-        for pname in "infer_processors", "learn_processors", "shared_processors":
-            for proc in locals()[pname]:
-                getattr(self, pname).append(
-                    init_instance_by_config(
-                        proc,
-                        None if (isinstance(proc, dict) and "module_path" in proc) else processor_module,
-                        accept_types=processor_module.Processor,
-                    )
-                )
-
-        self.process_type = process_type
-        self.drop_raw = drop_raw
-        super().__init__(instruments, start_time, end_time, data_loader, **kwargs)
-
-    def get_all_processors(self):
-        return self.shared_processors + self.infer_processors + self.learn_processors
-
-    def fit(self):
-        """
-        fit data without processing the data
-        """
-        for proc in self.get_all_processors():
-            with TimeInspector.logt(f"{proc.__class__.__name__}"):
-                proc.fit(self._data)
-
-    def fit_process_data(self):
-        """
-        fit and process data
-
-        The input of the `fit` will be the output of the previous processor
-        """
-        self.process_data(with_fit=True)
-
-    @staticmethod
-    def _run_proc_l(
-        df: pd.DataFrame, proc_l: List[processor_module.Processor], with_fit: bool, check_for_infer: bool
-    ) -> pd.DataFrame:
-        for proc in proc_l:
-            if check_for_infer and not proc.is_for_infer():
-                raise TypeError("Only processors usable for inference can be used in `infer_processors` ")
-            with TimeInspector.logt(f"{proc.__class__.__name__}"):
-                if with_fit:
-                    proc.fit(df)
-                df = proc(df)
-        return df
-
-    @staticmethod
-    def _is_proc_readonly(proc_l: List[processor_module.Processor]):
-        """
-        NOTE: it will return True if `len(proc_l) == 0`
-        """
-        for p in proc_l:
-            if not p.readonly():
-                return False
-        return True
-
-    def process_data(self, with_fit: bool = False):
-        """
-        process_data data. Fun `processor.fit` if necessary
-
-        Notation: (data)  [processor]
-
-        # data processing flow of self.process_type == DataHandlerLP.PTYPE_I
-
-        .. code-block:: text
-
-            (self._data)-[shared_processors]-(_shared_df)-[learn_processors]-(_learn_df)
-                                                   \\
-                                                    -[infer_processors]-(_infer_df)
-
-        # data processing flow of self.process_type == DataHandlerLP.PTYPE_A
-
-        .. code-block:: text
-
-            (self._data)-[shared_processors]-(_shared_df)-[infer_processors]-(_infer_df)-[learn_processors]-(_learn_df)
-
-        Parameters
-        ----------
-        with_fit : bool
-            The input of the `fit` will be the output of the previous processor
-        """
-        # shared data processors
-        # 1) assign
-        _shared_df = self._data
-        if not self._is_proc_readonly(self.shared_processors):  # avoid modifying the original data
-            _shared_df = _shared_df.copy()
-        # 2) process
-        _shared_df = self._run_proc_l(_shared_df, self.shared_processors, with_fit=with_fit, check_for_infer=True)
-
-        # data for inference
-        # 1) assign
-        _infer_df = _shared_df
-        if not self._is_proc_readonly(self.infer_processors):  # avoid modifying the original data
-            _infer_df = _infer_df.copy()
-        # 2) process
-        _infer_df = self._run_proc_l(_infer_df, self.infer_processors, with_fit=with_fit, check_for_infer=True)
-
-        self._infer = _infer_df
-
-        # data for learning
-        # 1) assign
-        if self.process_type == DataHandlerLP.PTYPE_I:
-            _learn_df = _shared_df
-        elif self.process_type == DataHandlerLP.PTYPE_A:
-            # based on `infer_df` and append the processor
-            _learn_df = _infer_df
-        else:
-            raise NotImplementedError(f"This type of input is not supported")
-        if not self._is_proc_readonly(self.learn_processors):  # avoid modifying the original  data
-            _learn_df = _learn_df.copy()
-        # 2) process
-        _learn_df = self._run_proc_l(_learn_df, self.learn_processors, with_fit=with_fit, check_for_infer=False)
-
-        self._learn = _learn_df
-
-        if self.drop_raw:
-            del self._data
-
-    def config(self, processor_kwargs: dict = None, **kwargs):
-        """
-        configuration of data.
-        # what data to be loaded from data source
-
-        This method will be used when loading pickled handler from dataset.
-        The data will be initialized with different time range.
-
-        """
-        super().config(**kwargs)
-        if processor_kwargs is not None:
-            for processor in self.get_all_processors():
-                processor.config(**processor_kwargs)
-
-    # init type
-    IT_FIT_SEQ = "fit_seq"  # the input of `fit` will be the output of the previous processor
-    IT_FIT_IND = "fit_ind"  # the input of `fit` will be the original df
-    IT_LS = "load_state"  # The state of the object has been load by pickle
-
-    def setup_data(self, init_type: str = IT_FIT_SEQ, **kwargs):
-        """
-        Set up the data in case of running initialization for multiple time
-
-        Parameters
-        ----------
-        init_type : str
-            The type `IT_*` listed above.
-        enable_cache : bool
-            default value is false:
-
-            - if `enable_cache` == True:
-
-                the processed data will be saved on disk, and handler will load the cached data from the disk directly
-                when we call `init` next time
-        """
-        # init raw data
-        super().setup_data(**kwargs)
-
-        with TimeInspector.logt("fit & process data"):
-            if init_type == DataHandlerLP.IT_FIT_IND:
-                self.fit()
-                self.process_data()
-            elif init_type == DataHandlerLP.IT_LS:
-                self.process_data()
-            elif init_type == DataHandlerLP.IT_FIT_SEQ:
-                self.fit_process_data()
-            else:
-                raise NotImplementedError(f"This type of input is not supported")
-
-        # TODO: Be able to cache handler data. Save the memory for data processing
-
-    def _get_df_by_key(self, data_key: DATA_KEY_TYPE = DK_I) -> pd.DataFrame:
-        if data_key == self.DK_R and self.drop_raw:
-            raise AttributeError(
-                "DataHandlerLP has not attribute _data, please set drop_raw = False if you want to use raw data"
-            )
-        df = getattr(self, self.ATTR_MAP[data_key])
-        return df
-
-    def fetch(
-        self,
-        selector: Union[pd.Timestamp, slice, str] = slice(None, None),
-        level: Union[str, int] = "datetime",
-        col_set=DataHandler.CS_ALL,
-        data_key: DATA_KEY_TYPE = DK_I,
-        squeeze: bool = False,
-        proc_func: Callable = None,
-    ) -> pd.DataFrame:
-        """
-        fetch data from underlying data source
-
-        Parameters
-        ----------
-        selector : Union[pd.Timestamp, slice, str]
-            describe how to select data by index.
-        level : Union[str, int]
-            which index level to select the data.
-        col_set : str
-            select a set of meaningful columns.(e.g. features, columns).
-        data_key : str
-            the data to fetch:  DK_*.
-        proc_func: Callable
-            please refer to the doc of DataHandler.fetch
-
-        Returns
-        -------
-        pd.DataFrame:
-        """
-
-        return self._fetch_data(
-            data_storage=self._get_df_by_key(data_key),
-            selector=selector,
-            level=level,
-            col_set=col_set,
-            squeeze=squeeze,
-            proc_func=proc_func,
-        )
-
-    def get_cols(self, col_set=DataHandler.CS_ALL, data_key: DATA_KEY_TYPE = DK_I) -> list:
-        """
-        get the column names
-
-        Parameters
-        ----------
-        col_set : str
-            select a set of meaningful columns.(e.g. features, columns).
-        data_key : DATA_KEY_TYPE
-            the data to fetch:  DK_*.
-
-        Returns
-        -------
-        list:
-            list of column names
-        """
-        df = self._get_df_by_key(data_key).head()
-        df = fetch_df_by_col(df, col_set)
-        return df.columns.to_list()
-
-    @classmethod
-    def cast(cls, handler: "DataHandlerLP") -> "DataHandlerLP":
-        """
-        Motivation
-
-        - A user creates a datahandler in his customized package. Then he wants to share the processed handler to
-          other users without introduce the package dependency and complicated data processing logic.
-        - This class make it possible by casting the class to DataHandlerLP and only keep the processed data
-
-        Parameters
-        ----------
-        handler : DataHandlerLP
-            A subclass of DataHandlerLP
-
-        Returns
-        -------
-        DataHandlerLP:
-            the converted processed data
-        """
-        new_hd: DataHandlerLP = object.__new__(DataHandlerLP)
-        new_hd.from_cast = True  # add a mark for the cast instance
-
-        for key in list(DataHandlerLP.ATTR_MAP.values()) + [
-            "instruments",
-            "start_time",
-            "end_time",
-            "fetch_orig",
-            "drop_raw",
-        ]:
-            setattr(new_hd, key, getattr(handler, key, None))
-        return new_hd
-
-    @classmethod
-    def from_df(cls, df: pd.DataFrame) -> "DataHandlerLP":
-        """
-        Motivation:
-        - When user want to get a quick data handler.
-
-        The created data handler will have only one shared Dataframe without processors.
-        After creating the handler, user may often want to dump the handler for reuse
-        Here is a typical use case
-
-        .. code-block:: python
-
-            from qlib.data.dataset import DataHandlerLP
-            dh = DataHandlerLP.from_df(df)
-            dh.to_pickle(fname, dump_all=True)
-
-        TODO:
-        - The StaticDataLoader is quite slow. It don't have to copy the data again...
-
-        """
-        loader = data_loader_module.StaticDataLoader(df)
-        return cls(data_loader=loader)
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+# coding=utf-8
+import warnings
+from typing import Callable, Union, Tuple, List, Iterator, Optional
+
+import pandas as pd
+
+from qlib.typehint import Literal
+from ...log import get_module_logger, TimeInspector
+from ...utils import init_instance_by_config
+from ...utils.serial import Serializable
+from .utils import fetch_df_by_index, fetch_df_by_col
+from ...utils import lazy_sort_index
+from .loader import DataLoader
+
+from . import processor as processor_module
+from . import loader as data_loader_module
+
+
+# TODO: A more general handler interface which does not relies on internal pd.DataFrame is needed.
+class DataHandler(Serializable):
+    """
+    The steps to using a handler
+    1. initialized data handler  (call by `init`).
+    2. use the data.
+
+
+    The data handler try to maintain a handler with 2 level.
+    `datetime` & `instruments`.
+
+    Any order of the index level can be supported (The order will be implied in the data).
+    The order  <`datetime`, `instruments`> will be used when the dataframe index name is missed.
+
+    Example of the data:
+    The multi-index of the columns is optional.
+
+    .. code-block:: text
+
+                                feature                                                            label
+                                $close     $volume  Ref($close, 1)  Mean($close, 3)  $high-$low  LABEL0
+        datetime   instrument
+        2010-01-04 SH600000    81.807068  17145150.0       83.737389        83.016739    2.741058  0.0032
+                   SH600004    13.313329  11800983.0       13.313329        13.317701    0.183632  0.0042
+                   SH600005    37.796539  12231662.0       38.258602        37.919757    0.970325  0.0289
+
+
+    Tips for improving the performance of datahandler
+    - Fetching data with `col_set=CS_RAW` will return the raw data and may avoid pandas from copying the data when calling `loc`
+    """
+
+    _data: pd.DataFrame  # underlying data.
+
+    def __init__(
+        self,
+        instruments=None,
+        start_time=None,
+        end_time=None,
+        data_loader: Union[dict, str, DataLoader] = None,
+        init_data=True,
+        fetch_orig=True,
+    ):
+        """
+        Parameters
+        ----------
+        instruments :
+            The stock list to retrieve.
+        start_time :
+            start_time of the original data.
+        end_time :
+            end_time of the original data.
+        data_loader : Union[dict, str, DataLoader]
+            data loader to load the data.
+        init_data :
+            initialize the original data in the constructor.
+        fetch_orig : bool
+            Return the original data instead of copy if possible.
+        """
+
+        # Setup data loader
+        assert data_loader is not None  # to make start_time end_time could have None default value
+
+        # what data source to load data
+        self.data_loader = init_instance_by_config(
+            data_loader,
+            None if (isinstance(data_loader, dict) and "module_path" in data_loader) else data_loader_module,
+            accept_types=DataLoader,
+        )
+
+        # what data to be loaded from data source
+        # For IDE auto-completion.
+        self.instruments = instruments
+        self.start_time = start_time
+        self.end_time = end_time
+
+        self.fetch_orig = fetch_orig
+        if init_data:
+            with TimeInspector.logt("Init data"):
+                self.setup_data()
+        super().__init__()
+
+    def config(self, **kwargs):
+        """
+        configuration of data.
+        # what data to be loaded from data source
+
+        This method will be used when loading pickled handler from dataset.
+        The data will be initialized with different time range.
+
+        """
+        attr_list = {"instruments", "start_time", "end_time"}
+        for k, v in kwargs.items():
+            if k in attr_list:
+                setattr(self, k, v)
+
+        for attr in attr_list:
+            if attr in kwargs:
+                kwargs.pop(attr)
+
+        super().config(**kwargs)
+
+    def setup_data(self, enable_cache: bool = False):
+        """
+        Set Up the data in case of running initialization for multiple time
+
+        It is responsible for maintaining following variable
+        1) self._data
+
+        Parameters
+        ----------
+        enable_cache : bool
+            default value is false:
+
+            - if `enable_cache` == True:
+
+                the processed data will be saved on disk, and handler will load the cached data from the disk directly
+                when we call `init` next time
+        """
+        # Setup data.
+        # _data may be with multiple column index level. The outer level indicates the feature set name
+        with TimeInspector.logt("Loading data"):
+            # make sure the fetch method is based on an index-sorted pd.DataFrame
+            self._data = lazy_sort_index(self.data_loader.load(self.instruments, self.start_time, self.end_time))
+        # TODO: cache
+
+    CS_ALL = "__all"  # return all columns with single-level index column
+    CS_RAW = "__raw"  # return raw data with multi-level index column
+
+    def fetch(
+        self,
+        selector: Union[pd.Timestamp, slice, str, pd.Index] = slice(None, None),
+        level: Union[str, int] = "datetime",
+        col_set: Union[str, List[str]] = CS_ALL,
+        squeeze: bool = False,
+        proc_func: Callable = None,
+    ) -> pd.DataFrame:
+        """
+        fetch data from underlying data source
+
+        Design motivation:
+        - providing a unified interface for underlying data.
+        - Potential to make the interface more friendly.
+        - User can improve performance when fetching data in this extra layer
+
+        Parameters
+        ----------
+        selector : Union[pd.Timestamp, slice, str]
+            describe how to select data by index
+            It can be categories as following
+
+            - fetch single index
+            - fetch a range of index
+
+                - a slice range
+                - pd.Index for specific indexes
+
+            Following conflicts may occur
+
+            - Does ["20200101", "20210101"] mean selecting this slice or these two days?
+
+                - slice have higher priorities
+
+        level : Union[str, int]
+            which index level to select the data
+
+        col_set : Union[str, List[str]]
+
+            - if isinstance(col_set, str):
+
+                select a set of meaningful, pd.Index columns.(e.g. features, columns)
+
+                - if col_set == CS_RAW:
+
+                    the raw dataset will be returned.
+
+            - if isinstance(col_set, List[str]):
+
+                select several sets of meaningful columns, the returned data has multiple levels
+
+        proc_func: Callable
+
+            - Give a hook for processing data before fetching
+            - An example to explain the necessity of the hook:
+
+                - A Dataset learned some processors to process data which is related to data segmentation
+                - It will apply them every time when preparing data.
+                - The learned processor require the dataframe remains the same format when fitting and applying
+                - However the data format will change according to the parameters.
+                - So the processors should be applied to the underlayer data.
+
+        squeeze : bool
+            whether squeeze columns and index
+
+        Returns
+        -------
+        pd.DataFrame.
+        """
+        return self._fetch_data(
+            data_storage=self._data,
+            selector=selector,
+            level=level,
+            col_set=col_set,
+            squeeze=squeeze,
+            proc_func=proc_func,
+        )
+
+    def _fetch_data(
+        self,
+        data_storage,
+        selector: Union[pd.Timestamp, slice, str, pd.Index] = slice(None, None),
+        level: Union[str, int] = "datetime",
+        col_set: Union[str, List[str]] = CS_ALL,
+        squeeze: bool = False,
+        proc_func: Callable = None,
+    ):
+        # This method is extracted for sharing in subclasses
+        from .storage import BaseHandlerStorage  # pylint: disable=C0415
+
+        # Following conflicts may occur
+        # - Does [20200101", "20210101"] mean selecting this slice or these two days?
+        # To solve this issue
+        #   - slice have higher priorities (except when level is none)
+        if isinstance(selector, (tuple, list)) and level is not None:
+            # when level is None, the argument will be passed in directly
+            # we don't have to convert it into slice
+            try:
+                selector = slice(*selector)
+            except ValueError:
+                get_module_logger("DataHandlerLP").info(f"Fail to converting to query to slice. It will used directly")
+
+        if isinstance(data_storage, pd.DataFrame):
+            data_df = data_storage
+            if proc_func is not None:
+                # FIXME: fetching by time first will be more friendly to `proc_func`
+                # Copy in case of `proc_func` changing the data inplace....
+                data_df = proc_func(fetch_df_by_index(data_df, selector, level, fetch_orig=self.fetch_orig).copy())
+                data_df = fetch_df_by_col(data_df, col_set)
+            else:
+                # Fetch column  first will be more friendly to SepDataFrame
+                data_df = fetch_df_by_col(data_df, col_set)
+                data_df = fetch_df_by_index(data_df, selector, level, fetch_orig=self.fetch_orig)
+        elif isinstance(data_storage, BaseHandlerStorage):
+            if not data_storage.is_proc_func_supported():
+                if proc_func is not None:
+                    raise ValueError(f"proc_func is not supported by the storage {type(data_storage)}")
+                data_df = data_storage.fetch(
+                    selector=selector, level=level, col_set=col_set, fetch_orig=self.fetch_orig
+                )
+            else:
+                data_df = data_storage.fetch(
+                    selector=selector, level=level, col_set=col_set, fetch_orig=self.fetch_orig, proc_func=proc_func
+                )
+        else:
+            raise TypeError(f"data_storage should be pd.DataFrame|HashingStockStorage, not {type(data_storage)}")
+
+        if squeeze:
+            # squeeze columns
+            data_df = data_df.squeeze()
+            # squeeze index
+            if isinstance(selector, (str, pd.Timestamp)):
+                data_df = data_df.reset_index(level=level, drop=True)
+        return data_df
+
+    def get_cols(self, col_set=CS_ALL) -> list:
+        """
+        get the column names
+
+        Parameters
+        ----------
+        col_set : str
+            select a set of meaningful columns.(e.g. features, columns)
+
+        Returns
+        -------
+        list:
+            list of column names
+        """
+        df = self._data.head()
+        df = fetch_df_by_col(df, col_set)
+        return df.columns.to_list()
+
+    def get_range_selector(self, cur_date: Union[pd.Timestamp, str], periods: int) -> slice:
+        """
+        get range selector by number of periods
+
+        Args:
+            cur_date (pd.Timestamp or str): current date
+            periods (int): number of periods
+        """
+        trading_dates = self._data.index.unique(level="datetime")
+        cur_loc = trading_dates.get_loc(cur_date)
+        pre_loc = cur_loc - periods + 1
+        if pre_loc < 0:
+            warnings.warn("`periods` is too large. the first date will be returned.")
+            pre_loc = 0
+        ref_date = trading_dates[pre_loc]
+        return slice(ref_date, cur_date)
+
+    def get_range_iterator(
+        self, periods: int, min_periods: Optional[int] = None, **kwargs
+    ) -> Iterator[Tuple[pd.Timestamp, pd.DataFrame]]:
+        """
+        get an iterator of sliced data with given periods
+
+        Args:
+            periods (int): number of periods.
+            min_periods (int): minimum periods for sliced dataframe.
+            kwargs (dict): will be passed to `self.fetch`.
+        """
+        trading_dates = self._data.index.unique(level="datetime")
+        if min_periods is None:
+            min_periods = periods
+        for cur_date in trading_dates[min_periods:]:
+            selector = self.get_range_selector(cur_date, periods)
+            yield cur_date, self.fetch(selector, **kwargs)
+
+
+DATA_KEY_TYPE = Literal["raw", "infer", "learn"]
+
+
+class DataHandlerLP(DataHandler):
+    """
+    DataHandler with **(L)earnable (P)rocessor**
+
+    This handler will produce three pieces of data in pd.DataFrame format.
+
+    - DK_R / self._data: the raw data loaded from the loader
+    - DK_I / self._infer: the data processed for inference
+    - DK_L / self._learn: the data processed for learning model.
+
+    The motivation of using different processor workflows for learning and inference
+    Here are some examples.
+
+    - The instrument universe for learning and inference may be different.
+    - The processing of some samples may rely on label (for example, some samples hit the limit may need extra processing or be dropped).
+
+        - These processors only apply to the learning phase.
+
+    Tips for data handler
+
+    - To reduce the memory cost
+
+        - `drop_raw=True`: this will modify the data inplace on raw data;
+
+    - Please note processed data like `self._infer` or `self._learn` are concepts different from `segments` in Qlib's `Dataset` like "train" and "test"
+
+        - Processed data like `self._infer` or `self._learn` are underlying data processed with different processors
+        - `segments` in Qlib's `Dataset` like "train" and "test" are simply the time segmentations when querying data("train" are often before "test" in time-series).
+        - For example, you can query `data._infer` processed by `infer_processors` in the "train" time segmentation.
+    """
+
+    # based on `self._data`, _infer and _learn are genrated after processors
+    _infer: pd.DataFrame  # data for inference
+    _learn: pd.DataFrame  # data for learning models
+
+    # data key
+    DK_R: DATA_KEY_TYPE = "raw"
+    DK_I: DATA_KEY_TYPE = "infer"
+    DK_L: DATA_KEY_TYPE = "learn"
+    # map data_key to attribute name
+    ATTR_MAP = {DK_R: "_data", DK_I: "_infer", DK_L: "_learn"}
+
+    # process type
+    PTYPE_I = "independent"
+    # - self._infer will be processed by shared_processors + infer_processors
+    # - self._learn will be processed by shared_processors + learn_processors
+
+    # NOTE:
+    PTYPE_A = "append"
+
+    # - self._infer will be processed by shared_processors + infer_processors
+    # - self._learn will be processed by shared_processors + infer_processors + learn_processors
+    #   - (e.g. self._infer processed by learn_processors )
+
+    def __init__(
+        self,
+        instruments=None,
+        start_time=None,
+        end_time=None,
+        data_loader: Union[dict, str, DataLoader] = None,
+        infer_processors: List = [],
+        learn_processors: List = [],
+        shared_processors: List = [],
+        process_type=PTYPE_A,
+        drop_raw=False,
+        **kwargs,
+    ):
+        """
+        Parameters
+        ----------
+        infer_processors : list
+            - list of <description info> of processors to generate data for inference
+
+            - example of <description info>:
+
+            .. code-block::
+
+                1) classname & kwargs:
+                    {
+                        "class": "MinMaxNorm",
+                        "kwargs": {
+                            "fit_start_time": "20080101",
+                            "fit_end_time": "20121231"
+                        }
+                    }
+                2) Only classname:
+                    "DropnaFeature"
+                3) object instance of Processor
+
+        learn_processors : list
+            similar to infer_processors, but for generating data for learning models
+
+        process_type: str
+            PTYPE_I = 'independent'
+
+            - self._infer will be processed by infer_processors
+
+            - self._learn will be processed by learn_processors
+
+            PTYPE_A = 'append'
+
+            - self._infer will be processed by infer_processors
+
+            - self._learn will be processed by infer_processors + learn_processors
+
+              - (e.g. self._infer processed by learn_processors )
+        drop_raw: bool
+            Whether to drop the raw data
+        """
+
+        # Setup preprocessor
+        self.infer_processors = []  # for lint
+        self.learn_processors = []  # for lint
+        self.shared_processors = []  # for lint
+        for pname in "infer_processors", "learn_processors", "shared_processors":
+            for proc in locals()[pname]:
+                getattr(self, pname).append(
+                    init_instance_by_config(
+                        proc,
+                        None if (isinstance(proc, dict) and "module_path" in proc) else processor_module,
+                        accept_types=processor_module.Processor,
+                    )
+                )
+
+        self.process_type = process_type
+        self.drop_raw = drop_raw
+        super().__init__(instruments, start_time, end_time, data_loader, **kwargs)
+
+    def get_all_processors(self):
+        return self.shared_processors + self.infer_processors + self.learn_processors
+
+    def fit(self):
+        """
+        fit data without processing the data
+        """
+        for proc in self.get_all_processors():
+            with TimeInspector.logt(f"{proc.__class__.__name__}"):
+                proc.fit(self._data)
+
+    def fit_process_data(self):
+        """
+        fit and process data
+
+        The input of the `fit` will be the output of the previous processor
+        """
+        self.process_data(with_fit=True)
+
+    @staticmethod
+    def _run_proc_l(
+        df: pd.DataFrame, proc_l: List[processor_module.Processor], with_fit: bool, check_for_infer: bool
+    ) -> pd.DataFrame:
+        for proc in proc_l:
+            if check_for_infer and not proc.is_for_infer():
+                raise TypeError("Only processors usable for inference can be used in `infer_processors` ")
+            with TimeInspector.logt(f"{proc.__class__.__name__}"):
+                if with_fit:
+                    proc.fit(df)
+                df = proc(df)
+        return df
+
+    @staticmethod
+    def _is_proc_readonly(proc_l: List[processor_module.Processor]):
+        """
+        NOTE: it will return True if `len(proc_l) == 0`
+        """
+        for p in proc_l:
+            if not p.readonly():
+                return False
+        return True
+
+    def process_data(self, with_fit: bool = False):
+        """
+        process_data data. Fun `processor.fit` if necessary
+
+        Notation: (data)  [processor]
+
+        # data processing flow of self.process_type == DataHandlerLP.PTYPE_I
+
+        .. code-block:: text
+
+            (self._data)-[shared_processors]-(_shared_df)-[learn_processors]-(_learn_df)
+                                                   \\
+                                                    -[infer_processors]-(_infer_df)
+
+        # data processing flow of self.process_type == DataHandlerLP.PTYPE_A
+
+        .. code-block:: text
+
+            (self._data)-[shared_processors]-(_shared_df)-[infer_processors]-(_infer_df)-[learn_processors]-(_learn_df)
+
+        Parameters
+        ----------
+        with_fit : bool
+            The input of the `fit` will be the output of the previous processor
+        """
+        # shared data processors
+        # 1) assign
+        _shared_df = self._data
+        if not self._is_proc_readonly(self.shared_processors):  # avoid modifying the original data
+            _shared_df = _shared_df.copy()
+        # 2) process
+        _shared_df = self._run_proc_l(_shared_df, self.shared_processors, with_fit=with_fit, check_for_infer=True)
+
+        # data for inference
+        # 1) assign
+        _infer_df = _shared_df
+        if not self._is_proc_readonly(self.infer_processors):  # avoid modifying the original data
+            _infer_df = _infer_df.copy()
+        # 2) process
+        _infer_df = self._run_proc_l(_infer_df, self.infer_processors, with_fit=with_fit, check_for_infer=True)
+
+        self._infer = _infer_df
+
+        # data for learning
+        # 1) assign
+        if self.process_type == DataHandlerLP.PTYPE_I:
+            _learn_df = _shared_df
+        elif self.process_type == DataHandlerLP.PTYPE_A:
+            # based on `infer_df` and append the processor
+            _learn_df = _infer_df
+        else:
+            raise NotImplementedError(f"This type of input is not supported")
+        if not self._is_proc_readonly(self.learn_processors):  # avoid modifying the original  data
+            _learn_df = _learn_df.copy()
+        # 2) process
+        _learn_df = self._run_proc_l(_learn_df, self.learn_processors, with_fit=with_fit, check_for_infer=False)
+
+        self._learn = _learn_df
+
+        if self.drop_raw:
+            del self._data
+
+    def config(self, processor_kwargs: dict = None, **kwargs):
+        """
+        configuration of data.
+        # what data to be loaded from data source
+
+        This method will be used when loading pickled handler from dataset.
+        The data will be initialized with different time range.
+
+        """
+        super().config(**kwargs)
+        if processor_kwargs is not None:
+            for processor in self.get_all_processors():
+                processor.config(**processor_kwargs)
+
+    # init type
+    IT_FIT_SEQ = "fit_seq"  # the input of `fit` will be the output of the previous processor
+    IT_FIT_IND = "fit_ind"  # the input of `fit` will be the original df
+    IT_LS = "load_state"  # The state of the object has been load by pickle
+
+    def setup_data(self, init_type: str = IT_FIT_SEQ, **kwargs):
+        """
+        Set up the data in case of running initialization for multiple time
+
+        Parameters
+        ----------
+        init_type : str
+            The type `IT_*` listed above.
+        enable_cache : bool
+            default value is false:
+
+            - if `enable_cache` == True:
+
+                the processed data will be saved on disk, and handler will load the cached data from the disk directly
+                when we call `init` next time
+        """
+        # init raw data
+        super().setup_data(**kwargs)
+
+        with TimeInspector.logt("fit & process data"):
+            if init_type == DataHandlerLP.IT_FIT_IND:
+                self.fit()
+                self.process_data()
+            elif init_type == DataHandlerLP.IT_LS:
+                self.process_data()
+            elif init_type == DataHandlerLP.IT_FIT_SEQ:
+                self.fit_process_data()
+            else:
+                raise NotImplementedError(f"This type of input is not supported")
+
+        # TODO: Be able to cache handler data. Save the memory for data processing
+
+    def _get_df_by_key(self, data_key: DATA_KEY_TYPE = DK_I) -> pd.DataFrame:
+        if data_key == self.DK_R and self.drop_raw:
+            raise AttributeError(
+                "DataHandlerLP has not attribute _data, please set drop_raw = False if you want to use raw data"
+            )
+        df = getattr(self, self.ATTR_MAP[data_key])
+        return df
+
+    def fetch(
+        self,
+        selector: Union[pd.Timestamp, slice, str] = slice(None, None),
+        level: Union[str, int] = "datetime",
+        col_set=DataHandler.CS_ALL,
+        data_key: DATA_KEY_TYPE = DK_I,
+        squeeze: bool = False,
+        proc_func: Callable = None,
+    ) -> pd.DataFrame:
+        """
+        fetch data from underlying data source
+
+        Parameters
+        ----------
+        selector : Union[pd.Timestamp, slice, str]
+            describe how to select data by index.
+        level : Union[str, int]
+            which index level to select the data.
+        col_set : str
+            select a set of meaningful columns.(e.g. features, columns).
+        data_key : str
+            the data to fetch:  DK_*.
+        proc_func: Callable
+            please refer to the doc of DataHandler.fetch
+
+        Returns
+        -------
+        pd.DataFrame:
+        """
+
+        return self._fetch_data(
+            data_storage=self._get_df_by_key(data_key),
+            selector=selector,
+            level=level,
+            col_set=col_set,
+            squeeze=squeeze,
+            proc_func=proc_func,
+        )
+
+    def get_cols(self, col_set=DataHandler.CS_ALL, data_key: DATA_KEY_TYPE = DK_I) -> list:
+        """
+        get the column names
+
+        Parameters
+        ----------
+        col_set : str
+            select a set of meaningful columns.(e.g. features, columns).
+        data_key : DATA_KEY_TYPE
+            the data to fetch:  DK_*.
+
+        Returns
+        -------
+        list:
+            list of column names
+        """
+        df = self._get_df_by_key(data_key).head()
+        df = fetch_df_by_col(df, col_set)
+        return df.columns.to_list()
+
+    @classmethod
+    def cast(cls, handler: "DataHandlerLP") -> "DataHandlerLP":
+        """
+        Motivation
+
+        - A user creates a datahandler in his customized package. Then he wants to share the processed handler to
+          other users without introduce the package dependency and complicated data processing logic.
+        - This class make it possible by casting the class to DataHandlerLP and only keep the processed data
+
+        Parameters
+        ----------
+        handler : DataHandlerLP
+            A subclass of DataHandlerLP
+
+        Returns
+        -------
+        DataHandlerLP:
+            the converted processed data
+        """
+        new_hd: DataHandlerLP = object.__new__(DataHandlerLP)
+        new_hd.from_cast = True  # add a mark for the cast instance
+
+        for key in list(DataHandlerLP.ATTR_MAP.values()) + [
+            "instruments",
+            "start_time",
+            "end_time",
+            "fetch_orig",
+            "drop_raw",
+        ]:
+            setattr(new_hd, key, getattr(handler, key, None))
+        return new_hd
+
+    @classmethod
+    def from_df(cls, df: pd.DataFrame) -> "DataHandlerLP":
+        """
+        Motivation:
+        - When user want to get a quick data handler.
+
+        The created data handler will have only one shared Dataframe without processors.
+        After creating the handler, user may often want to dump the handler for reuse
+        Here is a typical use case
+
+        .. code-block:: python
+
+            from qlib.data.dataset import DataHandlerLP
+            dh = DataHandlerLP.from_df(df)
+            dh.to_pickle(fname, dump_all=True)
+
+        TODO:
+        - The StaticDataLoader is quite slow. It don't have to copy the data again...
+
+        """
+        loader = data_loader_module.StaticDataLoader(df)
+        return cls(data_loader=loader)
```

## qlib/data/dataset/loader.py

 * *Ordering differences only*

```diff
@@ -1,342 +1,342 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-import abc
-import pickle
-from pathlib import Path
-import warnings
-import pandas as pd
-
-from typing import Tuple, Union, List
-
-from qlib.data import D
-from qlib.utils import load_dataset, init_instance_by_config, time_to_slc_point
-from qlib.log import get_module_logger
-from qlib.utils.serial import Serializable
-
-
-class DataLoader(abc.ABC):
-    """
-    DataLoader is designed for loading raw data from original data source.
-    """
-
-    @abc.abstractmethod
-    def load(self, instruments, start_time=None, end_time=None) -> pd.DataFrame:
-        """
-        load the data as pd.DataFrame.
-
-        Example of the data (The multi-index of the columns is optional.):
-
-            .. code-block:: text
-
-                                        feature                                                             label
-                                        $close     $volume     Ref($close, 1)  Mean($close, 3)  $high-$low  LABEL0
-                datetime    instrument
-                2010-01-04  SH600000    81.807068  17145150.0       83.737389        83.016739    2.741058  0.0032
-                            SH600004    13.313329  11800983.0       13.313329        13.317701    0.183632  0.0042
-                            SH600005    37.796539  12231662.0       38.258602        37.919757    0.970325  0.0289
-
-
-        Parameters
-        ----------
-        instruments : str or dict
-            it can either be the market name or the config file of instruments generated by InstrumentProvider.
-        start_time : str
-            start of the time range.
-        end_time : str
-            end of the time range.
-
-        Returns
-        -------
-        pd.DataFrame:
-            data load from the under layer source
-        """
-
-
-class DLWParser(DataLoader):
-    """
-    (D)ata(L)oader (W)ith (P)arser for features and names
-
-    Extracting this class so that QlibDataLoader and other dataloaders(such as QdbDataLoader) can share the fields.
-    """
-
-    def __init__(self, config: Union[list, tuple, dict]):
-        """
-        Parameters
-        ----------
-        config : Union[list, tuple, dict]
-            Config will be used to describe the fields and column names
-
-            .. code-block::
-
-                <config> := {
-                    "group_name1": <fields_info1>
-                    "group_name2": <fields_info2>
-                }
-                or
-                <config> := <fields_info>
-
-                <fields_info> := ["expr", ...] | (["expr", ...], ["col_name", ...])
-                # NOTE: list or tuple will be treated as the things when parsing
-        """
-        self.is_group = isinstance(config, dict)
-
-        if self.is_group:
-            self.fields = {grp: self._parse_fields_info(fields_info) for grp, fields_info in config.items()}
-        else:
-            self.fields = self._parse_fields_info(config)
-
-    def _parse_fields_info(self, fields_info: Union[list, tuple]) -> Tuple[list, list]:
-        if len(fields_info) == 0:
-            raise ValueError("The size of fields must be greater than 0")
-
-        if not isinstance(fields_info, (list, tuple)):
-            raise TypeError("Unsupported type")
-
-        if isinstance(fields_info[0], str):
-            exprs = names = fields_info
-        elif isinstance(fields_info[0], (list, tuple)):
-            exprs, names = fields_info
-        else:
-            raise NotImplementedError(f"This type of input is not supported")
-        return exprs, names
-
-    @abc.abstractmethod
-    def load_group_df(
-        self,
-        instruments,
-        exprs: list,
-        names: list,
-        start_time: Union[str, pd.Timestamp] = None,
-        end_time: Union[str, pd.Timestamp] = None,
-        gp_name: str = None,
-    ) -> pd.DataFrame:
-        """
-        load the dataframe for specific group
-
-        Parameters
-        ----------
-        instruments :
-            the instruments.
-        exprs : list
-            the expressions to describe the content of the data.
-        names : list
-            the name of the data.
-
-        Returns
-        -------
-        pd.DataFrame:
-            the queried dataframe.
-        """
-
-    def load(self, instruments=None, start_time=None, end_time=None) -> pd.DataFrame:
-        if self.is_group:
-            df = pd.concat(
-                {
-                    grp: self.load_group_df(instruments, exprs, names, start_time, end_time, grp)
-                    for grp, (exprs, names) in self.fields.items()
-                },
-                axis=1,
-            )
-        else:
-            exprs, names = self.fields
-            df = self.load_group_df(instruments, exprs, names, start_time, end_time)
-        return df
-
-
-class QlibDataLoader(DLWParser):
-    """Same as QlibDataLoader. The fields can be define by config"""
-
-    def __init__(
-        self,
-        config: Tuple[list, tuple, dict],
-        filter_pipe: List = None,
-        swap_level: bool = True,
-        freq: Union[str, dict] = "day",
-        inst_processors: Union[dict, list] = None,
-    ):
-        """
-        Parameters
-        ----------
-        config : Tuple[list, tuple, dict]
-            Please refer to the doc of DLWParser
-        filter_pipe :
-            Filter pipe for the instruments
-        swap_level :
-            Whether to swap level of MultiIndex
-        freq:  dict or str
-            If type(config) == dict and type(freq) == str, load config data using freq.
-            If type(config) == dict and type(freq) == dict, load config[<group_name>] data using freq[<group_name>]
-        inst_processors: dict | list
-            If inst_processors is not None and type(config) == dict; load config[<group_name>] data using inst_processors[<group_name>]
-            If inst_processors is a list, then it will be applied to all groups.
-        """
-        self.filter_pipe = filter_pipe
-        self.swap_level = swap_level
-        self.freq = freq
-
-        # sample
-        self.inst_processors = inst_processors if inst_processors is not None else {}
-        assert isinstance(
-            self.inst_processors, (dict, list)
-        ), f"inst_processors(={self.inst_processors}) must be dict or list"
-
-        super().__init__(config)
-
-        if self.is_group:
-            # check sample config
-            if isinstance(freq, dict):
-                for _gp in config.keys():
-                    if _gp not in freq:
-                        raise ValueError(f"freq(={freq}) missing group(={_gp})")
-                assert (
-                    self.inst_processors
-                ), f"freq(={self.freq}), inst_processors(={self.inst_processors}) cannot be None/empty"
-
-    def load_group_df(
-        self,
-        instruments,
-        exprs: list,
-        names: list,
-        start_time: Union[str, pd.Timestamp] = None,
-        end_time: Union[str, pd.Timestamp] = None,
-        gp_name: str = None,
-    ) -> pd.DataFrame:
-        if instruments is None:
-            warnings.warn("`instruments` is not set, will load all stocks")
-            instruments = "all"
-        if isinstance(instruments, str):
-            instruments = D.instruments(instruments, filter_pipe=self.filter_pipe)
-        elif self.filter_pipe is not None:
-            warnings.warn("`filter_pipe` is not None, but it will not be used with `instruments` as list")
-
-        freq = self.freq[gp_name] if isinstance(self.freq, dict) else self.freq
-        inst_processors = (
-            self.inst_processors if isinstance(self.inst_processors, list) else self.inst_processors.get(gp_name, [])
-        )
-        df = D.features(instruments, exprs, start_time, end_time, freq=freq, inst_processors=inst_processors)
-        df.columns = names
-        if self.swap_level:
-            df = df.swaplevel().sort_index()  # NOTE: if swaplevel, return <datetime, instrument>
-        return df
-
-
-class StaticDataLoader(DataLoader, Serializable):
-    """
-    DataLoader that supports loading data from file or as provided.
-    """
-
-    include_attr = ["_config"]
-
-    def __init__(self, config: Union[dict, str, pd.DataFrame], join="outer"):
-        """
-        Parameters
-        ----------
-        config : dict
-            {fields_group: <path or object>}
-        join : str
-            How to align different dataframes
-        """
-        self._config = config  # using "_" to avoid confliction with the method `config` of Serializable
-        self.join = join
-        self._data = None
-
-    def __getstate__(self) -> dict:
-        # avoid pickling `self._data`
-        return {k: v for k, v in self.__dict__.items() if not k.startswith("_")}
-
-    def load(self, instruments=None, start_time=None, end_time=None) -> pd.DataFrame:
-        self._maybe_load_raw_data()
-        if instruments is None:
-            df = self._data
-        else:
-            df = self._data.loc(axis=0)[:, instruments]
-        if start_time is None and end_time is None:
-            return df  # NOTE: avoid copy by loc
-        # pd.Timestamp(None) == NaT, use NaT as index can not fetch correct thing, so do not change None.
-        start_time = time_to_slc_point(start_time)
-        end_time = time_to_slc_point(end_time)
-        return df.loc[start_time:end_time]
-
-    def _maybe_load_raw_data(self):
-        if self._data is not None:
-            return
-        if isinstance(self._config, dict):
-            self._data = pd.concat(
-                {fields_group: load_dataset(path_or_obj) for fields_group, path_or_obj in self._config.items()},
-                axis=1,
-                join=self.join,
-            )
-            self._data.sort_index(inplace=True)
-        elif isinstance(self._config, (str, Path)):
-            with Path(self._config).open("rb") as f:
-                self._data = pickle.load(f)
-        elif isinstance(self._config, pd.DataFrame):
-            self._data = self._config
-
-
-class DataLoaderDH(DataLoader):
-    """DataLoaderDH
-    DataLoader based on (D)ata (H)andler
-    It is designed to load multiple data from data handler
-    - If you just want to load data from single datahandler, you can write them in single data handler
-
-    TODO: What make this module not that easy to use.
-
-    - For online scenario
-
-        - The underlayer data handler should be configured. But data loader doesn't provide such interface & hook.
-    """
-
-    def __init__(self, handler_config: dict, fetch_kwargs: dict = {}, is_group=False):
-        """
-        Parameters
-        ----------
-        handler_config : dict
-            handler_config will be used to describe the handlers
-
-            .. code-block::
-
-                <handler_config> := {
-                    "group_name1": <handler>
-                    "group_name2": <handler>
-                }
-                or
-                <handler_config> := <handler>
-                <handler> := DataHandler Instance | DataHandler Config
-
-        fetch_kwargs : dict
-            fetch_kwargs will be used to describe the different arguments of fetch method, such as col_set, squeeze, data_key, etc.
-
-        is_group: bool
-            is_group will be used to describe whether the key of handler_config is group
-
-        """
-        from qlib.data.dataset.handler import DataHandler  # pylint: disable=C0415
-
-        if is_group:
-            self.handlers = {
-                grp: init_instance_by_config(config, accept_types=DataHandler) for grp, config in handler_config.items()
-            }
-        else:
-            self.handlers = init_instance_by_config(handler_config, accept_types=DataHandler)
-
-        self.is_group = is_group
-        self.fetch_kwargs = {"col_set": DataHandler.CS_RAW}
-        self.fetch_kwargs.update(fetch_kwargs)
-
-    def load(self, instruments=None, start_time=None, end_time=None) -> pd.DataFrame:
-        if instruments is not None:
-            get_module_logger(self.__class__.__name__).warning(f"instruments[{instruments}] is ignored")
-
-        if self.is_group:
-            df = pd.concat(
-                {
-                    grp: dh.fetch(selector=slice(start_time, end_time), level="datetime", **self.fetch_kwargs)
-                    for grp, dh in self.handlers.items()
-                },
-                axis=1,
-            )
-        else:
-            df = self.handlers.fetch(selector=slice(start_time, end_time), level="datetime", **self.fetch_kwargs)
-        return df
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+import abc
+import pickle
+from pathlib import Path
+import warnings
+import pandas as pd
+
+from typing import Tuple, Union, List
+
+from qlib.data import D
+from qlib.utils import load_dataset, init_instance_by_config, time_to_slc_point
+from qlib.log import get_module_logger
+from qlib.utils.serial import Serializable
+
+
+class DataLoader(abc.ABC):
+    """
+    DataLoader is designed for loading raw data from original data source.
+    """
+
+    @abc.abstractmethod
+    def load(self, instruments, start_time=None, end_time=None) -> pd.DataFrame:
+        """
+        load the data as pd.DataFrame.
+
+        Example of the data (The multi-index of the columns is optional.):
+
+            .. code-block:: text
+
+                                        feature                                                             label
+                                        $close     $volume     Ref($close, 1)  Mean($close, 3)  $high-$low  LABEL0
+                datetime    instrument
+                2010-01-04  SH600000    81.807068  17145150.0       83.737389        83.016739    2.741058  0.0032
+                            SH600004    13.313329  11800983.0       13.313329        13.317701    0.183632  0.0042
+                            SH600005    37.796539  12231662.0       38.258602        37.919757    0.970325  0.0289
+
+
+        Parameters
+        ----------
+        instruments : str or dict
+            it can either be the market name or the config file of instruments generated by InstrumentProvider.
+        start_time : str
+            start of the time range.
+        end_time : str
+            end of the time range.
+
+        Returns
+        -------
+        pd.DataFrame:
+            data load from the under layer source
+        """
+
+
+class DLWParser(DataLoader):
+    """
+    (D)ata(L)oader (W)ith (P)arser for features and names
+
+    Extracting this class so that QlibDataLoader and other dataloaders(such as QdbDataLoader) can share the fields.
+    """
+
+    def __init__(self, config: Union[list, tuple, dict]):
+        """
+        Parameters
+        ----------
+        config : Union[list, tuple, dict]
+            Config will be used to describe the fields and column names
+
+            .. code-block::
+
+                <config> := {
+                    "group_name1": <fields_info1>
+                    "group_name2": <fields_info2>
+                }
+                or
+                <config> := <fields_info>
+
+                <fields_info> := ["expr", ...] | (["expr", ...], ["col_name", ...])
+                # NOTE: list or tuple will be treated as the things when parsing
+        """
+        self.is_group = isinstance(config, dict)
+
+        if self.is_group:
+            self.fields = {grp: self._parse_fields_info(fields_info) for grp, fields_info in config.items()}
+        else:
+            self.fields = self._parse_fields_info(config)
+
+    def _parse_fields_info(self, fields_info: Union[list, tuple]) -> Tuple[list, list]:
+        if len(fields_info) == 0:
+            raise ValueError("The size of fields must be greater than 0")
+
+        if not isinstance(fields_info, (list, tuple)):
+            raise TypeError("Unsupported type")
+
+        if isinstance(fields_info[0], str):
+            exprs = names = fields_info
+        elif isinstance(fields_info[0], (list, tuple)):
+            exprs, names = fields_info
+        else:
+            raise NotImplementedError(f"This type of input is not supported")
+        return exprs, names
+
+    @abc.abstractmethod
+    def load_group_df(
+        self,
+        instruments,
+        exprs: list,
+        names: list,
+        start_time: Union[str, pd.Timestamp] = None,
+        end_time: Union[str, pd.Timestamp] = None,
+        gp_name: str = None,
+    ) -> pd.DataFrame:
+        """
+        load the dataframe for specific group
+
+        Parameters
+        ----------
+        instruments :
+            the instruments.
+        exprs : list
+            the expressions to describe the content of the data.
+        names : list
+            the name of the data.
+
+        Returns
+        -------
+        pd.DataFrame:
+            the queried dataframe.
+        """
+
+    def load(self, instruments=None, start_time=None, end_time=None) -> pd.DataFrame:
+        if self.is_group:
+            df = pd.concat(
+                {
+                    grp: self.load_group_df(instruments, exprs, names, start_time, end_time, grp)
+                    for grp, (exprs, names) in self.fields.items()
+                },
+                axis=1,
+            )
+        else:
+            exprs, names = self.fields
+            df = self.load_group_df(instruments, exprs, names, start_time, end_time)
+        return df
+
+
+class QlibDataLoader(DLWParser):
+    """Same as QlibDataLoader. The fields can be define by config"""
+
+    def __init__(
+        self,
+        config: Tuple[list, tuple, dict],
+        filter_pipe: List = None,
+        swap_level: bool = True,
+        freq: Union[str, dict] = "day",
+        inst_processors: Union[dict, list] = None,
+    ):
+        """
+        Parameters
+        ----------
+        config : Tuple[list, tuple, dict]
+            Please refer to the doc of DLWParser
+        filter_pipe :
+            Filter pipe for the instruments
+        swap_level :
+            Whether to swap level of MultiIndex
+        freq:  dict or str
+            If type(config) == dict and type(freq) == str, load config data using freq.
+            If type(config) == dict and type(freq) == dict, load config[<group_name>] data using freq[<group_name>]
+        inst_processors: dict | list
+            If inst_processors is not None and type(config) == dict; load config[<group_name>] data using inst_processors[<group_name>]
+            If inst_processors is a list, then it will be applied to all groups.
+        """
+        self.filter_pipe = filter_pipe
+        self.swap_level = swap_level
+        self.freq = freq
+
+        # sample
+        self.inst_processors = inst_processors if inst_processors is not None else {}
+        assert isinstance(
+            self.inst_processors, (dict, list)
+        ), f"inst_processors(={self.inst_processors}) must be dict or list"
+
+        super().__init__(config)
+
+        if self.is_group:
+            # check sample config
+            if isinstance(freq, dict):
+                for _gp in config.keys():
+                    if _gp not in freq:
+                        raise ValueError(f"freq(={freq}) missing group(={_gp})")
+                assert (
+                    self.inst_processors
+                ), f"freq(={self.freq}), inst_processors(={self.inst_processors}) cannot be None/empty"
+
+    def load_group_df(
+        self,
+        instruments,
+        exprs: list,
+        names: list,
+        start_time: Union[str, pd.Timestamp] = None,
+        end_time: Union[str, pd.Timestamp] = None,
+        gp_name: str = None,
+    ) -> pd.DataFrame:
+        if instruments is None:
+            warnings.warn("`instruments` is not set, will load all stocks")
+            instruments = "all"
+        if isinstance(instruments, str):
+            instruments = D.instruments(instruments, filter_pipe=self.filter_pipe)
+        elif self.filter_pipe is not None:
+            warnings.warn("`filter_pipe` is not None, but it will not be used with `instruments` as list")
+
+        freq = self.freq[gp_name] if isinstance(self.freq, dict) else self.freq
+        inst_processors = (
+            self.inst_processors if isinstance(self.inst_processors, list) else self.inst_processors.get(gp_name, [])
+        )
+        df = D.features(instruments, exprs, start_time, end_time, freq=freq, inst_processors=inst_processors)
+        df.columns = names
+        if self.swap_level:
+            df = df.swaplevel().sort_index()  # NOTE: if swaplevel, return <datetime, instrument>
+        return df
+
+
+class StaticDataLoader(DataLoader, Serializable):
+    """
+    DataLoader that supports loading data from file or as provided.
+    """
+
+    include_attr = ["_config"]
+
+    def __init__(self, config: Union[dict, str, pd.DataFrame], join="outer"):
+        """
+        Parameters
+        ----------
+        config : dict
+            {fields_group: <path or object>}
+        join : str
+            How to align different dataframes
+        """
+        self._config = config  # using "_" to avoid confliction with the method `config` of Serializable
+        self.join = join
+        self._data = None
+
+    def __getstate__(self) -> dict:
+        # avoid pickling `self._data`
+        return {k: v for k, v in self.__dict__.items() if not k.startswith("_")}
+
+    def load(self, instruments=None, start_time=None, end_time=None) -> pd.DataFrame:
+        self._maybe_load_raw_data()
+        if instruments is None:
+            df = self._data
+        else:
+            df = self._data.loc(axis=0)[:, instruments]
+        if start_time is None and end_time is None:
+            return df  # NOTE: avoid copy by loc
+        # pd.Timestamp(None) == NaT, use NaT as index can not fetch correct thing, so do not change None.
+        start_time = time_to_slc_point(start_time)
+        end_time = time_to_slc_point(end_time)
+        return df.loc[start_time:end_time]
+
+    def _maybe_load_raw_data(self):
+        if self._data is not None:
+            return
+        if isinstance(self._config, dict):
+            self._data = pd.concat(
+                {fields_group: load_dataset(path_or_obj) for fields_group, path_or_obj in self._config.items()},
+                axis=1,
+                join=self.join,
+            )
+            self._data.sort_index(inplace=True)
+        elif isinstance(self._config, (str, Path)):
+            with Path(self._config).open("rb") as f:
+                self._data = pickle.load(f)
+        elif isinstance(self._config, pd.DataFrame):
+            self._data = self._config
+
+
+class DataLoaderDH(DataLoader):
+    """DataLoaderDH
+    DataLoader based on (D)ata (H)andler
+    It is designed to load multiple data from data handler
+    - If you just want to load data from single datahandler, you can write them in single data handler
+
+    TODO: What make this module not that easy to use.
+
+    - For online scenario
+
+        - The underlayer data handler should be configured. But data loader doesn't provide such interface & hook.
+    """
+
+    def __init__(self, handler_config: dict, fetch_kwargs: dict = {}, is_group=False):
+        """
+        Parameters
+        ----------
+        handler_config : dict
+            handler_config will be used to describe the handlers
+
+            .. code-block::
+
+                <handler_config> := {
+                    "group_name1": <handler>
+                    "group_name2": <handler>
+                }
+                or
+                <handler_config> := <handler>
+                <handler> := DataHandler Instance | DataHandler Config
+
+        fetch_kwargs : dict
+            fetch_kwargs will be used to describe the different arguments of fetch method, such as col_set, squeeze, data_key, etc.
+
+        is_group: bool
+            is_group will be used to describe whether the key of handler_config is group
+
+        """
+        from qlib.data.dataset.handler import DataHandler  # pylint: disable=C0415
+
+        if is_group:
+            self.handlers = {
+                grp: init_instance_by_config(config, accept_types=DataHandler) for grp, config in handler_config.items()
+            }
+        else:
+            self.handlers = init_instance_by_config(handler_config, accept_types=DataHandler)
+
+        self.is_group = is_group
+        self.fetch_kwargs = {"col_set": DataHandler.CS_RAW}
+        self.fetch_kwargs.update(fetch_kwargs)
+
+    def load(self, instruments=None, start_time=None, end_time=None) -> pd.DataFrame:
+        if instruments is not None:
+            get_module_logger(self.__class__.__name__).warning(f"instruments[{instruments}] is ignored")
+
+        if self.is_group:
+            df = pd.concat(
+                {
+                    grp: dh.fetch(selector=slice(start_time, end_time), level="datetime", **self.fetch_kwargs)
+                    for grp, dh in self.handlers.items()
+                },
+                axis=1,
+            )
+        else:
+            df = self.handlers.fetch(selector=slice(start_time, end_time), level="datetime", **self.fetch_kwargs)
+        return df
```

## qlib/data/dataset/processor.py

```diff
@@ -1,421 +1,420 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-import abc
-from typing import Union, Text, Optional
-import numpy as np
-import pandas as pd
-
-from qlib.utils.data import robust_zscore, zscore
-from ...constant import EPS
-from .utils import fetch_df_by_index
-from ...utils.serial import Serializable
-from ...utils.paral import datetime_groupby_apply
-from qlib.data.inst_processor import InstProcessor
-from qlib.data import D
-
-
-def get_group_columns(df: pd.DataFrame, group: Union[Text, None]):
-    """
-    get a group of columns from multi-index columns DataFrame
-
-    Parameters
-    ----------
-    df : pd.DataFrame
-        with multi of columns.
-    group : str
-        the name of the feature group, i.e. the first level value of the group index.
-    """
-    if group is None:
-        return df.columns
-    else:
-        return df.columns[df.columns.get_loc(group)]
-
-
-class Processor(Serializable):
-    def fit(self, df: pd.DataFrame = None):
-        """
-        learn data processing parameters
-
-        Parameters
-        ----------
-        df : pd.DataFrame
-            When we fit and process data with processor one by one. The fit function reiles on the output of previous
-            processor, i.e. `df`.
-
-        """
-
-    @abc.abstractmethod
-    def __call__(self, df: pd.DataFrame):
-        """
-        process the data
-
-        NOTE: **The processor could change the content of `df` inplace !!!!! **
-        User should keep a copy of data outside
-
-        Parameters
-        ----------
-        df : pd.DataFrame
-            The raw_df of handler or result from previous processor.
-        """
-
-    def is_for_infer(self) -> bool:
-        """
-        Is this processor usable for inference
-        Some processors are not usable for inference.
-
-        Returns
-        -------
-        bool:
-            if it is usable for infenrece.
-        """
-        return True
-
-    def readonly(self) -> bool:
-        """
-        Does the processor treat the input data readonly (i.e. does not write the input data) when processing
-
-        Knowning the readonly information is helpful to the Handler to avoid uncessary copy
-        """
-        return False
-
-    def config(self, **kwargs):
-        attr_list = {"fit_start_time", "fit_end_time"}
-        for k, v in kwargs.items():
-            if k in attr_list and hasattr(self, k):
-                setattr(self, k, v)
-
-        for attr in attr_list:
-            if attr in kwargs:
-                kwargs.pop(attr)
-        super().config(**kwargs)
-
-
-class DropnaProcessor(Processor):
-    def __init__(self, fields_group=None):
-        self.fields_group = fields_group
-
-    def __call__(self, df):
-        return df.dropna(subset=get_group_columns(df, self.fields_group))
-
-    def readonly(self):
-        return True
-
-
-class DropnaLabel(DropnaProcessor):
-    def __init__(self, fields_group="label"):
-        super().__init__(fields_group=fields_group)
-
-    def is_for_infer(self) -> bool:
-        """The samples are dropped according to label. So it is not usable for inference"""
-        return False
-
-
-class DropCol(Processor):
-    def __init__(self, col_list=[]):
-        self.col_list = col_list
-
-    def __call__(self, df):
-        if isinstance(df.columns, pd.MultiIndex):
-            mask = df.columns.get_level_values(-1).isin(self.col_list)
-        else:
-            mask = df.columns.isin(self.col_list)
-        return df.loc[:, ~mask]
-
-    def readonly(self):
-        return True
-
-
-class FilterCol(Processor):
-    def __init__(self, fields_group="feature", col_list=[]):
-        self.fields_group = fields_group
-        self.col_list = col_list
-
-    def __call__(self, df):
-
-        cols = get_group_columns(df, self.fields_group)
-        all_cols = df.columns
-        diff_cols = np.setdiff1d(all_cols.get_level_values(-1), cols.get_level_values(-1))
-        self.col_list = np.union1d(diff_cols, self.col_list)
-        mask = df.columns.get_level_values(-1).isin(self.col_list)
-        return df.loc[:, mask]
-
-    def readonly(self):
-        return True
-
-
-class TanhProcess(Processor):
-    """Use tanh to process noise data"""
-
-    def __call__(self, df):
-        def tanh_denoise(data):
-            mask = data.columns.get_level_values(1).str.contains("LABEL")
-            col = df.columns[~mask]
-            data[col] = data[col] - 1
-            data[col] = np.tanh(data[col])
-
-            return data
-
-        return tanh_denoise(df)
-
-
-class ProcessInf(Processor):
-    """Process infinity"""
-
-    def __call__(self, df):
-        def replace_inf(data):
-            def process_inf(df):
-                for col in df.columns:
-                    # FIXME: Such behavior is very weird
-                    df[col] = df[col].replace([np.inf, -np.inf], df[col][~np.isinf(df[col])].mean())
-                return df
-
-            data = datetime_groupby_apply(data, process_inf)
-            data.sort_index(inplace=True)
-            return data
-
-        return replace_inf(df)
-
-
-class Fillna(Processor):
-    """Process NaN"""
-
-    def __init__(self, fields_group=None, fill_value=0):
-        self.fields_group = fields_group
-        self.fill_value = fill_value
-
-    def __call__(self, df):
-        if self.fields_group is None:
-            df.fillna(self.fill_value, inplace=True)
-        else:
-            cols = get_group_columns(df, self.fields_group)
-            # this implementation is extremely slow
-            # df.fillna({col: self.fill_value for col in cols}, inplace=True)
-
-            # So we use numpy to accelerate filling values
-            nan_select = np.isnan(df.values)
-            nan_select[:, ~df.columns.isin(cols)] = False
-            df.values[nan_select] = self.fill_value
-        return df
-
-
-class MinMaxNorm(Processor):
-    def __init__(self, fit_start_time, fit_end_time, fields_group=None):
-        # NOTE: correctly set the `fit_start_time` and `fit_end_time` is very important !!!
-        # `fit_end_time` **must not** include any information from the test data!!!
-        self.fit_start_time = fit_start_time
-        self.fit_end_time = fit_end_time
-        self.fields_group = fields_group
-
-    def fit(self, df: pd.DataFrame = None):
-        df = fetch_df_by_index(df, slice(self.fit_start_time, self.fit_end_time), level="datetime")
-        cols = get_group_columns(df, self.fields_group)
-        self.min_val = np.nanmin(df[cols].values, axis=0)
-        self.max_val = np.nanmax(df[cols].values, axis=0)
-        self.ignore = self.min_val == self.max_val
-        # To improve the speed, we set the value of `min_val` to `0` for the columns that do not need to be processed,
-        # and the value of `max_val` to `1`, when using `(x - min_val) / (max_val - min_val)` for uniform calculation,
-        # the columns that do not need to be processed will be calculated by `(x - 0) / (1 - 0)`,
-        # as you can see, the columns that do not need to be processed, will not be affected.
-        for _i, _con in enumerate(self.ignore):
-            if _con:
-                self.min_val[_i] = 0
-                self.max_val[_i] = 1
-        self.cols = cols
-
-    def __call__(self, df):
-        def normalize(x, min_val=self.min_val, max_val=self.max_val):
-            return (x - min_val) / (max_val - min_val)
-
-        df.loc(axis=1)[self.cols] = normalize(df[self.cols].values)
-        return df
-
-
-class ZScoreNorm(Processor):
-    """ZScore Normalization"""
-
-    def __init__(self, fit_start_time, fit_end_time, fields_group=None):
-        # NOTE: correctly set the `fit_start_time` and `fit_end_time` is very important !!!
-        # `fit_end_time` **must not** include any information from the test data!!!
-        self.fit_start_time = fit_start_time
-        self.fit_end_time = fit_end_time
-        self.fields_group = fields_group
-
-    def fit(self, df: pd.DataFrame = None):
-        df = fetch_df_by_index(df, slice(self.fit_start_time, self.fit_end_time), level="datetime")
-        cols = get_group_columns(df, self.fields_group)
-        self.mean_train = np.nanmean(df[cols].values, axis=0)
-        self.std_train = np.nanstd(df[cols].values, axis=0)
-        self.ignore = self.std_train == 0
-        # To improve the speed, we set the value of `std_train` to `1` for the columns that do not need to be processed,
-        # and the value of `mean_train` to `0`, when using `(x - mean_train) / std_train` for uniform calculation,
-        # the columns that do not need to be processed will be calculated by `(x - 0) / 1`,
-        # as you can see, the columns that do not need to be processed, will not be affected.
-        for _i, _con in enumerate(self.ignore):
-            if _con:
-                self.std_train[_i] = 1
-                self.mean_train[_i] = 0
-        self.cols = cols
-
-    def __call__(self, df):
-        def normalize(x, mean_train=self.mean_train, std_train=self.std_train):
-            return (x - mean_train) / std_train
-
-        df.loc(axis=1)[self.cols] = normalize(df[self.cols].values)
-        return df
-
-
-class RobustZScoreNorm(Processor):
-    """Robust ZScore Normalization
-
-    Use robust statistics for Z-Score normalization:
-        mean(x) = median(x)
-        std(x) = MAD(x) * 1.4826
-
-    Reference:
-        https://en.wikipedia.org/wiki/Median_absolute_deviation.
-    """
-
-    def __init__(self, fit_start_time, fit_end_time, fields_group=None, clip_outlier=True):
-        # NOTE: correctly set the `fit_start_time` and `fit_end_time` is very important !!!
-        # `fit_end_time` **must not** include any information from the test data!!!
-        self.fit_start_time = fit_start_time
-        self.fit_end_time = fit_end_time
-        self.fields_group = fields_group
-        self.clip_outlier = clip_outlier
-
-    def fit(self, df: pd.DataFrame = None):
-        df = fetch_df_by_index(df, slice(self.fit_start_time, self.fit_end_time), level="datetime")
-        self.cols = get_group_columns(df, self.fields_group)
-        X = df[self.cols].values
-        self.mean_train = np.nanmedian(X, axis=0)
-        self.std_train = np.nanmedian(np.abs(X - self.mean_train), axis=0)
-        self.std_train += EPS
-        self.std_train *= 1.4826
-
-    def __call__(self, df):
-        X = df[self.cols]
-        X -= self.mean_train
-        X /= self.std_train
-        if self.clip_outlier:
-            X = np.clip(X, -3, 3)
-        df[self.cols] = X
-        return df
-
-
-class CSZScoreNorm(Processor):
-    """Cross Sectional ZScore Normalization"""
-
-    def __init__(self, fields_group=None, method="zscore"):
-        self.fields_group = fields_group
-        if method == "zscore":
-            self.zscore_func = zscore
-        elif method == "robust":
-            self.zscore_func = robust_zscore
-        else:
-            raise NotImplementedError(f"This type of input is not supported")
-
-    def __call__(self, df):
-        # try not modify original dataframe
-        if not isinstance(self.fields_group, list):
-            self.fields_group = [self.fields_group]
-        for g in self.fields_group:
-            cols = get_group_columns(df, g)
-            df[cols] = df[cols].groupby("datetime", group_keys=False).apply(self.zscore_func)
-        return df
-
-
-class CSRankNorm(Processor):
-    """
-    Cross Sectional Rank Normalization.
-    "Cross Sectional" is often used to describe data operations.
-    The operations across different stocks are often called Cross Sectional Operation.
-
-    For example, CSRankNorm is an operation that grouping the data by each day and rank `across` all the stocks in each day.
-
-    Explanation about 3.46 & 0.5
-
-    .. code-block:: python
-
-        import numpy as np
-        import pandas as pd
-        x = np.random.random(10000)  # for any variable
-        x_rank = pd.Series(x).rank(pct=True)  # if it is converted to rank, it will be a uniform distributed
-        x_rank_norm = (x_rank - x_rank.mean()) / x_rank.std()  # Normally, we will normalize it to make it like normal distribution
-
-        x_rank.mean()   # accounts for 0.5
-        1 / x_rank.std()  # accounts for 3.46
-
-    """
-
-    def __init__(self, fields_group=None):
-        self.fields_group = fields_group
-
-    def __call__(self, df):
-        # try not modify original dataframe
-        cols = get_group_columns(df, self.fields_group)
-        t = df[cols].groupby("datetime").rank(pct=True)
-        t -= 0.5
-        t *= 3.46  # NOTE: towards unit std
-        df[cols] = t
-        return df
-
-
-class CSZFillna(Processor):
-    """Cross Sectional Fill Nan"""
-
-    def __init__(self, fields_group=None):
-        self.fields_group = fields_group
-
-    def __call__(self, df):
-        cols = get_group_columns(df, self.fields_group)
-        df[cols] = df[cols].groupby("datetime", group_keys=False).apply(lambda x: x.fillna(x.mean()))
-        return df
-
-
-class HashStockFormat(Processor):
-    """Process the storage of from df into hasing stock format"""
-
-    def __call__(self, df: pd.DataFrame):
-        from .storage import HashingStockStorage  # pylint: disable=C0415
-
-        return HashingStockStorage.from_df(df)
-
-
-class TimeRangeFlt(InstProcessor):
-    """
-    This is a filter to filter stock.
-    Only keep the data that exist from start_time to end_time (the existence in the middle is not checked.)
-    WARNING:  It may induce leakage!!!
-    """
-
-    def __init__(
-        self,
-        start_time: Optional[Union[pd.Timestamp, str]] = None,
-        end_time: Optional[Union[pd.Timestamp, str]] = None,
-        freq: str = "day",
-    ):
-        """
-        Parameters
-        ----------
-        start_time : Optional[Union[pd.Timestamp, str]]
-            The data must start earlier (or equal) than `start_time`
-            None indicates data will not be filtered based on `start_time`
-        end_time : Optional[Union[pd.Timestamp, str]]
-            similar to start_time
-        freq : str
-            The frequency of the calendar
-        """
-        # Align to calendar before filtering
-        cal = D.calendar(start_time=start_time, end_time=end_time, freq=freq)
-        self.start_time = None if start_time is None else cal[0]
-        self.end_time = None if end_time is None else cal[-1]
-
-    def __call__(self, df: pd.DataFrame, instrument, *args, **kwargs):
-        if (
-            df.empty
-            or (self.start_time is None or df.index.min() <= self.start_time)
-            and (self.end_time is None or df.index.max() >= self.end_time)
-        ):
-            return df
-        return df.head(0)
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+import abc
+from typing import Union, Text, Optional
+import numpy as np
+import pandas as pd
+
+from qlib.utils.data import robust_zscore, zscore
+from ...constant import EPS
+from .utils import fetch_df_by_index
+from ...utils.serial import Serializable
+from ...utils.paral import datetime_groupby_apply
+from qlib.data.inst_processor import InstProcessor
+from qlib.data import D
+
+
+def get_group_columns(df: pd.DataFrame, group: Union[Text, None]):
+    """
+    get a group of columns from multi-index columns DataFrame
+
+    Parameters
+    ----------
+    df : pd.DataFrame
+        with multi of columns.
+    group : str
+        the name of the feature group, i.e. the first level value of the group index.
+    """
+    if group is None:
+        return df.columns
+    else:
+        return df.columns[df.columns.get_loc(group)]
+
+
+class Processor(Serializable):
+    def fit(self, df: pd.DataFrame = None):
+        """
+        learn data processing parameters
+
+        Parameters
+        ----------
+        df : pd.DataFrame
+            When we fit and process data with processor one by one. The fit function reiles on the output of previous
+            processor, i.e. `df`.
+
+        """
+
+    @abc.abstractmethod
+    def __call__(self, df: pd.DataFrame):
+        """
+        process the data
+
+        NOTE: **The processor could change the content of `df` inplace !!!!! **
+        User should keep a copy of data outside
+
+        Parameters
+        ----------
+        df : pd.DataFrame
+            The raw_df of handler or result from previous processor.
+        """
+
+    def is_for_infer(self) -> bool:
+        """
+        Is this processor usable for inference
+        Some processors are not usable for inference.
+
+        Returns
+        -------
+        bool:
+            if it is usable for infenrece.
+        """
+        return True
+
+    def readonly(self) -> bool:
+        """
+        Does the processor treat the input data readonly (i.e. does not write the input data) when processing
+
+        Knowning the readonly information is helpful to the Handler to avoid uncessary copy
+        """
+        return False
+
+    def config(self, **kwargs):
+        attr_list = {"fit_start_time", "fit_end_time"}
+        for k, v in kwargs.items():
+            if k in attr_list and hasattr(self, k):
+                setattr(self, k, v)
+
+        for attr in attr_list:
+            if attr in kwargs:
+                kwargs.pop(attr)
+        super().config(**kwargs)
+
+
+class DropnaProcessor(Processor):
+    def __init__(self, fields_group=None):
+        self.fields_group = fields_group
+
+    def __call__(self, df):
+        return df.dropna(subset=get_group_columns(df, self.fields_group))
+
+    def readonly(self):
+        return True
+
+
+class DropnaLabel(DropnaProcessor):
+    def __init__(self, fields_group="label"):
+        super().__init__(fields_group=fields_group)
+
+    def is_for_infer(self) -> bool:
+        """The samples are dropped according to label. So it is not usable for inference"""
+        return False
+
+
+class DropCol(Processor):
+    def __init__(self, col_list=[]):
+        self.col_list = col_list
+
+    def __call__(self, df):
+        if isinstance(df.columns, pd.MultiIndex):
+            mask = df.columns.get_level_values(-1).isin(self.col_list)
+        else:
+            mask = df.columns.isin(self.col_list)
+        return df.loc[:, ~mask]
+
+    def readonly(self):
+        return True
+
+
+class FilterCol(Processor):
+    def __init__(self, fields_group="feature", col_list=[]):
+        self.fields_group = fields_group
+        self.col_list = col_list
+
+    def __call__(self, df):
+        cols = get_group_columns(df, self.fields_group)
+        all_cols = df.columns
+        diff_cols = np.setdiff1d(all_cols.get_level_values(-1), cols.get_level_values(-1))
+        self.col_list = np.union1d(diff_cols, self.col_list)
+        mask = df.columns.get_level_values(-1).isin(self.col_list)
+        return df.loc[:, mask]
+
+    def readonly(self):
+        return True
+
+
+class TanhProcess(Processor):
+    """Use tanh to process noise data"""
+
+    def __call__(self, df):
+        def tanh_denoise(data):
+            mask = data.columns.get_level_values(1).str.contains("LABEL")
+            col = df.columns[~mask]
+            data[col] = data[col] - 1
+            data[col] = np.tanh(data[col])
+
+            return data
+
+        return tanh_denoise(df)
+
+
+class ProcessInf(Processor):
+    """Process infinity"""
+
+    def __call__(self, df):
+        def replace_inf(data):
+            def process_inf(df):
+                for col in df.columns:
+                    # FIXME: Such behavior is very weird
+                    df[col] = df[col].replace([np.inf, -np.inf], df[col][~np.isinf(df[col])].mean())
+                return df
+
+            data = datetime_groupby_apply(data, process_inf)
+            data.sort_index(inplace=True)
+            return data
+
+        return replace_inf(df)
+
+
+class Fillna(Processor):
+    """Process NaN"""
+
+    def __init__(self, fields_group=None, fill_value=0):
+        self.fields_group = fields_group
+        self.fill_value = fill_value
+
+    def __call__(self, df):
+        if self.fields_group is None:
+            df.fillna(self.fill_value, inplace=True)
+        else:
+            cols = get_group_columns(df, self.fields_group)
+            # this implementation is extremely slow
+            # df.fillna({col: self.fill_value for col in cols}, inplace=True)
+
+            # So we use numpy to accelerate filling values
+            nan_select = np.isnan(df.values)
+            nan_select[:, ~df.columns.isin(cols)] = False
+            df.values[nan_select] = self.fill_value
+        return df
+
+
+class MinMaxNorm(Processor):
+    def __init__(self, fit_start_time, fit_end_time, fields_group=None):
+        # NOTE: correctly set the `fit_start_time` and `fit_end_time` is very important !!!
+        # `fit_end_time` **must not** include any information from the test data!!!
+        self.fit_start_time = fit_start_time
+        self.fit_end_time = fit_end_time
+        self.fields_group = fields_group
+
+    def fit(self, df: pd.DataFrame = None):
+        df = fetch_df_by_index(df, slice(self.fit_start_time, self.fit_end_time), level="datetime")
+        cols = get_group_columns(df, self.fields_group)
+        self.min_val = np.nanmin(df[cols].values, axis=0)
+        self.max_val = np.nanmax(df[cols].values, axis=0)
+        self.ignore = self.min_val == self.max_val
+        # To improve the speed, we set the value of `min_val` to `0` for the columns that do not need to be processed,
+        # and the value of `max_val` to `1`, when using `(x - min_val) / (max_val - min_val)` for uniform calculation,
+        # the columns that do not need to be processed will be calculated by `(x - 0) / (1 - 0)`,
+        # as you can see, the columns that do not need to be processed, will not be affected.
+        for _i, _con in enumerate(self.ignore):
+            if _con:
+                self.min_val[_i] = 0
+                self.max_val[_i] = 1
+        self.cols = cols
+
+    def __call__(self, df):
+        def normalize(x, min_val=self.min_val, max_val=self.max_val):
+            return (x - min_val) / (max_val - min_val)
+
+        df.loc(axis=1)[self.cols] = normalize(df[self.cols].values)
+        return df
+
+
+class ZScoreNorm(Processor):
+    """ZScore Normalization"""
+
+    def __init__(self, fit_start_time, fit_end_time, fields_group=None):
+        # NOTE: correctly set the `fit_start_time` and `fit_end_time` is very important !!!
+        # `fit_end_time` **must not** include any information from the test data!!!
+        self.fit_start_time = fit_start_time
+        self.fit_end_time = fit_end_time
+        self.fields_group = fields_group
+
+    def fit(self, df: pd.DataFrame = None):
+        df = fetch_df_by_index(df, slice(self.fit_start_time, self.fit_end_time), level="datetime")
+        cols = get_group_columns(df, self.fields_group)
+        self.mean_train = np.nanmean(df[cols].values, axis=0)
+        self.std_train = np.nanstd(df[cols].values, axis=0)
+        self.ignore = self.std_train == 0
+        # To improve the speed, we set the value of `std_train` to `1` for the columns that do not need to be processed,
+        # and the value of `mean_train` to `0`, when using `(x - mean_train) / std_train` for uniform calculation,
+        # the columns that do not need to be processed will be calculated by `(x - 0) / 1`,
+        # as you can see, the columns that do not need to be processed, will not be affected.
+        for _i, _con in enumerate(self.ignore):
+            if _con:
+                self.std_train[_i] = 1
+                self.mean_train[_i] = 0
+        self.cols = cols
+
+    def __call__(self, df):
+        def normalize(x, mean_train=self.mean_train, std_train=self.std_train):
+            return (x - mean_train) / std_train
+
+        df.loc(axis=1)[self.cols] = normalize(df[self.cols].values)
+        return df
+
+
+class RobustZScoreNorm(Processor):
+    """Robust ZScore Normalization
+
+    Use robust statistics for Z-Score normalization:
+        mean(x) = median(x)
+        std(x) = MAD(x) * 1.4826
+
+    Reference:
+        https://en.wikipedia.org/wiki/Median_absolute_deviation.
+    """
+
+    def __init__(self, fit_start_time, fit_end_time, fields_group=None, clip_outlier=True):
+        # NOTE: correctly set the `fit_start_time` and `fit_end_time` is very important !!!
+        # `fit_end_time` **must not** include any information from the test data!!!
+        self.fit_start_time = fit_start_time
+        self.fit_end_time = fit_end_time
+        self.fields_group = fields_group
+        self.clip_outlier = clip_outlier
+
+    def fit(self, df: pd.DataFrame = None):
+        df = fetch_df_by_index(df, slice(self.fit_start_time, self.fit_end_time), level="datetime")
+        self.cols = get_group_columns(df, self.fields_group)
+        X = df[self.cols].values
+        self.mean_train = np.nanmedian(X, axis=0)
+        self.std_train = np.nanmedian(np.abs(X - self.mean_train), axis=0)
+        self.std_train += EPS
+        self.std_train *= 1.4826
+
+    def __call__(self, df):
+        X = df[self.cols]
+        X -= self.mean_train
+        X /= self.std_train
+        if self.clip_outlier:
+            X = np.clip(X, -3, 3)
+        df[self.cols] = X
+        return df
+
+
+class CSZScoreNorm(Processor):
+    """Cross Sectional ZScore Normalization"""
+
+    def __init__(self, fields_group=None, method="zscore"):
+        self.fields_group = fields_group
+        if method == "zscore":
+            self.zscore_func = zscore
+        elif method == "robust":
+            self.zscore_func = robust_zscore
+        else:
+            raise NotImplementedError(f"This type of input is not supported")
+
+    def __call__(self, df):
+        # try not modify original dataframe
+        if not isinstance(self.fields_group, list):
+            self.fields_group = [self.fields_group]
+        for g in self.fields_group:
+            cols = get_group_columns(df, g)
+            df[cols] = df[cols].groupby("datetime", group_keys=False).apply(self.zscore_func)
+        return df
+
+
+class CSRankNorm(Processor):
+    """
+    Cross Sectional Rank Normalization.
+    "Cross Sectional" is often used to describe data operations.
+    The operations across different stocks are often called Cross Sectional Operation.
+
+    For example, CSRankNorm is an operation that grouping the data by each day and rank `across` all the stocks in each day.
+
+    Explanation about 3.46 & 0.5
+
+    .. code-block:: python
+
+        import numpy as np
+        import pandas as pd
+        x = np.random.random(10000)  # for any variable
+        x_rank = pd.Series(x).rank(pct=True)  # if it is converted to rank, it will be a uniform distributed
+        x_rank_norm = (x_rank - x_rank.mean()) / x_rank.std()  # Normally, we will normalize it to make it like normal distribution
+
+        x_rank.mean()   # accounts for 0.5
+        1 / x_rank.std()  # accounts for 3.46
+
+    """
+
+    def __init__(self, fields_group=None):
+        self.fields_group = fields_group
+
+    def __call__(self, df):
+        # try not modify original dataframe
+        cols = get_group_columns(df, self.fields_group)
+        t = df[cols].groupby("datetime").rank(pct=True)
+        t -= 0.5
+        t *= 3.46  # NOTE: towards unit std
+        df[cols] = t
+        return df
+
+
+class CSZFillna(Processor):
+    """Cross Sectional Fill Nan"""
+
+    def __init__(self, fields_group=None):
+        self.fields_group = fields_group
+
+    def __call__(self, df):
+        cols = get_group_columns(df, self.fields_group)
+        df[cols] = df[cols].groupby("datetime", group_keys=False).apply(lambda x: x.fillna(x.mean()))
+        return df
+
+
+class HashStockFormat(Processor):
+    """Process the storage of from df into hasing stock format"""
+
+    def __call__(self, df: pd.DataFrame):
+        from .storage import HashingStockStorage  # pylint: disable=C0415
+
+        return HashingStockStorage.from_df(df)
+
+
+class TimeRangeFlt(InstProcessor):
+    """
+    This is a filter to filter stock.
+    Only keep the data that exist from start_time to end_time (the existence in the middle is not checked.)
+    WARNING:  It may induce leakage!!!
+    """
+
+    def __init__(
+        self,
+        start_time: Optional[Union[pd.Timestamp, str]] = None,
+        end_time: Optional[Union[pd.Timestamp, str]] = None,
+        freq: str = "day",
+    ):
+        """
+        Parameters
+        ----------
+        start_time : Optional[Union[pd.Timestamp, str]]
+            The data must start earlier (or equal) than `start_time`
+            None indicates data will not be filtered based on `start_time`
+        end_time : Optional[Union[pd.Timestamp, str]]
+            similar to start_time
+        freq : str
+            The frequency of the calendar
+        """
+        # Align to calendar before filtering
+        cal = D.calendar(start_time=start_time, end_time=end_time, freq=freq)
+        self.start_time = None if start_time is None else cal[0]
+        self.end_time = None if end_time is None else cal[-1]
+
+    def __call__(self, df: pd.DataFrame, instrument, *args, **kwargs):
+        if (
+            df.empty
+            or (self.start_time is None or df.index.min() <= self.start_time)
+            and (self.end_time is None or df.index.max() >= self.end_time)
+        ):
+            return df
+        return df.head(0)
```

## qlib/data/dataset/storage.py

 * *Ordering differences only*

```diff
@@ -1,158 +1,158 @@
-import pandas as pd
-import numpy as np
-
-from .handler import DataHandler
-from typing import Union, List, Callable
-
-from .utils import get_level_index, fetch_df_by_index, fetch_df_by_col
-
-
-class BaseHandlerStorage:
-    """
-    Base data storage for datahandler
-    - pd.DataFrame is the default data storage format in Qlib datahandler
-    - If users want to use custom data storage, they should define subclass inherited BaseHandlerStorage, and implement the following method
-    """
-
-    def fetch(
-        self,
-        selector: Union[pd.Timestamp, slice, str, list] = slice(None, None),
-        level: Union[str, int] = "datetime",
-        col_set: Union[str, List[str]] = DataHandler.CS_ALL,
-        fetch_orig: bool = True,
-        proc_func: Callable = None,
-        **kwargs,
-    ) -> pd.DataFrame:
-        """fetch data from the data storage
-
-        Parameters
-        ----------
-        selector : Union[pd.Timestamp, slice, str]
-            describe how to select data by index
-        level : Union[str, int]
-            which index level to select the data
-            - if level is None, apply selector to df directly
-        col_set : Union[str, List[str]]
-            - if isinstance(col_set, str):
-                select a set of meaningful columns.(e.g. features, columns)
-                if col_set == DataHandler.CS_RAW:
-                    the raw dataset will be returned.
-            - if isinstance(col_set, List[str]):
-                select several sets of meaningful columns, the returned data has multiple level
-        fetch_orig : bool
-            Return the original data instead of copy if possible.
-        proc_func: Callable
-            please refer to the doc of DataHandler.fetch
-
-        Returns
-        -------
-        pd.DataFrame
-            the dataframe fetched
-        """
-        raise NotImplementedError("fetch is method not implemented!")
-
-    @staticmethod
-    def from_df(df: pd.DataFrame):
-        raise NotImplementedError("from_df method is not implemented!")
-
-    def is_proc_func_supported(self):
-        """whether the arg `proc_func` in `fetch` method is supported."""
-        raise NotImplementedError("is_proc_func_supported method is not implemented!")
-
-
-class HashingStockStorage(BaseHandlerStorage):
-    """Hashing data storage for datahanlder
-    - The default data storage pandas.DataFrame is too slow when randomly accessing one stock's data
-    - HashingStockStorage hashes the multiple stocks' data(pandas.DataFrame) by the key `stock_id`.
-    - HashingStockStorage hashes the pandas.DataFrame into a dict, whose key is the stock_id(str) and value this stock data(panda.DataFrame), it has the following format:
-        {
-            stock1_id: stock1_data,
-            stock2_id: stock2_data,
-            ...
-            stockn_id: stockn_data,
-        }
-    - By the `fetch` method, users can access any stock data with much lower time cost than default data storage
-    """
-
-    def __init__(self, df):
-        self.hash_df = dict()
-        self.stock_level = get_level_index(df, "instrument")
-        for k, v in df.groupby(level="instrument"):
-            self.hash_df[k] = v
-        self.columns = df.columns
-
-    @staticmethod
-    def from_df(df):
-        return HashingStockStorage(df)
-
-    def _fetch_hash_df_by_stock(self, selector, level):
-        """fetch the data with stock selector
-
-        Parameters
-        ----------
-        selector : Union[pd.Timestamp, slice, str]
-            describe how to select data by index
-        level : Union[str, int]
-            which index level to select the data
-            - if level is None, apply selector to df directly
-            - the `_fetch_hash_df_by_stock` will parse the stock selector in arg `selector`
-
-        Returns
-        -------
-        Dict
-            The dict whose key is stock_id, value is the stock's data
-        """
-
-        stock_selector = slice(None)
-
-        if level is None:
-            if isinstance(selector, tuple) and self.stock_level < len(selector):
-                stock_selector = selector[self.stock_level]
-            elif isinstance(selector, (list, str)) and self.stock_level == 0:
-                stock_selector = selector
-        elif level in ("instrument", self.stock_level):
-            if isinstance(selector, tuple):
-                stock_selector = selector[0]
-            elif isinstance(selector, (list, str)):
-                stock_selector = selector
-
-        if not isinstance(stock_selector, (list, str)) and stock_selector != slice(None):
-            raise TypeError(f"stock selector must be type str|list, or slice(None), rather than {stock_selector}")
-
-        if stock_selector == slice(None):
-            return self.hash_df
-
-        if isinstance(stock_selector, str):
-            stock_selector = [stock_selector]
-
-        select_dict = dict()
-        for each_stock in sorted(stock_selector):
-            if each_stock in self.hash_df:
-                select_dict[each_stock] = self.hash_df[each_stock]
-        return select_dict
-
-    def fetch(
-        self,
-        selector: Union[pd.Timestamp, slice, str] = slice(None, None),
-        level: Union[str, int] = "datetime",
-        col_set: Union[str, List[str]] = DataHandler.CS_ALL,
-        fetch_orig: bool = True,
-    ) -> pd.DataFrame:
-        fetch_stock_df_list = list(self._fetch_hash_df_by_stock(selector=selector, level=level).values())
-        for _index, stock_df in enumerate(fetch_stock_df_list):
-            fetch_col_df = fetch_df_by_col(df=stock_df, col_set=col_set)
-            fetch_index_df = fetch_df_by_index(df=fetch_col_df, selector=selector, level=level, fetch_orig=fetch_orig)
-            fetch_stock_df_list[_index] = fetch_index_df
-        if len(fetch_stock_df_list) == 0:
-            index_names = ("instrument", "datetime") if self.stock_level == 0 else ("datetime", "instrument")
-            return pd.DataFrame(
-                index=pd.MultiIndex.from_arrays([[], []], names=index_names), columns=self.columns, dtype=np.float32
-            )
-        elif len(fetch_stock_df_list) == 1:
-            return fetch_stock_df_list[0]
-        else:
-            return pd.concat(fetch_stock_df_list, sort=False, copy=~fetch_orig)
-
-    def is_proc_func_supported(self):
-        """the arg `proc_func` in `fetch` method is not supported in HashingStockStorage"""
-        return False
+import pandas as pd
+import numpy as np
+
+from .handler import DataHandler
+from typing import Union, List, Callable
+
+from .utils import get_level_index, fetch_df_by_index, fetch_df_by_col
+
+
+class BaseHandlerStorage:
+    """
+    Base data storage for datahandler
+    - pd.DataFrame is the default data storage format in Qlib datahandler
+    - If users want to use custom data storage, they should define subclass inherited BaseHandlerStorage, and implement the following method
+    """
+
+    def fetch(
+        self,
+        selector: Union[pd.Timestamp, slice, str, list] = slice(None, None),
+        level: Union[str, int] = "datetime",
+        col_set: Union[str, List[str]] = DataHandler.CS_ALL,
+        fetch_orig: bool = True,
+        proc_func: Callable = None,
+        **kwargs,
+    ) -> pd.DataFrame:
+        """fetch data from the data storage
+
+        Parameters
+        ----------
+        selector : Union[pd.Timestamp, slice, str]
+            describe how to select data by index
+        level : Union[str, int]
+            which index level to select the data
+            - if level is None, apply selector to df directly
+        col_set : Union[str, List[str]]
+            - if isinstance(col_set, str):
+                select a set of meaningful columns.(e.g. features, columns)
+                if col_set == DataHandler.CS_RAW:
+                    the raw dataset will be returned.
+            - if isinstance(col_set, List[str]):
+                select several sets of meaningful columns, the returned data has multiple level
+        fetch_orig : bool
+            Return the original data instead of copy if possible.
+        proc_func: Callable
+            please refer to the doc of DataHandler.fetch
+
+        Returns
+        -------
+        pd.DataFrame
+            the dataframe fetched
+        """
+        raise NotImplementedError("fetch is method not implemented!")
+
+    @staticmethod
+    def from_df(df: pd.DataFrame):
+        raise NotImplementedError("from_df method is not implemented!")
+
+    def is_proc_func_supported(self):
+        """whether the arg `proc_func` in `fetch` method is supported."""
+        raise NotImplementedError("is_proc_func_supported method is not implemented!")
+
+
+class HashingStockStorage(BaseHandlerStorage):
+    """Hashing data storage for datahanlder
+    - The default data storage pandas.DataFrame is too slow when randomly accessing one stock's data
+    - HashingStockStorage hashes the multiple stocks' data(pandas.DataFrame) by the key `stock_id`.
+    - HashingStockStorage hashes the pandas.DataFrame into a dict, whose key is the stock_id(str) and value this stock data(panda.DataFrame), it has the following format:
+        {
+            stock1_id: stock1_data,
+            stock2_id: stock2_data,
+            ...
+            stockn_id: stockn_data,
+        }
+    - By the `fetch` method, users can access any stock data with much lower time cost than default data storage
+    """
+
+    def __init__(self, df):
+        self.hash_df = dict()
+        self.stock_level = get_level_index(df, "instrument")
+        for k, v in df.groupby(level="instrument"):
+            self.hash_df[k] = v
+        self.columns = df.columns
+
+    @staticmethod
+    def from_df(df):
+        return HashingStockStorage(df)
+
+    def _fetch_hash_df_by_stock(self, selector, level):
+        """fetch the data with stock selector
+
+        Parameters
+        ----------
+        selector : Union[pd.Timestamp, slice, str]
+            describe how to select data by index
+        level : Union[str, int]
+            which index level to select the data
+            - if level is None, apply selector to df directly
+            - the `_fetch_hash_df_by_stock` will parse the stock selector in arg `selector`
+
+        Returns
+        -------
+        Dict
+            The dict whose key is stock_id, value is the stock's data
+        """
+
+        stock_selector = slice(None)
+
+        if level is None:
+            if isinstance(selector, tuple) and self.stock_level < len(selector):
+                stock_selector = selector[self.stock_level]
+            elif isinstance(selector, (list, str)) and self.stock_level == 0:
+                stock_selector = selector
+        elif level in ("instrument", self.stock_level):
+            if isinstance(selector, tuple):
+                stock_selector = selector[0]
+            elif isinstance(selector, (list, str)):
+                stock_selector = selector
+
+        if not isinstance(stock_selector, (list, str)) and stock_selector != slice(None):
+            raise TypeError(f"stock selector must be type str|list, or slice(None), rather than {stock_selector}")
+
+        if stock_selector == slice(None):
+            return self.hash_df
+
+        if isinstance(stock_selector, str):
+            stock_selector = [stock_selector]
+
+        select_dict = dict()
+        for each_stock in sorted(stock_selector):
+            if each_stock in self.hash_df:
+                select_dict[each_stock] = self.hash_df[each_stock]
+        return select_dict
+
+    def fetch(
+        self,
+        selector: Union[pd.Timestamp, slice, str] = slice(None, None),
+        level: Union[str, int] = "datetime",
+        col_set: Union[str, List[str]] = DataHandler.CS_ALL,
+        fetch_orig: bool = True,
+    ) -> pd.DataFrame:
+        fetch_stock_df_list = list(self._fetch_hash_df_by_stock(selector=selector, level=level).values())
+        for _index, stock_df in enumerate(fetch_stock_df_list):
+            fetch_col_df = fetch_df_by_col(df=stock_df, col_set=col_set)
+            fetch_index_df = fetch_df_by_index(df=fetch_col_df, selector=selector, level=level, fetch_orig=fetch_orig)
+            fetch_stock_df_list[_index] = fetch_index_df
+        if len(fetch_stock_df_list) == 0:
+            index_names = ("instrument", "datetime") if self.stock_level == 0 else ("datetime", "instrument")
+            return pd.DataFrame(
+                index=pd.MultiIndex.from_arrays([[], []], names=index_names), columns=self.columns, dtype=np.float32
+            )
+        elif len(fetch_stock_df_list) == 1:
+            return fetch_stock_df_list[0]
+        else:
+            return pd.concat(fetch_stock_df_list, sort=False, copy=~fetch_orig)
+
+    def is_proc_func_supported(self):
+        """the arg `proc_func` in `fetch` method is not supported in HashingStockStorage"""
+        return False
```

## qlib/data/dataset/utils.py

```diff
@@ -1,146 +1,142 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-from __future__ import annotations
-import pandas as pd
-from typing import Union, List, TYPE_CHECKING
-from qlib.utils import init_instance_by_config
-
-if TYPE_CHECKING:
-    from qlib.data.dataset import DataHandler
-
-
-def get_level_index(df: pd.DataFrame, level=Union[str, int]) -> int:
-    """
-
-    get the level index of `df` given `level`
-
-    Parameters
-    ----------
-    df : pd.DataFrame
-        data
-    level : Union[str, int]
-        index level
-
-    Returns
-    -------
-    int:
-        The level index in the multiple index
-    """
-    if isinstance(level, str):
-        try:
-            return df.index.names.index(level)
-        except (AttributeError, ValueError):
-            # NOTE: If level index is not given in the data, the default level index will be ('datetime', 'instrument')
-            return ("datetime", "instrument").index(level)
-    elif isinstance(level, int):
-        return level
-    else:
-        raise NotImplementedError(f"This type of input is not supported")
-
-
-def fetch_df_by_index(
-    df: pd.DataFrame,
-    selector: Union[pd.Timestamp, slice, str, list, pd.Index],
-    level: Union[str, int],
-    fetch_orig=True,
-) -> pd.DataFrame:
-    """
-    fetch data from `data` with `selector` and `level`
-
-    selector are assumed to be well processed.
-    `fetch_df_by_index` is only responsible for get the right level
-
-    Parameters
-    ----------
-    selector : Union[pd.Timestamp, slice, str, list]
-        selector
-    level : Union[int, str]
-        the level to use the selector
-
-    Returns
-    -------
-    Data of the given index.
-    """
-    # level = None -> use selector directly
-    if level is None or isinstance(selector, pd.MultiIndex):
-        return df.loc(axis=0)[selector]
-    # Try to get the right index
-    idx_slc = (selector, slice(None, None))
-    if get_level_index(df, level) == 1:
-        idx_slc = idx_slc[1], idx_slc[0]
-    if fetch_orig:
-        for slc in idx_slc:
-            if slc != slice(None, None):
-                return df.loc[
-                    pd.IndexSlice[idx_slc],
-                ]
-        else:  # pylint: disable=W0120
-            return df
-    else:
-        return df.loc[
-            pd.IndexSlice[idx_slc],
-        ]
-
-
-def fetch_df_by_col(df: pd.DataFrame, col_set: Union[str, List[str]]) -> pd.DataFrame:
-    from .handler import DataHandler  # pylint: disable=C0415
-
-    if not isinstance(df.columns, pd.MultiIndex) or col_set == DataHandler.CS_RAW:
-        return df
-    elif col_set == DataHandler.CS_ALL:
-        return df.droplevel(axis=1, level=0)
-    else:
-        return df.loc(axis=1)[col_set]
-
-
-def convert_index_format(df: Union[pd.DataFrame, pd.Series], level: str = "datetime") -> Union[pd.DataFrame, pd.Series]:
-    """
-    Convert the format of df.MultiIndex according to the following rules:
-        - If `level` is the first level of df.MultiIndex, do nothing
-        - If `level` is the second level of df.MultiIndex, swap the level of index.
-
-    NOTE:
-        the number of levels of df.MultiIndex should be 2
-
-    Parameters
-    ----------
-    df : Union[pd.DataFrame, pd.Series]
-        raw DataFrame/Series
-    level : str, optional
-        the level that will be converted to the first one, by default "datetime"
-
-    Returns
-    -------
-    Union[pd.DataFrame, pd.Series]
-        converted DataFrame/Series
-    """
-
-    if get_level_index(df, level=level) == 1:
-        df = df.swaplevel().sort_index()
-    return df
-
-
-def init_task_handler(task: dict) -> DataHandler:
-    """
-    initialize the handler part of the task **inplace**
-
-    Parameters
-    ----------
-    task : dict
-        the task to be handled
-
-    Returns
-    -------
-    Union[DataHandler, None]:
-        returns
-    """
-    # avoid recursive import
-    from .handler import DataHandler  # pylint: disable=C0415
-
-    h_conf = task["dataset"]["kwargs"].get("handler")
-    if h_conf is not None:
-        handler = init_instance_by_config(h_conf, accept_types=DataHandler)
-        task["dataset"]["kwargs"]["handler"] = handler
-        return handler
-    else:
-        raise ValueError("The task does not contains a handler part.")
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+from __future__ import annotations
+import pandas as pd
+from typing import Union, List, TYPE_CHECKING
+from qlib.utils import init_instance_by_config
+
+if TYPE_CHECKING:
+    from qlib.data.dataset import DataHandler
+
+
+def get_level_index(df: pd.DataFrame, level=Union[str, int]) -> int:
+    """
+
+    get the level index of `df` given `level`
+
+    Parameters
+    ----------
+    df : pd.DataFrame
+        data
+    level : Union[str, int]
+        index level
+
+    Returns
+    -------
+    int:
+        The level index in the multiple index
+    """
+    if isinstance(level, str):
+        try:
+            return df.index.names.index(level)
+        except (AttributeError, ValueError):
+            # NOTE: If level index is not given in the data, the default level index will be ('datetime', 'instrument')
+            return ("datetime", "instrument").index(level)
+    elif isinstance(level, int):
+        return level
+    else:
+        raise NotImplementedError(f"This type of input is not supported")
+
+
+def fetch_df_by_index(
+    df: pd.DataFrame,
+    selector: Union[pd.Timestamp, slice, str, list, pd.Index],
+    level: Union[str, int],
+    fetch_orig=True,
+) -> pd.DataFrame:
+    """
+    fetch data from `data` with `selector` and `level`
+
+    selector are assumed to be well processed.
+    `fetch_df_by_index` is only responsible for get the right level
+
+    Parameters
+    ----------
+    selector : Union[pd.Timestamp, slice, str, list]
+        selector
+    level : Union[int, str]
+        the level to use the selector
+
+    Returns
+    -------
+    Data of the given index.
+    """
+    # level = None -> use selector directly
+    if level is None or isinstance(selector, pd.MultiIndex):
+        return df.loc(axis=0)[selector]
+    # Try to get the right index
+    idx_slc = (selector, slice(None, None))
+    if get_level_index(df, level) == 1:
+        idx_slc = idx_slc[1], idx_slc[0]
+    if fetch_orig:
+        for slc in idx_slc:
+            if slc != slice(None, None):
+                return df.loc[pd.IndexSlice[idx_slc],]  # noqa: E231
+        else:  # pylint: disable=W0120
+            return df
+    else:
+        return df.loc[pd.IndexSlice[idx_slc],]  # noqa: E231
+
+
+def fetch_df_by_col(df: pd.DataFrame, col_set: Union[str, List[str]]) -> pd.DataFrame:
+    from .handler import DataHandler  # pylint: disable=C0415
+
+    if not isinstance(df.columns, pd.MultiIndex) or col_set == DataHandler.CS_RAW:
+        return df
+    elif col_set == DataHandler.CS_ALL:
+        return df.droplevel(axis=1, level=0)
+    else:
+        return df.loc(axis=1)[col_set]
+
+
+def convert_index_format(df: Union[pd.DataFrame, pd.Series], level: str = "datetime") -> Union[pd.DataFrame, pd.Series]:
+    """
+    Convert the format of df.MultiIndex according to the following rules:
+        - If `level` is the first level of df.MultiIndex, do nothing
+        - If `level` is the second level of df.MultiIndex, swap the level of index.
+
+    NOTE:
+        the number of levels of df.MultiIndex should be 2
+
+    Parameters
+    ----------
+    df : Union[pd.DataFrame, pd.Series]
+        raw DataFrame/Series
+    level : str, optional
+        the level that will be converted to the first one, by default "datetime"
+
+    Returns
+    -------
+    Union[pd.DataFrame, pd.Series]
+        converted DataFrame/Series
+    """
+
+    if get_level_index(df, level=level) == 1:
+        df = df.swaplevel().sort_index()
+    return df
+
+
+def init_task_handler(task: dict) -> DataHandler:
+    """
+    initialize the handler part of the task **inplace**
+
+    Parameters
+    ----------
+    task : dict
+        the task to be handled
+
+    Returns
+    -------
+    Union[DataHandler, None]:
+        returns
+    """
+    # avoid recursive import
+    from .handler import DataHandler  # pylint: disable=C0415
+
+    h_conf = task["dataset"]["kwargs"].get("handler")
+    if h_conf is not None:
+        handler = init_instance_by_config(h_conf, accept_types=DataHandler)
+        task["dataset"]["kwargs"]["handler"] = handler
+        return handler
+    else:
+        raise ValueError("The task does not contains a handler part.")
```

## qlib/data/dataset/weight.py

 * *Ordering differences only*

```diff
@@ -1,27 +1,27 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-
-class Reweighter:
-    def __init__(self, *args, **kwargs):
-        """
-        To initialize the Reweighter, users should provide specific methods to let reweighter do the reweighting (such as sample-wise, rule-based).
-        """
-        raise NotImplementedError()
-
-    def reweight(self, data: object) -> object:
-        """
-        Get weights for data
-
-        Parameters
-        ----------
-        data : object
-            The input data.
-            The first dimension is the index of samples
-
-        Returns
-        -------
-        object:
-            the weights info for the data
-        """
-        raise NotImplementedError(f"This type of input is not supported")
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+
+class Reweighter:
+    def __init__(self, *args, **kwargs):
+        """
+        To initialize the Reweighter, users should provide specific methods to let reweighter do the reweighting (such as sample-wise, rule-based).
+        """
+        raise NotImplementedError()
+
+    def reweight(self, data: object) -> object:
+        """
+        Get weights for data
+
+        Parameters
+        ----------
+        data : object
+            The input data.
+            The first dimension is the index of samples
+
+        Returns
+        -------
+        object:
+            the weights info for the data
+        """
+        raise NotImplementedError(f"This type of input is not supported")
```

## qlib/data/storage/__init__.py

 * *Ordering differences only*

```diff
@@ -1,7 +1,7 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-from .storage import CalendarStorage, InstrumentStorage, FeatureStorage, CalVT, InstVT, InstKT
-
-
-__all__ = ["CalendarStorage", "InstrumentStorage", "FeatureStorage", "CalVT", "InstVT", "InstKT"]
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+from .storage import CalendarStorage, InstrumentStorage, FeatureStorage, CalVT, InstVT, InstKT
+
+
+__all__ = ["CalendarStorage", "InstrumentStorage", "FeatureStorage", "CalVT", "InstVT", "InstKT"]
```

## qlib/data/storage/file_storage.py

```diff
@@ -1,382 +1,379 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-import struct
-from pathlib import Path
-from typing import Iterable, Union, Dict, Mapping, Tuple, List
-
-import numpy as np
-import pandas as pd
-
-from qlib.utils.time import Freq
-from qlib.utils.resam import resam_calendar
-from qlib.config import C
-from qlib.data.cache import H
-from qlib.log import get_module_logger
-from qlib.data.storage import CalendarStorage, InstrumentStorage, FeatureStorage, CalVT, InstKT, InstVT
-
-logger = get_module_logger("file_storage")
-
-
-class FileStorageMixin:
-    """FileStorageMixin, applicable to FileXXXStorage
-    Subclasses need to have provider_uri, freq, storage_name, file_name attributes
-
-    """
-
-    # NOTE: provider_uri priority:
-    #   1. self._provider_uri : if provider_uri is provided.
-    #   2. provider_uri in qlib.config.C
-
-    @property
-    def provider_uri(self):
-        return C["provider_uri"] if getattr(self, "_provider_uri", None) is None else self._provider_uri
-
-    @property
-    def dpm(self):
-        return (
-            C.dpm
-            if getattr(self, "_provider_uri", None) is None
-            else C.DataPathManager(self._provider_uri, C.mount_path)
-        )
-
-    @property
-    def support_freq(self) -> List[str]:
-        _v = "_support_freq"
-        if hasattr(self, _v):
-            return getattr(self, _v)
-        if len(self.provider_uri) == 1 and C.DEFAULT_FREQ in self.provider_uri:
-            freq_l = filter(
-                lambda _freq: not _freq.endswith("_future"),
-                map(lambda x: x.stem, self.dpm.get_data_uri(C.DEFAULT_FREQ).joinpath("calendars").glob("*.txt")),
-            )
-        else:
-            freq_l = self.provider_uri.keys()
-        freq_l = [Freq(freq) for freq in freq_l]
-        setattr(self, _v, freq_l)
-        return freq_l
-
-    @property
-    def uri(self) -> Path:
-        if self.freq not in self.support_freq:
-            raise ValueError(f"{self.storage_name}: {self.provider_uri} does not contain data for {self.freq}")
-        return self.dpm.get_data_uri(self.freq).joinpath(f"{self.storage_name}s", self.file_name)
-
-    def check(self):
-        """check self.uri
-
-        Raises
-        -------
-        ValueError
-        """
-        if not self.uri.exists():
-            raise ValueError(f"{self.storage_name} not exists: {self.uri}")
-
-
-class FileCalendarStorage(FileStorageMixin, CalendarStorage):
-    def __init__(self, freq: str, future: bool, provider_uri: dict = None, **kwargs):
-        super(FileCalendarStorage, self).__init__(freq, future, **kwargs)
-        self.future = future
-        self._provider_uri = None if provider_uri is None else C.DataPathManager.format_provider_uri(provider_uri)
-        self.enable_read_cache = True  # TODO: make it configurable
-        self.region = C["region"]
-
-    @property
-    def file_name(self) -> str:
-        return f"{self._freq_file}_future.txt" if self.future else f"{self._freq_file}.txt".lower()
-
-    @property
-    def _freq_file(self) -> str:
-        """the freq to read from file"""
-        if not hasattr(self, "_freq_file_cache"):
-            freq = Freq(self.freq)
-            if freq not in self.support_freq:
-                # NOTE: uri
-                #   1. If `uri` does not exist
-                #       - Get the `min_uri` of the closest `freq` under the same "directory" as the `uri`
-                #       - Read data from `min_uri` and resample to `freq`
-
-                freq = Freq.get_recent_freq(freq, self.support_freq)
-                if freq is None:
-                    raise ValueError(f"can't find a freq from {self.support_freq} that can resample to {self.freq}!")
-            self._freq_file_cache = freq
-        return self._freq_file_cache
-
-    def _read_calendar(self) -> List[CalVT]:
-        # NOTE:
-        # if we want to accelerate partial reading calendar
-        # we can add parameters like `skip_rows: int = 0, n_rows: int = None` to the interface.
-        # Currently, it is not supported for the txt-based calendar
-
-        if not self.uri.exists():
-            self._write_calendar(values=[])
-
-        with self.uri.open("r") as fp:
-            res = []
-            for line in fp.readlines():
-                line = line.strip()
-                if len(line) > 0:
-                    res.append(line)
-            return res
-
-    def _write_calendar(self, values: Iterable[CalVT], mode: str = "wb"):
-        with self.uri.open(mode=mode) as fp:
-            np.savetxt(fp, values, fmt="%s", encoding="utf-8")
-
-    @property
-    def uri(self) -> Path:
-        return self.dpm.get_data_uri(self._freq_file).joinpath(f"{self.storage_name}s", self.file_name)
-
-    @property
-    def data(self) -> List[CalVT]:
-        self.check()
-        # If cache is enabled, then return cache directly
-        if self.enable_read_cache:
-            key = "orig_file" + str(self.uri)
-            if key not in H["c"]:
-                H["c"][key] = self._read_calendar()
-            _calendar = H["c"][key]
-        else:
-            _calendar = self._read_calendar()
-        if Freq(self._freq_file) != Freq(self.freq):
-            _calendar = resam_calendar(
-                np.array(list(map(pd.Timestamp, _calendar))), self._freq_file, self.freq, self.region
-            )
-        return _calendar
-
-    def _get_storage_freq(self) -> List[str]:
-        return sorted(set(map(lambda x: x.stem.split("_")[0], self.uri.parent.glob("*.txt"))))
-
-    def extend(self, values: Iterable[CalVT]) -> None:
-        self._write_calendar(values, mode="ab")
-
-    def clear(self) -> None:
-        self._write_calendar(values=[])
-
-    def index(self, value: CalVT) -> int:
-        self.check()
-        calendar = self._read_calendar()
-        return int(np.argwhere(calendar == value)[0])
-
-    def insert(self, index: int, value: CalVT):
-        calendar = self._read_calendar()
-        calendar = np.insert(calendar, index, value)
-        self._write_calendar(values=calendar)
-
-    def remove(self, value: CalVT) -> None:
-        self.check()
-        index = self.index(value)
-        calendar = self._read_calendar()
-        calendar = np.delete(calendar, index)
-        self._write_calendar(values=calendar)
-
-    def __setitem__(self, i: Union[int, slice], values: Union[CalVT, Iterable[CalVT]]) -> None:
-        calendar = self._read_calendar()
-        calendar[i] = values
-        self._write_calendar(values=calendar)
-
-    def __delitem__(self, i: Union[int, slice]) -> None:
-        self.check()
-        calendar = self._read_calendar()
-        calendar = np.delete(calendar, i)
-        self._write_calendar(values=calendar)
-
-    def __getitem__(self, i: Union[int, slice]) -> Union[CalVT, List[CalVT]]:
-        self.check()
-        return self._read_calendar()[i]
-
-    def __len__(self) -> int:
-        return len(self.data)
-
-
-class FileInstrumentStorage(FileStorageMixin, InstrumentStorage):
-
-    INSTRUMENT_SEP = "\t"
-    INSTRUMENT_START_FIELD = "start_datetime"
-    INSTRUMENT_END_FIELD = "end_datetime"
-    SYMBOL_FIELD_NAME = "instrument"
-
-    def __init__(self, market: str, freq: str, provider_uri: dict = None, **kwargs):
-        super(FileInstrumentStorage, self).__init__(market, freq, **kwargs)
-        self._provider_uri = None if provider_uri is None else C.DataPathManager.format_provider_uri(provider_uri)
-        self.file_name = f"{market.lower()}.txt"
-
-    def _read_instrument(self) -> Dict[InstKT, InstVT]:
-        if not self.uri.exists():
-            self._write_instrument()
-
-        _instruments = dict()
-        df = pd.read_csv(
-            self.uri,
-            sep="\t",
-            usecols=[0, 1, 2],
-            names=[self.SYMBOL_FIELD_NAME, self.INSTRUMENT_START_FIELD, self.INSTRUMENT_END_FIELD],
-            dtype={self.SYMBOL_FIELD_NAME: str},
-            parse_dates=[self.INSTRUMENT_START_FIELD, self.INSTRUMENT_END_FIELD],
-        )
-        for row in df.itertuples(index=False):
-            _instruments.setdefault(row[0], []).append((row[1], row[2]))
-        return _instruments
-
-    def _write_instrument(self, data: Dict[InstKT, InstVT] = None) -> None:
-        if not data:
-            with self.uri.open("w") as _:
-                pass
-            return
-
-        res = []
-        for inst, v_list in data.items():
-            _df = pd.DataFrame(v_list, columns=[self.INSTRUMENT_START_FIELD, self.INSTRUMENT_END_FIELD])
-            _df[self.SYMBOL_FIELD_NAME] = inst
-            res.append(_df)
-
-        df = pd.concat(res, sort=False)
-        df.loc[:, [self.SYMBOL_FIELD_NAME, self.INSTRUMENT_START_FIELD, self.INSTRUMENT_END_FIELD]].to_csv(
-            self.uri, header=False, sep=self.INSTRUMENT_SEP, index=False
-        )
-        df.to_csv(self.uri, sep="\t", encoding="utf-8", header=False, index=False)
-
-    def clear(self) -> None:
-        self._write_instrument(data={})
-
-    @property
-    def data(self) -> Dict[InstKT, InstVT]:
-        self.check()
-        return self._read_instrument()
-
-    def __setitem__(self, k: InstKT, v: InstVT) -> None:
-        inst = self._read_instrument()
-        inst[k] = v
-        self._write_instrument(inst)
-
-    def __delitem__(self, k: InstKT) -> None:
-        self.check()
-        inst = self._read_instrument()
-        del inst[k]
-        self._write_instrument(inst)
-
-    def __getitem__(self, k: InstKT) -> InstVT:
-        self.check()
-        return self._read_instrument()[k]
-
-    def update(self, *args, **kwargs) -> None:
-
-        if len(args) > 1:
-            raise TypeError(f"update expected at most 1 arguments, got {len(args)}")
-        inst = self._read_instrument()
-        if args:
-            other = args[0]  # type: dict
-            if isinstance(other, Mapping):
-                for key in other:
-                    inst[key] = other[key]
-            elif hasattr(other, "keys"):
-                for key in other.keys():
-                    inst[key] = other[key]
-            else:
-                for key, value in other:
-                    inst[key] = value
-        for key, value in kwargs.items():
-            inst[key] = value
-
-        self._write_instrument(inst)
-
-    def __len__(self) -> int:
-        return len(self.data)
-
-
-class FileFeatureStorage(FileStorageMixin, FeatureStorage):
-    def __init__(self, instrument: str, field: str, freq: str, provider_uri: dict = None, **kwargs):
-        super(FileFeatureStorage, self).__init__(instrument, field, freq, **kwargs)
-        self._provider_uri = None if provider_uri is None else C.DataPathManager.format_provider_uri(provider_uri)
-        self.file_name = f"{instrument.lower()}/{field.lower()}.{freq.lower()}.bin"
-
-    def clear(self):
-        with self.uri.open("wb") as _:
-            pass
-
-    @property
-    def data(self) -> pd.Series:
-        return self[:]
-
-    def write(self, data_array: Union[List, np.ndarray], index: int = None) -> None:
-        if len(data_array) == 0:
-            logger.info(
-                "len(data_array) == 0, write"
-                "if you need to clear the FeatureStorage, please execute: FeatureStorage.clear"
-            )
-            return
-        if not self.uri.exists():
-            # write
-            index = 0 if index is None else index
-            with self.uri.open("wb") as fp:
-                np.hstack([index, data_array]).astype("<f").tofile(fp)
-        else:
-            if index is None or index > self.end_index:
-                # append
-                index = 0 if index is None else index
-                with self.uri.open("ab+") as fp:
-                    np.hstack([[np.nan] * (index - self.end_index - 1), data_array]).astype("<f").tofile(fp)
-            else:
-                # rewrite
-                with self.uri.open("rb+") as fp:
-                    _old_data = np.fromfile(fp, dtype="<f")
-                    _old_index = _old_data[0]
-                    _old_df = pd.DataFrame(
-                        _old_data[1:], index=range(_old_index, _old_index + len(_old_data) - 1), columns=["old"]
-                    )
-                    fp.seek(0)
-                    _new_df = pd.DataFrame(data_array, index=range(index, index + len(data_array)), columns=["new"])
-                    _df = pd.concat([_old_df, _new_df], sort=False, axis=1)
-                    _df = _df.reindex(range(_df.index.min(), _df.index.max() + 1))
-                    _df["new"].fillna(_df["old"]).values.astype("<f").tofile(fp)
-
-    @property
-    def start_index(self) -> Union[int, None]:
-        if not self.uri.exists():
-            return None
-        with self.uri.open("rb") as fp:
-            index = int(np.frombuffer(fp.read(4), dtype="<f")[0])
-        return index
-
-    @property
-    def end_index(self) -> Union[int, None]:
-        if not self.uri.exists():
-            return None
-        # The next  data appending index point will be  `end_index + 1`
-        return self.start_index + len(self) - 1
-
-    def __getitem__(self, i: Union[int, slice]) -> Union[Tuple[int, float], pd.Series]:
-        if not self.uri.exists():
-            if isinstance(i, int):
-                return None, None
-            elif isinstance(i, slice):
-                return pd.Series(dtype=np.float32)
-            else:
-                raise TypeError(f"type(i) = {type(i)}")
-
-        storage_start_index = self.start_index
-        storage_end_index = self.end_index
-        with self.uri.open("rb") as fp:
-            if isinstance(i, int):
-
-                if storage_start_index > i:
-                    raise IndexError(f"{i}: start index is {storage_start_index}")
-                fp.seek(4 * (i - storage_start_index) + 4)
-                return i, struct.unpack("f", fp.read(4))[0]
-            elif isinstance(i, slice):
-                start_index = storage_start_index if i.start is None else i.start
-                end_index = storage_end_index if i.stop is None else i.stop - 1
-                si = max(start_index, storage_start_index)
-                if si > end_index:
-                    return pd.Series(dtype=np.float32)
-                fp.seek(4 * (si - storage_start_index) + 4)
-                # read n bytes
-                count = end_index - si + 1
-                data = np.frombuffer(fp.read(4 * count), dtype="<f")
-                return pd.Series(data, index=pd.RangeIndex(si, si + len(data)))
-            else:
-                raise TypeError(f"type(i) = {type(i)}")
-
-    def __len__(self) -> int:
-        self.check()
-        return self.uri.stat().st_size // 4 - 1
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+import struct
+from pathlib import Path
+from typing import Iterable, Union, Dict, Mapping, Tuple, List
+
+import numpy as np
+import pandas as pd
+
+from qlib.utils.time import Freq
+from qlib.utils.resam import resam_calendar
+from qlib.config import C
+from qlib.data.cache import H
+from qlib.log import get_module_logger
+from qlib.data.storage import CalendarStorage, InstrumentStorage, FeatureStorage, CalVT, InstKT, InstVT
+
+logger = get_module_logger("file_storage")
+
+
+class FileStorageMixin:
+    """FileStorageMixin, applicable to FileXXXStorage
+    Subclasses need to have provider_uri, freq, storage_name, file_name attributes
+
+    """
+
+    # NOTE: provider_uri priority:
+    #   1. self._provider_uri : if provider_uri is provided.
+    #   2. provider_uri in qlib.config.C
+
+    @property
+    def provider_uri(self):
+        return C["provider_uri"] if getattr(self, "_provider_uri", None) is None else self._provider_uri
+
+    @property
+    def dpm(self):
+        return (
+            C.dpm
+            if getattr(self, "_provider_uri", None) is None
+            else C.DataPathManager(self._provider_uri, C.mount_path)
+        )
+
+    @property
+    def support_freq(self) -> List[str]:
+        _v = "_support_freq"
+        if hasattr(self, _v):
+            return getattr(self, _v)
+        if len(self.provider_uri) == 1 and C.DEFAULT_FREQ in self.provider_uri:
+            freq_l = filter(
+                lambda _freq: not _freq.endswith("_future"),
+                map(lambda x: x.stem, self.dpm.get_data_uri(C.DEFAULT_FREQ).joinpath("calendars").glob("*.txt")),
+            )
+        else:
+            freq_l = self.provider_uri.keys()
+        freq_l = [Freq(freq) for freq in freq_l]
+        setattr(self, _v, freq_l)
+        return freq_l
+
+    @property
+    def uri(self) -> Path:
+        if self.freq not in self.support_freq:
+            raise ValueError(f"{self.storage_name}: {self.provider_uri} does not contain data for {self.freq}")
+        return self.dpm.get_data_uri(self.freq).joinpath(f"{self.storage_name}s", self.file_name)
+
+    def check(self):
+        """check self.uri
+
+        Raises
+        -------
+        ValueError
+        """
+        if not self.uri.exists():
+            raise ValueError(f"{self.storage_name} not exists: {self.uri}")
+
+
+class FileCalendarStorage(FileStorageMixin, CalendarStorage):
+    def __init__(self, freq: str, future: bool, provider_uri: dict = None, **kwargs):
+        super(FileCalendarStorage, self).__init__(freq, future, **kwargs)
+        self.future = future
+        self._provider_uri = None if provider_uri is None else C.DataPathManager.format_provider_uri(provider_uri)
+        self.enable_read_cache = True  # TODO: make it configurable
+        self.region = C["region"]
+
+    @property
+    def file_name(self) -> str:
+        return f"{self._freq_file}_future.txt" if self.future else f"{self._freq_file}.txt".lower()
+
+    @property
+    def _freq_file(self) -> str:
+        """the freq to read from file"""
+        if not hasattr(self, "_freq_file_cache"):
+            freq = Freq(self.freq)
+            if freq not in self.support_freq:
+                # NOTE: uri
+                #   1. If `uri` does not exist
+                #       - Get the `min_uri` of the closest `freq` under the same "directory" as the `uri`
+                #       - Read data from `min_uri` and resample to `freq`
+
+                freq = Freq.get_recent_freq(freq, self.support_freq)
+                if freq is None:
+                    raise ValueError(f"can't find a freq from {self.support_freq} that can resample to {self.freq}!")
+            self._freq_file_cache = freq
+        return self._freq_file_cache
+
+    def _read_calendar(self) -> List[CalVT]:
+        # NOTE:
+        # if we want to accelerate partial reading calendar
+        # we can add parameters like `skip_rows: int = 0, n_rows: int = None` to the interface.
+        # Currently, it is not supported for the txt-based calendar
+
+        if not self.uri.exists():
+            self._write_calendar(values=[])
+
+        with self.uri.open("r") as fp:
+            res = []
+            for line in fp.readlines():
+                line = line.strip()
+                if len(line) > 0:
+                    res.append(line)
+            return res
+
+    def _write_calendar(self, values: Iterable[CalVT], mode: str = "wb"):
+        with self.uri.open(mode=mode) as fp:
+            np.savetxt(fp, values, fmt="%s", encoding="utf-8")
+
+    @property
+    def uri(self) -> Path:
+        return self.dpm.get_data_uri(self._freq_file).joinpath(f"{self.storage_name}s", self.file_name)
+
+    @property
+    def data(self) -> List[CalVT]:
+        self.check()
+        # If cache is enabled, then return cache directly
+        if self.enable_read_cache:
+            key = "orig_file" + str(self.uri)
+            if key not in H["c"]:
+                H["c"][key] = self._read_calendar()
+            _calendar = H["c"][key]
+        else:
+            _calendar = self._read_calendar()
+        if Freq(self._freq_file) != Freq(self.freq):
+            _calendar = resam_calendar(
+                np.array(list(map(pd.Timestamp, _calendar))), self._freq_file, self.freq, self.region
+            )
+        return _calendar
+
+    def _get_storage_freq(self) -> List[str]:
+        return sorted(set(map(lambda x: x.stem.split("_")[0], self.uri.parent.glob("*.txt"))))
+
+    def extend(self, values: Iterable[CalVT]) -> None:
+        self._write_calendar(values, mode="ab")
+
+    def clear(self) -> None:
+        self._write_calendar(values=[])
+
+    def index(self, value: CalVT) -> int:
+        self.check()
+        calendar = self._read_calendar()
+        return int(np.argwhere(calendar == value)[0])
+
+    def insert(self, index: int, value: CalVT):
+        calendar = self._read_calendar()
+        calendar = np.insert(calendar, index, value)
+        self._write_calendar(values=calendar)
+
+    def remove(self, value: CalVT) -> None:
+        self.check()
+        index = self.index(value)
+        calendar = self._read_calendar()
+        calendar = np.delete(calendar, index)
+        self._write_calendar(values=calendar)
+
+    def __setitem__(self, i: Union[int, slice], values: Union[CalVT, Iterable[CalVT]]) -> None:
+        calendar = self._read_calendar()
+        calendar[i] = values
+        self._write_calendar(values=calendar)
+
+    def __delitem__(self, i: Union[int, slice]) -> None:
+        self.check()
+        calendar = self._read_calendar()
+        calendar = np.delete(calendar, i)
+        self._write_calendar(values=calendar)
+
+    def __getitem__(self, i: Union[int, slice]) -> Union[CalVT, List[CalVT]]:
+        self.check()
+        return self._read_calendar()[i]
+
+    def __len__(self) -> int:
+        return len(self.data)
+
+
+class FileInstrumentStorage(FileStorageMixin, InstrumentStorage):
+    INSTRUMENT_SEP = "\t"
+    INSTRUMENT_START_FIELD = "start_datetime"
+    INSTRUMENT_END_FIELD = "end_datetime"
+    SYMBOL_FIELD_NAME = "instrument"
+
+    def __init__(self, market: str, freq: str, provider_uri: dict = None, **kwargs):
+        super(FileInstrumentStorage, self).__init__(market, freq, **kwargs)
+        self._provider_uri = None if provider_uri is None else C.DataPathManager.format_provider_uri(provider_uri)
+        self.file_name = f"{market.lower()}.txt"
+
+    def _read_instrument(self) -> Dict[InstKT, InstVT]:
+        if not self.uri.exists():
+            self._write_instrument()
+
+        _instruments = dict()
+        df = pd.read_csv(
+            self.uri,
+            sep="\t",
+            usecols=[0, 1, 2],
+            names=[self.SYMBOL_FIELD_NAME, self.INSTRUMENT_START_FIELD, self.INSTRUMENT_END_FIELD],
+            dtype={self.SYMBOL_FIELD_NAME: str},
+            parse_dates=[self.INSTRUMENT_START_FIELD, self.INSTRUMENT_END_FIELD],
+        )
+        for row in df.itertuples(index=False):
+            _instruments.setdefault(row[0], []).append((row[1], row[2]))
+        return _instruments
+
+    def _write_instrument(self, data: Dict[InstKT, InstVT] = None) -> None:
+        if not data:
+            with self.uri.open("w") as _:
+                pass
+            return
+
+        res = []
+        for inst, v_list in data.items():
+            _df = pd.DataFrame(v_list, columns=[self.INSTRUMENT_START_FIELD, self.INSTRUMENT_END_FIELD])
+            _df[self.SYMBOL_FIELD_NAME] = inst
+            res.append(_df)
+
+        df = pd.concat(res, sort=False)
+        df.loc[:, [self.SYMBOL_FIELD_NAME, self.INSTRUMENT_START_FIELD, self.INSTRUMENT_END_FIELD]].to_csv(
+            self.uri, header=False, sep=self.INSTRUMENT_SEP, index=False
+        )
+        df.to_csv(self.uri, sep="\t", encoding="utf-8", header=False, index=False)
+
+    def clear(self) -> None:
+        self._write_instrument(data={})
+
+    @property
+    def data(self) -> Dict[InstKT, InstVT]:
+        self.check()
+        return self._read_instrument()
+
+    def __setitem__(self, k: InstKT, v: InstVT) -> None:
+        inst = self._read_instrument()
+        inst[k] = v
+        self._write_instrument(inst)
+
+    def __delitem__(self, k: InstKT) -> None:
+        self.check()
+        inst = self._read_instrument()
+        del inst[k]
+        self._write_instrument(inst)
+
+    def __getitem__(self, k: InstKT) -> InstVT:
+        self.check()
+        return self._read_instrument()[k]
+
+    def update(self, *args, **kwargs) -> None:
+        if len(args) > 1:
+            raise TypeError(f"update expected at most 1 arguments, got {len(args)}")
+        inst = self._read_instrument()
+        if args:
+            other = args[0]  # type: dict
+            if isinstance(other, Mapping):
+                for key in other:
+                    inst[key] = other[key]
+            elif hasattr(other, "keys"):
+                for key in other.keys():
+                    inst[key] = other[key]
+            else:
+                for key, value in other:
+                    inst[key] = value
+        for key, value in kwargs.items():
+            inst[key] = value
+
+        self._write_instrument(inst)
+
+    def __len__(self) -> int:
+        return len(self.data)
+
+
+class FileFeatureStorage(FileStorageMixin, FeatureStorage):
+    def __init__(self, instrument: str, field: str, freq: str, provider_uri: dict = None, **kwargs):
+        super(FileFeatureStorage, self).__init__(instrument, field, freq, **kwargs)
+        self._provider_uri = None if provider_uri is None else C.DataPathManager.format_provider_uri(provider_uri)
+        self.file_name = f"{instrument.lower()}/{field.lower()}.{freq.lower()}.bin"
+
+    def clear(self):
+        with self.uri.open("wb") as _:
+            pass
+
+    @property
+    def data(self) -> pd.Series:
+        return self[:]
+
+    def write(self, data_array: Union[List, np.ndarray], index: int = None) -> None:
+        if len(data_array) == 0:
+            logger.info(
+                "len(data_array) == 0, write"
+                "if you need to clear the FeatureStorage, please execute: FeatureStorage.clear"
+            )
+            return
+        if not self.uri.exists():
+            # write
+            index = 0 if index is None else index
+            with self.uri.open("wb") as fp:
+                np.hstack([index, data_array]).astype("<f").tofile(fp)
+        else:
+            if index is None or index > self.end_index:
+                # append
+                index = 0 if index is None else index
+                with self.uri.open("ab+") as fp:
+                    np.hstack([[np.nan] * (index - self.end_index - 1), data_array]).astype("<f").tofile(fp)
+            else:
+                # rewrite
+                with self.uri.open("rb+") as fp:
+                    _old_data = np.fromfile(fp, dtype="<f")
+                    _old_index = _old_data[0]
+                    _old_df = pd.DataFrame(
+                        _old_data[1:], index=range(_old_index, _old_index + len(_old_data) - 1), columns=["old"]
+                    )
+                    fp.seek(0)
+                    _new_df = pd.DataFrame(data_array, index=range(index, index + len(data_array)), columns=["new"])
+                    _df = pd.concat([_old_df, _new_df], sort=False, axis=1)
+                    _df = _df.reindex(range(_df.index.min(), _df.index.max() + 1))
+                    _df["new"].fillna(_df["old"]).values.astype("<f").tofile(fp)
+
+    @property
+    def start_index(self) -> Union[int, None]:
+        if not self.uri.exists():
+            return None
+        with self.uri.open("rb") as fp:
+            index = int(np.frombuffer(fp.read(4), dtype="<f")[0])
+        return index
+
+    @property
+    def end_index(self) -> Union[int, None]:
+        if not self.uri.exists():
+            return None
+        # The next  data appending index point will be  `end_index + 1`
+        return self.start_index + len(self) - 1
+
+    def __getitem__(self, i: Union[int, slice]) -> Union[Tuple[int, float], pd.Series]:
+        if not self.uri.exists():
+            if isinstance(i, int):
+                return None, None
+            elif isinstance(i, slice):
+                return pd.Series(dtype=np.float32)
+            else:
+                raise TypeError(f"type(i) = {type(i)}")
+
+        storage_start_index = self.start_index
+        storage_end_index = self.end_index
+        with self.uri.open("rb") as fp:
+            if isinstance(i, int):
+                if storage_start_index > i:
+                    raise IndexError(f"{i}: start index is {storage_start_index}")
+                fp.seek(4 * (i - storage_start_index) + 4)
+                return i, struct.unpack("f", fp.read(4))[0]
+            elif isinstance(i, slice):
+                start_index = storage_start_index if i.start is None else i.start
+                end_index = storage_end_index if i.stop is None else i.stop - 1
+                si = max(start_index, storage_start_index)
+                if si > end_index:
+                    return pd.Series(dtype=np.float32)
+                fp.seek(4 * (si - storage_start_index) + 4)
+                # read n bytes
+                count = end_index - si + 1
+                data = np.frombuffer(fp.read(4 * count), dtype="<f")
+                return pd.Series(data, index=pd.RangeIndex(si, si + len(data)))
+            else:
+                raise TypeError(f"type(i) = {type(i)}")
+
+    def __len__(self) -> int:
+        self.check()
+        return self.uri.stat().st_size // 4 - 1
```

## qlib/data/storage/storage.py

 * *Ordering differences only*

```diff
@@ -1,494 +1,494 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-import re
-from typing import Iterable, overload, Tuple, List, Text, Union, Dict
-
-import numpy as np
-import pandas as pd
-from qlib.log import get_module_logger
-
-# calendar value type
-CalVT = str
-
-# instrument value
-InstVT = List[Tuple[CalVT, CalVT]]
-# instrument key
-InstKT = Text
-
-logger = get_module_logger("storage")
-
-"""
-If the user is only using it in `qlib`, you can customize Storage to implement only the following methods:
-
-class UserCalendarStorage(CalendarStorage):
-
-    @property
-    def data(self) -> Iterable[CalVT]:
-        '''get all data
-
-        Raises
-        ------
-        ValueError
-            If the data(storage) does not exist, raise ValueError
-        '''
-        raise NotImplementedError("Subclass of CalendarStorage must implement `data` method")
-
-
-class UserInstrumentStorage(InstrumentStorage):
-
-    @property
-    def data(self) -> Dict[InstKT, InstVT]:
-        '''get all data
-
-        Raises
-        ------
-        ValueError
-            If the data(storage) does not exist, raise ValueError
-        '''
-        raise NotImplementedError("Subclass of InstrumentStorage must implement `data` method")
-
-
-class UserFeatureStorage(FeatureStorage):
-
-    def __getitem__(self, s: slice) -> pd.Series:
-        '''x.__getitem__(slice(start: int, stop: int, step: int)) <==> x[start:stop:step]
-
-        Returns
-        -------
-            pd.Series(values, index=pd.RangeIndex(start, len(values))
-
-        Notes
-        -------
-        if data(storage) does not exist:
-            if isinstance(i, int):
-                return (None, None)
-            if isinstance(i,  slice):
-                # return empty pd.Series
-                return pd.Series(dtype=np.float32)
-        '''
-        raise NotImplementedError(
-            "Subclass of FeatureStorage must implement `__getitem__(s: slice)` method"
-        )
-
-
-"""
-
-
-class BaseStorage:
-    @property
-    def storage_name(self) -> str:
-        return re.findall("[A-Z][^A-Z]*", self.__class__.__name__)[-2].lower()
-
-
-class CalendarStorage(BaseStorage):
-    """
-    The behavior of CalendarStorage's methods and List's methods of the same name remain consistent
-    """
-
-    def __init__(self, freq: str, future: bool, **kwargs):
-        self.freq = freq
-        self.future = future
-        self.kwargs = kwargs
-
-    @property
-    def data(self) -> Iterable[CalVT]:
-        """get all data
-
-        Raises
-        ------
-        ValueError
-            If the data(storage) does not exist, raise ValueError
-        """
-        raise NotImplementedError("Subclass of CalendarStorage must implement `data` method")
-
-    def clear(self) -> None:
-        raise NotImplementedError("Subclass of CalendarStorage must implement `clear` method")
-
-    def extend(self, iterable: Iterable[CalVT]) -> None:
-        raise NotImplementedError("Subclass of CalendarStorage must implement `extend` method")
-
-    def index(self, value: CalVT) -> int:
-        """
-        Raises
-        ------
-        ValueError
-            If the data(storage) does not exist, raise ValueError
-        """
-        raise NotImplementedError("Subclass of CalendarStorage must implement `index` method")
-
-    def insert(self, index: int, value: CalVT) -> None:
-        raise NotImplementedError("Subclass of CalendarStorage must implement `insert` method")
-
-    def remove(self, value: CalVT) -> None:
-        raise NotImplementedError("Subclass of CalendarStorage must implement `remove` method")
-
-    @overload
-    def __setitem__(self, i: int, value: CalVT) -> None:
-        """x.__setitem__(i, o) <==> (x[i] = o)"""
-
-    @overload
-    def __setitem__(self, s: slice, value: Iterable[CalVT]) -> None:
-        """x.__setitem__(s, o) <==> (x[s] = o)"""
-
-    def __setitem__(self, i, value) -> None:
-        raise NotImplementedError(
-            "Subclass of CalendarStorage must implement `__setitem__(i: int, o: CalVT)`/`__setitem__(s: slice, o: Iterable[CalVT])`  method"
-        )
-
-    @overload
-    def __delitem__(self, i: int) -> None:
-        """x.__delitem__(i) <==> del x[i]"""
-
-    @overload
-    def __delitem__(self, i: slice) -> None:
-        """x.__delitem__(slice(start: int, stop: int, step: int)) <==> del x[start:stop:step]"""
-
-    def __delitem__(self, i) -> None:
-        """
-        Raises
-        ------
-        ValueError
-            If the data(storage) does not exist, raise ValueError
-        """
-        raise NotImplementedError(
-            "Subclass of CalendarStorage must implement `__delitem__(i: int)`/`__delitem__(s: slice)`  method"
-        )
-
-    @overload
-    def __getitem__(self, s: slice) -> Iterable[CalVT]:
-        """x.__getitem__(slice(start: int, stop: int, step: int)) <==> x[start:stop:step]"""
-
-    @overload
-    def __getitem__(self, i: int) -> CalVT:
-        """x.__getitem__(i) <==> x[i]"""
-
-    def __getitem__(self, i) -> CalVT:
-        """
-
-        Raises
-        ------
-        ValueError
-            If the data(storage) does not exist, raise ValueError
-
-        """
-        raise NotImplementedError(
-            "Subclass of CalendarStorage must implement `__getitem__(i: int)`/`__getitem__(s: slice)`  method"
-        )
-
-    def __len__(self) -> int:
-        """
-
-        Raises
-        ------
-        ValueError
-            If the data(storage) does not exist, raise ValueError
-
-        """
-        raise NotImplementedError("Subclass of CalendarStorage must implement `__len__`  method")
-
-
-class InstrumentStorage(BaseStorage):
-    def __init__(self, market: str, freq: str, **kwargs):
-        self.market = market
-        self.freq = freq
-        self.kwargs = kwargs
-
-    @property
-    def data(self) -> Dict[InstKT, InstVT]:
-        """get all data
-
-        Raises
-        ------
-        ValueError
-            If the data(storage) does not exist, raise ValueError
-        """
-        raise NotImplementedError("Subclass of InstrumentStorage must implement `data` method")
-
-    def clear(self) -> None:
-        raise NotImplementedError("Subclass of InstrumentStorage must implement `clear` method")
-
-    def update(self, *args, **kwargs) -> None:
-        """D.update([E, ]**F) -> None.  Update D from mapping/iterable E and F.
-
-        Notes
-        ------
-            If E present and has a .keys() method, does:     for k in E: D[k] = E[k]
-
-            If E present and lacks .keys() method, does:     for (k, v) in E: D[k] = v
-
-            In either case, this is followed by: for k, v in F.items(): D[k] = v
-
-        """
-        raise NotImplementedError("Subclass of InstrumentStorage must implement `update` method")
-
-    def __setitem__(self, k: InstKT, v: InstVT) -> None:
-        """Set self[key] to value."""
-        raise NotImplementedError("Subclass of InstrumentStorage must implement `__setitem__` method")
-
-    def __delitem__(self, k: InstKT) -> None:
-        """Delete self[key].
-
-        Raises
-        ------
-        ValueError
-            If the data(storage) does not exist, raise ValueError
-        """
-        raise NotImplementedError("Subclass of InstrumentStorage must implement `__delitem__` method")
-
-    def __getitem__(self, k: InstKT) -> InstVT:
-        """x.__getitem__(k) <==> x[k]"""
-        raise NotImplementedError("Subclass of InstrumentStorage must implement `__getitem__` method")
-
-    def __len__(self) -> int:
-        """
-
-        Raises
-        ------
-        ValueError
-            If the data(storage) does not exist, raise ValueError
-
-        """
-        raise NotImplementedError("Subclass of InstrumentStorage must implement `__len__`  method")
-
-
-class FeatureStorage(BaseStorage):
-    def __init__(self, instrument: str, field: str, freq: str, **kwargs):
-        self.instrument = instrument
-        self.field = field
-        self.freq = freq
-        self.kwargs = kwargs
-
-    @property
-    def data(self) -> pd.Series:
-        """get all data
-
-        Notes
-        ------
-        if data(storage) does not exist, return empty pd.Series: `return pd.Series(dtype=np.float32)`
-        """
-        raise NotImplementedError("Subclass of FeatureStorage must implement `data` method")
-
-    @property
-    def start_index(self) -> Union[int, None]:
-        """get FeatureStorage start index
-
-        Notes
-        -----
-        If the data(storage) does not exist, return None
-        """
-        raise NotImplementedError("Subclass of FeatureStorage must implement `start_index` method")
-
-    @property
-    def end_index(self) -> Union[int, None]:
-        """get FeatureStorage end index
-
-        Notes
-        -----
-        The  right index of the data range (both sides are closed)
-
-            The next  data appending point will be  `end_index + 1`
-
-        If the data(storage) does not exist, return None
-        """
-        raise NotImplementedError("Subclass of FeatureStorage must implement `end_index` method")
-
-    def clear(self) -> None:
-        raise NotImplementedError("Subclass of FeatureStorage must implement `clear` method")
-
-    def write(self, data_array: Union[List, np.ndarray, Tuple], index: int = None):
-        """Write data_array to FeatureStorage starting from index.
-
-        Notes
-        ------
-            If index is None, append data_array to feature.
-
-            If len(data_array) == 0; return
-
-            If (index - self.end_index) >= 1, self[end_index+1: index] will be filled with np.nan
-
-        Examples
-        ---------
-            .. code-block::
-
-                feature:
-                    3   4
-                    4   5
-                    5   6
-
-
-            >>> self.write([6, 7], index=6)
-
-                feature:
-                    3   4
-                    4   5
-                    5   6
-                    6   6
-                    7   7
-
-            >>> self.write([8], index=9)
-
-                feature:
-                    3   4
-                    4   5
-                    5   6
-                    6   6
-                    7   7
-                    8   np.nan
-                    9   8
-
-            >>> self.write([1, np.nan], index=3)
-
-                feature:
-                    3   1
-                    4   np.nan
-                    5   6
-                    6   6
-                    7   7
-                    8   np.nan
-                    9   8
-
-        """
-        raise NotImplementedError("Subclass of FeatureStorage must implement `write` method")
-
-    def rebase(self, start_index: int = None, end_index: int = None):
-        """Rebase the start_index and end_index of the FeatureStorage.
-
-        start_index and end_index are closed intervals: [start_index, end_index]
-
-        Examples
-        ---------
-
-            .. code-block::
-
-                    feature:
-                        3   4
-                        4   5
-                        5   6
-
-
-                >>> self.rebase(start_index=4)
-
-                    feature:
-                        4   5
-                        5   6
-
-                >>> self.rebase(start_index=3)
-
-                    feature:
-                        3   np.nan
-                        4   5
-                        5   6
-
-                >>> self.write([3], index=3)
-
-                    feature:
-                        3   3
-                        4   5
-                        5   6
-
-                >>> self.rebase(end_index=4)
-
-                    feature:
-                        3   3
-                        4   5
-
-                >>> self.write([6, 7, 8], index=4)
-
-                    feature:
-                        3   3
-                        4   6
-                        5   7
-                        6   8
-
-                >>> self.rebase(start_index=4, end_index=5)
-
-                    feature:
-                        4   6
-                        5   7
-
-        """
-        storage_si = self.start_index
-        storage_ei = self.end_index
-        if storage_si is None or storage_ei is None:
-            raise ValueError("storage.start_index or storage.end_index is None, storage may not exist")
-
-        start_index = storage_si if start_index is None else start_index
-        end_index = storage_ei if end_index is None else end_index
-
-        if start_index is None or end_index is None:
-            logger.warning("both start_index and end_index are None, or storage does not exist; rebase is ignored")
-            return
-
-        if start_index < 0 or end_index < 0:
-            logger.warning("start_index or end_index cannot be less than 0")
-            return
-        if start_index > end_index:
-            logger.warning(
-                f"start_index({start_index}) > end_index({end_index}), rebase is ignored; "
-                f"if you need to clear the FeatureStorage, please execute: FeatureStorage.clear"
-            )
-            return
-
-        if start_index <= storage_si:
-            self.write([np.nan] * (storage_si - start_index), start_index)
-        else:
-            self.rewrite(self[start_index:].values, start_index)
-
-        if end_index >= self.end_index:
-            self.write([np.nan] * (end_index - self.end_index))
-        else:
-            self.rewrite(self[: end_index + 1].values, start_index)
-
-    def rewrite(self, data: Union[List, np.ndarray, Tuple], index: int):
-        """overwrite all data in FeatureStorage with data
-
-        Parameters
-        ----------
-        data: Union[List, np.ndarray, Tuple]
-            data
-        index: int
-            data start index
-        """
-        self.clear()
-        self.write(data, index)
-
-    @overload
-    def __getitem__(self, s: slice) -> pd.Series:
-        """x.__getitem__(slice(start: int, stop: int, step: int)) <==> x[start:stop:step]
-
-        Returns
-        -------
-            pd.Series(values, index=pd.RangeIndex(start, len(values))
-        """
-
-    @overload
-    def __getitem__(self, i: int) -> Tuple[int, float]:
-        """x.__getitem__(y) <==> x[y]"""
-
-    def __getitem__(self, i) -> Union[Tuple[int, float], pd.Series]:
-        """x.__getitem__(y) <==> x[y]
-
-        Notes
-        -------
-        if data(storage) does not exist:
-            if isinstance(i, int):
-                return (None, None)
-            if isinstance(i,  slice):
-                # return empty pd.Series
-                return pd.Series(dtype=np.float32)
-        """
-        raise NotImplementedError(
-            "Subclass of FeatureStorage must implement `__getitem__(i: int)`/`__getitem__(s: slice)` method"
-        )
-
-    def __len__(self) -> int:
-        """
-
-        Raises
-        ------
-        ValueError
-            If the data(storage) does not exist, raise ValueError
-
-        """
-        raise NotImplementedError("Subclass of FeatureStorage must implement `__len__`  method")
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+import re
+from typing import Iterable, overload, Tuple, List, Text, Union, Dict
+
+import numpy as np
+import pandas as pd
+from qlib.log import get_module_logger
+
+# calendar value type
+CalVT = str
+
+# instrument value
+InstVT = List[Tuple[CalVT, CalVT]]
+# instrument key
+InstKT = Text
+
+logger = get_module_logger("storage")
+
+"""
+If the user is only using it in `qlib`, you can customize Storage to implement only the following methods:
+
+class UserCalendarStorage(CalendarStorage):
+
+    @property
+    def data(self) -> Iterable[CalVT]:
+        '''get all data
+
+        Raises
+        ------
+        ValueError
+            If the data(storage) does not exist, raise ValueError
+        '''
+        raise NotImplementedError("Subclass of CalendarStorage must implement `data` method")
+
+
+class UserInstrumentStorage(InstrumentStorage):
+
+    @property
+    def data(self) -> Dict[InstKT, InstVT]:
+        '''get all data
+
+        Raises
+        ------
+        ValueError
+            If the data(storage) does not exist, raise ValueError
+        '''
+        raise NotImplementedError("Subclass of InstrumentStorage must implement `data` method")
+
+
+class UserFeatureStorage(FeatureStorage):
+
+    def __getitem__(self, s: slice) -> pd.Series:
+        '''x.__getitem__(slice(start: int, stop: int, step: int)) <==> x[start:stop:step]
+
+        Returns
+        -------
+            pd.Series(values, index=pd.RangeIndex(start, len(values))
+
+        Notes
+        -------
+        if data(storage) does not exist:
+            if isinstance(i, int):
+                return (None, None)
+            if isinstance(i,  slice):
+                # return empty pd.Series
+                return pd.Series(dtype=np.float32)
+        '''
+        raise NotImplementedError(
+            "Subclass of FeatureStorage must implement `__getitem__(s: slice)` method"
+        )
+
+
+"""
+
+
+class BaseStorage:
+    @property
+    def storage_name(self) -> str:
+        return re.findall("[A-Z][^A-Z]*", self.__class__.__name__)[-2].lower()
+
+
+class CalendarStorage(BaseStorage):
+    """
+    The behavior of CalendarStorage's methods and List's methods of the same name remain consistent
+    """
+
+    def __init__(self, freq: str, future: bool, **kwargs):
+        self.freq = freq
+        self.future = future
+        self.kwargs = kwargs
+
+    @property
+    def data(self) -> Iterable[CalVT]:
+        """get all data
+
+        Raises
+        ------
+        ValueError
+            If the data(storage) does not exist, raise ValueError
+        """
+        raise NotImplementedError("Subclass of CalendarStorage must implement `data` method")
+
+    def clear(self) -> None:
+        raise NotImplementedError("Subclass of CalendarStorage must implement `clear` method")
+
+    def extend(self, iterable: Iterable[CalVT]) -> None:
+        raise NotImplementedError("Subclass of CalendarStorage must implement `extend` method")
+
+    def index(self, value: CalVT) -> int:
+        """
+        Raises
+        ------
+        ValueError
+            If the data(storage) does not exist, raise ValueError
+        """
+        raise NotImplementedError("Subclass of CalendarStorage must implement `index` method")
+
+    def insert(self, index: int, value: CalVT) -> None:
+        raise NotImplementedError("Subclass of CalendarStorage must implement `insert` method")
+
+    def remove(self, value: CalVT) -> None:
+        raise NotImplementedError("Subclass of CalendarStorage must implement `remove` method")
+
+    @overload
+    def __setitem__(self, i: int, value: CalVT) -> None:
+        """x.__setitem__(i, o) <==> (x[i] = o)"""
+
+    @overload
+    def __setitem__(self, s: slice, value: Iterable[CalVT]) -> None:
+        """x.__setitem__(s, o) <==> (x[s] = o)"""
+
+    def __setitem__(self, i, value) -> None:
+        raise NotImplementedError(
+            "Subclass of CalendarStorage must implement `__setitem__(i: int, o: CalVT)`/`__setitem__(s: slice, o: Iterable[CalVT])`  method"
+        )
+
+    @overload
+    def __delitem__(self, i: int) -> None:
+        """x.__delitem__(i) <==> del x[i]"""
+
+    @overload
+    def __delitem__(self, i: slice) -> None:
+        """x.__delitem__(slice(start: int, stop: int, step: int)) <==> del x[start:stop:step]"""
+
+    def __delitem__(self, i) -> None:
+        """
+        Raises
+        ------
+        ValueError
+            If the data(storage) does not exist, raise ValueError
+        """
+        raise NotImplementedError(
+            "Subclass of CalendarStorage must implement `__delitem__(i: int)`/`__delitem__(s: slice)`  method"
+        )
+
+    @overload
+    def __getitem__(self, s: slice) -> Iterable[CalVT]:
+        """x.__getitem__(slice(start: int, stop: int, step: int)) <==> x[start:stop:step]"""
+
+    @overload
+    def __getitem__(self, i: int) -> CalVT:
+        """x.__getitem__(i) <==> x[i]"""
+
+    def __getitem__(self, i) -> CalVT:
+        """
+
+        Raises
+        ------
+        ValueError
+            If the data(storage) does not exist, raise ValueError
+
+        """
+        raise NotImplementedError(
+            "Subclass of CalendarStorage must implement `__getitem__(i: int)`/`__getitem__(s: slice)`  method"
+        )
+
+    def __len__(self) -> int:
+        """
+
+        Raises
+        ------
+        ValueError
+            If the data(storage) does not exist, raise ValueError
+
+        """
+        raise NotImplementedError("Subclass of CalendarStorage must implement `__len__`  method")
+
+
+class InstrumentStorage(BaseStorage):
+    def __init__(self, market: str, freq: str, **kwargs):
+        self.market = market
+        self.freq = freq
+        self.kwargs = kwargs
+
+    @property
+    def data(self) -> Dict[InstKT, InstVT]:
+        """get all data
+
+        Raises
+        ------
+        ValueError
+            If the data(storage) does not exist, raise ValueError
+        """
+        raise NotImplementedError("Subclass of InstrumentStorage must implement `data` method")
+
+    def clear(self) -> None:
+        raise NotImplementedError("Subclass of InstrumentStorage must implement `clear` method")
+
+    def update(self, *args, **kwargs) -> None:
+        """D.update([E, ]**F) -> None.  Update D from mapping/iterable E and F.
+
+        Notes
+        ------
+            If E present and has a .keys() method, does:     for k in E: D[k] = E[k]
+
+            If E present and lacks .keys() method, does:     for (k, v) in E: D[k] = v
+
+            In either case, this is followed by: for k, v in F.items(): D[k] = v
+
+        """
+        raise NotImplementedError("Subclass of InstrumentStorage must implement `update` method")
+
+    def __setitem__(self, k: InstKT, v: InstVT) -> None:
+        """Set self[key] to value."""
+        raise NotImplementedError("Subclass of InstrumentStorage must implement `__setitem__` method")
+
+    def __delitem__(self, k: InstKT) -> None:
+        """Delete self[key].
+
+        Raises
+        ------
+        ValueError
+            If the data(storage) does not exist, raise ValueError
+        """
+        raise NotImplementedError("Subclass of InstrumentStorage must implement `__delitem__` method")
+
+    def __getitem__(self, k: InstKT) -> InstVT:
+        """x.__getitem__(k) <==> x[k]"""
+        raise NotImplementedError("Subclass of InstrumentStorage must implement `__getitem__` method")
+
+    def __len__(self) -> int:
+        """
+
+        Raises
+        ------
+        ValueError
+            If the data(storage) does not exist, raise ValueError
+
+        """
+        raise NotImplementedError("Subclass of InstrumentStorage must implement `__len__`  method")
+
+
+class FeatureStorage(BaseStorage):
+    def __init__(self, instrument: str, field: str, freq: str, **kwargs):
+        self.instrument = instrument
+        self.field = field
+        self.freq = freq
+        self.kwargs = kwargs
+
+    @property
+    def data(self) -> pd.Series:
+        """get all data
+
+        Notes
+        ------
+        if data(storage) does not exist, return empty pd.Series: `return pd.Series(dtype=np.float32)`
+        """
+        raise NotImplementedError("Subclass of FeatureStorage must implement `data` method")
+
+    @property
+    def start_index(self) -> Union[int, None]:
+        """get FeatureStorage start index
+
+        Notes
+        -----
+        If the data(storage) does not exist, return None
+        """
+        raise NotImplementedError("Subclass of FeatureStorage must implement `start_index` method")
+
+    @property
+    def end_index(self) -> Union[int, None]:
+        """get FeatureStorage end index
+
+        Notes
+        -----
+        The  right index of the data range (both sides are closed)
+
+            The next  data appending point will be  `end_index + 1`
+
+        If the data(storage) does not exist, return None
+        """
+        raise NotImplementedError("Subclass of FeatureStorage must implement `end_index` method")
+
+    def clear(self) -> None:
+        raise NotImplementedError("Subclass of FeatureStorage must implement `clear` method")
+
+    def write(self, data_array: Union[List, np.ndarray, Tuple], index: int = None):
+        """Write data_array to FeatureStorage starting from index.
+
+        Notes
+        ------
+            If index is None, append data_array to feature.
+
+            If len(data_array) == 0; return
+
+            If (index - self.end_index) >= 1, self[end_index+1: index] will be filled with np.nan
+
+        Examples
+        ---------
+            .. code-block::
+
+                feature:
+                    3   4
+                    4   5
+                    5   6
+
+
+            >>> self.write([6, 7], index=6)
+
+                feature:
+                    3   4
+                    4   5
+                    5   6
+                    6   6
+                    7   7
+
+            >>> self.write([8], index=9)
+
+                feature:
+                    3   4
+                    4   5
+                    5   6
+                    6   6
+                    7   7
+                    8   np.nan
+                    9   8
+
+            >>> self.write([1, np.nan], index=3)
+
+                feature:
+                    3   1
+                    4   np.nan
+                    5   6
+                    6   6
+                    7   7
+                    8   np.nan
+                    9   8
+
+        """
+        raise NotImplementedError("Subclass of FeatureStorage must implement `write` method")
+
+    def rebase(self, start_index: int = None, end_index: int = None):
+        """Rebase the start_index and end_index of the FeatureStorage.
+
+        start_index and end_index are closed intervals: [start_index, end_index]
+
+        Examples
+        ---------
+
+            .. code-block::
+
+                    feature:
+                        3   4
+                        4   5
+                        5   6
+
+
+                >>> self.rebase(start_index=4)
+
+                    feature:
+                        4   5
+                        5   6
+
+                >>> self.rebase(start_index=3)
+
+                    feature:
+                        3   np.nan
+                        4   5
+                        5   6
+
+                >>> self.write([3], index=3)
+
+                    feature:
+                        3   3
+                        4   5
+                        5   6
+
+                >>> self.rebase(end_index=4)
+
+                    feature:
+                        3   3
+                        4   5
+
+                >>> self.write([6, 7, 8], index=4)
+
+                    feature:
+                        3   3
+                        4   6
+                        5   7
+                        6   8
+
+                >>> self.rebase(start_index=4, end_index=5)
+
+                    feature:
+                        4   6
+                        5   7
+
+        """
+        storage_si = self.start_index
+        storage_ei = self.end_index
+        if storage_si is None or storage_ei is None:
+            raise ValueError("storage.start_index or storage.end_index is None, storage may not exist")
+
+        start_index = storage_si if start_index is None else start_index
+        end_index = storage_ei if end_index is None else end_index
+
+        if start_index is None or end_index is None:
+            logger.warning("both start_index and end_index are None, or storage does not exist; rebase is ignored")
+            return
+
+        if start_index < 0 or end_index < 0:
+            logger.warning("start_index or end_index cannot be less than 0")
+            return
+        if start_index > end_index:
+            logger.warning(
+                f"start_index({start_index}) > end_index({end_index}), rebase is ignored; "
+                f"if you need to clear the FeatureStorage, please execute: FeatureStorage.clear"
+            )
+            return
+
+        if start_index <= storage_si:
+            self.write([np.nan] * (storage_si - start_index), start_index)
+        else:
+            self.rewrite(self[start_index:].values, start_index)
+
+        if end_index >= self.end_index:
+            self.write([np.nan] * (end_index - self.end_index))
+        else:
+            self.rewrite(self[: end_index + 1].values, start_index)
+
+    def rewrite(self, data: Union[List, np.ndarray, Tuple], index: int):
+        """overwrite all data in FeatureStorage with data
+
+        Parameters
+        ----------
+        data: Union[List, np.ndarray, Tuple]
+            data
+        index: int
+            data start index
+        """
+        self.clear()
+        self.write(data, index)
+
+    @overload
+    def __getitem__(self, s: slice) -> pd.Series:
+        """x.__getitem__(slice(start: int, stop: int, step: int)) <==> x[start:stop:step]
+
+        Returns
+        -------
+            pd.Series(values, index=pd.RangeIndex(start, len(values))
+        """
+
+    @overload
+    def __getitem__(self, i: int) -> Tuple[int, float]:
+        """x.__getitem__(y) <==> x[y]"""
+
+    def __getitem__(self, i) -> Union[Tuple[int, float], pd.Series]:
+        """x.__getitem__(y) <==> x[y]
+
+        Notes
+        -------
+        if data(storage) does not exist:
+            if isinstance(i, int):
+                return (None, None)
+            if isinstance(i,  slice):
+                # return empty pd.Series
+                return pd.Series(dtype=np.float32)
+        """
+        raise NotImplementedError(
+            "Subclass of FeatureStorage must implement `__getitem__(i: int)`/`__getitem__(s: slice)` method"
+        )
+
+    def __len__(self) -> int:
+        """
+
+        Raises
+        ------
+        ValueError
+            If the data(storage) does not exist, raise ValueError
+
+        """
+        raise NotImplementedError("Subclass of FeatureStorage must implement `__len__`  method")
```

## qlib/model/__init__.py

 * *Ordering differences only*

```diff
@@ -1,9 +1,9 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-import warnings
-
-from .base import Model
-
-
-__all__ = ["Model", "warnings"]
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+import warnings
+
+from .base import Model
+
+
+__all__ = ["Model", "warnings"]
```

## qlib/model/base.py

 * *Ordering differences only*

```diff
@@ -1,110 +1,110 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-import abc
-from typing import Text, Union
-from ..utils.serial import Serializable
-from ..data.dataset import Dataset
-from ..data.dataset.weight import Reweighter
-
-
-class BaseModel(Serializable, metaclass=abc.ABCMeta):
-    """Modeling things"""
-
-    @abc.abstractmethod
-    def predict(self, *args, **kwargs) -> object:
-        """Make predictions after modeling things"""
-
-    def __call__(self, *args, **kwargs) -> object:
-        """leverage Python syntactic sugar to make the models' behaviors like functions"""
-        return self.predict(*args, **kwargs)
-
-
-class Model(BaseModel):
-    """Learnable Models"""
-
-    def fit(self, dataset: Dataset, reweighter: Reweighter):
-        """
-        Learn model from the base model
-
-        .. note::
-
-            The attribute names of learned model should `not` start with '_'. So that the model could be
-            dumped to disk.
-
-        The following code example shows how to retrieve `x_train`, `y_train` and `w_train` from the `dataset`:
-
-            .. code-block:: Python
-
-                # get features and labels
-                df_train, df_valid = dataset.prepare(
-                    ["train", "valid"], col_set=["feature", "label"], data_key=DataHandlerLP.DK_L
-                )
-                x_train, y_train = df_train["feature"], df_train["label"]
-                x_valid, y_valid = df_valid["feature"], df_valid["label"]
-
-                # get weights
-                try:
-                    wdf_train, wdf_valid = dataset.prepare(["train", "valid"], col_set=["weight"],
-                                                           data_key=DataHandlerLP.DK_L)
-                    w_train, w_valid = wdf_train["weight"], wdf_valid["weight"]
-                except KeyError as e:
-                    w_train = pd.DataFrame(np.ones_like(y_train.values), index=y_train.index)
-                    w_valid = pd.DataFrame(np.ones_like(y_valid.values), index=y_valid.index)
-
-        Parameters
-        ----------
-        dataset : Dataset
-            dataset will generate the processed data from model training.
-
-        """
-        raise NotImplementedError()
-
-    @abc.abstractmethod
-    def predict(self, dataset: Dataset, segment: Union[Text, slice] = "test") -> object:
-        """give prediction given Dataset
-
-        Parameters
-        ----------
-        dataset : Dataset
-            dataset will generate the processed dataset from model training.
-
-        segment : Text or slice
-            dataset will use this segment to prepare data. (default=test)
-
-        Returns
-        -------
-        Prediction results with certain type such as `pandas.Series`.
-        """
-        raise NotImplementedError()
-
-
-class ModelFT(Model):
-    """Model (F)ine(t)unable"""
-
-    @abc.abstractmethod
-    def finetune(self, dataset: Dataset):
-        """finetune model based given dataset
-
-        A typical use case of finetuning model with qlib.workflow.R
-
-        .. code-block:: python
-
-            # start exp to train init model
-            with R.start(experiment_name="init models"):
-                model.fit(dataset)
-                R.save_objects(init_model=model)
-                rid = R.get_recorder().id
-
-            # Finetune model based on previous trained model
-            with R.start(experiment_name="finetune model"):
-                recorder = R.get_recorder(recorder_id=rid, experiment_name="init models")
-                model = recorder.load_object("init_model")
-                model.finetune(dataset, num_boost_round=10)
-
-
-        Parameters
-        ----------
-        dataset : Dataset
-            dataset will generate the processed dataset from model training.
-        """
-        raise NotImplementedError()
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+import abc
+from typing import Text, Union
+from ..utils.serial import Serializable
+from ..data.dataset import Dataset
+from ..data.dataset.weight import Reweighter
+
+
+class BaseModel(Serializable, metaclass=abc.ABCMeta):
+    """Modeling things"""
+
+    @abc.abstractmethod
+    def predict(self, *args, **kwargs) -> object:
+        """Make predictions after modeling things"""
+
+    def __call__(self, *args, **kwargs) -> object:
+        """leverage Python syntactic sugar to make the models' behaviors like functions"""
+        return self.predict(*args, **kwargs)
+
+
+class Model(BaseModel):
+    """Learnable Models"""
+
+    def fit(self, dataset: Dataset, reweighter: Reweighter):
+        """
+        Learn model from the base model
+
+        .. note::
+
+            The attribute names of learned model should `not` start with '_'. So that the model could be
+            dumped to disk.
+
+        The following code example shows how to retrieve `x_train`, `y_train` and `w_train` from the `dataset`:
+
+            .. code-block:: Python
+
+                # get features and labels
+                df_train, df_valid = dataset.prepare(
+                    ["train", "valid"], col_set=["feature", "label"], data_key=DataHandlerLP.DK_L
+                )
+                x_train, y_train = df_train["feature"], df_train["label"]
+                x_valid, y_valid = df_valid["feature"], df_valid["label"]
+
+                # get weights
+                try:
+                    wdf_train, wdf_valid = dataset.prepare(["train", "valid"], col_set=["weight"],
+                                                           data_key=DataHandlerLP.DK_L)
+                    w_train, w_valid = wdf_train["weight"], wdf_valid["weight"]
+                except KeyError as e:
+                    w_train = pd.DataFrame(np.ones_like(y_train.values), index=y_train.index)
+                    w_valid = pd.DataFrame(np.ones_like(y_valid.values), index=y_valid.index)
+
+        Parameters
+        ----------
+        dataset : Dataset
+            dataset will generate the processed data from model training.
+
+        """
+        raise NotImplementedError()
+
+    @abc.abstractmethod
+    def predict(self, dataset: Dataset, segment: Union[Text, slice] = "test") -> object:
+        """give prediction given Dataset
+
+        Parameters
+        ----------
+        dataset : Dataset
+            dataset will generate the processed dataset from model training.
+
+        segment : Text or slice
+            dataset will use this segment to prepare data. (default=test)
+
+        Returns
+        -------
+        Prediction results with certain type such as `pandas.Series`.
+        """
+        raise NotImplementedError()
+
+
+class ModelFT(Model):
+    """Model (F)ine(t)unable"""
+
+    @abc.abstractmethod
+    def finetune(self, dataset: Dataset):
+        """finetune model based given dataset
+
+        A typical use case of finetuning model with qlib.workflow.R
+
+        .. code-block:: python
+
+            # start exp to train init model
+            with R.start(experiment_name="init models"):
+                model.fit(dataset)
+                R.save_objects(init_model=model)
+                rid = R.get_recorder().id
+
+            # Finetune model based on previous trained model
+            with R.start(experiment_name="finetune model"):
+                recorder = R.get_recorder(recorder_id=rid, experiment_name="init models")
+                model = recorder.load_object("init_model")
+                model.finetune(dataset, num_boost_round=10)
+
+
+        Parameters
+        ----------
+        dataset : Dataset
+            dataset will generate the processed dataset from model training.
+        """
+        raise NotImplementedError()
```

## qlib/model/trainer.py

 * *Ordering differences only*

```diff
@@ -1,617 +1,617 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-"""
-The Trainer will train a list of tasks and return a list of model recorders.
-There are two steps in each Trainer including ``train`` (make model recorder) and ``end_train`` (modify model recorder).
-
-This is a concept called ``DelayTrainer``, which can be used in online simulating for parallel training.
-In ``DelayTrainer``, the first step is only to save some necessary info to model recorders, and the second step which will be finished in the end can do some concurrent and time-consuming operations such as model fitting.
-
-``Qlib`` offer two kinds of Trainer, ``TrainerR`` is the simplest way and ``TrainerRM`` is based on TaskManager to help manager tasks lifecycle automatically.
-"""
-
-import socket
-from typing import Callable, List, Optional
-
-from tqdm.auto import tqdm
-
-from qlib.config import C
-from qlib.data.dataset import Dataset
-from qlib.data.dataset.weight import Reweighter
-from qlib.log import get_module_logger
-from qlib.model.base import Model
-from qlib.utils import (
-    auto_filter_kwargs,
-    fill_placeholder,
-    flatten_dict,
-    init_instance_by_config,
-)
-from qlib.utils.paral import call_in_subproc
-from qlib.workflow import R
-from qlib.workflow.recorder import Recorder
-from qlib.workflow.task.manage import TaskManager, run_task
-
-
-def _log_task_info(task_config: dict):
-    R.log_params(**flatten_dict(task_config))
-    R.save_objects(**{"task": task_config})  # keep the original format and datatype
-    R.set_tags(**{"hostname": socket.gethostname()})
-
-
-def _exe_task(task_config: dict):
-    rec = R.get_recorder()
-    # model & dataset initiation
-    model: Model = init_instance_by_config(task_config["model"], accept_types=Model)
-    dataset: Dataset = init_instance_by_config(task_config["dataset"], accept_types=Dataset)
-    reweighter: Reweighter = task_config.get("reweighter", None)
-    # model training
-    auto_filter_kwargs(model.fit)(dataset, reweighter=reweighter)
-    R.save_objects(**{"params.pkl": model})
-    # this dataset is saved for online inference. So the concrete data should not be dumped
-    dataset.config(dump_all=False, recursive=True)
-    R.save_objects(**{"dataset": dataset})
-    # fill placehorder
-    placehorder_value = {"<MODEL>": model, "<DATASET>": dataset}
-    task_config = fill_placeholder(task_config, placehorder_value)
-    # generate records: prediction, backtest, and analysis
-    records = task_config.get("record", [])
-    if isinstance(records, dict):  # prevent only one dict
-        records = [records]
-    for record in records:
-        # Some recorder require the parameter `model` and `dataset`.
-        # try to automatically pass in them to the initialization function
-        # to make defining the tasking easier
-        r = init_instance_by_config(
-            record,
-            recorder=rec,
-            default_module="qlib.workflow.record_temp",
-            try_kwargs={"model": model, "dataset": dataset},
-        )
-        r.generate()
-
-
-def begin_task_train(task_config: dict, experiment_name: str, recorder_name: str = None) -> Recorder:
-    """
-    Begin task training to start a recorder and save the task config.
-
-    Args:
-        task_config (dict): the config of a task
-        experiment_name (str): the name of experiment
-        recorder_name (str): the given name will be the recorder name. None for using rid.
-
-    Returns:
-        Recorder: the model recorder
-    """
-    with R.start(experiment_name=experiment_name, recorder_name=recorder_name):
-        _log_task_info(task_config)
-        return R.get_recorder()
-
-
-def end_task_train(rec: Recorder, experiment_name: str) -> Recorder:
-    """
-    Finish task training with real model fitting and saving.
-
-    Args:
-        rec (Recorder): the recorder will be resumed
-        experiment_name (str): the name of experiment
-
-    Returns:
-        Recorder: the model recorder
-    """
-    with R.start(experiment_name=experiment_name, recorder_id=rec.info["id"], resume=True):
-        task_config = R.load_object("task")
-        _exe_task(task_config)
-    return rec
-
-
-def task_train(task_config: dict, experiment_name: str, recorder_name: str = None) -> Recorder:
-    """
-    Task based training, will be divided into two steps.
-
-    Parameters
-    ----------
-    task_config : dict
-        The config of a task.
-    experiment_name: str
-        The name of experiment
-    recorder_name: str
-        The name of recorder
-
-    Returns
-    ----------
-    Recorder: The instance of the recorder
-    """
-    with R.start(experiment_name=experiment_name, recorder_name=recorder_name):
-        _log_task_info(task_config)
-        _exe_task(task_config)
-        return R.get_recorder()
-
-
-class Trainer:
-    """
-    The trainer can train a list of models.
-    There are Trainer and DelayTrainer, which can be distinguished by when it will finish real training.
-    """
-
-    def __init__(self):
-        self.delay = False
-
-    def train(self, tasks: list, *args, **kwargs) -> list:
-        """
-        Given a list of task definitions, begin training, and return the models.
-
-        For Trainer, it finishes real training in this method.
-        For DelayTrainer, it only does some preparation in this method.
-
-        Args:
-            tasks: a list of tasks
-
-        Returns:
-            list: a list of models
-        """
-        raise NotImplementedError(f"Please implement the `train` method.")
-
-    def end_train(self, models: list, *args, **kwargs) -> list:
-        """
-        Given a list of models, finished something at the end of training if you need.
-        The models may be Recorder, txt file, database, and so on.
-
-        For Trainer, it does some finishing touches in this method.
-        For DelayTrainer, it finishes real training in this method.
-
-        Args:
-            models: a list of models
-
-        Returns:
-            list: a list of models
-        """
-        # do nothing if you finished all work in `train` method
-        return models
-
-    def is_delay(self) -> bool:
-        """
-        If Trainer will delay finishing `end_train`.
-
-        Returns:
-            bool: if DelayTrainer
-        """
-        return self.delay
-
-    def __call__(self, *args, **kwargs) -> list:
-        return self.end_train(self.train(*args, **kwargs))
-
-    def has_worker(self) -> bool:
-        """
-        Some trainer has backend worker to support parallel training
-        This method can tell if the worker is enabled.
-
-        Returns
-        -------
-        bool:
-            if the worker is enabled
-
-        """
-        return False
-
-    def worker(self):
-        """
-        start the worker
-
-        Raises
-        ------
-        NotImplementedError:
-            If the worker is not supported
-        """
-        raise NotImplementedError(f"Please implement the `worker` method")
-
-
-class TrainerR(Trainer):
-    """
-    Trainer based on (R)ecorder.
-    It will train a list of tasks and return a list of model recorders in a linear way.
-
-    Assumption: models were defined by `task` and the results will be saved to `Recorder`.
-    """
-
-    # Those tag will help you distinguish whether the Recorder has finished traning
-    STATUS_KEY = "train_status"
-    STATUS_BEGIN = "begin_task_train"
-    STATUS_END = "end_task_train"
-
-    def __init__(
-        self,
-        experiment_name: Optional[str] = None,
-        train_func: Callable = task_train,
-        call_in_subproc: bool = False,
-        default_rec_name: Optional[str] = None,
-    ):
-        """
-        Init TrainerR.
-
-        Args:
-            experiment_name (str, optional): the default name of experiment.
-            train_func (Callable, optional): default training method. Defaults to `task_train`.
-            call_in_subproc (bool): call the process in subprocess to force memory release
-        """
-        super().__init__()
-        self.experiment_name = experiment_name
-        self.default_rec_name = default_rec_name
-        self.train_func = train_func
-        self._call_in_subproc = call_in_subproc
-
-    def train(self, tasks: list, train_func: Callable = None, experiment_name: str = None, **kwargs) -> List[Recorder]:
-        """
-        Given a list of `tasks` and return a list of trained Recorder. The order can be guaranteed.
-
-        Args:
-            tasks (list): a list of definitions based on `task` dict
-            train_func (Callable): the training method which needs at least `tasks` and `experiment_name`. None for the default training method.
-            experiment_name (str): the experiment name, None for use default name.
-            kwargs: the params for train_func.
-
-        Returns:
-            List[Recorder]: a list of Recorders
-        """
-        if isinstance(tasks, dict):
-            tasks = [tasks]
-        if len(tasks) == 0:
-            return []
-        if train_func is None:
-            train_func = self.train_func
-        if experiment_name is None:
-            experiment_name = self.experiment_name
-        recs = []
-        for task in tqdm(tasks, desc="train tasks"):
-            if self._call_in_subproc:
-                get_module_logger("TrainerR").info("running models in sub process (for forcing release memroy).")
-                train_func = call_in_subproc(train_func, C)
-            rec = train_func(task, experiment_name, recorder_name=self.default_rec_name, **kwargs)
-            rec.set_tags(**{self.STATUS_KEY: self.STATUS_BEGIN})
-            recs.append(rec)
-        return recs
-
-    def end_train(self, models: list, **kwargs) -> List[Recorder]:
-        """
-        Set STATUS_END tag to the recorders.
-
-        Args:
-            models (list): a list of trained recorders.
-
-        Returns:
-            List[Recorder]: the same list as the param.
-        """
-        if isinstance(models, Recorder):
-            models = [models]
-        for rec in models:
-            rec.set_tags(**{self.STATUS_KEY: self.STATUS_END})
-        return models
-
-
-class DelayTrainerR(TrainerR):
-    """
-    A delayed implementation based on TrainerR, which means `train` method may only do some preparation and `end_train` method can do the real model fitting.
-    """
-
-    def __init__(
-        self, experiment_name: str = None, train_func=begin_task_train, end_train_func=end_task_train, **kwargs
-    ):
-        """
-        Init TrainerRM.
-
-        Args:
-            experiment_name (str): the default name of experiment.
-            train_func (Callable, optional): default train method. Defaults to `begin_task_train`.
-            end_train_func (Callable, optional): default end_train method. Defaults to `end_task_train`.
-        """
-        super().__init__(experiment_name, train_func, **kwargs)
-        self.end_train_func = end_train_func
-        self.delay = True
-
-    def end_train(self, models, end_train_func=None, experiment_name: str = None, **kwargs) -> List[Recorder]:
-        """
-        Given a list of Recorder and return a list of trained Recorder.
-        This class will finish real data loading and model fitting.
-
-        Args:
-            models (list): a list of Recorder, the tasks have been saved to them
-            end_train_func (Callable, optional): the end_train method which needs at least `recorders` and `experiment_name`. Defaults to None for using self.end_train_func.
-            experiment_name (str): the experiment name, None for use default name.
-            kwargs: the params for end_train_func.
-
-        Returns:
-            List[Recorder]: a list of Recorders
-        """
-        if isinstance(models, Recorder):
-            models = [models]
-        if end_train_func is None:
-            end_train_func = self.end_train_func
-        if experiment_name is None:
-            experiment_name = self.experiment_name
-        for rec in models:
-            if rec.list_tags()[self.STATUS_KEY] == self.STATUS_END:
-                continue
-            end_train_func(rec, experiment_name, **kwargs)
-            rec.set_tags(**{self.STATUS_KEY: self.STATUS_END})
-        return models
-
-
-class TrainerRM(Trainer):
-    """
-    Trainer based on (R)ecorder and Task(M)anager.
-    It can train a list of tasks and return a list of model recorders in a multiprocessing way.
-
-    Assumption: `task` will be saved to TaskManager and `task` will be fetched and trained from TaskManager
-    """
-
-    # Those tag will help you distinguish whether the Recorder has finished traning
-    STATUS_KEY = "train_status"
-    STATUS_BEGIN = "begin_task_train"
-    STATUS_END = "end_task_train"
-
-    # This tag is the _id in TaskManager to distinguish tasks.
-    TM_ID = "_id in TaskManager"
-
-    def __init__(
-        self,
-        experiment_name: str = None,
-        task_pool: str = None,
-        train_func=task_train,
-        skip_run_task: bool = False,
-        default_rec_name: Optional[str] = None,
-    ):
-        """
-        Init TrainerR.
-
-        Args:
-            experiment_name (str): the default name of experiment.
-            task_pool (str): task pool name in TaskManager. None for use same name as experiment_name.
-            train_func (Callable, optional): default training method. Defaults to `task_train`.
-            skip_run_task (bool):
-                If skip_run_task == True:
-                Only run_task in the worker. Otherwise skip run_task.
-        """
-
-        super().__init__()
-        self.experiment_name = experiment_name
-        self.task_pool = task_pool
-        self.train_func = train_func
-        self.skip_run_task = skip_run_task
-        self.default_rec_name = default_rec_name
-
-    def train(
-        self,
-        tasks: list,
-        train_func: Callable = None,
-        experiment_name: str = None,
-        before_status: str = TaskManager.STATUS_WAITING,
-        after_status: str = TaskManager.STATUS_DONE,
-        default_rec_name: Optional[str] = None,
-        **kwargs,
-    ) -> List[Recorder]:
-        """
-        Given a list of `tasks` and return a list of trained Recorder. The order can be guaranteed.
-
-        This method defaults to a single process, but TaskManager offered a great way to parallel training.
-        Users can customize their train_func to realize multiple processes or even multiple machines.
-
-        Args:
-            tasks (list): a list of definitions based on `task` dict
-            train_func (Callable): the training method which needs at least `tasks` and `experiment_name`. None for the default training method.
-            experiment_name (str): the experiment name, None for use default name.
-            before_status (str): the tasks in before_status will be fetched and trained. Can be STATUS_WAITING, STATUS_PART_DONE.
-            after_status (str): the tasks after trained will become after_status. Can be STATUS_WAITING, STATUS_PART_DONE.
-            kwargs: the params for train_func.
-
-        Returns:
-            List[Recorder]: a list of Recorders
-        """
-        if isinstance(tasks, dict):
-            tasks = [tasks]
-        if len(tasks) == 0:
-            return []
-        if train_func is None:
-            train_func = self.train_func
-        if experiment_name is None:
-            experiment_name = self.experiment_name
-        if default_rec_name is None:
-            default_rec_name = self.default_rec_name
-        task_pool = self.task_pool
-        if task_pool is None:
-            task_pool = experiment_name
-        tm = TaskManager(task_pool=task_pool)
-        _id_list = tm.create_task(tasks)  # all tasks will be saved to MongoDB
-        query = {"_id": {"$in": _id_list}}
-        if not self.skip_run_task:
-            run_task(
-                train_func,
-                task_pool,
-                query=query,  # only train these tasks
-                experiment_name=experiment_name,
-                before_status=before_status,
-                after_status=after_status,
-                recorder_name=default_rec_name,
-                **kwargs,
-            )
-
-        if not self.is_delay():
-            tm.wait(query=query)
-
-        recs = []
-        for _id in _id_list:
-            rec = tm.re_query(_id)["res"]
-            rec.set_tags(**{self.STATUS_KEY: self.STATUS_BEGIN})
-            rec.set_tags(**{self.TM_ID: _id})
-            recs.append(rec)
-        return recs
-
-    def end_train(self, recs: list, **kwargs) -> List[Recorder]:
-        """
-        Set STATUS_END tag to the recorders.
-
-        Args:
-            recs (list): a list of trained recorders.
-
-        Returns:
-            List[Recorder]: the same list as the param.
-        """
-        if isinstance(recs, Recorder):
-            recs = [recs]
-        for rec in recs:
-            rec.set_tags(**{self.STATUS_KEY: self.STATUS_END})
-        return recs
-
-    def worker(
-        self,
-        train_func: Callable = None,
-        experiment_name: str = None,
-    ):
-        """
-        The multiprocessing method for `train`. It can share a same task_pool with `train` and can run in other progress or other machines.
-
-        Args:
-            train_func (Callable): the training method which needs at least `tasks` and `experiment_name`. None for the default training method.
-            experiment_name (str): the experiment name, None for use default name.
-        """
-        if train_func is None:
-            train_func = self.train_func
-        if experiment_name is None:
-            experiment_name = self.experiment_name
-        task_pool = self.task_pool
-        if task_pool is None:
-            task_pool = experiment_name
-        run_task(train_func, task_pool=task_pool, experiment_name=experiment_name)
-
-    def has_worker(self) -> bool:
-        return True
-
-
-class DelayTrainerRM(TrainerRM):
-    """
-    A delayed implementation based on TrainerRM, which means `train` method may only do some preparation and `end_train` method can do the real model fitting.
-
-    """
-
-    def __init__(
-        self,
-        experiment_name: str = None,
-        task_pool: str = None,
-        train_func=begin_task_train,
-        end_train_func=end_task_train,
-        skip_run_task: bool = False,
-        **kwargs,
-    ):
-        """
-        Init DelayTrainerRM.
-
-        Args:
-            experiment_name (str): the default name of experiment.
-            task_pool (str): task pool name in TaskManager. None for use same name as experiment_name.
-            train_func (Callable, optional): default train method. Defaults to `begin_task_train`.
-            end_train_func (Callable, optional): default end_train method. Defaults to `end_task_train`.
-            skip_run_task (bool):
-                If skip_run_task == True:
-                Only run_task in the worker. Otherwise skip run_task.
-                E.g. Starting trainer on a CPU VM and then waiting tasks to be finished on GPU VMs.
-        """
-        super().__init__(experiment_name, task_pool, train_func, **kwargs)
-        self.end_train_func = end_train_func
-        self.delay = True
-        self.skip_run_task = skip_run_task
-
-    def train(self, tasks: list, train_func=None, experiment_name: str = None, **kwargs) -> List[Recorder]:
-        """
-        Same as `train` of TrainerRM, after_status will be STATUS_PART_DONE.
-
-        Args:
-            tasks (list): a list of definition based on `task` dict
-            train_func (Callable): the train method which need at least `tasks` and `experiment_name`. Defaults to None for using self.train_func.
-            experiment_name (str): the experiment name, None for use default name.
-
-        Returns:
-            List[Recorder]: a list of Recorders
-        """
-        if isinstance(tasks, dict):
-            tasks = [tasks]
-        if len(tasks) == 0:
-            return []
-        _skip_run_task = self.skip_run_task
-        self.skip_run_task = False  # The task preparation can't be skipped
-        res = super().train(
-            tasks,
-            train_func=train_func,
-            experiment_name=experiment_name,
-            after_status=TaskManager.STATUS_PART_DONE,
-            **kwargs,
-        )
-        self.skip_run_task = _skip_run_task
-        return res
-
-    def end_train(self, recs, end_train_func=None, experiment_name: str = None, **kwargs) -> List[Recorder]:
-        """
-        Given a list of Recorder and return a list of trained Recorder.
-        This class will finish real data loading and model fitting.
-
-        Args:
-            recs (list): a list of Recorder, the tasks have been saved to them.
-            end_train_func (Callable, optional): the end_train method which need at least `recorders` and `experiment_name`. Defaults to None for using self.end_train_func.
-            experiment_name (str): the experiment name, None for use default name.
-            kwargs: the params for end_train_func.
-
-        Returns:
-            List[Recorder]: a list of Recorders
-        """
-        if isinstance(recs, Recorder):
-            recs = [recs]
-        if end_train_func is None:
-            end_train_func = self.end_train_func
-        if experiment_name is None:
-            experiment_name = self.experiment_name
-        task_pool = self.task_pool
-        if task_pool is None:
-            task_pool = experiment_name
-        _id_list = []
-        for rec in recs:
-            _id_list.append(rec.list_tags()[self.TM_ID])
-
-        query = {"_id": {"$in": _id_list}}
-        if not self.skip_run_task:
-            run_task(
-                end_train_func,
-                task_pool,
-                query=query,  # only train these tasks
-                experiment_name=experiment_name,
-                before_status=TaskManager.STATUS_PART_DONE,
-                **kwargs,
-            )
-
-        TaskManager(task_pool=task_pool).wait(query=query)
-
-        for rec in recs:
-            rec.set_tags(**{self.STATUS_KEY: self.STATUS_END})
-        return recs
-
-    def worker(self, end_train_func=None, experiment_name: str = None):
-        """
-        The multiprocessing method for `end_train`. It can share a same task_pool with `end_train` and can run in other progress or other machines.
-
-        Args:
-            end_train_func (Callable, optional): the end_train method which need at least `recorders` and `experiment_name`. Defaults to None for using self.end_train_func.
-            experiment_name (str): the experiment name, None for use default name.
-        """
-        if end_train_func is None:
-            end_train_func = self.end_train_func
-        if experiment_name is None:
-            experiment_name = self.experiment_name
-        task_pool = self.task_pool
-        if task_pool is None:
-            task_pool = experiment_name
-        run_task(
-            end_train_func,
-            task_pool=task_pool,
-            experiment_name=experiment_name,
-            before_status=TaskManager.STATUS_PART_DONE,
-        )
-
-    def has_worker(self) -> bool:
-        return True
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+"""
+The Trainer will train a list of tasks and return a list of model recorders.
+There are two steps in each Trainer including ``train`` (make model recorder) and ``end_train`` (modify model recorder).
+
+This is a concept called ``DelayTrainer``, which can be used in online simulating for parallel training.
+In ``DelayTrainer``, the first step is only to save some necessary info to model recorders, and the second step which will be finished in the end can do some concurrent and time-consuming operations such as model fitting.
+
+``Qlib`` offer two kinds of Trainer, ``TrainerR`` is the simplest way and ``TrainerRM`` is based on TaskManager to help manager tasks lifecycle automatically.
+"""
+
+import socket
+from typing import Callable, List, Optional
+
+from tqdm.auto import tqdm
+
+from qlib.config import C
+from qlib.data.dataset import Dataset
+from qlib.data.dataset.weight import Reweighter
+from qlib.log import get_module_logger
+from qlib.model.base import Model
+from qlib.utils import (
+    auto_filter_kwargs,
+    fill_placeholder,
+    flatten_dict,
+    init_instance_by_config,
+)
+from qlib.utils.paral import call_in_subproc
+from qlib.workflow import R
+from qlib.workflow.recorder import Recorder
+from qlib.workflow.task.manage import TaskManager, run_task
+
+
+def _log_task_info(task_config: dict):
+    R.log_params(**flatten_dict(task_config))
+    R.save_objects(**{"task": task_config})  # keep the original format and datatype
+    R.set_tags(**{"hostname": socket.gethostname()})
+
+
+def _exe_task(task_config: dict):
+    rec = R.get_recorder()
+    # model & dataset initiation
+    model: Model = init_instance_by_config(task_config["model"], accept_types=Model)
+    dataset: Dataset = init_instance_by_config(task_config["dataset"], accept_types=Dataset)
+    reweighter: Reweighter = task_config.get("reweighter", None)
+    # model training
+    auto_filter_kwargs(model.fit)(dataset, reweighter=reweighter)
+    R.save_objects(**{"params.pkl": model})
+    # this dataset is saved for online inference. So the concrete data should not be dumped
+    dataset.config(dump_all=False, recursive=True)
+    R.save_objects(**{"dataset": dataset})
+    # fill placehorder
+    placehorder_value = {"<MODEL>": model, "<DATASET>": dataset}
+    task_config = fill_placeholder(task_config, placehorder_value)
+    # generate records: prediction, backtest, and analysis
+    records = task_config.get("record", [])
+    if isinstance(records, dict):  # prevent only one dict
+        records = [records]
+    for record in records:
+        # Some recorder require the parameter `model` and `dataset`.
+        # try to automatically pass in them to the initialization function
+        # to make defining the tasking easier
+        r = init_instance_by_config(
+            record,
+            recorder=rec,
+            default_module="qlib.workflow.record_temp",
+            try_kwargs={"model": model, "dataset": dataset},
+        )
+        r.generate()
+
+
+def begin_task_train(task_config: dict, experiment_name: str, recorder_name: str = None) -> Recorder:
+    """
+    Begin task training to start a recorder and save the task config.
+
+    Args:
+        task_config (dict): the config of a task
+        experiment_name (str): the name of experiment
+        recorder_name (str): the given name will be the recorder name. None for using rid.
+
+    Returns:
+        Recorder: the model recorder
+    """
+    with R.start(experiment_name=experiment_name, recorder_name=recorder_name):
+        _log_task_info(task_config)
+        return R.get_recorder()
+
+
+def end_task_train(rec: Recorder, experiment_name: str) -> Recorder:
+    """
+    Finish task training with real model fitting and saving.
+
+    Args:
+        rec (Recorder): the recorder will be resumed
+        experiment_name (str): the name of experiment
+
+    Returns:
+        Recorder: the model recorder
+    """
+    with R.start(experiment_name=experiment_name, recorder_id=rec.info["id"], resume=True):
+        task_config = R.load_object("task")
+        _exe_task(task_config)
+    return rec
+
+
+def task_train(task_config: dict, experiment_name: str, recorder_name: str = None) -> Recorder:
+    """
+    Task based training, will be divided into two steps.
+
+    Parameters
+    ----------
+    task_config : dict
+        The config of a task.
+    experiment_name: str
+        The name of experiment
+    recorder_name: str
+        The name of recorder
+
+    Returns
+    ----------
+    Recorder: The instance of the recorder
+    """
+    with R.start(experiment_name=experiment_name, recorder_name=recorder_name):
+        _log_task_info(task_config)
+        _exe_task(task_config)
+        return R.get_recorder()
+
+
+class Trainer:
+    """
+    The trainer can train a list of models.
+    There are Trainer and DelayTrainer, which can be distinguished by when it will finish real training.
+    """
+
+    def __init__(self):
+        self.delay = False
+
+    def train(self, tasks: list, *args, **kwargs) -> list:
+        """
+        Given a list of task definitions, begin training, and return the models.
+
+        For Trainer, it finishes real training in this method.
+        For DelayTrainer, it only does some preparation in this method.
+
+        Args:
+            tasks: a list of tasks
+
+        Returns:
+            list: a list of models
+        """
+        raise NotImplementedError(f"Please implement the `train` method.")
+
+    def end_train(self, models: list, *args, **kwargs) -> list:
+        """
+        Given a list of models, finished something at the end of training if you need.
+        The models may be Recorder, txt file, database, and so on.
+
+        For Trainer, it does some finishing touches in this method.
+        For DelayTrainer, it finishes real training in this method.
+
+        Args:
+            models: a list of models
+
+        Returns:
+            list: a list of models
+        """
+        # do nothing if you finished all work in `train` method
+        return models
+
+    def is_delay(self) -> bool:
+        """
+        If Trainer will delay finishing `end_train`.
+
+        Returns:
+            bool: if DelayTrainer
+        """
+        return self.delay
+
+    def __call__(self, *args, **kwargs) -> list:
+        return self.end_train(self.train(*args, **kwargs))
+
+    def has_worker(self) -> bool:
+        """
+        Some trainer has backend worker to support parallel training
+        This method can tell if the worker is enabled.
+
+        Returns
+        -------
+        bool:
+            if the worker is enabled
+
+        """
+        return False
+
+    def worker(self):
+        """
+        start the worker
+
+        Raises
+        ------
+        NotImplementedError:
+            If the worker is not supported
+        """
+        raise NotImplementedError(f"Please implement the `worker` method")
+
+
+class TrainerR(Trainer):
+    """
+    Trainer based on (R)ecorder.
+    It will train a list of tasks and return a list of model recorders in a linear way.
+
+    Assumption: models were defined by `task` and the results will be saved to `Recorder`.
+    """
+
+    # Those tag will help you distinguish whether the Recorder has finished traning
+    STATUS_KEY = "train_status"
+    STATUS_BEGIN = "begin_task_train"
+    STATUS_END = "end_task_train"
+
+    def __init__(
+        self,
+        experiment_name: Optional[str] = None,
+        train_func: Callable = task_train,
+        call_in_subproc: bool = False,
+        default_rec_name: Optional[str] = None,
+    ):
+        """
+        Init TrainerR.
+
+        Args:
+            experiment_name (str, optional): the default name of experiment.
+            train_func (Callable, optional): default training method. Defaults to `task_train`.
+            call_in_subproc (bool): call the process in subprocess to force memory release
+        """
+        super().__init__()
+        self.experiment_name = experiment_name
+        self.default_rec_name = default_rec_name
+        self.train_func = train_func
+        self._call_in_subproc = call_in_subproc
+
+    def train(self, tasks: list, train_func: Callable = None, experiment_name: str = None, **kwargs) -> List[Recorder]:
+        """
+        Given a list of `tasks` and return a list of trained Recorder. The order can be guaranteed.
+
+        Args:
+            tasks (list): a list of definitions based on `task` dict
+            train_func (Callable): the training method which needs at least `tasks` and `experiment_name`. None for the default training method.
+            experiment_name (str): the experiment name, None for use default name.
+            kwargs: the params for train_func.
+
+        Returns:
+            List[Recorder]: a list of Recorders
+        """
+        if isinstance(tasks, dict):
+            tasks = [tasks]
+        if len(tasks) == 0:
+            return []
+        if train_func is None:
+            train_func = self.train_func
+        if experiment_name is None:
+            experiment_name = self.experiment_name
+        recs = []
+        for task in tqdm(tasks, desc="train tasks"):
+            if self._call_in_subproc:
+                get_module_logger("TrainerR").info("running models in sub process (for forcing release memroy).")
+                train_func = call_in_subproc(train_func, C)
+            rec = train_func(task, experiment_name, recorder_name=self.default_rec_name, **kwargs)
+            rec.set_tags(**{self.STATUS_KEY: self.STATUS_BEGIN})
+            recs.append(rec)
+        return recs
+
+    def end_train(self, models: list, **kwargs) -> List[Recorder]:
+        """
+        Set STATUS_END tag to the recorders.
+
+        Args:
+            models (list): a list of trained recorders.
+
+        Returns:
+            List[Recorder]: the same list as the param.
+        """
+        if isinstance(models, Recorder):
+            models = [models]
+        for rec in models:
+            rec.set_tags(**{self.STATUS_KEY: self.STATUS_END})
+        return models
+
+
+class DelayTrainerR(TrainerR):
+    """
+    A delayed implementation based on TrainerR, which means `train` method may only do some preparation and `end_train` method can do the real model fitting.
+    """
+
+    def __init__(
+        self, experiment_name: str = None, train_func=begin_task_train, end_train_func=end_task_train, **kwargs
+    ):
+        """
+        Init TrainerRM.
+
+        Args:
+            experiment_name (str): the default name of experiment.
+            train_func (Callable, optional): default train method. Defaults to `begin_task_train`.
+            end_train_func (Callable, optional): default end_train method. Defaults to `end_task_train`.
+        """
+        super().__init__(experiment_name, train_func, **kwargs)
+        self.end_train_func = end_train_func
+        self.delay = True
+
+    def end_train(self, models, end_train_func=None, experiment_name: str = None, **kwargs) -> List[Recorder]:
+        """
+        Given a list of Recorder and return a list of trained Recorder.
+        This class will finish real data loading and model fitting.
+
+        Args:
+            models (list): a list of Recorder, the tasks have been saved to them
+            end_train_func (Callable, optional): the end_train method which needs at least `recorders` and `experiment_name`. Defaults to None for using self.end_train_func.
+            experiment_name (str): the experiment name, None for use default name.
+            kwargs: the params for end_train_func.
+
+        Returns:
+            List[Recorder]: a list of Recorders
+        """
+        if isinstance(models, Recorder):
+            models = [models]
+        if end_train_func is None:
+            end_train_func = self.end_train_func
+        if experiment_name is None:
+            experiment_name = self.experiment_name
+        for rec in models:
+            if rec.list_tags()[self.STATUS_KEY] == self.STATUS_END:
+                continue
+            end_train_func(rec, experiment_name, **kwargs)
+            rec.set_tags(**{self.STATUS_KEY: self.STATUS_END})
+        return models
+
+
+class TrainerRM(Trainer):
+    """
+    Trainer based on (R)ecorder and Task(M)anager.
+    It can train a list of tasks and return a list of model recorders in a multiprocessing way.
+
+    Assumption: `task` will be saved to TaskManager and `task` will be fetched and trained from TaskManager
+    """
+
+    # Those tag will help you distinguish whether the Recorder has finished traning
+    STATUS_KEY = "train_status"
+    STATUS_BEGIN = "begin_task_train"
+    STATUS_END = "end_task_train"
+
+    # This tag is the _id in TaskManager to distinguish tasks.
+    TM_ID = "_id in TaskManager"
+
+    def __init__(
+        self,
+        experiment_name: str = None,
+        task_pool: str = None,
+        train_func=task_train,
+        skip_run_task: bool = False,
+        default_rec_name: Optional[str] = None,
+    ):
+        """
+        Init TrainerR.
+
+        Args:
+            experiment_name (str): the default name of experiment.
+            task_pool (str): task pool name in TaskManager. None for use same name as experiment_name.
+            train_func (Callable, optional): default training method. Defaults to `task_train`.
+            skip_run_task (bool):
+                If skip_run_task == True:
+                Only run_task in the worker. Otherwise skip run_task.
+        """
+
+        super().__init__()
+        self.experiment_name = experiment_name
+        self.task_pool = task_pool
+        self.train_func = train_func
+        self.skip_run_task = skip_run_task
+        self.default_rec_name = default_rec_name
+
+    def train(
+        self,
+        tasks: list,
+        train_func: Callable = None,
+        experiment_name: str = None,
+        before_status: str = TaskManager.STATUS_WAITING,
+        after_status: str = TaskManager.STATUS_DONE,
+        default_rec_name: Optional[str] = None,
+        **kwargs,
+    ) -> List[Recorder]:
+        """
+        Given a list of `tasks` and return a list of trained Recorder. The order can be guaranteed.
+
+        This method defaults to a single process, but TaskManager offered a great way to parallel training.
+        Users can customize their train_func to realize multiple processes or even multiple machines.
+
+        Args:
+            tasks (list): a list of definitions based on `task` dict
+            train_func (Callable): the training method which needs at least `tasks` and `experiment_name`. None for the default training method.
+            experiment_name (str): the experiment name, None for use default name.
+            before_status (str): the tasks in before_status will be fetched and trained. Can be STATUS_WAITING, STATUS_PART_DONE.
+            after_status (str): the tasks after trained will become after_status. Can be STATUS_WAITING, STATUS_PART_DONE.
+            kwargs: the params for train_func.
+
+        Returns:
+            List[Recorder]: a list of Recorders
+        """
+        if isinstance(tasks, dict):
+            tasks = [tasks]
+        if len(tasks) == 0:
+            return []
+        if train_func is None:
+            train_func = self.train_func
+        if experiment_name is None:
+            experiment_name = self.experiment_name
+        if default_rec_name is None:
+            default_rec_name = self.default_rec_name
+        task_pool = self.task_pool
+        if task_pool is None:
+            task_pool = experiment_name
+        tm = TaskManager(task_pool=task_pool)
+        _id_list = tm.create_task(tasks)  # all tasks will be saved to MongoDB
+        query = {"_id": {"$in": _id_list}}
+        if not self.skip_run_task:
+            run_task(
+                train_func,
+                task_pool,
+                query=query,  # only train these tasks
+                experiment_name=experiment_name,
+                before_status=before_status,
+                after_status=after_status,
+                recorder_name=default_rec_name,
+                **kwargs,
+            )
+
+        if not self.is_delay():
+            tm.wait(query=query)
+
+        recs = []
+        for _id in _id_list:
+            rec = tm.re_query(_id)["res"]
+            rec.set_tags(**{self.STATUS_KEY: self.STATUS_BEGIN})
+            rec.set_tags(**{self.TM_ID: _id})
+            recs.append(rec)
+        return recs
+
+    def end_train(self, recs: list, **kwargs) -> List[Recorder]:
+        """
+        Set STATUS_END tag to the recorders.
+
+        Args:
+            recs (list): a list of trained recorders.
+
+        Returns:
+            List[Recorder]: the same list as the param.
+        """
+        if isinstance(recs, Recorder):
+            recs = [recs]
+        for rec in recs:
+            rec.set_tags(**{self.STATUS_KEY: self.STATUS_END})
+        return recs
+
+    def worker(
+        self,
+        train_func: Callable = None,
+        experiment_name: str = None,
+    ):
+        """
+        The multiprocessing method for `train`. It can share a same task_pool with `train` and can run in other progress or other machines.
+
+        Args:
+            train_func (Callable): the training method which needs at least `tasks` and `experiment_name`. None for the default training method.
+            experiment_name (str): the experiment name, None for use default name.
+        """
+        if train_func is None:
+            train_func = self.train_func
+        if experiment_name is None:
+            experiment_name = self.experiment_name
+        task_pool = self.task_pool
+        if task_pool is None:
+            task_pool = experiment_name
+        run_task(train_func, task_pool=task_pool, experiment_name=experiment_name)
+
+    def has_worker(self) -> bool:
+        return True
+
+
+class DelayTrainerRM(TrainerRM):
+    """
+    A delayed implementation based on TrainerRM, which means `train` method may only do some preparation and `end_train` method can do the real model fitting.
+
+    """
+
+    def __init__(
+        self,
+        experiment_name: str = None,
+        task_pool: str = None,
+        train_func=begin_task_train,
+        end_train_func=end_task_train,
+        skip_run_task: bool = False,
+        **kwargs,
+    ):
+        """
+        Init DelayTrainerRM.
+
+        Args:
+            experiment_name (str): the default name of experiment.
+            task_pool (str): task pool name in TaskManager. None for use same name as experiment_name.
+            train_func (Callable, optional): default train method. Defaults to `begin_task_train`.
+            end_train_func (Callable, optional): default end_train method. Defaults to `end_task_train`.
+            skip_run_task (bool):
+                If skip_run_task == True:
+                Only run_task in the worker. Otherwise skip run_task.
+                E.g. Starting trainer on a CPU VM and then waiting tasks to be finished on GPU VMs.
+        """
+        super().__init__(experiment_name, task_pool, train_func, **kwargs)
+        self.end_train_func = end_train_func
+        self.delay = True
+        self.skip_run_task = skip_run_task
+
+    def train(self, tasks: list, train_func=None, experiment_name: str = None, **kwargs) -> List[Recorder]:
+        """
+        Same as `train` of TrainerRM, after_status will be STATUS_PART_DONE.
+
+        Args:
+            tasks (list): a list of definition based on `task` dict
+            train_func (Callable): the train method which need at least `tasks` and `experiment_name`. Defaults to None for using self.train_func.
+            experiment_name (str): the experiment name, None for use default name.
+
+        Returns:
+            List[Recorder]: a list of Recorders
+        """
+        if isinstance(tasks, dict):
+            tasks = [tasks]
+        if len(tasks) == 0:
+            return []
+        _skip_run_task = self.skip_run_task
+        self.skip_run_task = False  # The task preparation can't be skipped
+        res = super().train(
+            tasks,
+            train_func=train_func,
+            experiment_name=experiment_name,
+            after_status=TaskManager.STATUS_PART_DONE,
+            **kwargs,
+        )
+        self.skip_run_task = _skip_run_task
+        return res
+
+    def end_train(self, recs, end_train_func=None, experiment_name: str = None, **kwargs) -> List[Recorder]:
+        """
+        Given a list of Recorder and return a list of trained Recorder.
+        This class will finish real data loading and model fitting.
+
+        Args:
+            recs (list): a list of Recorder, the tasks have been saved to them.
+            end_train_func (Callable, optional): the end_train method which need at least `recorders` and `experiment_name`. Defaults to None for using self.end_train_func.
+            experiment_name (str): the experiment name, None for use default name.
+            kwargs: the params for end_train_func.
+
+        Returns:
+            List[Recorder]: a list of Recorders
+        """
+        if isinstance(recs, Recorder):
+            recs = [recs]
+        if end_train_func is None:
+            end_train_func = self.end_train_func
+        if experiment_name is None:
+            experiment_name = self.experiment_name
+        task_pool = self.task_pool
+        if task_pool is None:
+            task_pool = experiment_name
+        _id_list = []
+        for rec in recs:
+            _id_list.append(rec.list_tags()[self.TM_ID])
+
+        query = {"_id": {"$in": _id_list}}
+        if not self.skip_run_task:
+            run_task(
+                end_train_func,
+                task_pool,
+                query=query,  # only train these tasks
+                experiment_name=experiment_name,
+                before_status=TaskManager.STATUS_PART_DONE,
+                **kwargs,
+            )
+
+        TaskManager(task_pool=task_pool).wait(query=query)
+
+        for rec in recs:
+            rec.set_tags(**{self.STATUS_KEY: self.STATUS_END})
+        return recs
+
+    def worker(self, end_train_func=None, experiment_name: str = None):
+        """
+        The multiprocessing method for `end_train`. It can share a same task_pool with `end_train` and can run in other progress or other machines.
+
+        Args:
+            end_train_func (Callable, optional): the end_train method which need at least `recorders` and `experiment_name`. Defaults to None for using self.end_train_func.
+            experiment_name (str): the experiment name, None for use default name.
+        """
+        if end_train_func is None:
+            end_train_func = self.end_train_func
+        if experiment_name is None:
+            experiment_name = self.experiment_name
+        task_pool = self.task_pool
+        if task_pool is None:
+            task_pool = experiment_name
+        run_task(
+            end_train_func,
+            task_pool=task_pool,
+            experiment_name=experiment_name,
+            before_status=TaskManager.STATUS_PART_DONE,
+        )
+
+    def has_worker(self) -> bool:
+        return True
```

## qlib/model/utils.py

 * *Ordering differences only*

```diff
@@ -1,26 +1,26 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-from torch.utils.data import Dataset
-
-
-class ConcatDataset(Dataset):
-    def __init__(self, *datasets):
-        self.datasets = datasets
-
-    def __getitem__(self, i):
-        return tuple(d[i] for d in self.datasets)
-
-    def __len__(self):
-        return min(len(d) for d in self.datasets)
-
-
-class IndexSampler:
-    def __init__(self, sampler):
-        self.sampler = sampler
-
-    def __getitem__(self, i: int):
-        return self.sampler[i], i
-
-    def __len__(self):
-        return len(self.sampler)
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+from torch.utils.data import Dataset
+
+
+class ConcatDataset(Dataset):
+    def __init__(self, *datasets):
+        self.datasets = datasets
+
+    def __getitem__(self, i):
+        return tuple(d[i] for d in self.datasets)
+
+    def __len__(self):
+        return min(len(d) for d in self.datasets)
+
+
+class IndexSampler:
+    def __init__(self, sampler):
+        self.sampler = sampler
+
+    def __getitem__(self, i: int):
+        return self.sampler[i], i
+
+    def __len__(self):
+        return len(self.sampler)
```

## qlib/model/ens/ensemble.py

 * *Ordering differences only*

```diff
@@ -1,134 +1,134 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-"""
-Ensemble module can merge the objects in an Ensemble. For example, if there are many submodels predictions, we may need to merge them into an ensemble prediction.
-"""
-
-from typing import Union
-import pandas as pd
-from qlib.utils import FLATTEN_TUPLE, flatten_dict
-from qlib.log import get_module_logger
-
-
-class Ensemble:
-    """Merge the ensemble_dict into an ensemble object.
-
-    For example: {Rollinga_b: object, Rollingb_c: object} -> object
-
-    When calling this class:
-
-        Args:
-            ensemble_dict (dict): the ensemble dict like {name: things} waiting for merging
-
-        Returns:
-            object: the ensemble object
-    """
-
-    def __call__(self, ensemble_dict: dict, *args, **kwargs):
-        raise NotImplementedError(f"Please implement the `__call__` method.")
-
-
-class SingleKeyEnsemble(Ensemble):
-
-    """
-    Extract the object if there is only one key and value in the dict. Make the result more readable.
-    {Only key: Only value} -> Only value
-
-    If there is more than 1 key or less than 1 key, then do nothing.
-    Even you can run this recursively to make dict more readable.
-
-    NOTE: Default runs recursively.
-
-    When calling this class:
-
-        Args:
-            ensemble_dict (dict): the dict. The key of the dict will be ignored.
-
-        Returns:
-            dict: the readable dict.
-    """
-
-    def __call__(self, ensemble_dict: Union[dict, object], recursion: bool = True) -> object:
-        if not isinstance(ensemble_dict, dict):
-            return ensemble_dict
-        if recursion:
-            tmp_dict = {}
-            for k, v in ensemble_dict.items():
-                tmp_dict[k] = self(v, recursion)
-            ensemble_dict = tmp_dict
-        keys = list(ensemble_dict.keys())
-        if len(keys) == 1:
-            ensemble_dict = ensemble_dict[keys[0]]
-        return ensemble_dict
-
-
-class RollingEnsemble(Ensemble):
-
-    """Merge a dict of rolling dataframe like `prediction` or `IC` into an ensemble.
-
-    NOTE: The values of dict must be pd.DataFrame, and have the index "datetime".
-
-    When calling this class:
-
-        Args:
-            ensemble_dict (dict): a dict like {"A": pd.DataFrame, "B": pd.DataFrame}.
-            The key of the dict will be ignored.
-
-        Returns:
-            pd.DataFrame: the complete result of rolling.
-    """
-
-    def __call__(self, ensemble_dict: dict) -> pd.DataFrame:
-        get_module_logger("RollingEnsemble").info(f"keys in group: {list(ensemble_dict.keys())}")
-        artifact_list = list(ensemble_dict.values())
-        artifact_list.sort(key=lambda x: x.index.get_level_values("datetime").min())
-        artifact = pd.concat(artifact_list)
-        # If there are duplicated predition, use the latest perdiction
-        artifact = artifact[~artifact.index.duplicated(keep="last")]
-        artifact = artifact.sort_index()
-        return artifact
-
-
-class AverageEnsemble(Ensemble):
-    """
-    Average and standardize a dict of same shape dataframe like `prediction` or `IC` into an ensemble.
-
-    NOTE: The values of dict must be pd.DataFrame, and have the index "datetime". If it is a nested dict, then flat it.
-
-    When calling this class:
-
-        Args:
-            ensemble_dict (dict): a dict like {"A": pd.DataFrame, "B": pd.DataFrame}.
-            The key of the dict will be ignored.
-
-        Returns:
-            pd.DataFrame: the complete result of averaging and standardizing.
-    """
-
-    def __call__(self, ensemble_dict: dict) -> pd.DataFrame:
-        """using sample:
-        from qlib.model.ens.ensemble import AverageEnsemble
-        pred_res['new_key_name'] = AverageEnsemble()(predict_dict)
-
-        Parameters
-        ----------
-        ensemble_dict : dict
-            Dictionary you want to ensemble
-
-        Returns
-        -------
-        pd.DataFrame
-            The dictionary including ensenbling result
-        """
-        # need to flatten the nested dict
-        ensemble_dict = flatten_dict(ensemble_dict, sep=FLATTEN_TUPLE)
-        get_module_logger("AverageEnsemble").info(f"keys in group: {list(ensemble_dict.keys())}")
-        values = list(ensemble_dict.values())
-        # NOTE: this may change the style underlying data!!!!
-        # from pd.DataFrame to pd.Series
-        results = pd.concat(values, axis=1)
-        results = results.groupby("datetime").apply(lambda df: (df - df.mean()) / df.std())
-        results = results.mean(axis=1)
-        results = results.sort_index()
-        return results
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+"""
+Ensemble module can merge the objects in an Ensemble. For example, if there are many submodels predictions, we may need to merge them into an ensemble prediction.
+"""
+
+from typing import Union
+import pandas as pd
+from qlib.utils import FLATTEN_TUPLE, flatten_dict
+from qlib.log import get_module_logger
+
+
+class Ensemble:
+    """Merge the ensemble_dict into an ensemble object.
+
+    For example: {Rollinga_b: object, Rollingb_c: object} -> object
+
+    When calling this class:
+
+        Args:
+            ensemble_dict (dict): the ensemble dict like {name: things} waiting for merging
+
+        Returns:
+            object: the ensemble object
+    """
+
+    def __call__(self, ensemble_dict: dict, *args, **kwargs):
+        raise NotImplementedError(f"Please implement the `__call__` method.")
+
+
+class SingleKeyEnsemble(Ensemble):
+
+    """
+    Extract the object if there is only one key and value in the dict. Make the result more readable.
+    {Only key: Only value} -> Only value
+
+    If there is more than 1 key or less than 1 key, then do nothing.
+    Even you can run this recursively to make dict more readable.
+
+    NOTE: Default runs recursively.
+
+    When calling this class:
+
+        Args:
+            ensemble_dict (dict): the dict. The key of the dict will be ignored.
+
+        Returns:
+            dict: the readable dict.
+    """
+
+    def __call__(self, ensemble_dict: Union[dict, object], recursion: bool = True) -> object:
+        if not isinstance(ensemble_dict, dict):
+            return ensemble_dict
+        if recursion:
+            tmp_dict = {}
+            for k, v in ensemble_dict.items():
+                tmp_dict[k] = self(v, recursion)
+            ensemble_dict = tmp_dict
+        keys = list(ensemble_dict.keys())
+        if len(keys) == 1:
+            ensemble_dict = ensemble_dict[keys[0]]
+        return ensemble_dict
+
+
+class RollingEnsemble(Ensemble):
+
+    """Merge a dict of rolling dataframe like `prediction` or `IC` into an ensemble.
+
+    NOTE: The values of dict must be pd.DataFrame, and have the index "datetime".
+
+    When calling this class:
+
+        Args:
+            ensemble_dict (dict): a dict like {"A": pd.DataFrame, "B": pd.DataFrame}.
+            The key of the dict will be ignored.
+
+        Returns:
+            pd.DataFrame: the complete result of rolling.
+    """
+
+    def __call__(self, ensemble_dict: dict) -> pd.DataFrame:
+        get_module_logger("RollingEnsemble").info(f"keys in group: {list(ensemble_dict.keys())}")
+        artifact_list = list(ensemble_dict.values())
+        artifact_list.sort(key=lambda x: x.index.get_level_values("datetime").min())
+        artifact = pd.concat(artifact_list)
+        # If there are duplicated predition, use the latest perdiction
+        artifact = artifact[~artifact.index.duplicated(keep="last")]
+        artifact = artifact.sort_index()
+        return artifact
+
+
+class AverageEnsemble(Ensemble):
+    """
+    Average and standardize a dict of same shape dataframe like `prediction` or `IC` into an ensemble.
+
+    NOTE: The values of dict must be pd.DataFrame, and have the index "datetime". If it is a nested dict, then flat it.
+
+    When calling this class:
+
+        Args:
+            ensemble_dict (dict): a dict like {"A": pd.DataFrame, "B": pd.DataFrame}.
+            The key of the dict will be ignored.
+
+        Returns:
+            pd.DataFrame: the complete result of averaging and standardizing.
+    """
+
+    def __call__(self, ensemble_dict: dict) -> pd.DataFrame:
+        """using sample:
+        from qlib.model.ens.ensemble import AverageEnsemble
+        pred_res['new_key_name'] = AverageEnsemble()(predict_dict)
+
+        Parameters
+        ----------
+        ensemble_dict : dict
+            Dictionary you want to ensemble
+
+        Returns
+        -------
+        pd.DataFrame
+            The dictionary including ensenbling result
+        """
+        # need to flatten the nested dict
+        ensemble_dict = flatten_dict(ensemble_dict, sep=FLATTEN_TUPLE)
+        get_module_logger("AverageEnsemble").info(f"keys in group: {list(ensemble_dict.keys())}")
+        values = list(ensemble_dict.values())
+        # NOTE: this may change the style underlying data!!!!
+        # from pd.DataFrame to pd.Series
+        results = pd.concat(values, axis=1)
+        results = results.groupby("datetime").apply(lambda df: (df - df.mean()) / df.std())
+        results = results.mean(axis=1)
+        results = results.sort_index()
+        return results
```

## qlib/model/ens/group.py

 * *Ordering differences only*

```diff
@@ -1,115 +1,115 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-"""
-Group can group a set of objects based on `group_func` and change them to a dict.
-After group, we provide a method to reduce them.
-
-For example:
-
-group: {(A,B,C1): object, (A,B,C2): object} -> {(A,B): {C1: object, C2: object}}
-reduce: {(A,B): {C1: object, C2: object}} -> {(A,B): object}
-
-"""
-
-from qlib.model.ens.ensemble import Ensemble, RollingEnsemble
-from typing import Callable
-from joblib import Parallel, delayed
-
-
-class Group:
-    """Group the objects based on dict"""
-
-    def __init__(self, group_func=None, ens: Ensemble = None):
-        """
-        Init Group.
-
-        Args:
-            group_func (Callable, optional): Given a dict and return the group key and one of the group elements.
-
-                For example: {(A,B,C1): object, (A,B,C2): object} -> {(A,B): {C1: object, C2: object}}
-
-            Defaults to None.
-
-            ens (Ensemble, optional): If not None, do ensemble for grouped value after grouping.
-        """
-        self._group_func = group_func
-        self._ens_func = ens
-
-    def group(self, *args, **kwargs) -> dict:
-        """
-        Group a set of objects and change them to a dict.
-
-        For example: {(A,B,C1): object, (A,B,C2): object} -> {(A,B): {C1: object, C2: object}}
-
-        Returns:
-            dict: grouped dict
-        """
-        if isinstance(getattr(self, "_group_func", None), Callable):
-            return self._group_func(*args, **kwargs)
-        else:
-            raise NotImplementedError(f"Please specify valid `group_func`.")
-
-    def reduce(self, *args, **kwargs) -> dict:
-        """
-        Reduce grouped dict.
-
-        For example: {(A,B): {C1: object, C2: object}} -> {(A,B): object}
-
-        Returns:
-            dict: reduced dict
-        """
-        if isinstance(getattr(self, "_ens_func", None), Callable):
-            return self._ens_func(*args, **kwargs)
-        else:
-            raise NotImplementedError(f"Please specify valid `_ens_func`.")
-
-    def __call__(self, ungrouped_dict: dict, n_jobs: int = 1, verbose: int = 0, *args, **kwargs) -> dict:
-        """
-        Group the ungrouped_dict into different groups.
-
-        Args:
-            ungrouped_dict (dict): the ungrouped dict waiting for grouping like {name: things}
-
-        Returns:
-            dict: grouped_dict like {G1: object, G2: object}
-            n_jobs: how many progress you need.
-            verbose: the print mode for Parallel.
-        """
-
-        # NOTE: The multiprocessing will raise error if you use `Serializable`
-        # Because the `Serializable` will affect the behaviors of pickle
-        grouped_dict = self.group(ungrouped_dict, *args, **kwargs)
-
-        key_l = []
-        job_l = []
-        for key, value in grouped_dict.items():
-            key_l.append(key)
-            job_l.append(delayed(Group.reduce)(self, value))
-        return dict(zip(key_l, Parallel(n_jobs=n_jobs, verbose=verbose)(job_l)))
-
-
-class RollingGroup(Group):
-    """Group the rolling dict"""
-
-    def group(self, rolling_dict: dict) -> dict:
-        """Given an rolling dict likes {(A,B,R): things}, return the grouped dict likes {(A,B): {R:things}}
-
-        NOTE: There is an assumption which is the rolling key is at the end of the key tuple, because the rolling results always need to be ensemble firstly.
-
-        Args:
-            rolling_dict (dict): an rolling dict. If the key is not a tuple, then do nothing.
-
-        Returns:
-            dict: grouped dict
-        """
-        grouped_dict = {}
-        for key, values in rolling_dict.items():
-            if isinstance(key, tuple):
-                grouped_dict.setdefault(key[:-1], {})[key[-1]] = values
-            else:
-                raise TypeError(f"Expected `tuple` type, but got a value `{key}`")
-        return grouped_dict
-
-    def __init__(self, ens=RollingEnsemble()):
-        super().__init__(ens=ens)
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+"""
+Group can group a set of objects based on `group_func` and change them to a dict.
+After group, we provide a method to reduce them.
+
+For example:
+
+group: {(A,B,C1): object, (A,B,C2): object} -> {(A,B): {C1: object, C2: object}}
+reduce: {(A,B): {C1: object, C2: object}} -> {(A,B): object}
+
+"""
+
+from qlib.model.ens.ensemble import Ensemble, RollingEnsemble
+from typing import Callable
+from joblib import Parallel, delayed
+
+
+class Group:
+    """Group the objects based on dict"""
+
+    def __init__(self, group_func=None, ens: Ensemble = None):
+        """
+        Init Group.
+
+        Args:
+            group_func (Callable, optional): Given a dict and return the group key and one of the group elements.
+
+                For example: {(A,B,C1): object, (A,B,C2): object} -> {(A,B): {C1: object, C2: object}}
+
+            Defaults to None.
+
+            ens (Ensemble, optional): If not None, do ensemble for grouped value after grouping.
+        """
+        self._group_func = group_func
+        self._ens_func = ens
+
+    def group(self, *args, **kwargs) -> dict:
+        """
+        Group a set of objects and change them to a dict.
+
+        For example: {(A,B,C1): object, (A,B,C2): object} -> {(A,B): {C1: object, C2: object}}
+
+        Returns:
+            dict: grouped dict
+        """
+        if isinstance(getattr(self, "_group_func", None), Callable):
+            return self._group_func(*args, **kwargs)
+        else:
+            raise NotImplementedError(f"Please specify valid `group_func`.")
+
+    def reduce(self, *args, **kwargs) -> dict:
+        """
+        Reduce grouped dict.
+
+        For example: {(A,B): {C1: object, C2: object}} -> {(A,B): object}
+
+        Returns:
+            dict: reduced dict
+        """
+        if isinstance(getattr(self, "_ens_func", None), Callable):
+            return self._ens_func(*args, **kwargs)
+        else:
+            raise NotImplementedError(f"Please specify valid `_ens_func`.")
+
+    def __call__(self, ungrouped_dict: dict, n_jobs: int = 1, verbose: int = 0, *args, **kwargs) -> dict:
+        """
+        Group the ungrouped_dict into different groups.
+
+        Args:
+            ungrouped_dict (dict): the ungrouped dict waiting for grouping like {name: things}
+
+        Returns:
+            dict: grouped_dict like {G1: object, G2: object}
+            n_jobs: how many progress you need.
+            verbose: the print mode for Parallel.
+        """
+
+        # NOTE: The multiprocessing will raise error if you use `Serializable`
+        # Because the `Serializable` will affect the behaviors of pickle
+        grouped_dict = self.group(ungrouped_dict, *args, **kwargs)
+
+        key_l = []
+        job_l = []
+        for key, value in grouped_dict.items():
+            key_l.append(key)
+            job_l.append(delayed(Group.reduce)(self, value))
+        return dict(zip(key_l, Parallel(n_jobs=n_jobs, verbose=verbose)(job_l)))
+
+
+class RollingGroup(Group):
+    """Group the rolling dict"""
+
+    def group(self, rolling_dict: dict) -> dict:
+        """Given an rolling dict likes {(A,B,R): things}, return the grouped dict likes {(A,B): {R:things}}
+
+        NOTE: There is an assumption which is the rolling key is at the end of the key tuple, because the rolling results always need to be ensemble firstly.
+
+        Args:
+            rolling_dict (dict): an rolling dict. If the key is not a tuple, then do nothing.
+
+        Returns:
+            dict: grouped dict
+        """
+        grouped_dict = {}
+        for key, values in rolling_dict.items():
+            if isinstance(key, tuple):
+                grouped_dict.setdefault(key[:-1], {})[key[-1]] = values
+            else:
+                raise TypeError(f"Expected `tuple` type, but got a value `{key}`")
+        return grouped_dict
+
+    def __init__(self, ens=RollingEnsemble()):
+        super().__init__(ens=ens)
```

## qlib/model/interpret/base.py

 * *Ordering differences only*

```diff
@@ -1,45 +1,45 @@
-#  Copyright (c) Microsoft Corporation.
-#  Licensed under the MIT License.
-
-"""
-Interfaces to interpret models
-"""
-
-import pandas as pd
-from abc import abstractmethod
-
-
-class FeatureInt:
-    """Feature (Int)erpreter"""
-
-    @abstractmethod
-    def get_feature_importance(self) -> pd.Series:
-        """get feature importance
-
-        Returns
-        -------
-            The index is the feature name.
-
-            The greater the value, the higher importance.
-        """
-
-
-class LightGBMFInt(FeatureInt):
-    """LightGBM (F)eature (Int)erpreter"""
-
-    def __init__(self):
-        self.model = None
-
-    def get_feature_importance(self, *args, **kwargs) -> pd.Series:
-        """get feature importance
-
-        Notes
-        -----
-            parameters reference:
-            https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.Booster.html?highlight=feature_importance#lightgbm.Booster.feature_importance
-        """
-        return pd.Series(
-            self.model.feature_importance(*args, **kwargs), index=self.model.feature_name()
-        ).sort_values(  # pylint: disable=E1101
-            ascending=False
-        )
+#  Copyright (c) Microsoft Corporation.
+#  Licensed under the MIT License.
+
+"""
+Interfaces to interpret models
+"""
+
+import pandas as pd
+from abc import abstractmethod
+
+
+class FeatureInt:
+    """Feature (Int)erpreter"""
+
+    @abstractmethod
+    def get_feature_importance(self) -> pd.Series:
+        """get feature importance
+
+        Returns
+        -------
+            The index is the feature name.
+
+            The greater the value, the higher importance.
+        """
+
+
+class LightGBMFInt(FeatureInt):
+    """LightGBM (F)eature (Int)erpreter"""
+
+    def __init__(self):
+        self.model = None
+
+    def get_feature_importance(self, *args, **kwargs) -> pd.Series:
+        """get feature importance
+
+        Notes
+        -----
+            parameters reference:
+            https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.Booster.html?highlight=feature_importance#lightgbm.Booster.feature_importance
+        """
+        return pd.Series(
+            self.model.feature_importance(*args, **kwargs), index=self.model.feature_name()
+        ).sort_values(  # pylint: disable=E1101
+            ascending=False
+        )
```

## qlib/model/meta/__init__.py

 * *Ordering differences only*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-from .task import MetaTask
-from .dataset import MetaTaskDataset
-
-
-__all__ = ["MetaTask", "MetaTaskDataset"]
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+from .task import MetaTask
+from .dataset import MetaTaskDataset
+
+
+__all__ = ["MetaTask", "MetaTaskDataset"]
```

## qlib/model/meta/dataset.py

 * *Ordering differences only*

```diff
@@ -1,77 +1,77 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-import abc
-from qlib.model.meta.task import MetaTask
-from typing import Dict, Union, List, Tuple, Text
-from ...utils.serial import Serializable
-
-
-class MetaTaskDataset(Serializable, metaclass=abc.ABCMeta):
-    """
-    A dataset fetching the data in a meta-level.
-
-    A Meta Dataset is responsible for
-
-    - input tasks(e.g. Qlib tasks) and prepare meta tasks
-
-        - meta task contains more information than normal tasks (e.g. input data for meta model)
-
-    The learnt pattern could transfer to other meta dataset. The following cases should be supported
-
-    - A meta-model trained on meta-dataset A and then applied to meta-dataset B
-
-        - Some pattern are shared between meta-dataset A and B, so meta-input on meta-dataset A are used when meta model are applied on meta-dataset-B
-    """
-
-    def __init__(self, segments: Union[Dict[Text, Tuple], float], *args, **kwargs):
-        """
-        The meta-dataset maintains a list of meta-tasks when it is initialized.
-
-        The segments indicates the way to divide the data
-
-        The duty of the `__init__` function of MetaTaskDataset
-        - initialize the tasks
-        """
-        super().__init__(*args, **kwargs)
-        self.segments = segments
-
-    def prepare_tasks(self, segments: Union[List[Text], Text], *args, **kwargs) -> List[MetaTask]:
-        """
-        Prepare the data in each meta-task and ready for training.
-
-        The following code example shows how to retrieve a list of meta-tasks from the `meta_dataset`:
-
-            .. code-block:: Python
-
-                # get the train segment and the test segment, both of them are lists
-                train_meta_tasks, test_meta_tasks = meta_dataset.prepare_tasks(["train", "test"])
-
-        Parameters
-        ----------
-        segments: Union[List[Text], Tuple[Text], Text]
-            the info to select data
-
-        Returns
-        -------
-        list:
-            A list of the prepared data of each meta-task for training the meta-model. For multiple segments [seg1, seg2, ... , segN], the returned list will be [[tasks in seg1], [tasks in seg2], ... , [tasks in segN]].
-            Each task is a meta task
-        """
-        if isinstance(segments, (list, tuple)):
-            return [self._prepare_seg(seg) for seg in segments]
-        elif isinstance(segments, str):
-            return self._prepare_seg(segments)
-        else:
-            raise NotImplementedError(f"This type of input is not supported")
-
-    @abc.abstractmethod
-    def _prepare_seg(self, segment: Text):
-        """
-        prepare a single segment of data for training data
-
-        Parameters
-        ----------
-        seg : Text
-            the name of the segment
-        """
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+import abc
+from qlib.model.meta.task import MetaTask
+from typing import Dict, Union, List, Tuple, Text
+from ...utils.serial import Serializable
+
+
+class MetaTaskDataset(Serializable, metaclass=abc.ABCMeta):
+    """
+    A dataset fetching the data in a meta-level.
+
+    A Meta Dataset is responsible for
+
+    - input tasks(e.g. Qlib tasks) and prepare meta tasks
+
+        - meta task contains more information than normal tasks (e.g. input data for meta model)
+
+    The learnt pattern could transfer to other meta dataset. The following cases should be supported
+
+    - A meta-model trained on meta-dataset A and then applied to meta-dataset B
+
+        - Some pattern are shared between meta-dataset A and B, so meta-input on meta-dataset A are used when meta model are applied on meta-dataset-B
+    """
+
+    def __init__(self, segments: Union[Dict[Text, Tuple], float], *args, **kwargs):
+        """
+        The meta-dataset maintains a list of meta-tasks when it is initialized.
+
+        The segments indicates the way to divide the data
+
+        The duty of the `__init__` function of MetaTaskDataset
+        - initialize the tasks
+        """
+        super().__init__(*args, **kwargs)
+        self.segments = segments
+
+    def prepare_tasks(self, segments: Union[List[Text], Text], *args, **kwargs) -> List[MetaTask]:
+        """
+        Prepare the data in each meta-task and ready for training.
+
+        The following code example shows how to retrieve a list of meta-tasks from the `meta_dataset`:
+
+            .. code-block:: Python
+
+                # get the train segment and the test segment, both of them are lists
+                train_meta_tasks, test_meta_tasks = meta_dataset.prepare_tasks(["train", "test"])
+
+        Parameters
+        ----------
+        segments: Union[List[Text], Tuple[Text], Text]
+            the info to select data
+
+        Returns
+        -------
+        list:
+            A list of the prepared data of each meta-task for training the meta-model. For multiple segments [seg1, seg2, ... , segN], the returned list will be [[tasks in seg1], [tasks in seg2], ... , [tasks in segN]].
+            Each task is a meta task
+        """
+        if isinstance(segments, (list, tuple)):
+            return [self._prepare_seg(seg) for seg in segments]
+        elif isinstance(segments, str):
+            return self._prepare_seg(segments)
+        else:
+            raise NotImplementedError(f"This type of input is not supported")
+
+    @abc.abstractmethod
+    def _prepare_seg(self, segment: Text):
+        """
+        prepare a single segment of data for training data
+
+        Parameters
+        ----------
+        seg : Text
+            the name of the segment
+        """
```

## qlib/model/meta/model.py

 * *Ordering differences only*

```diff
@@ -1,75 +1,75 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-import abc
-from typing import List
-
-from .dataset import MetaTaskDataset
-
-
-class MetaModel(metaclass=abc.ABCMeta):
-    """
-    The meta-model guiding the model learning.
-
-    The word `Guiding` can be categorized into two types based on the stage of model learning
-    - The definition of learning tasks:  Please refer to docs of `MetaTaskModel`
-    - Controlling the learning process of models: Please refer to the docs of `MetaGuideModel`
-    """
-
-    @abc.abstractmethod
-    def fit(self, *args, **kwargs):
-        """
-        The training process of the meta-model.
-        """
-
-    @abc.abstractmethod
-    def inference(self, *args, **kwargs) -> object:
-        """
-        The inference process of the meta-model.
-
-        Returns
-        -------
-        object:
-            Some information to guide the model learning
-        """
-
-
-class MetaTaskModel(MetaModel):
-    """
-    This type of meta-model deals with base task definitions. The meta-model creates tasks for training new base forecasting models after it is trained. `prepare_tasks` directly modifies the task definitions.
-    """
-
-    def fit(self, meta_dataset: MetaTaskDataset):
-        """
-        The MetaTaskModel is expected to get prepared MetaTask from meta_dataset.
-        And then it will learn knowledge from the meta tasks
-        """
-        raise NotImplementedError(f"Please implement the `fit` method")
-
-    def inference(self, meta_dataset: MetaTaskDataset) -> List[dict]:
-        """
-        MetaTaskModel will make inference on the meta_dataset
-        The MetaTaskModel is expected to get prepared MetaTask from meta_dataset.
-        Then it will create modified task with Qlib format which can be executed by Qlib trainer.
-
-        Returns
-        -------
-        List[dict]:
-            A list of modified task definitions.
-
-        """
-        raise NotImplementedError(f"Please implement the `inference` method")
-
-
-class MetaGuideModel(MetaModel):
-    """
-    This type of meta-model aims to guide the training process of the base model. The meta-model interacts with the base forecasting models during their training process.
-    """
-
-    @abc.abstractmethod
-    def fit(self, *args, **kwargs):
-        pass
-
-    @abc.abstractmethod
-    def inference(self, *args, **kwargs):
-        pass
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+import abc
+from typing import List
+
+from .dataset import MetaTaskDataset
+
+
+class MetaModel(metaclass=abc.ABCMeta):
+    """
+    The meta-model guiding the model learning.
+
+    The word `Guiding` can be categorized into two types based on the stage of model learning
+    - The definition of learning tasks:  Please refer to docs of `MetaTaskModel`
+    - Controlling the learning process of models: Please refer to the docs of `MetaGuideModel`
+    """
+
+    @abc.abstractmethod
+    def fit(self, *args, **kwargs):
+        """
+        The training process of the meta-model.
+        """
+
+    @abc.abstractmethod
+    def inference(self, *args, **kwargs) -> object:
+        """
+        The inference process of the meta-model.
+
+        Returns
+        -------
+        object:
+            Some information to guide the model learning
+        """
+
+
+class MetaTaskModel(MetaModel):
+    """
+    This type of meta-model deals with base task definitions. The meta-model creates tasks for training new base forecasting models after it is trained. `prepare_tasks` directly modifies the task definitions.
+    """
+
+    def fit(self, meta_dataset: MetaTaskDataset):
+        """
+        The MetaTaskModel is expected to get prepared MetaTask from meta_dataset.
+        And then it will learn knowledge from the meta tasks
+        """
+        raise NotImplementedError(f"Please implement the `fit` method")
+
+    def inference(self, meta_dataset: MetaTaskDataset) -> List[dict]:
+        """
+        MetaTaskModel will make inference on the meta_dataset
+        The MetaTaskModel is expected to get prepared MetaTask from meta_dataset.
+        Then it will create modified task with Qlib format which can be executed by Qlib trainer.
+
+        Returns
+        -------
+        List[dict]:
+            A list of modified task definitions.
+
+        """
+        raise NotImplementedError(f"Please implement the `inference` method")
+
+
+class MetaGuideModel(MetaModel):
+    """
+    This type of meta-model aims to guide the training process of the base model. The meta-model interacts with the base forecasting models during their training process.
+    """
+
+    @abc.abstractmethod
+    def fit(self, *args, **kwargs):
+        pass
+
+    @abc.abstractmethod
+    def inference(self, *args, **kwargs):
+        pass
```

## qlib/model/meta/task.py

 * *Ordering differences only*

```diff
@@ -1,53 +1,53 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-from qlib.data.dataset import Dataset
-from ...utils import init_instance_by_config
-
-
-class MetaTask:
-    """
-    A single meta-task, a meta-dataset contains a list of them.
-    It serves as a component as in MetaDatasetDS
-
-    The data processing is different
-
-    - the processed input may be different between training and testing
-
-        - When training, the X, y, X_test, y_test in training tasks are necessary (# PROC_MODE_FULL #)
-          but not necessary in test tasks. (# PROC_MODE_TEST #)
-        - When the meta model can be transferred into other dataset, only meta_info is necessary  (# PROC_MODE_TRANSFER #)
-    """
-
-    PROC_MODE_FULL = "full"
-    PROC_MODE_TEST = "test"
-    PROC_MODE_TRANSFER = "transfer"
-
-    def __init__(self, task: dict, meta_info: object, mode: str = PROC_MODE_FULL):
-        """
-        The `__init__` func is responsible for
-
-        - store the task
-        - store the origin input data for
-        - process the input data for meta data
-
-        Parameters
-        ----------
-        task : dict
-            the task to be enhanced by meta model
-
-        meta_info : object
-            the input for meta model
-        """
-        self.task = task
-        self.meta_info = meta_info  # the original meta input information, it will be processed later
-        self.mode = mode
-
-    def get_dataset(self) -> Dataset:
-        return init_instance_by_config(self.task["dataset"], accept_types=Dataset)
-
-    def get_meta_input(self) -> object:
-        """
-        Return the **processed** meta_info
-        """
-        return self.meta_info
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+from qlib.data.dataset import Dataset
+from ...utils import init_instance_by_config
+
+
+class MetaTask:
+    """
+    A single meta-task, a meta-dataset contains a list of them.
+    It serves as a component as in MetaDatasetDS
+
+    The data processing is different
+
+    - the processed input may be different between training and testing
+
+        - When training, the X, y, X_test, y_test in training tasks are necessary (# PROC_MODE_FULL #)
+          but not necessary in test tasks. (# PROC_MODE_TEST #)
+        - When the meta model can be transferred into other dataset, only meta_info is necessary  (# PROC_MODE_TRANSFER #)
+    """
+
+    PROC_MODE_FULL = "full"
+    PROC_MODE_TEST = "test"
+    PROC_MODE_TRANSFER = "transfer"
+
+    def __init__(self, task: dict, meta_info: object, mode: str = PROC_MODE_FULL):
+        """
+        The `__init__` func is responsible for
+
+        - store the task
+        - store the origin input data for
+        - process the input data for meta data
+
+        Parameters
+        ----------
+        task : dict
+            the task to be enhanced by meta model
+
+        meta_info : object
+            the input for meta model
+        """
+        self.task = task
+        self.meta_info = meta_info  # the original meta input information, it will be processed later
+        self.mode = mode
+
+    def get_dataset(self) -> Dataset:
+        return init_instance_by_config(self.task["dataset"], accept_types=Dataset)
+
+    def get_meta_input(self) -> object:
+        """
+        Return the **processed** meta_info
+        """
+        return self.meta_info
```

## qlib/model/riskmodel/__init__.py

 * *Ordering differences only*

```diff
@@ -1,15 +1,15 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-from .base import RiskModel
-from .poet import POETCovEstimator
-from .shrink import ShrinkCovEstimator
-from .structured import StructuredCovEstimator
-
-
-__all__ = [
-    "RiskModel",
-    "POETCovEstimator",
-    "ShrinkCovEstimator",
-    "StructuredCovEstimator",
-]
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+from .base import RiskModel
+from .poet import POETCovEstimator
+from .shrink import ShrinkCovEstimator
+from .structured import StructuredCovEstimator
+
+
+__all__ = [
+    "RiskModel",
+    "POETCovEstimator",
+    "ShrinkCovEstimator",
+    "StructuredCovEstimator",
+]
```

## qlib/model/riskmodel/base.py

 * *Ordering differences only*

```diff
@@ -1,147 +1,147 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-import inspect
-import numpy as np
-import pandas as pd
-from typing import Union
-
-from qlib.model.base import BaseModel
-
-
-class RiskModel(BaseModel):
-    """Risk Model
-
-    A risk model is used to estimate the covariance matrix of stock returns.
-    """
-
-    MASK_NAN = "mask"
-    FILL_NAN = "fill"
-    IGNORE_NAN = "ignore"
-
-    def __init__(self, nan_option: str = "ignore", assume_centered: bool = False, scale_return: bool = True):
-        """
-        Args:
-            nan_option (str): nan handling option (`ignore`/`mask`/`fill`).
-            assume_centered (bool): whether the data is assumed to be centered.
-            scale_return (bool): whether scale returns as percentage.
-        """
-        # nan
-        assert nan_option in [
-            self.MASK_NAN,
-            self.FILL_NAN,
-            self.IGNORE_NAN,
-        ], f"`nan_option={nan_option}` is not supported"
-        self.nan_option = nan_option
-
-        self.assume_centered = assume_centered
-        self.scale_return = scale_return
-
-    def predict(
-        self,
-        X: Union[pd.Series, pd.DataFrame, np.ndarray],
-        return_corr: bool = False,
-        is_price: bool = True,
-        return_decomposed_components=False,
-    ) -> Union[pd.DataFrame, np.ndarray, tuple]:
-        """
-        Args:
-            X (pd.Series, pd.DataFrame or np.ndarray): data from which to estimate the covariance,
-                with variables as columns and observations as rows.
-            return_corr (bool): whether return the correlation matrix.
-            is_price (bool): whether `X` contains price (if not assume stock returns).
-            return_decomposed_components (bool): whether return decomposed components of the covariance matrix.
-
-        Returns:
-            pd.DataFrame or np.ndarray: estimated covariance (or correlation).
-        """
-        assert (
-            not return_corr or not return_decomposed_components
-        ), "Can only return either correlation matrix or decomposed components."
-
-        # transform input into 2D array
-        if not isinstance(X, (pd.Series, pd.DataFrame)):
-            columns = None
-        else:
-            if isinstance(X.index, pd.MultiIndex):
-                if isinstance(X, pd.DataFrame):
-                    X = X.iloc[:, 0].unstack(level="instrument")  # always use the first column
-                else:
-                    X = X.unstack(level="instrument")
-            else:
-                # X is 2D DataFrame
-                pass
-            columns = X.columns  # will be used to restore dataframe
-            X = X.values
-
-        # calculate pct_change
-        if is_price:
-            X = X[1:] / X[:-1] - 1  # NOTE: resulting `n - 1` rows
-
-        # scale return
-        if self.scale_return:
-            X *= 100
-
-        # handle nan and centered
-        X = self._preprocess(X)
-
-        # return decomposed components if needed
-        if return_decomposed_components:
-            assert (
-                "return_decomposed_components" in inspect.getfullargspec(self._predict).args
-            ), "This risk model does not support return decomposed components of the covariance matrix "
-
-            F, cov_b, var_u = self._predict(X, return_decomposed_components=True)  # pylint: disable=E1123
-            return F, cov_b, var_u
-
-        # estimate covariance
-        S = self._predict(X)
-
-        # return correlation if needed
-        if return_corr:
-            vola = np.sqrt(np.diag(S))
-            corr = S / np.outer(vola, vola)
-            if columns is None:
-                return corr
-            return pd.DataFrame(corr, index=columns, columns=columns)
-
-        # return covariance
-        if columns is None:
-            return S
-        return pd.DataFrame(S, index=columns, columns=columns)
-
-    def _predict(self, X: np.ndarray) -> np.ndarray:
-        """covariance estimation implementation
-
-        This method should be overridden by child classes.
-
-        By default, this method implements the empirical covariance estimation.
-
-        Args:
-            X (np.ndarray): data matrix containing multiple variables (columns) and observations (rows).
-
-        Returns:
-            np.ndarray: covariance matrix.
-        """
-        xTx = np.asarray(X.T.dot(X))
-        N = len(X)
-        if isinstance(X, np.ma.MaskedArray):
-            M = 1 - X.mask
-            N = M.T.dot(M)  # each pair has distinct number of samples
-        return xTx / N
-
-    def _preprocess(self, X: np.ndarray) -> Union[np.ndarray, np.ma.MaskedArray]:
-        """handle nan and centerize data
-
-        Note:
-            if `nan_option='mask'` then the returned array will be `np.ma.MaskedArray`.
-        """
-        # handle nan
-        if self.nan_option == self.FILL_NAN:
-            X = np.nan_to_num(X)
-        elif self.nan_option == self.MASK_NAN:
-            X = np.ma.masked_invalid(X)
-        # centralize
-        if not self.assume_centered:
-            X = X - np.nanmean(X, axis=0)
-        return X
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+import inspect
+import numpy as np
+import pandas as pd
+from typing import Union
+
+from qlib.model.base import BaseModel
+
+
+class RiskModel(BaseModel):
+    """Risk Model
+
+    A risk model is used to estimate the covariance matrix of stock returns.
+    """
+
+    MASK_NAN = "mask"
+    FILL_NAN = "fill"
+    IGNORE_NAN = "ignore"
+
+    def __init__(self, nan_option: str = "ignore", assume_centered: bool = False, scale_return: bool = True):
+        """
+        Args:
+            nan_option (str): nan handling option (`ignore`/`mask`/`fill`).
+            assume_centered (bool): whether the data is assumed to be centered.
+            scale_return (bool): whether scale returns as percentage.
+        """
+        # nan
+        assert nan_option in [
+            self.MASK_NAN,
+            self.FILL_NAN,
+            self.IGNORE_NAN,
+        ], f"`nan_option={nan_option}` is not supported"
+        self.nan_option = nan_option
+
+        self.assume_centered = assume_centered
+        self.scale_return = scale_return
+
+    def predict(
+        self,
+        X: Union[pd.Series, pd.DataFrame, np.ndarray],
+        return_corr: bool = False,
+        is_price: bool = True,
+        return_decomposed_components=False,
+    ) -> Union[pd.DataFrame, np.ndarray, tuple]:
+        """
+        Args:
+            X (pd.Series, pd.DataFrame or np.ndarray): data from which to estimate the covariance,
+                with variables as columns and observations as rows.
+            return_corr (bool): whether return the correlation matrix.
+            is_price (bool): whether `X` contains price (if not assume stock returns).
+            return_decomposed_components (bool): whether return decomposed components of the covariance matrix.
+
+        Returns:
+            pd.DataFrame or np.ndarray: estimated covariance (or correlation).
+        """
+        assert (
+            not return_corr or not return_decomposed_components
+        ), "Can only return either correlation matrix or decomposed components."
+
+        # transform input into 2D array
+        if not isinstance(X, (pd.Series, pd.DataFrame)):
+            columns = None
+        else:
+            if isinstance(X.index, pd.MultiIndex):
+                if isinstance(X, pd.DataFrame):
+                    X = X.iloc[:, 0].unstack(level="instrument")  # always use the first column
+                else:
+                    X = X.unstack(level="instrument")
+            else:
+                # X is 2D DataFrame
+                pass
+            columns = X.columns  # will be used to restore dataframe
+            X = X.values
+
+        # calculate pct_change
+        if is_price:
+            X = X[1:] / X[:-1] - 1  # NOTE: resulting `n - 1` rows
+
+        # scale return
+        if self.scale_return:
+            X *= 100
+
+        # handle nan and centered
+        X = self._preprocess(X)
+
+        # return decomposed components if needed
+        if return_decomposed_components:
+            assert (
+                "return_decomposed_components" in inspect.getfullargspec(self._predict).args
+            ), "This risk model does not support return decomposed components of the covariance matrix "
+
+            F, cov_b, var_u = self._predict(X, return_decomposed_components=True)  # pylint: disable=E1123
+            return F, cov_b, var_u
+
+        # estimate covariance
+        S = self._predict(X)
+
+        # return correlation if needed
+        if return_corr:
+            vola = np.sqrt(np.diag(S))
+            corr = S / np.outer(vola, vola)
+            if columns is None:
+                return corr
+            return pd.DataFrame(corr, index=columns, columns=columns)
+
+        # return covariance
+        if columns is None:
+            return S
+        return pd.DataFrame(S, index=columns, columns=columns)
+
+    def _predict(self, X: np.ndarray) -> np.ndarray:
+        """covariance estimation implementation
+
+        This method should be overridden by child classes.
+
+        By default, this method implements the empirical covariance estimation.
+
+        Args:
+            X (np.ndarray): data matrix containing multiple variables (columns) and observations (rows).
+
+        Returns:
+            np.ndarray: covariance matrix.
+        """
+        xTx = np.asarray(X.T.dot(X))
+        N = len(X)
+        if isinstance(X, np.ma.MaskedArray):
+            M = 1 - X.mask
+            N = M.T.dot(M)  # each pair has distinct number of samples
+        return xTx / N
+
+    def _preprocess(self, X: np.ndarray) -> Union[np.ndarray, np.ma.MaskedArray]:
+        """handle nan and centerize data
+
+        Note:
+            if `nan_option='mask'` then the returned array will be `np.ma.MaskedArray`.
+        """
+        # handle nan
+        if self.nan_option == self.FILL_NAN:
+            X = np.nan_to_num(X)
+        elif self.nan_option == self.MASK_NAN:
+            X = np.ma.masked_invalid(X)
+        # centralize
+        if not self.assume_centered:
+            X = X - np.nanmean(X, axis=0)
+        return X
```

## qlib/model/riskmodel/poet.py

```diff
@@ -1,84 +1,83 @@
-import numpy as np
-
-from qlib.model.riskmodel import RiskModel
-
-
-class POETCovEstimator(RiskModel):
-    """Principal Orthogonal Complement Thresholding Estimator (POET)
-
-    Reference:
-        [1] Fan, J., Liao, Y., & Mincheva, M. (2013). Large covariance estimation by thresholding principal orthogonal complements.
-            Journal of the Royal Statistical Society. Series B: Statistical Methodology, 75(4), 603–680. https://doi.org/10.1111/rssb.12016
-        [2] http://econweb.rutgers.edu/yl1114/papers/poet/POET.m
-    """
-
-    THRESH_SOFT = "soft"
-    THRESH_HARD = "hard"
-    THRESH_SCAD = "scad"
-
-    def __init__(self, num_factors: int = 0, thresh: float = 1.0, thresh_method: str = "soft", **kwargs):
-        """
-        Args:
-            num_factors (int): number of factors (if set to zero, no factor model will be used).
-            thresh (float): the positive constant for thresholding.
-            thresh_method (str): thresholding method, which can be
-                - 'soft': soft thresholding.
-                - 'hard': hard thresholding.
-                - 'scad': scad thresholding.
-            kwargs: see `RiskModel` for more information.
-        """
-        super().__init__(**kwargs)
-
-        assert num_factors >= 0, "`num_factors` requires a positive integer"
-        self.num_factors = num_factors
-
-        assert thresh >= 0, "`thresh` requires a positive float number"
-        self.thresh = thresh
-
-        assert thresh_method in [
-            self.THRESH_HARD,
-            self.THRESH_SOFT,
-            self.THRESH_SCAD,
-        ], "`thresh_method` should be `soft`/`hard`/`scad`"
-        self.thresh_method = thresh_method
-
-    def _predict(self, X: np.ndarray) -> np.ndarray:
-
-        Y = X.T  # NOTE: to match POET's implementation
-        p, n = Y.shape
-
-        if self.num_factors > 0:
-            Dd, V = np.linalg.eig(Y.T.dot(Y))
-            V = V[:, np.argsort(Dd)]
-            F = V[:, -self.num_factors :][:, ::-1] * np.sqrt(n)
-            LamPCA = Y.dot(F) / n
-            uhat = np.asarray(Y - LamPCA.dot(F.T))
-            Lowrank = np.asarray(LamPCA.dot(LamPCA.T))
-            rate = 1 / np.sqrt(p) + np.sqrt(np.log(p) / n)
-        else:
-            uhat = np.asarray(Y)
-            rate = np.sqrt(np.log(p) / n)
-            Lowrank = 0
-
-        lamb = rate * self.thresh
-        SuPCA = uhat.dot(uhat.T) / n
-        SuDiag = np.diag(np.diag(SuPCA))
-        R = np.linalg.inv(SuDiag**0.5).dot(SuPCA).dot(np.linalg.inv(SuDiag**0.5))
-
-        if self.thresh_method == self.THRESH_HARD:
-            M = R * (np.abs(R) > lamb)
-        elif self.thresh_method == self.THRESH_SOFT:
-            res = np.abs(R) - lamb
-            res = (res + np.abs(res)) / 2
-            M = np.sign(R) * res
-        else:
-            M1 = (np.abs(R) < 2 * lamb) * np.sign(R) * (np.abs(R) - lamb) * (np.abs(R) > lamb)
-            M2 = (np.abs(R) < 3.7 * lamb) * (np.abs(R) >= 2 * lamb) * (2.7 * R - 3.7 * np.sign(R) * lamb) / 1.7
-            M3 = (np.abs(R) >= 3.7 * lamb) * R
-            M = M1 + M2 + M3
-
-        Rthresh = M - np.diag(np.diag(M)) + np.eye(p)
-        SigmaU = (SuDiag**0.5).dot(Rthresh).dot(SuDiag**0.5)
-        SigmaY = SigmaU + Lowrank
-
-        return SigmaY
+import numpy as np
+
+from qlib.model.riskmodel import RiskModel
+
+
+class POETCovEstimator(RiskModel):
+    """Principal Orthogonal Complement Thresholding Estimator (POET)
+
+    Reference:
+        [1] Fan, J., Liao, Y., & Mincheva, M. (2013). Large covariance estimation by thresholding principal orthogonal complements.
+            Journal of the Royal Statistical Society. Series B: Statistical Methodology, 75(4), 603–680. https://doi.org/10.1111/rssb.12016
+        [2] http://econweb.rutgers.edu/yl1114/papers/poet/POET.m
+    """
+
+    THRESH_SOFT = "soft"
+    THRESH_HARD = "hard"
+    THRESH_SCAD = "scad"
+
+    def __init__(self, num_factors: int = 0, thresh: float = 1.0, thresh_method: str = "soft", **kwargs):
+        """
+        Args:
+            num_factors (int): number of factors (if set to zero, no factor model will be used).
+            thresh (float): the positive constant for thresholding.
+            thresh_method (str): thresholding method, which can be
+                - 'soft': soft thresholding.
+                - 'hard': hard thresholding.
+                - 'scad': scad thresholding.
+            kwargs: see `RiskModel` for more information.
+        """
+        super().__init__(**kwargs)
+
+        assert num_factors >= 0, "`num_factors` requires a positive integer"
+        self.num_factors = num_factors
+
+        assert thresh >= 0, "`thresh` requires a positive float number"
+        self.thresh = thresh
+
+        assert thresh_method in [
+            self.THRESH_HARD,
+            self.THRESH_SOFT,
+            self.THRESH_SCAD,
+        ], "`thresh_method` should be `soft`/`hard`/`scad`"
+        self.thresh_method = thresh_method
+
+    def _predict(self, X: np.ndarray) -> np.ndarray:
+        Y = X.T  # NOTE: to match POET's implementation
+        p, n = Y.shape
+
+        if self.num_factors > 0:
+            Dd, V = np.linalg.eig(Y.T.dot(Y))
+            V = V[:, np.argsort(Dd)]
+            F = V[:, -self.num_factors :][:, ::-1] * np.sqrt(n)
+            LamPCA = Y.dot(F) / n
+            uhat = np.asarray(Y - LamPCA.dot(F.T))
+            Lowrank = np.asarray(LamPCA.dot(LamPCA.T))
+            rate = 1 / np.sqrt(p) + np.sqrt(np.log(p) / n)
+        else:
+            uhat = np.asarray(Y)
+            rate = np.sqrt(np.log(p) / n)
+            Lowrank = 0
+
+        lamb = rate * self.thresh
+        SuPCA = uhat.dot(uhat.T) / n
+        SuDiag = np.diag(np.diag(SuPCA))
+        R = np.linalg.inv(SuDiag**0.5).dot(SuPCA).dot(np.linalg.inv(SuDiag**0.5))
+
+        if self.thresh_method == self.THRESH_HARD:
+            M = R * (np.abs(R) > lamb)
+        elif self.thresh_method == self.THRESH_SOFT:
+            res = np.abs(R) - lamb
+            res = (res + np.abs(res)) / 2
+            M = np.sign(R) * res
+        else:
+            M1 = (np.abs(R) < 2 * lamb) * np.sign(R) * (np.abs(R) - lamb) * (np.abs(R) > lamb)
+            M2 = (np.abs(R) < 3.7 * lamb) * (np.abs(R) >= 2 * lamb) * (2.7 * R - 3.7 * np.sign(R) * lamb) / 1.7
+            M3 = (np.abs(R) >= 3.7 * lamb) * R
+            M = M1 + M2 + M3
+
+        Rthresh = M - np.diag(np.diag(M)) + np.eye(p)
+        SigmaU = (SuDiag**0.5).dot(Rthresh).dot(SuDiag**0.5)
+        SigmaY = SigmaU + Lowrank
+
+        return SigmaY
```

## qlib/model/riskmodel/shrink.py

 * *Ordering differences only*

```diff
@@ -1,261 +1,261 @@
-import numpy as np
-from typing import Union
-
-from qlib.model.riskmodel import RiskModel
-
-
-class ShrinkCovEstimator(RiskModel):
-    """Shrinkage Covariance Estimator
-
-    This estimator will shrink the sample covariance matrix towards
-    an identify matrix:
-        S_hat = (1 - alpha) * S + alpha * F
-    where `alpha` is the shrink parameter and `F` is the shrinking target.
-
-    The following shrinking parameters (`alpha`) are supported:
-        - `lw` [1][2][3]: use Ledoit-Wolf shrinking parameter.
-        - `oas` [4]: use Oracle Approximating Shrinkage shrinking parameter.
-        - float: directly specify the shrink parameter, should be between [0, 1].
-
-    The following shrinking targets (`F`) are supported:
-        - `const_var` [1][4][5]: assume stocks have the same constant variance and zero correlation.
-        - `const_corr` [2][6]: assume stocks have different variance but equal correlation.
-        - `single_factor` [3][7]: assume single factor model as the shrinking target.
-        - np.ndarray: provide the shrinking targets directly.
-
-    Note:
-        - The optimal shrinking parameter depends on the selection of the shrinking target.
-            Currently, `oas` is not supported for `const_corr` and `single_factor`.
-        - Remember to set `nan_option` to `fill` or `mask` if your data has missing values.
-
-    References:
-        [1] Ledoit, O., & Wolf, M. (2004). A well-conditioned estimator for large-dimensional covariance matrices.
-            Journal of Multivariate Analysis, 88(2), 365–411. https://doi.org/10.1016/S0047-259X(03)00096-4
-        [2] Ledoit, O., & Wolf, M. (2004). Honey, I shrunk the sample covariance matrix.
-            Journal of Portfolio Management, 30(4), 1–22. https://doi.org/10.3905/jpm.2004.110
-        [3] Ledoit, O., & Wolf, M. (2003). Improved estimation of the covariance matrix of stock returns
-            with an application to portfolio selection.
-            Journal of Empirical Finance, 10(5), 603–621. https://doi.org/10.1016/S0927-5398(03)00007-0
-        [4] Chen, Y., Wiesel, A., Eldar, Y. C., & Hero, A. O. (2010). Shrinkage algorithms for MMSE covariance
-            estimation. IEEE Transactions on Signal Processing, 58(10), 5016–5029.
-            https://doi.org/10.1109/TSP.2010.2053029
-        [5] https://www.econ.uzh.ch/dam/jcr:ffffffff-935a-b0d6-0000-00007f64e5b9/cov1para.m.zip
-        [6] https://www.econ.uzh.ch/dam/jcr:ffffffff-935a-b0d6-ffff-ffffde5e2d4e/covCor.m.zip
-        [7] https://www.econ.uzh.ch/dam/jcr:ffffffff-935a-b0d6-0000-0000648dfc98/covMarket.m.zip
-    """
-
-    SHR_LW = "lw"
-    SHR_OAS = "oas"
-
-    TGT_CONST_VAR = "const_var"
-    TGT_CONST_CORR = "const_corr"
-    TGT_SINGLE_FACTOR = "single_factor"
-
-    def __init__(self, alpha: Union[str, float] = 0.0, target: Union[str, np.ndarray] = "const_var", **kwargs):
-        """
-        Args:
-            alpha (str or float): shrinking parameter or estimator (`lw`/`oas`)
-            target (str or np.ndarray): shrinking target (`const_var`/`const_corr`/`single_factor`)
-            kwargs: see `RiskModel` for more information
-        """
-        super().__init__(**kwargs)
-
-        # alpha
-        if isinstance(alpha, str):
-            assert alpha in [self.SHR_LW, self.SHR_OAS], f"shrinking method `{alpha}` is not supported"
-        elif isinstance(alpha, (float, np.floating)):
-            assert 0 <= alpha <= 1, "alpha should be between [0, 1]"
-        else:
-            raise TypeError("invalid argument type for `alpha`")
-        self.alpha = alpha
-
-        # target
-        if isinstance(target, str):
-            assert target in [
-                self.TGT_CONST_VAR,
-                self.TGT_CONST_CORR,
-                self.TGT_SINGLE_FACTOR,
-            ], f"shrinking target `{target} is not supported"
-        elif isinstance(target, np.ndarray):
-            pass
-        else:
-            raise TypeError("invalid argument type for `target`")
-        if alpha == self.SHR_OAS and target != self.TGT_CONST_VAR:
-            raise NotImplementedError("currently `oas` can only support `const_var` as target")
-        self.target = target
-
-    def _predict(self, X: np.ndarray) -> np.ndarray:
-        # sample covariance
-        S = super()._predict(X)
-
-        # shrinking target
-        F = self._get_shrink_target(X, S)
-
-        # get shrinking parameter
-        alpha = self._get_shrink_param(X, S, F)
-
-        # shrink covariance
-        if alpha > 0:
-            S *= 1 - alpha
-            F *= alpha
-            S += F
-
-        return S
-
-    def _get_shrink_target(self, X: np.ndarray, S: np.ndarray) -> np.ndarray:
-        """get shrinking target `F`"""
-        if self.target == self.TGT_CONST_VAR:
-            return self._get_shrink_target_const_var(X, S)
-        if self.target == self.TGT_CONST_CORR:
-            return self._get_shrink_target_const_corr(X, S)
-        if self.target == self.TGT_SINGLE_FACTOR:
-            return self._get_shrink_target_single_factor(X, S)
-        return self.target
-
-    def _get_shrink_target_const_var(self, X: np.ndarray, S: np.ndarray) -> np.ndarray:
-        """get shrinking target with constant variance
-
-        This target assumes zero pair-wise correlation and constant variance.
-        The constant variance is estimated by averaging all sample's variances.
-        """
-        n = len(S)
-        F = np.eye(n)
-        np.fill_diagonal(F, np.mean(np.diag(S)))
-        return F
-
-    def _get_shrink_target_const_corr(self, X: np.ndarray, S: np.ndarray) -> np.ndarray:
-        """get shrinking target with constant correlation
-
-        This target assumes constant pair-wise correlation but keep the sample variance.
-        The constant correlation is estimated by averaging all pairwise correlations.
-        """
-        n = len(S)
-        var = np.diag(S)
-        sqrt_var = np.sqrt(var)
-        covar = np.outer(sqrt_var, sqrt_var)
-        r_bar = (np.sum(S / covar) - n) / (n * (n - 1))
-        F = r_bar * covar
-        np.fill_diagonal(F, var)
-        return F
-
-    def _get_shrink_target_single_factor(self, X: np.ndarray, S: np.ndarray) -> np.ndarray:
-        """get shrinking target with single factor model"""
-        X_mkt = np.nanmean(X, axis=1)
-        cov_mkt = np.asarray(X.T.dot(X_mkt) / len(X))
-        var_mkt = np.asarray(X_mkt.dot(X_mkt) / len(X))
-        F = np.outer(cov_mkt, cov_mkt) / var_mkt
-        np.fill_diagonal(F, np.diag(S))
-        return F
-
-    def _get_shrink_param(self, X: np.ndarray, S: np.ndarray, F: np.ndarray) -> float:
-        """get shrinking parameter `alpha`
-
-        Note:
-            The Ledoit-Wolf shrinking parameter estimator consists of three different methods.
-        """
-        if self.alpha == self.SHR_OAS:
-            return self._get_shrink_param_oas(X, S, F)
-        elif self.alpha == self.SHR_LW:
-            if self.target == self.TGT_CONST_VAR:
-                return self._get_shrink_param_lw_const_var(X, S, F)
-            if self.target == self.TGT_CONST_CORR:
-                return self._get_shrink_param_lw_const_corr(X, S, F)
-            if self.target == self.TGT_SINGLE_FACTOR:
-                return self._get_shrink_param_lw_single_factor(X, S, F)
-        return self.alpha
-
-    def _get_shrink_param_oas(self, X: np.ndarray, S: np.ndarray, F: np.ndarray) -> float:
-        """Oracle Approximating Shrinkage Estimator
-
-        This method uses the following formula to estimate the `alpha`
-        parameter for the shrink covariance estimator:
-            A = (1 - 2 / p) * trace(S^2) + trace^2(S)
-            B = (n + 1 - 2 / p) * (trace(S^2) - trace^2(S) / p)
-            alpha = A / B
-        where `n`, `p` are the dim of observations and variables respectively.
-        """
-        trS2 = np.sum(S**2)
-        tr2S = np.trace(S) ** 2
-
-        n, p = X.shape
-
-        A = (1 - 2 / p) * (trS2 + tr2S)
-        B = (n + 1 - 2 / p) * (trS2 + tr2S / p)
-        alpha = A / B
-
-        return alpha
-
-    def _get_shrink_param_lw_const_var(self, X: np.ndarray, S: np.ndarray, F: np.ndarray) -> float:
-        """Ledoit-Wolf Shrinkage Estimator (Constant Variance)
-
-        This method shrinks the covariance matrix towards the constand variance target.
-        """
-        t, n = X.shape
-
-        y = X**2
-        phi = np.sum(y.T.dot(y) / t - S**2)
-
-        gamma = np.linalg.norm(S - F, "fro") ** 2
-
-        kappa = phi / gamma
-        alpha = max(0, min(1, kappa / t))
-
-        return alpha
-
-    def _get_shrink_param_lw_const_corr(self, X: np.ndarray, S: np.ndarray, F: np.ndarray) -> float:
-        """Ledoit-Wolf Shrinkage Estimator (Constant Correlation)
-
-        This method shrinks the covariance matrix towards the constand correlation target.
-        """
-        t, n = X.shape
-
-        var = np.diag(S)
-        sqrt_var = np.sqrt(var)
-        r_bar = (np.sum(S / np.outer(sqrt_var, sqrt_var)) - n) / (n * (n - 1))
-
-        y = X**2
-        phi_mat = y.T.dot(y) / t - S**2
-        phi = np.sum(phi_mat)
-
-        theta_mat = (X**3).T.dot(X) / t - var[:, None] * S
-        np.fill_diagonal(theta_mat, 0)
-        rho = np.sum(np.diag(phi_mat)) + r_bar * np.sum(np.outer(1 / sqrt_var, sqrt_var) * theta_mat)
-
-        gamma = np.linalg.norm(S - F, "fro") ** 2
-
-        kappa = (phi - rho) / gamma
-        alpha = max(0, min(1, kappa / t))
-
-        return alpha
-
-    def _get_shrink_param_lw_single_factor(self, X: np.ndarray, S: np.ndarray, F: np.ndarray) -> float:
-        """Ledoit-Wolf Shrinkage Estimator (Single Factor Model)
-
-        This method shrinks the covariance matrix towards the single factor model target.
-        """
-        t, n = X.shape
-
-        X_mkt = np.nanmean(X, axis=1)
-        cov_mkt = np.asarray(X.T.dot(X_mkt) / len(X))
-        var_mkt = np.asarray(X_mkt.dot(X_mkt) / len(X))
-
-        y = X**2
-        phi = np.sum(y.T.dot(y)) / t - np.sum(S**2)
-
-        rdiag = np.sum(y**2) / t - np.sum(np.diag(S) ** 2)
-        z = X * X_mkt[:, None]
-        v1 = y.T.dot(z) / t - cov_mkt[:, None] * S
-        roff1 = np.sum(v1 * cov_mkt[:, None].T) / var_mkt - np.sum(np.diag(v1) * cov_mkt) / var_mkt
-        v3 = z.T.dot(z) / t - var_mkt * S
-        roff3 = (
-            np.sum(v3 * np.outer(cov_mkt, cov_mkt)) / var_mkt**2 - np.sum(np.diag(v3) * cov_mkt**2) / var_mkt**2
-        )
-        roff = 2 * roff1 - roff3
-        rho = rdiag + roff
-
-        gamma = np.linalg.norm(S - F, "fro") ** 2
-
-        kappa = (phi - rho) / gamma
-        alpha = max(0, min(1, kappa / t))
-
-        return alpha
+import numpy as np
+from typing import Union
+
+from qlib.model.riskmodel import RiskModel
+
+
+class ShrinkCovEstimator(RiskModel):
+    """Shrinkage Covariance Estimator
+
+    This estimator will shrink the sample covariance matrix towards
+    an identify matrix:
+        S_hat = (1 - alpha) * S + alpha * F
+    where `alpha` is the shrink parameter and `F` is the shrinking target.
+
+    The following shrinking parameters (`alpha`) are supported:
+        - `lw` [1][2][3]: use Ledoit-Wolf shrinking parameter.
+        - `oas` [4]: use Oracle Approximating Shrinkage shrinking parameter.
+        - float: directly specify the shrink parameter, should be between [0, 1].
+
+    The following shrinking targets (`F`) are supported:
+        - `const_var` [1][4][5]: assume stocks have the same constant variance and zero correlation.
+        - `const_corr` [2][6]: assume stocks have different variance but equal correlation.
+        - `single_factor` [3][7]: assume single factor model as the shrinking target.
+        - np.ndarray: provide the shrinking targets directly.
+
+    Note:
+        - The optimal shrinking parameter depends on the selection of the shrinking target.
+            Currently, `oas` is not supported for `const_corr` and `single_factor`.
+        - Remember to set `nan_option` to `fill` or `mask` if your data has missing values.
+
+    References:
+        [1] Ledoit, O., & Wolf, M. (2004). A well-conditioned estimator for large-dimensional covariance matrices.
+            Journal of Multivariate Analysis, 88(2), 365–411. https://doi.org/10.1016/S0047-259X(03)00096-4
+        [2] Ledoit, O., & Wolf, M. (2004). Honey, I shrunk the sample covariance matrix.
+            Journal of Portfolio Management, 30(4), 1–22. https://doi.org/10.3905/jpm.2004.110
+        [3] Ledoit, O., & Wolf, M. (2003). Improved estimation of the covariance matrix of stock returns
+            with an application to portfolio selection.
+            Journal of Empirical Finance, 10(5), 603–621. https://doi.org/10.1016/S0927-5398(03)00007-0
+        [4] Chen, Y., Wiesel, A., Eldar, Y. C., & Hero, A. O. (2010). Shrinkage algorithms for MMSE covariance
+            estimation. IEEE Transactions on Signal Processing, 58(10), 5016–5029.
+            https://doi.org/10.1109/TSP.2010.2053029
+        [5] https://www.econ.uzh.ch/dam/jcr:ffffffff-935a-b0d6-0000-00007f64e5b9/cov1para.m.zip
+        [6] https://www.econ.uzh.ch/dam/jcr:ffffffff-935a-b0d6-ffff-ffffde5e2d4e/covCor.m.zip
+        [7] https://www.econ.uzh.ch/dam/jcr:ffffffff-935a-b0d6-0000-0000648dfc98/covMarket.m.zip
+    """
+
+    SHR_LW = "lw"
+    SHR_OAS = "oas"
+
+    TGT_CONST_VAR = "const_var"
+    TGT_CONST_CORR = "const_corr"
+    TGT_SINGLE_FACTOR = "single_factor"
+
+    def __init__(self, alpha: Union[str, float] = 0.0, target: Union[str, np.ndarray] = "const_var", **kwargs):
+        """
+        Args:
+            alpha (str or float): shrinking parameter or estimator (`lw`/`oas`)
+            target (str or np.ndarray): shrinking target (`const_var`/`const_corr`/`single_factor`)
+            kwargs: see `RiskModel` for more information
+        """
+        super().__init__(**kwargs)
+
+        # alpha
+        if isinstance(alpha, str):
+            assert alpha in [self.SHR_LW, self.SHR_OAS], f"shrinking method `{alpha}` is not supported"
+        elif isinstance(alpha, (float, np.floating)):
+            assert 0 <= alpha <= 1, "alpha should be between [0, 1]"
+        else:
+            raise TypeError("invalid argument type for `alpha`")
+        self.alpha = alpha
+
+        # target
+        if isinstance(target, str):
+            assert target in [
+                self.TGT_CONST_VAR,
+                self.TGT_CONST_CORR,
+                self.TGT_SINGLE_FACTOR,
+            ], f"shrinking target `{target} is not supported"
+        elif isinstance(target, np.ndarray):
+            pass
+        else:
+            raise TypeError("invalid argument type for `target`")
+        if alpha == self.SHR_OAS and target != self.TGT_CONST_VAR:
+            raise NotImplementedError("currently `oas` can only support `const_var` as target")
+        self.target = target
+
+    def _predict(self, X: np.ndarray) -> np.ndarray:
+        # sample covariance
+        S = super()._predict(X)
+
+        # shrinking target
+        F = self._get_shrink_target(X, S)
+
+        # get shrinking parameter
+        alpha = self._get_shrink_param(X, S, F)
+
+        # shrink covariance
+        if alpha > 0:
+            S *= 1 - alpha
+            F *= alpha
+            S += F
+
+        return S
+
+    def _get_shrink_target(self, X: np.ndarray, S: np.ndarray) -> np.ndarray:
+        """get shrinking target `F`"""
+        if self.target == self.TGT_CONST_VAR:
+            return self._get_shrink_target_const_var(X, S)
+        if self.target == self.TGT_CONST_CORR:
+            return self._get_shrink_target_const_corr(X, S)
+        if self.target == self.TGT_SINGLE_FACTOR:
+            return self._get_shrink_target_single_factor(X, S)
+        return self.target
+
+    def _get_shrink_target_const_var(self, X: np.ndarray, S: np.ndarray) -> np.ndarray:
+        """get shrinking target with constant variance
+
+        This target assumes zero pair-wise correlation and constant variance.
+        The constant variance is estimated by averaging all sample's variances.
+        """
+        n = len(S)
+        F = np.eye(n)
+        np.fill_diagonal(F, np.mean(np.diag(S)))
+        return F
+
+    def _get_shrink_target_const_corr(self, X: np.ndarray, S: np.ndarray) -> np.ndarray:
+        """get shrinking target with constant correlation
+
+        This target assumes constant pair-wise correlation but keep the sample variance.
+        The constant correlation is estimated by averaging all pairwise correlations.
+        """
+        n = len(S)
+        var = np.diag(S)
+        sqrt_var = np.sqrt(var)
+        covar = np.outer(sqrt_var, sqrt_var)
+        r_bar = (np.sum(S / covar) - n) / (n * (n - 1))
+        F = r_bar * covar
+        np.fill_diagonal(F, var)
+        return F
+
+    def _get_shrink_target_single_factor(self, X: np.ndarray, S: np.ndarray) -> np.ndarray:
+        """get shrinking target with single factor model"""
+        X_mkt = np.nanmean(X, axis=1)
+        cov_mkt = np.asarray(X.T.dot(X_mkt) / len(X))
+        var_mkt = np.asarray(X_mkt.dot(X_mkt) / len(X))
+        F = np.outer(cov_mkt, cov_mkt) / var_mkt
+        np.fill_diagonal(F, np.diag(S))
+        return F
+
+    def _get_shrink_param(self, X: np.ndarray, S: np.ndarray, F: np.ndarray) -> float:
+        """get shrinking parameter `alpha`
+
+        Note:
+            The Ledoit-Wolf shrinking parameter estimator consists of three different methods.
+        """
+        if self.alpha == self.SHR_OAS:
+            return self._get_shrink_param_oas(X, S, F)
+        elif self.alpha == self.SHR_LW:
+            if self.target == self.TGT_CONST_VAR:
+                return self._get_shrink_param_lw_const_var(X, S, F)
+            if self.target == self.TGT_CONST_CORR:
+                return self._get_shrink_param_lw_const_corr(X, S, F)
+            if self.target == self.TGT_SINGLE_FACTOR:
+                return self._get_shrink_param_lw_single_factor(X, S, F)
+        return self.alpha
+
+    def _get_shrink_param_oas(self, X: np.ndarray, S: np.ndarray, F: np.ndarray) -> float:
+        """Oracle Approximating Shrinkage Estimator
+
+        This method uses the following formula to estimate the `alpha`
+        parameter for the shrink covariance estimator:
+            A = (1 - 2 / p) * trace(S^2) + trace^2(S)
+            B = (n + 1 - 2 / p) * (trace(S^2) - trace^2(S) / p)
+            alpha = A / B
+        where `n`, `p` are the dim of observations and variables respectively.
+        """
+        trS2 = np.sum(S**2)
+        tr2S = np.trace(S) ** 2
+
+        n, p = X.shape
+
+        A = (1 - 2 / p) * (trS2 + tr2S)
+        B = (n + 1 - 2 / p) * (trS2 + tr2S / p)
+        alpha = A / B
+
+        return alpha
+
+    def _get_shrink_param_lw_const_var(self, X: np.ndarray, S: np.ndarray, F: np.ndarray) -> float:
+        """Ledoit-Wolf Shrinkage Estimator (Constant Variance)
+
+        This method shrinks the covariance matrix towards the constand variance target.
+        """
+        t, n = X.shape
+
+        y = X**2
+        phi = np.sum(y.T.dot(y) / t - S**2)
+
+        gamma = np.linalg.norm(S - F, "fro") ** 2
+
+        kappa = phi / gamma
+        alpha = max(0, min(1, kappa / t))
+
+        return alpha
+
+    def _get_shrink_param_lw_const_corr(self, X: np.ndarray, S: np.ndarray, F: np.ndarray) -> float:
+        """Ledoit-Wolf Shrinkage Estimator (Constant Correlation)
+
+        This method shrinks the covariance matrix towards the constand correlation target.
+        """
+        t, n = X.shape
+
+        var = np.diag(S)
+        sqrt_var = np.sqrt(var)
+        r_bar = (np.sum(S / np.outer(sqrt_var, sqrt_var)) - n) / (n * (n - 1))
+
+        y = X**2
+        phi_mat = y.T.dot(y) / t - S**2
+        phi = np.sum(phi_mat)
+
+        theta_mat = (X**3).T.dot(X) / t - var[:, None] * S
+        np.fill_diagonal(theta_mat, 0)
+        rho = np.sum(np.diag(phi_mat)) + r_bar * np.sum(np.outer(1 / sqrt_var, sqrt_var) * theta_mat)
+
+        gamma = np.linalg.norm(S - F, "fro") ** 2
+
+        kappa = (phi - rho) / gamma
+        alpha = max(0, min(1, kappa / t))
+
+        return alpha
+
+    def _get_shrink_param_lw_single_factor(self, X: np.ndarray, S: np.ndarray, F: np.ndarray) -> float:
+        """Ledoit-Wolf Shrinkage Estimator (Single Factor Model)
+
+        This method shrinks the covariance matrix towards the single factor model target.
+        """
+        t, n = X.shape
+
+        X_mkt = np.nanmean(X, axis=1)
+        cov_mkt = np.asarray(X.T.dot(X_mkt) / len(X))
+        var_mkt = np.asarray(X_mkt.dot(X_mkt) / len(X))
+
+        y = X**2
+        phi = np.sum(y.T.dot(y)) / t - np.sum(S**2)
+
+        rdiag = np.sum(y**2) / t - np.sum(np.diag(S) ** 2)
+        z = X * X_mkt[:, None]
+        v1 = y.T.dot(z) / t - cov_mkt[:, None] * S
+        roff1 = np.sum(v1 * cov_mkt[:, None].T) / var_mkt - np.sum(np.diag(v1) * cov_mkt) / var_mkt
+        v3 = z.T.dot(z) / t - var_mkt * S
+        roff3 = (
+            np.sum(v3 * np.outer(cov_mkt, cov_mkt)) / var_mkt**2 - np.sum(np.diag(v3) * cov_mkt**2) / var_mkt**2
+        )
+        roff = 2 * roff1 - roff3
+        rho = rdiag + roff
+
+        gamma = np.linalg.norm(S - F, "fro") ** 2
+
+        kappa = (phi - rho) / gamma
+        alpha = max(0, min(1, kappa / t))
+
+        return alpha
```

## qlib/model/riskmodel/structured.py

 * *Ordering differences only*

```diff
@@ -1,94 +1,94 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-import numpy as np
-from typing import Union
-from sklearn.decomposition import PCA, FactorAnalysis
-
-from qlib.model.riskmodel import RiskModel
-
-
-class StructuredCovEstimator(RiskModel):
-    """Structured Covariance Estimator
-
-    This estimator assumes observations can be predicted by multiple factors
-        X = B @ F.T + U
-    where `X` contains observations (row) of multiple variables (column),
-    `F` contains factor exposures (column) for all variables (row),
-    `B` is the regression coefficients matrix for all observations (row) on
-    all factors (columns), and `U` is the residual matrix with shape like `X`.
-
-    Therefore, the structured covariance can be estimated by
-        cov(X.T) = F @ cov(B.T) @ F.T + diag(var(U))
-
-    In finance domain, there are mainly three methods to design `F` [1][2]:
-        - Statistical Risk Model (SRM): latent factor models major components
-        - Fundamental Risk Model (FRM): human designed factors
-        - Deep Risk Model (DRM): neural network designed factors (like a blend of SRM & DRM)
-
-    In this implementation we use latent factor models to specify `F`.
-    Specifically, the following two latent factor models are supported:
-        - `pca`: Principal Component Analysis
-        - `fa`: Factor Analysis
-
-    Reference:
-        [1] Fan, J., Liao, Y., & Liu, H. (2016). An overview of the estimation of large covariance and
-            precision matrices. Econometrics Journal, 19(1), C1–C32. https://doi.org/10.1111/ectj.12061
-        [2] Lin, H., Zhou, D., Liu, W., & Bian, J. (2021). Deep Risk Model: A Deep Learning Solution for
-            Mining Latent Risk Factors to Improve Covariance Matrix Estimation. arXiv preprint arXiv:2107.05201.
-    """
-
-    FACTOR_MODEL_PCA = "pca"
-    FACTOR_MODEL_FA = "fa"
-    DEFAULT_NAN_OPTION = "fill"
-
-    def __init__(self, factor_model: str = "pca", num_factors: int = 10, **kwargs):
-        """
-        Args:
-            factor_model (str): the latent factor models used to estimate the structured covariance (`pca`/`fa`).
-            num_factors (int): number of components to keep.
-            kwargs: see `RiskModel` for more information
-        """
-        if "nan_option" in kwargs:
-            assert kwargs["nan_option"] in [self.DEFAULT_NAN_OPTION], "nan_option={} is not supported".format(
-                kwargs["nan_option"]
-            )
-        else:
-            kwargs["nan_option"] = self.DEFAULT_NAN_OPTION
-
-        super().__init__(**kwargs)
-
-        assert factor_model in [
-            self.FACTOR_MODEL_PCA,
-            self.FACTOR_MODEL_FA,
-        ], "factor_model={} is not supported".format(factor_model)
-        self.solver = PCA if factor_model == self.FACTOR_MODEL_PCA else FactorAnalysis
-
-        self.num_factors = num_factors
-
-    def _predict(self, X: np.ndarray, return_decomposed_components=False) -> Union[np.ndarray, tuple]:
-        """
-        covariance estimation implementation
-
-        Args:
-            X (np.ndarray): data matrix containing multiple variables (columns) and observations (rows).
-            return_decomposed_components (bool): whether return decomposed components of the covariance matrix.
-
-        Returns:
-            tuple or np.ndarray: decomposed covariance matrix or covariance matrix.
-        """
-
-        model = self.solver(self.num_factors, random_state=0).fit(X)
-
-        F = model.components_.T  # variables x factors
-        B = model.transform(X)  # observations x factors
-        U = X - B @ F.T
-        cov_b = np.cov(B.T)  # factors x factors
-        var_u = np.var(U, axis=0)  # diagonal
-
-        if return_decomposed_components:
-            return F, cov_b, var_u
-
-        cov_x = F @ cov_b @ F.T + np.diag(var_u)
-
-        return cov_x
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+import numpy as np
+from typing import Union
+from sklearn.decomposition import PCA, FactorAnalysis
+
+from qlib.model.riskmodel import RiskModel
+
+
+class StructuredCovEstimator(RiskModel):
+    """Structured Covariance Estimator
+
+    This estimator assumes observations can be predicted by multiple factors
+        X = B @ F.T + U
+    where `X` contains observations (row) of multiple variables (column),
+    `F` contains factor exposures (column) for all variables (row),
+    `B` is the regression coefficients matrix for all observations (row) on
+    all factors (columns), and `U` is the residual matrix with shape like `X`.
+
+    Therefore, the structured covariance can be estimated by
+        cov(X.T) = F @ cov(B.T) @ F.T + diag(var(U))
+
+    In finance domain, there are mainly three methods to design `F` [1][2]:
+        - Statistical Risk Model (SRM): latent factor models major components
+        - Fundamental Risk Model (FRM): human designed factors
+        - Deep Risk Model (DRM): neural network designed factors (like a blend of SRM & DRM)
+
+    In this implementation we use latent factor models to specify `F`.
+    Specifically, the following two latent factor models are supported:
+        - `pca`: Principal Component Analysis
+        - `fa`: Factor Analysis
+
+    Reference:
+        [1] Fan, J., Liao, Y., & Liu, H. (2016). An overview of the estimation of large covariance and
+            precision matrices. Econometrics Journal, 19(1), C1–C32. https://doi.org/10.1111/ectj.12061
+        [2] Lin, H., Zhou, D., Liu, W., & Bian, J. (2021). Deep Risk Model: A Deep Learning Solution for
+            Mining Latent Risk Factors to Improve Covariance Matrix Estimation. arXiv preprint arXiv:2107.05201.
+    """
+
+    FACTOR_MODEL_PCA = "pca"
+    FACTOR_MODEL_FA = "fa"
+    DEFAULT_NAN_OPTION = "fill"
+
+    def __init__(self, factor_model: str = "pca", num_factors: int = 10, **kwargs):
+        """
+        Args:
+            factor_model (str): the latent factor models used to estimate the structured covariance (`pca`/`fa`).
+            num_factors (int): number of components to keep.
+            kwargs: see `RiskModel` for more information
+        """
+        if "nan_option" in kwargs:
+            assert kwargs["nan_option"] in [self.DEFAULT_NAN_OPTION], "nan_option={} is not supported".format(
+                kwargs["nan_option"]
+            )
+        else:
+            kwargs["nan_option"] = self.DEFAULT_NAN_OPTION
+
+        super().__init__(**kwargs)
+
+        assert factor_model in [
+            self.FACTOR_MODEL_PCA,
+            self.FACTOR_MODEL_FA,
+        ], "factor_model={} is not supported".format(factor_model)
+        self.solver = PCA if factor_model == self.FACTOR_MODEL_PCA else FactorAnalysis
+
+        self.num_factors = num_factors
+
+    def _predict(self, X: np.ndarray, return_decomposed_components=False) -> Union[np.ndarray, tuple]:
+        """
+        covariance estimation implementation
+
+        Args:
+            X (np.ndarray): data matrix containing multiple variables (columns) and observations (rows).
+            return_decomposed_components (bool): whether return decomposed components of the covariance matrix.
+
+        Returns:
+            tuple or np.ndarray: decomposed covariance matrix or covariance matrix.
+        """
+
+        model = self.solver(self.num_factors, random_state=0).fit(X)
+
+        F = model.components_.T  # variables x factors
+        B = model.transform(X)  # observations x factors
+        U = X - B @ F.T
+        cov_b = np.cov(B.T)  # factors x factors
+        var_u = np.var(U, axis=0)  # diagonal
+
+        if return_decomposed_components:
+            return F, cov_b, var_u
+
+        cov_x = F @ cov_b @ F.T + np.diag(var_u)
+
+        return cov_x
```

## qlib/rl/__init__.py

 * *Ordering differences only*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-from .interpreter import Interpreter, StateInterpreter, ActionInterpreter
-from .reward import Reward, RewardCombination
-from .simulator import Simulator
-
-__all__ = ["Interpreter", "StateInterpreter", "ActionInterpreter", "Reward", "RewardCombination", "Simulator"]
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+from .interpreter import Interpreter, StateInterpreter, ActionInterpreter
+from .reward import Reward, RewardCombination
+from .simulator import Simulator
+
+__all__ = ["Interpreter", "StateInterpreter", "ActionInterpreter", "Reward", "RewardCombination", "Simulator"]
```

## qlib/rl/aux_info.py

 * *Ordering differences only*

```diff
@@ -1,43 +1,43 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-from __future__ import annotations
-
-from typing import TYPE_CHECKING, Generic, Optional, TypeVar
-
-from qlib.typehint import final
-
-from .simulator import StateType
-
-if TYPE_CHECKING:
-    from .utils.env_wrapper import EnvWrapper
-
-
-__all__ = ["AuxiliaryInfoCollector"]
-
-AuxInfoType = TypeVar("AuxInfoType")
-
-
-class AuxiliaryInfoCollector(Generic[StateType, AuxInfoType]):
-    """Override this class to collect customized auxiliary information from environment."""
-
-    env: Optional[EnvWrapper] = None
-
-    @final
-    def __call__(self, simulator_state: StateType) -> AuxInfoType:
-        return self.collect(simulator_state)
-
-    def collect(self, simulator_state: StateType) -> AuxInfoType:
-        """Override this for customized auxiliary info.
-        Usually useful in Multi-agent RL.
-
-        Parameters
-        ----------
-        simulator_state
-            Retrieved with ``simulator.get_state()``.
-
-        Returns
-        -------
-        Auxiliary information.
-        """
-        raise NotImplementedError("collect is not implemented!")
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+from __future__ import annotations
+
+from typing import TYPE_CHECKING, Generic, Optional, TypeVar
+
+from qlib.typehint import final
+
+from .simulator import StateType
+
+if TYPE_CHECKING:
+    from .utils.env_wrapper import EnvWrapper
+
+
+__all__ = ["AuxiliaryInfoCollector"]
+
+AuxInfoType = TypeVar("AuxInfoType")
+
+
+class AuxiliaryInfoCollector(Generic[StateType, AuxInfoType]):
+    """Override this class to collect customized auxiliary information from environment."""
+
+    env: Optional[EnvWrapper] = None
+
+    @final
+    def __call__(self, simulator_state: StateType) -> AuxInfoType:
+        return self.collect(simulator_state)
+
+    def collect(self, simulator_state: StateType) -> AuxInfoType:
+        """Override this for customized auxiliary info.
+        Usually useful in Multi-agent RL.
+
+        Parameters
+        ----------
+        simulator_state
+            Retrieved with ``simulator.get_state()``.
+
+        Returns
+        -------
+        Auxiliary information.
+        """
+        raise NotImplementedError("collect is not implemented!")
```

## qlib/rl/interpreter.py

 * *Ordering differences only*

```diff
@@ -1,141 +1,141 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-from __future__ import annotations
-
-from typing import Any, Generic, TypeVar
-
-import gym
-import numpy as np
-from gym import spaces
-
-from qlib.typehint import final
-from .simulator import ActType, StateType
-
-ObsType = TypeVar("ObsType")
-PolicyActType = TypeVar("PolicyActType")
-
-
-class Interpreter:
-    """Interpreter is a media between states produced by simulators and states needed by RL policies.
-    Interpreters are two-way:
-
-    1. From simulator state to policy state (aka observation), see :class:`StateInterpreter`.
-    2. From policy action to action accepted by simulator, see :class:`ActionInterpreter`.
-
-    Inherit one of the two sub-classes to define your own interpreter.
-    This super-class is only used for isinstance check.
-
-    Interpreters are recommended to be stateless, meaning that storing temporary information with ``self.xxx``
-    in interpreter is anti-pattern. In future, we might support register some interpreter-related
-    states by calling ``self.env.register_state()``, but it's not planned for first iteration.
-    """
-
-
-class StateInterpreter(Generic[StateType, ObsType], Interpreter):
-    """State Interpreter that interpret execution result of qlib executor into rl env state"""
-
-    @property
-    def observation_space(self) -> gym.Space:
-        raise NotImplementedError()
-
-    @final  # no overridden
-    def __call__(self, simulator_state: StateType) -> ObsType:
-        obs = self.interpret(simulator_state)
-        self.validate(obs)
-        return obs
-
-    def validate(self, obs: ObsType) -> None:
-        """Validate whether an observation belongs to the pre-defined observation space."""
-        _gym_space_contains(self.observation_space, obs)
-
-    def interpret(self, simulator_state: StateType) -> ObsType:
-        """Interpret the state of simulator.
-
-        Parameters
-        ----------
-        simulator_state
-            Retrieved with ``simulator.get_state()``.
-
-        Returns
-        -------
-        State needed by policy. Should conform with the state space defined in ``observation_space``.
-        """
-        raise NotImplementedError("interpret is not implemented!")
-
-
-class ActionInterpreter(Generic[StateType, PolicyActType, ActType], Interpreter):
-    """Action Interpreter that interpret rl agent action into qlib orders"""
-
-    @property
-    def action_space(self) -> gym.Space:
-        raise NotImplementedError()
-
-    @final  # no overridden
-    def __call__(self, simulator_state: StateType, action: PolicyActType) -> ActType:
-        self.validate(action)
-        obs = self.interpret(simulator_state, action)
-        return obs
-
-    def validate(self, action: PolicyActType) -> None:
-        """Validate whether an action belongs to the pre-defined action space."""
-        _gym_space_contains(self.action_space, action)
-
-    def interpret(self, simulator_state: StateType, action: PolicyActType) -> ActType:
-        """Convert the policy action to simulator action.
-
-        Parameters
-        ----------
-        simulator_state
-            Retrieved with ``simulator.get_state()``.
-        action
-            Raw action given by policy.
-
-        Returns
-        -------
-        The action needed by simulator,
-        """
-        raise NotImplementedError("interpret is not implemented!")
-
-
-def _gym_space_contains(space: gym.Space, x: Any) -> None:
-    """Strengthened version of gym.Space.contains.
-    Giving more diagnostic information on why validation fails.
-
-    Throw exception rather than returning true or false.
-    """
-    if isinstance(space, spaces.Dict):
-        if not isinstance(x, dict) or len(x) != len(space):
-            raise GymSpaceValidationError("Sample must be a dict with same length as space.", space, x)
-        for k, subspace in space.spaces.items():
-            if k not in x:
-                raise GymSpaceValidationError(f"Key {k} not found in sample.", space, x)
-            try:
-                _gym_space_contains(subspace, x[k])
-            except GymSpaceValidationError as e:
-                raise GymSpaceValidationError(f"Subspace of key {k} validation error.", space, x) from e
-
-    elif isinstance(space, spaces.Tuple):
-        if isinstance(x, (list, np.ndarray)):
-            x = tuple(x)  # Promote list and ndarray to tuple for contains check
-        if not isinstance(x, tuple) or len(x) != len(space):
-            raise GymSpaceValidationError("Sample must be a tuple with same length as space.", space, x)
-        for i, (subspace, part) in enumerate(zip(space, x)):
-            try:
-                _gym_space_contains(subspace, part)
-            except GymSpaceValidationError as e:
-                raise GymSpaceValidationError(f"Subspace of index {i} validation error.", space, x) from e
-
-    else:
-        if not space.contains(x):
-            raise GymSpaceValidationError("Validation error reported by gym.", space, x)
-
-
-class GymSpaceValidationError(Exception):
-    def __init__(self, message: str, space: gym.Space, x: Any) -> None:
-        self.message = message
-        self.space = space
-        self.x = x
-
-    def __str__(self) -> str:
-        return f"{self.message}\n  Space: {self.space}\n  Sample: {self.x}"
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+from __future__ import annotations
+
+from typing import Any, Generic, TypeVar
+
+import gym
+import numpy as np
+from gym import spaces
+
+from qlib.typehint import final
+from .simulator import ActType, StateType
+
+ObsType = TypeVar("ObsType")
+PolicyActType = TypeVar("PolicyActType")
+
+
+class Interpreter:
+    """Interpreter is a media between states produced by simulators and states needed by RL policies.
+    Interpreters are two-way:
+
+    1. From simulator state to policy state (aka observation), see :class:`StateInterpreter`.
+    2. From policy action to action accepted by simulator, see :class:`ActionInterpreter`.
+
+    Inherit one of the two sub-classes to define your own interpreter.
+    This super-class is only used for isinstance check.
+
+    Interpreters are recommended to be stateless, meaning that storing temporary information with ``self.xxx``
+    in interpreter is anti-pattern. In future, we might support register some interpreter-related
+    states by calling ``self.env.register_state()``, but it's not planned for first iteration.
+    """
+
+
+class StateInterpreter(Generic[StateType, ObsType], Interpreter):
+    """State Interpreter that interpret execution result of qlib executor into rl env state"""
+
+    @property
+    def observation_space(self) -> gym.Space:
+        raise NotImplementedError()
+
+    @final  # no overridden
+    def __call__(self, simulator_state: StateType) -> ObsType:
+        obs = self.interpret(simulator_state)
+        self.validate(obs)
+        return obs
+
+    def validate(self, obs: ObsType) -> None:
+        """Validate whether an observation belongs to the pre-defined observation space."""
+        _gym_space_contains(self.observation_space, obs)
+
+    def interpret(self, simulator_state: StateType) -> ObsType:
+        """Interpret the state of simulator.
+
+        Parameters
+        ----------
+        simulator_state
+            Retrieved with ``simulator.get_state()``.
+
+        Returns
+        -------
+        State needed by policy. Should conform with the state space defined in ``observation_space``.
+        """
+        raise NotImplementedError("interpret is not implemented!")
+
+
+class ActionInterpreter(Generic[StateType, PolicyActType, ActType], Interpreter):
+    """Action Interpreter that interpret rl agent action into qlib orders"""
+
+    @property
+    def action_space(self) -> gym.Space:
+        raise NotImplementedError()
+
+    @final  # no overridden
+    def __call__(self, simulator_state: StateType, action: PolicyActType) -> ActType:
+        self.validate(action)
+        obs = self.interpret(simulator_state, action)
+        return obs
+
+    def validate(self, action: PolicyActType) -> None:
+        """Validate whether an action belongs to the pre-defined action space."""
+        _gym_space_contains(self.action_space, action)
+
+    def interpret(self, simulator_state: StateType, action: PolicyActType) -> ActType:
+        """Convert the policy action to simulator action.
+
+        Parameters
+        ----------
+        simulator_state
+            Retrieved with ``simulator.get_state()``.
+        action
+            Raw action given by policy.
+
+        Returns
+        -------
+        The action needed by simulator,
+        """
+        raise NotImplementedError("interpret is not implemented!")
+
+
+def _gym_space_contains(space: gym.Space, x: Any) -> None:
+    """Strengthened version of gym.Space.contains.
+    Giving more diagnostic information on why validation fails.
+
+    Throw exception rather than returning true or false.
+    """
+    if isinstance(space, spaces.Dict):
+        if not isinstance(x, dict) or len(x) != len(space):
+            raise GymSpaceValidationError("Sample must be a dict with same length as space.", space, x)
+        for k, subspace in space.spaces.items():
+            if k not in x:
+                raise GymSpaceValidationError(f"Key {k} not found in sample.", space, x)
+            try:
+                _gym_space_contains(subspace, x[k])
+            except GymSpaceValidationError as e:
+                raise GymSpaceValidationError(f"Subspace of key {k} validation error.", space, x) from e
+
+    elif isinstance(space, spaces.Tuple):
+        if isinstance(x, (list, np.ndarray)):
+            x = tuple(x)  # Promote list and ndarray to tuple for contains check
+        if not isinstance(x, tuple) or len(x) != len(space):
+            raise GymSpaceValidationError("Sample must be a tuple with same length as space.", space, x)
+        for i, (subspace, part) in enumerate(zip(space, x)):
+            try:
+                _gym_space_contains(subspace, part)
+            except GymSpaceValidationError as e:
+                raise GymSpaceValidationError(f"Subspace of index {i} validation error.", space, x) from e
+
+    else:
+        if not space.contains(x):
+            raise GymSpaceValidationError("Validation error reported by gym.", space, x)
+
+
+class GymSpaceValidationError(Exception):
+    def __init__(self, message: str, space: gym.Space, x: Any) -> None:
+        self.message = message
+        self.space = space
+        self.x = x
+
+    def __str__(self) -> str:
+        return f"{self.message}\n  Space: {self.space}\n  Sample: {self.x}"
```

## qlib/rl/reward.py

 * *Ordering differences only*

```diff
@@ -1,85 +1,85 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-from __future__ import annotations
-
-from typing import TYPE_CHECKING, Any, Dict, Generic, Optional, Tuple, TypeVar
-
-from qlib.typehint import final
-
-if TYPE_CHECKING:
-    from .utils.env_wrapper import EnvWrapper
-
-SimulatorState = TypeVar("SimulatorState")
-
-
-class Reward(Generic[SimulatorState]):
-    """
-    Reward calculation component that takes a single argument: state of simulator. Returns a real number: reward.
-
-    Subclass should implement ``reward(simulator_state)`` to implement their own reward calculation recipe.
-    """
-
-    env: Optional[EnvWrapper] = None
-
-    @final
-    def __call__(self, simulator_state: SimulatorState) -> float:
-        return self.reward(simulator_state)
-
-    def reward(self, simulator_state: SimulatorState) -> float:
-        """Implement this method for your own reward."""
-        raise NotImplementedError("Implement reward calculation recipe in `reward()`.")
-
-    def log(self, name: str, value: Any) -> None:
-        assert self.env is not None
-        self.env.logger.add_scalar(name, value)
-
-
-class RewardCombination(Reward):
-    """Combination of multiple reward."""
-
-    def __init__(self, rewards: Dict[str, Tuple[Reward, float]]) -> None:
-        self.rewards = rewards
-
-    def reward(self, simulator_state: Any) -> float:
-        total_reward = 0.0
-        for name, (reward_fn, weight) in self.rewards.items():
-            rew = reward_fn(simulator_state) * weight
-            total_reward += rew
-            self.log(name, rew)
-        return total_reward
-
-
-# TODO:
-# reward_factory is disabled for now
-
-# _RegistryConfigReward = RegistryConfig[REWARDS]
-
-
-# @configclass
-# class _WeightedRewardConfig:
-#     weight: float
-#     reward: _RegistryConfigReward
-
-
-# RewardConfig = Union[_RegistryConfigReward, Dict[str, Union[_RegistryConfigReward, _WeightedRewardConfig]]]
-
-
-# def reward_factory(reward_config: RewardConfig) -> Reward:
-#     """
-#     Use this factory to instantiate the reward from config.
-#     Simply using ``reward_config.build()`` might not work because reward can have complex combinations.
-#     """
-#     if isinstance(reward_config, dict):
-#         # as reward combination
-#         rewards = {}
-#         for name, rew in reward_config.items():
-#             if not isinstance(rew, _WeightedRewardConfig):
-#                 # default weight is 1.
-#                 rew = _WeightedRewardConfig(weight=1., rew=rew)
-#             # no recursive build in this step
-#             rewards[name] = (rew.reward.build(), rew.weight)
-#         return RewardCombination(rewards)
-#     else:
-#         # single reward
-#         return reward_config.build()
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+from __future__ import annotations
+
+from typing import TYPE_CHECKING, Any, Dict, Generic, Optional, Tuple, TypeVar
+
+from qlib.typehint import final
+
+if TYPE_CHECKING:
+    from .utils.env_wrapper import EnvWrapper
+
+SimulatorState = TypeVar("SimulatorState")
+
+
+class Reward(Generic[SimulatorState]):
+    """
+    Reward calculation component that takes a single argument: state of simulator. Returns a real number: reward.
+
+    Subclass should implement ``reward(simulator_state)`` to implement their own reward calculation recipe.
+    """
+
+    env: Optional[EnvWrapper] = None
+
+    @final
+    def __call__(self, simulator_state: SimulatorState) -> float:
+        return self.reward(simulator_state)
+
+    def reward(self, simulator_state: SimulatorState) -> float:
+        """Implement this method for your own reward."""
+        raise NotImplementedError("Implement reward calculation recipe in `reward()`.")
+
+    def log(self, name: str, value: Any) -> None:
+        assert self.env is not None
+        self.env.logger.add_scalar(name, value)
+
+
+class RewardCombination(Reward):
+    """Combination of multiple reward."""
+
+    def __init__(self, rewards: Dict[str, Tuple[Reward, float]]) -> None:
+        self.rewards = rewards
+
+    def reward(self, simulator_state: Any) -> float:
+        total_reward = 0.0
+        for name, (reward_fn, weight) in self.rewards.items():
+            rew = reward_fn(simulator_state) * weight
+            total_reward += rew
+            self.log(name, rew)
+        return total_reward
+
+
+# TODO:
+# reward_factory is disabled for now
+
+# _RegistryConfigReward = RegistryConfig[REWARDS]
+
+
+# @configclass
+# class _WeightedRewardConfig:
+#     weight: float
+#     reward: _RegistryConfigReward
+
+
+# RewardConfig = Union[_RegistryConfigReward, Dict[str, Union[_RegistryConfigReward, _WeightedRewardConfig]]]
+
+
+# def reward_factory(reward_config: RewardConfig) -> Reward:
+#     """
+#     Use this factory to instantiate the reward from config.
+#     Simply using ``reward_config.build()`` might not work because reward can have complex combinations.
+#     """
+#     if isinstance(reward_config, dict):
+#         # as reward combination
+#         rewards = {}
+#         for name, rew in reward_config.items():
+#             if not isinstance(rew, _WeightedRewardConfig):
+#                 # default weight is 1.
+#                 rew = _WeightedRewardConfig(weight=1., rew=rew)
+#             # no recursive build in this step
+#             rewards[name] = (rew.reward.build(), rew.weight)
+#         return RewardCombination(rewards)
+#     else:
+#         # single reward
+#         return reward_config.build()
```

## qlib/rl/seed.py

 * *Ordering differences only*

```diff
@@ -1,12 +1,12 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-"""Defines a set of initial state definitions and state-set definitions.
-
-With single-asset order execution only, the only seed is order.
-"""
-
-from typing import TypeVar
-
-InitialStateType = TypeVar("InitialStateType")
-"""Type of data that creates the simulator."""
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+"""Defines a set of initial state definitions and state-set definitions.
+
+With single-asset order execution only, the only seed is order.
+"""
+
+from typing import TypeVar
+
+InitialStateType = TypeVar("InitialStateType")
+"""Type of data that creates the simulator."""
```

## qlib/rl/simulator.py

 * *Ordering differences only*

```diff
@@ -1,75 +1,75 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-from __future__ import annotations
-
-from typing import TYPE_CHECKING, Any, Generic, Optional, TypeVar
-
-from .seed import InitialStateType
-
-if TYPE_CHECKING:
-    from .utils.env_wrapper import EnvWrapper
-
-StateType = TypeVar("StateType")
-"""StateType stores all the useful data in the simulation process
-(as well as utilities to generate/retrieve data when needed)."""
-
-ActType = TypeVar("ActType")
-"""This ActType is the type of action at the simulator end."""
-
-
-class Simulator(Generic[InitialStateType, StateType, ActType]):
-    """
-    Simulator that resets with ``__init__``, and transits with ``step(action)``.
-
-    To make the data-flow clear, we make the following restrictions to Simulator:
-
-    1. The only way to modify the inner status of a simulator is by using ``step(action)``.
-    2. External modules can *read* the status of a simulator by using ``simulator.get_state()``,
-       and check whether the simulator is in the ending state by calling ``simulator.done()``.
-
-    A simulator is defined to be bounded with three types:
-
-    - *InitialStateType* that is the type of the data used to create the simulator.
-    - *StateType* that is the type of the **status** (state) of the simulator.
-    - *ActType* that is the type of the **action**, which is the input received in each step.
-
-    Different simulators might share the same StateType. For example, when they are dealing with the same task,
-    but with different simulation implementation. With the same type, they can safely share other components in the MDP.
-
-    Simulators are ephemeral. The lifecycle of a simulator starts with an initial state, and ends with the trajectory.
-    In another word, when the trajectory ends, simulator is recycled.
-    If simulators want to share context between (e.g., for speed-up purposes),
-    this could be done by accessing the weak reference of environment wrapper.
-
-    Attributes
-    ----------
-    env
-        A reference of env-wrapper, which could be useful in some corner cases.
-        Simulators are discouraged to use this, because it's prone to induce errors.
-    """
-
-    env: Optional[EnvWrapper] = None
-
-    def __init__(self, initial: InitialStateType, **kwargs: Any) -> None:
-        pass
-
-    def step(self, action: ActType) -> None:
-        """Receives an action of ActType.
-
-        Simulator should update its internal state, and return None.
-        The updated state can be retrieved with ``simulator.get_state()``.
-        """
-        raise NotImplementedError()
-
-    def get_state(self) -> StateType:
-        raise NotImplementedError()
-
-    def done(self) -> bool:
-        """Check whether the simulator is in a "done" state.
-        When simulator is in a "done" state,
-        it should no longer receives any ``step`` request.
-        As simulators are ephemeral, to reset the simulator,
-        the old one should be destroyed and a new simulator can be created.
-        """
-        raise NotImplementedError()
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+from __future__ import annotations
+
+from typing import TYPE_CHECKING, Any, Generic, Optional, TypeVar
+
+from .seed import InitialStateType
+
+if TYPE_CHECKING:
+    from .utils.env_wrapper import EnvWrapper
+
+StateType = TypeVar("StateType")
+"""StateType stores all the useful data in the simulation process
+(as well as utilities to generate/retrieve data when needed)."""
+
+ActType = TypeVar("ActType")
+"""This ActType is the type of action at the simulator end."""
+
+
+class Simulator(Generic[InitialStateType, StateType, ActType]):
+    """
+    Simulator that resets with ``__init__``, and transits with ``step(action)``.
+
+    To make the data-flow clear, we make the following restrictions to Simulator:
+
+    1. The only way to modify the inner status of a simulator is by using ``step(action)``.
+    2. External modules can *read* the status of a simulator by using ``simulator.get_state()``,
+       and check whether the simulator is in the ending state by calling ``simulator.done()``.
+
+    A simulator is defined to be bounded with three types:
+
+    - *InitialStateType* that is the type of the data used to create the simulator.
+    - *StateType* that is the type of the **status** (state) of the simulator.
+    - *ActType* that is the type of the **action**, which is the input received in each step.
+
+    Different simulators might share the same StateType. For example, when they are dealing with the same task,
+    but with different simulation implementation. With the same type, they can safely share other components in the MDP.
+
+    Simulators are ephemeral. The lifecycle of a simulator starts with an initial state, and ends with the trajectory.
+    In another word, when the trajectory ends, simulator is recycled.
+    If simulators want to share context between (e.g., for speed-up purposes),
+    this could be done by accessing the weak reference of environment wrapper.
+
+    Attributes
+    ----------
+    env
+        A reference of env-wrapper, which could be useful in some corner cases.
+        Simulators are discouraged to use this, because it's prone to induce errors.
+    """
+
+    env: Optional[EnvWrapper] = None
+
+    def __init__(self, initial: InitialStateType, **kwargs: Any) -> None:
+        pass
+
+    def step(self, action: ActType) -> None:
+        """Receives an action of ActType.
+
+        Simulator should update its internal state, and return None.
+        The updated state can be retrieved with ``simulator.get_state()``.
+        """
+        raise NotImplementedError()
+
+    def get_state(self) -> StateType:
+        raise NotImplementedError()
+
+    def done(self) -> bool:
+        """Check whether the simulator is in a "done" state.
+        When simulator is in a "done" state,
+        it should no longer receives any ``step`` request.
+        As simulators are ephemeral, to reset the simulator,
+        the old one should be destroyed and a new simulator can be created.
+        """
+        raise NotImplementedError()
```

## qlib/rl/contrib/backtest.py

 * *Ordering differences only*

```diff
@@ -1,384 +1,384 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-from __future__ import annotations
-
-import argparse
-import copy
-import os
-import pickle
-from collections import defaultdict
-from pathlib import Path
-from typing import Dict, List, Optional, Tuple, Union, cast
-
-import numpy as np
-import pandas as pd
-import torch
-from joblib import Parallel, delayed
-
-from qlib.backtest import INDICATOR_METRIC, collect_data_loop, get_strategy_executor
-from qlib.backtest.decision import BaseTradeDecision, Order, OrderDir, TradeRangeByTime
-from qlib.backtest.executor import SimulatorExecutor
-from qlib.backtest.high_performance_ds import BaseOrderIndicator
-from qlib.rl.contrib.naive_config_parser import get_backtest_config_fromfile
-from qlib.rl.contrib.utils import read_order_file
-from qlib.rl.data.integration import init_qlib
-from qlib.rl.order_execution.simulator_qlib import SingleAssetOrderExecution
-from qlib.typehint import Literal
-
-
-def _get_multi_level_executor_config(
-    strategy_config: dict,
-    cash_limit: float | None = None,
-    generate_report: bool = False,
-    data_granularity: str = "1min",
-) -> dict:
-    executor_config = {
-        "class": "SimulatorExecutor",
-        "module_path": "qlib.backtest.executor",
-        "kwargs": {
-            "time_per_step": data_granularity,
-            "verbose": False,
-            "trade_type": SimulatorExecutor.TT_PARAL if cash_limit is not None else SimulatorExecutor.TT_SERIAL,
-            "generate_report": generate_report,
-            "track_data": True,
-        },
-    }
-
-    freqs = list(strategy_config.keys())
-    freqs.sort(key=pd.Timedelta)
-    for freq in freqs:
-        executor_config = {
-            "class": "NestedExecutor",
-            "module_path": "qlib.backtest.executor",
-            "kwargs": {
-                "time_per_step": freq,
-                "inner_strategy": strategy_config[freq],
-                "inner_executor": executor_config,
-                "track_data": True,
-            },
-        }
-
-    return executor_config
-
-
-def _convert_indicator_to_dataframe(indicator: dict) -> Optional[pd.DataFrame]:
-    record_list = []
-    for time, value_dict in indicator.items():
-        if isinstance(value_dict, BaseOrderIndicator):
-            # HACK: for qlib v0.8
-            value_dict = value_dict.to_series()
-        try:
-            value_dict = copy.deepcopy(value_dict)
-            if value_dict["ffr"].empty:
-                continue
-        except Exception:
-            value_dict = {k: v for k, v in value_dict.items() if k != "pa"}
-        value_dict = pd.DataFrame(value_dict)
-        value_dict["datetime"] = time
-        record_list.append(value_dict)
-
-    if not record_list:
-        return None
-
-    records: pd.DataFrame = pd.concat(record_list, 0).reset_index().rename(columns={"index": "instrument"})
-    records = records.set_index(["instrument", "datetime"])
-    return records
-
-
-def _generate_report(
-    decisions: List[BaseTradeDecision],
-    report_indicators: List[INDICATOR_METRIC],
-) -> Dict[str, Tuple[pd.DataFrame, pd.DataFrame]]:
-    """Generate backtest reports
-
-    Parameters
-    ----------
-    decisions:
-        List of trade decisions.
-    report_indicators
-        List of indicator reports.
-    Returns
-    -------
-
-    """
-    indicator_dict: Dict[str, List[pd.DataFrame]] = defaultdict(list)
-    indicator_his: Dict[str, List[dict]] = defaultdict(list)
-
-    for report_indicator in report_indicators:
-        for key, (indicator_df, indicator_obj) in report_indicator.items():
-            indicator_dict[key].append(indicator_df)
-            indicator_his[key].append(indicator_obj.order_indicator_his)
-
-    report = {}
-    decision_details = pd.concat([getattr(d, "details") for d in decisions if hasattr(d, "details")])
-    for key in indicator_dict:
-        cur_dict = pd.concat(indicator_dict[key])
-        cur_his = pd.concat([_convert_indicator_to_dataframe(his) for his in indicator_his[key]])
-        cur_details = decision_details[decision_details.freq == key].set_index(["instrument", "datetime"])
-        if len(cur_details) > 0:
-            cur_details.pop("freq")
-            cur_his = cur_his.join(cur_details, how="outer")
-
-        report[key] = (cur_dict, cur_his)
-
-    return report
-
-
-def single_with_simulator(
-    backtest_config: dict,
-    orders: pd.DataFrame,
-    split: Literal["stock", "day"] = "stock",
-    cash_limit: float | None = None,
-    generate_report: bool = False,
-) -> Union[Tuple[pd.DataFrame, dict], pd.DataFrame]:
-    """Run backtest in a single thread with SingleAssetOrderExecution simulator. The orders will be executed day by day.
-    A new simulator will be created and used for every single-day order.
-
-    Parameters
-    ----------
-    backtest_config:
-        Backtest config
-    orders:
-        Orders to be executed. Example format:
-                 datetime instrument  amount  direction
-            0  2020-06-01       INST   600.0          0
-            1  2020-06-02       INST   700.0          1
-            ...
-    split
-        Method to split orders. If it is "stock", split orders by stock. If it is "day", split orders by date.
-    cash_limit
-        Limitation of cash.
-    generate_report
-        Whether to generate reports.
-
-    Returns
-    -------
-        If generate_report is True, return execution records and the generated report. Otherwise, return only records.
-    """
-    init_qlib(backtest_config["qlib"])
-
-    stocks = orders.instrument.unique().tolist()
-
-    reports = []
-    decisions = []
-    for _, row in orders.iterrows():
-        date = pd.Timestamp(row["datetime"])
-        start_time = pd.Timestamp(backtest_config["start_time"]).replace(year=date.year, month=date.month, day=date.day)
-        end_time = pd.Timestamp(backtest_config["end_time"]).replace(year=date.year, month=date.month, day=date.day)
-        order = Order(
-            stock_id=row["instrument"],
-            amount=row["amount"],
-            direction=OrderDir(row["direction"]),
-            start_time=start_time,
-            end_time=end_time,
-        )
-
-        executor_config = _get_multi_level_executor_config(
-            strategy_config=backtest_config["strategies"],
-            cash_limit=cash_limit,
-            generate_report=generate_report,
-            data_granularity=backtest_config["data_granularity"],
-        )
-
-        exchange_config = copy.deepcopy(backtest_config["exchange"])
-        exchange_config.update(
-            {
-                "codes": stocks,
-                "freq": backtest_config["data_granularity"],
-            }
-        )
-
-        simulator = SingleAssetOrderExecution(
-            order=order,
-            executor_config=executor_config,
-            exchange_config=exchange_config,
-            qlib_config=None,
-            cash_limit=None,
-        )
-
-        reports.append(simulator.report_dict)
-        decisions += simulator.decisions
-
-    indicator_1day_objs = [report["indicator_dict"]["1day"][1] for report in reports]
-    indicator_info = {k: v for obj in indicator_1day_objs for k, v in obj.order_indicator_his.items()}
-    records = _convert_indicator_to_dataframe(indicator_info)
-    assert records is None or not np.isnan(records["ffr"]).any()
-
-    if generate_report:
-        _report = _generate_report(decisions, [report["indicator"] for report in reports])
-
-        if split == "stock":
-            stock_id = orders.iloc[0].instrument
-            report = {stock_id: _report}
-        else:
-            day = orders.iloc[0].datetime
-            report = {day: _report}
-
-        return records, report
-    else:
-        return records
-
-
-def single_with_collect_data_loop(
-    backtest_config: dict,
-    orders: pd.DataFrame,
-    split: Literal["stock", "day"] = "stock",
-    cash_limit: float | None = None,
-    generate_report: bool = False,
-) -> Union[Tuple[pd.DataFrame, dict], pd.DataFrame]:
-    """Run backtest in a single thread with collect_data_loop.
-
-    Parameters
-    ----------
-    backtest_config:
-        Backtest config
-    orders:
-        Orders to be executed. Example format:
-                 datetime instrument  amount  direction
-            0  2020-06-01       INST   600.0          0
-            1  2020-06-02       INST   700.0          1
-            ...
-    split
-        Method to split orders. If it is "stock", split orders by stock. If it is "day", split orders by date.
-    cash_limit
-        Limitation of cash.
-    generate_report
-        Whether to generate reports.
-
-    Returns
-    -------
-        If generate_report is True, return execution records and the generated report. Otherwise, return only records.
-    """
-
-    init_qlib(backtest_config["qlib"])
-
-    trade_start_time = orders["datetime"].min()
-    trade_end_time = orders["datetime"].max()
-    stocks = orders.instrument.unique().tolist()
-
-    strategy_config = {
-        "class": "FileOrderStrategy",
-        "module_path": "qlib.contrib.strategy.rule_strategy",
-        "kwargs": {
-            "file": orders,
-            "trade_range": TradeRangeByTime(
-                pd.Timestamp(backtest_config["start_time"]).time(),
-                pd.Timestamp(backtest_config["end_time"]).time(),
-            ),
-        },
-    }
-
-    executor_config = _get_multi_level_executor_config(
-        strategy_config=backtest_config["strategies"],
-        cash_limit=cash_limit,
-        generate_report=generate_report,
-        data_granularity=backtest_config["data_granularity"],
-    )
-
-    exchange_config = copy.deepcopy(backtest_config["exchange"])
-    exchange_config.update(
-        {
-            "codes": stocks,
-            "freq": backtest_config["data_granularity"],
-        }
-    )
-
-    strategy, executor = get_strategy_executor(
-        start_time=pd.Timestamp(trade_start_time),
-        end_time=pd.Timestamp(trade_end_time) + pd.DateOffset(1),
-        strategy=strategy_config,
-        executor=executor_config,
-        benchmark=None,
-        account=cash_limit if cash_limit is not None else int(1e12),
-        exchange_kwargs=exchange_config,
-        pos_type="Position" if cash_limit is not None else "InfPosition",
-    )
-
-    report_dict: dict = {}
-    decisions = list(collect_data_loop(trade_start_time, trade_end_time, strategy, executor, report_dict))
-
-    indicator_dict = cast(INDICATOR_METRIC, report_dict.get("indicator_dict"))
-    records = _convert_indicator_to_dataframe(indicator_dict["1day"][1].order_indicator_his)
-    assert records is None or not np.isnan(records["ffr"]).any()
-
-    if generate_report:
-        _report = _generate_report(decisions, [indicator_dict])
-        if split == "stock":
-            stock_id = orders.iloc[0].instrument
-            report = {stock_id: _report}
-        else:
-            day = orders.iloc[0].datetime
-            report = {day: _report}
-        return records, report
-    else:
-        return records
-
-
-def backtest(backtest_config: dict, with_simulator: bool = False) -> pd.DataFrame:
-    order_df = read_order_file(backtest_config["order_file"])
-
-    cash_limit = backtest_config["exchange"].pop("cash_limit")
-    generate_report = backtest_config.pop("generate_report")
-
-    stock_pool = order_df["instrument"].unique().tolist()
-    stock_pool.sort()
-
-    single = single_with_simulator if with_simulator else single_with_collect_data_loop
-    mp_config = {"n_jobs": backtest_config["concurrency"], "verbose": 10, "backend": "multiprocessing"}
-    torch.set_num_threads(1)  # https://github.com/pytorch/pytorch/issues/17199
-    res = Parallel(**mp_config)(
-        delayed(single)(
-            backtest_config=backtest_config,
-            orders=order_df[order_df["instrument"] == stock].copy(),
-            split="stock",
-            cash_limit=cash_limit,
-            generate_report=generate_report,
-        )
-        for stock in stock_pool
-    )
-
-    output_path = Path(backtest_config["output_dir"])
-    if generate_report:
-        with (output_path / "report.pkl").open("wb") as f:
-            report = {}
-            for r in res:
-                report.update(r[1])
-            pickle.dump(report, f)
-        res = pd.concat([r[0] for r in res], 0)
-    else:
-        res = pd.concat(res)
-
-    if not output_path.exists():
-        os.makedirs(output_path)
-
-    if "pa" in res.columns:
-        res["pa"] = res["pa"] * 10000.0  # align with training metrics
-    res.to_csv(output_path / "backtest_result.csv")
-    return res
-
-
-if __name__ == "__main__":
-    import warnings
-
-    warnings.filterwarnings("ignore", category=DeprecationWarning)
-    warnings.filterwarnings("ignore", category=RuntimeWarning)
-
-    parser = argparse.ArgumentParser()
-    parser.add_argument("--config_path", type=str, required=True, help="Path to the config file")
-    parser.add_argument("--use_simulator", action="store_true", help="Whether to use simulator as the backend")
-    parser.add_argument(
-        "--n_jobs",
-        type=int,
-        required=False,
-        help="The number of jobs for running backtest parallely(1 for single process)",
-    )
-    args = parser.parse_args()
-
-    config = get_backtest_config_fromfile(args.config_path)
-    if args.n_jobs is not None:
-        config["concurrency"] = args.n_jobs
-
-    backtest(
-        backtest_config=config,
-        with_simulator=args.use_simulator,
-    )
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+from __future__ import annotations
+
+import argparse
+import copy
+import os
+import pickle
+from collections import defaultdict
+from pathlib import Path
+from typing import Dict, List, Optional, Tuple, Union, cast
+
+import numpy as np
+import pandas as pd
+import torch
+from joblib import Parallel, delayed
+
+from qlib.backtest import INDICATOR_METRIC, collect_data_loop, get_strategy_executor
+from qlib.backtest.decision import BaseTradeDecision, Order, OrderDir, TradeRangeByTime
+from qlib.backtest.executor import SimulatorExecutor
+from qlib.backtest.high_performance_ds import BaseOrderIndicator
+from qlib.rl.contrib.naive_config_parser import get_backtest_config_fromfile
+from qlib.rl.contrib.utils import read_order_file
+from qlib.rl.data.integration import init_qlib
+from qlib.rl.order_execution.simulator_qlib import SingleAssetOrderExecution
+from qlib.typehint import Literal
+
+
+def _get_multi_level_executor_config(
+    strategy_config: dict,
+    cash_limit: float | None = None,
+    generate_report: bool = False,
+    data_granularity: str = "1min",
+) -> dict:
+    executor_config = {
+        "class": "SimulatorExecutor",
+        "module_path": "qlib.backtest.executor",
+        "kwargs": {
+            "time_per_step": data_granularity,
+            "verbose": False,
+            "trade_type": SimulatorExecutor.TT_PARAL if cash_limit is not None else SimulatorExecutor.TT_SERIAL,
+            "generate_report": generate_report,
+            "track_data": True,
+        },
+    }
+
+    freqs = list(strategy_config.keys())
+    freqs.sort(key=pd.Timedelta)
+    for freq in freqs:
+        executor_config = {
+            "class": "NestedExecutor",
+            "module_path": "qlib.backtest.executor",
+            "kwargs": {
+                "time_per_step": freq,
+                "inner_strategy": strategy_config[freq],
+                "inner_executor": executor_config,
+                "track_data": True,
+            },
+        }
+
+    return executor_config
+
+
+def _convert_indicator_to_dataframe(indicator: dict) -> Optional[pd.DataFrame]:
+    record_list = []
+    for time, value_dict in indicator.items():
+        if isinstance(value_dict, BaseOrderIndicator):
+            # HACK: for qlib v0.8
+            value_dict = value_dict.to_series()
+        try:
+            value_dict = copy.deepcopy(value_dict)
+            if value_dict["ffr"].empty:
+                continue
+        except Exception:
+            value_dict = {k: v for k, v in value_dict.items() if k != "pa"}
+        value_dict = pd.DataFrame(value_dict)
+        value_dict["datetime"] = time
+        record_list.append(value_dict)
+
+    if not record_list:
+        return None
+
+    records: pd.DataFrame = pd.concat(record_list, 0).reset_index().rename(columns={"index": "instrument"})
+    records = records.set_index(["instrument", "datetime"])
+    return records
+
+
+def _generate_report(
+    decisions: List[BaseTradeDecision],
+    report_indicators: List[INDICATOR_METRIC],
+) -> Dict[str, Tuple[pd.DataFrame, pd.DataFrame]]:
+    """Generate backtest reports
+
+    Parameters
+    ----------
+    decisions:
+        List of trade decisions.
+    report_indicators
+        List of indicator reports.
+    Returns
+    -------
+
+    """
+    indicator_dict: Dict[str, List[pd.DataFrame]] = defaultdict(list)
+    indicator_his: Dict[str, List[dict]] = defaultdict(list)
+
+    for report_indicator in report_indicators:
+        for key, (indicator_df, indicator_obj) in report_indicator.items():
+            indicator_dict[key].append(indicator_df)
+            indicator_his[key].append(indicator_obj.order_indicator_his)
+
+    report = {}
+    decision_details = pd.concat([getattr(d, "details") for d in decisions if hasattr(d, "details")])
+    for key in indicator_dict:
+        cur_dict = pd.concat(indicator_dict[key])
+        cur_his = pd.concat([_convert_indicator_to_dataframe(his) for his in indicator_his[key]])
+        cur_details = decision_details[decision_details.freq == key].set_index(["instrument", "datetime"])
+        if len(cur_details) > 0:
+            cur_details.pop("freq")
+            cur_his = cur_his.join(cur_details, how="outer")
+
+        report[key] = (cur_dict, cur_his)
+
+    return report
+
+
+def single_with_simulator(
+    backtest_config: dict,
+    orders: pd.DataFrame,
+    split: Literal["stock", "day"] = "stock",
+    cash_limit: float | None = None,
+    generate_report: bool = False,
+) -> Union[Tuple[pd.DataFrame, dict], pd.DataFrame]:
+    """Run backtest in a single thread with SingleAssetOrderExecution simulator. The orders will be executed day by day.
+    A new simulator will be created and used for every single-day order.
+
+    Parameters
+    ----------
+    backtest_config:
+        Backtest config
+    orders:
+        Orders to be executed. Example format:
+                 datetime instrument  amount  direction
+            0  2020-06-01       INST   600.0          0
+            1  2020-06-02       INST   700.0          1
+            ...
+    split
+        Method to split orders. If it is "stock", split orders by stock. If it is "day", split orders by date.
+    cash_limit
+        Limitation of cash.
+    generate_report
+        Whether to generate reports.
+
+    Returns
+    -------
+        If generate_report is True, return execution records and the generated report. Otherwise, return only records.
+    """
+    init_qlib(backtest_config["qlib"])
+
+    stocks = orders.instrument.unique().tolist()
+
+    reports = []
+    decisions = []
+    for _, row in orders.iterrows():
+        date = pd.Timestamp(row["datetime"])
+        start_time = pd.Timestamp(backtest_config["start_time"]).replace(year=date.year, month=date.month, day=date.day)
+        end_time = pd.Timestamp(backtest_config["end_time"]).replace(year=date.year, month=date.month, day=date.day)
+        order = Order(
+            stock_id=row["instrument"],
+            amount=row["amount"],
+            direction=OrderDir(row["direction"]),
+            start_time=start_time,
+            end_time=end_time,
+        )
+
+        executor_config = _get_multi_level_executor_config(
+            strategy_config=backtest_config["strategies"],
+            cash_limit=cash_limit,
+            generate_report=generate_report,
+            data_granularity=backtest_config["data_granularity"],
+        )
+
+        exchange_config = copy.deepcopy(backtest_config["exchange"])
+        exchange_config.update(
+            {
+                "codes": stocks,
+                "freq": backtest_config["data_granularity"],
+            }
+        )
+
+        simulator = SingleAssetOrderExecution(
+            order=order,
+            executor_config=executor_config,
+            exchange_config=exchange_config,
+            qlib_config=None,
+            cash_limit=None,
+        )
+
+        reports.append(simulator.report_dict)
+        decisions += simulator.decisions
+
+    indicator_1day_objs = [report["indicator_dict"]["1day"][1] for report in reports]
+    indicator_info = {k: v for obj in indicator_1day_objs for k, v in obj.order_indicator_his.items()}
+    records = _convert_indicator_to_dataframe(indicator_info)
+    assert records is None or not np.isnan(records["ffr"]).any()
+
+    if generate_report:
+        _report = _generate_report(decisions, [report["indicator"] for report in reports])
+
+        if split == "stock":
+            stock_id = orders.iloc[0].instrument
+            report = {stock_id: _report}
+        else:
+            day = orders.iloc[0].datetime
+            report = {day: _report}
+
+        return records, report
+    else:
+        return records
+
+
+def single_with_collect_data_loop(
+    backtest_config: dict,
+    orders: pd.DataFrame,
+    split: Literal["stock", "day"] = "stock",
+    cash_limit: float | None = None,
+    generate_report: bool = False,
+) -> Union[Tuple[pd.DataFrame, dict], pd.DataFrame]:
+    """Run backtest in a single thread with collect_data_loop.
+
+    Parameters
+    ----------
+    backtest_config:
+        Backtest config
+    orders:
+        Orders to be executed. Example format:
+                 datetime instrument  amount  direction
+            0  2020-06-01       INST   600.0          0
+            1  2020-06-02       INST   700.0          1
+            ...
+    split
+        Method to split orders. If it is "stock", split orders by stock. If it is "day", split orders by date.
+    cash_limit
+        Limitation of cash.
+    generate_report
+        Whether to generate reports.
+
+    Returns
+    -------
+        If generate_report is True, return execution records and the generated report. Otherwise, return only records.
+    """
+
+    init_qlib(backtest_config["qlib"])
+
+    trade_start_time = orders["datetime"].min()
+    trade_end_time = orders["datetime"].max()
+    stocks = orders.instrument.unique().tolist()
+
+    strategy_config = {
+        "class": "FileOrderStrategy",
+        "module_path": "qlib.contrib.strategy.rule_strategy",
+        "kwargs": {
+            "file": orders,
+            "trade_range": TradeRangeByTime(
+                pd.Timestamp(backtest_config["start_time"]).time(),
+                pd.Timestamp(backtest_config["end_time"]).time(),
+            ),
+        },
+    }
+
+    executor_config = _get_multi_level_executor_config(
+        strategy_config=backtest_config["strategies"],
+        cash_limit=cash_limit,
+        generate_report=generate_report,
+        data_granularity=backtest_config["data_granularity"],
+    )
+
+    exchange_config = copy.deepcopy(backtest_config["exchange"])
+    exchange_config.update(
+        {
+            "codes": stocks,
+            "freq": backtest_config["data_granularity"],
+        }
+    )
+
+    strategy, executor = get_strategy_executor(
+        start_time=pd.Timestamp(trade_start_time),
+        end_time=pd.Timestamp(trade_end_time) + pd.DateOffset(1),
+        strategy=strategy_config,
+        executor=executor_config,
+        benchmark=None,
+        account=cash_limit if cash_limit is not None else int(1e12),
+        exchange_kwargs=exchange_config,
+        pos_type="Position" if cash_limit is not None else "InfPosition",
+    )
+
+    report_dict: dict = {}
+    decisions = list(collect_data_loop(trade_start_time, trade_end_time, strategy, executor, report_dict))
+
+    indicator_dict = cast(INDICATOR_METRIC, report_dict.get("indicator_dict"))
+    records = _convert_indicator_to_dataframe(indicator_dict["1day"][1].order_indicator_his)
+    assert records is None or not np.isnan(records["ffr"]).any()
+
+    if generate_report:
+        _report = _generate_report(decisions, [indicator_dict])
+        if split == "stock":
+            stock_id = orders.iloc[0].instrument
+            report = {stock_id: _report}
+        else:
+            day = orders.iloc[0].datetime
+            report = {day: _report}
+        return records, report
+    else:
+        return records
+
+
+def backtest(backtest_config: dict, with_simulator: bool = False) -> pd.DataFrame:
+    order_df = read_order_file(backtest_config["order_file"])
+
+    cash_limit = backtest_config["exchange"].pop("cash_limit")
+    generate_report = backtest_config.pop("generate_report")
+
+    stock_pool = order_df["instrument"].unique().tolist()
+    stock_pool.sort()
+
+    single = single_with_simulator if with_simulator else single_with_collect_data_loop
+    mp_config = {"n_jobs": backtest_config["concurrency"], "verbose": 10, "backend": "multiprocessing"}
+    torch.set_num_threads(1)  # https://github.com/pytorch/pytorch/issues/17199
+    res = Parallel(**mp_config)(
+        delayed(single)(
+            backtest_config=backtest_config,
+            orders=order_df[order_df["instrument"] == stock].copy(),
+            split="stock",
+            cash_limit=cash_limit,
+            generate_report=generate_report,
+        )
+        for stock in stock_pool
+    )
+
+    output_path = Path(backtest_config["output_dir"])
+    if generate_report:
+        with (output_path / "report.pkl").open("wb") as f:
+            report = {}
+            for r in res:
+                report.update(r[1])
+            pickle.dump(report, f)
+        res = pd.concat([r[0] for r in res], 0)
+    else:
+        res = pd.concat(res)
+
+    if not output_path.exists():
+        os.makedirs(output_path)
+
+    if "pa" in res.columns:
+        res["pa"] = res["pa"] * 10000.0  # align with training metrics
+    res.to_csv(output_path / "backtest_result.csv")
+    return res
+
+
+if __name__ == "__main__":
+    import warnings
+
+    warnings.filterwarnings("ignore", category=DeprecationWarning)
+    warnings.filterwarnings("ignore", category=RuntimeWarning)
+
+    parser = argparse.ArgumentParser()
+    parser.add_argument("--config_path", type=str, required=True, help="Path to the config file")
+    parser.add_argument("--use_simulator", action="store_true", help="Whether to use simulator as the backend")
+    parser.add_argument(
+        "--n_jobs",
+        type=int,
+        required=False,
+        help="The number of jobs for running backtest parallely(1 for single process)",
+    )
+    args = parser.parse_args()
+
+    config = get_backtest_config_fromfile(args.config_path)
+    if args.n_jobs is not None:
+        config["concurrency"] = args.n_jobs
+
+    backtest(
+        backtest_config=config,
+        with_simulator=args.use_simulator,
+    )
```

## qlib/rl/contrib/naive_config_parser.py

 * *Ordering differences only*

```diff
@@ -1,107 +1,107 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-import os
-import platform
-import shutil
-import sys
-import tempfile
-from importlib import import_module
-
-import yaml
-
-
-DELETE_KEY = "_delete_"
-
-
-def merge_a_into_b(a: dict, b: dict) -> dict:
-    b = b.copy()
-    for k, v in a.items():
-        if isinstance(v, dict) and k in b:
-            v.pop(DELETE_KEY, False)
-            b[k] = merge_a_into_b(v, b[k])
-        else:
-            b[k] = v
-    return b
-
-
-def check_file_exist(filename: str, msg_tmpl: str = 'file "{}" does not exist') -> None:
-    if not os.path.isfile(filename):
-        raise FileNotFoundError(msg_tmpl.format(filename))
-
-
-def parse_backtest_config(path: str) -> dict:
-    abs_path = os.path.abspath(path)
-    check_file_exist(abs_path)
-
-    file_ext_name = os.path.splitext(abs_path)[1]
-    if file_ext_name not in (".py", ".json", ".yaml", ".yml"):
-        raise IOError("Only py/yml/yaml/json type are supported now!")
-
-    with tempfile.TemporaryDirectory() as tmp_config_dir:
-        with tempfile.NamedTemporaryFile(dir=tmp_config_dir, suffix=file_ext_name) as tmp_config_file:
-            if platform.system() == "Windows":
-                tmp_config_file.close()
-
-            tmp_config_name = os.path.basename(tmp_config_file.name)
-            shutil.copyfile(abs_path, tmp_config_file.name)
-
-            if abs_path.endswith(".py"):
-                tmp_module_name = os.path.splitext(tmp_config_name)[0]
-                sys.path.insert(0, tmp_config_dir)
-                module = import_module(tmp_module_name)
-                sys.path.pop(0)
-
-                config = {k: v for k, v in module.__dict__.items() if not k.startswith("__")}
-
-                del sys.modules[tmp_module_name]
-            else:
-                with open(tmp_config_file.name) as input_stream:
-                    config = yaml.safe_load(input_stream)
-
-    if "_base_" in config:
-        base_file_name = config.pop("_base_")
-        if not isinstance(base_file_name, list):
-            base_file_name = [base_file_name]
-
-        for f in base_file_name:
-            base_config = parse_backtest_config(os.path.join(os.path.dirname(abs_path), f))
-            config = merge_a_into_b(a=config, b=base_config)
-
-    return config
-
-
-def _convert_all_list_to_tuple(config: dict) -> dict:
-    for k, v in config.items():
-        if isinstance(v, list):
-            config[k] = tuple(v)
-        elif isinstance(v, dict):
-            config[k] = _convert_all_list_to_tuple(v)
-    return config
-
-
-def get_backtest_config_fromfile(path: str) -> dict:
-    backtest_config = parse_backtest_config(path)
-
-    exchange_config_default = {
-        "open_cost": 0.0005,
-        "close_cost": 0.0015,
-        "min_cost": 5.0,
-        "trade_unit": 100.0,
-        "cash_limit": None,
-    }
-    backtest_config["exchange"] = merge_a_into_b(a=backtest_config["exchange"], b=exchange_config_default)
-    backtest_config["exchange"] = _convert_all_list_to_tuple(backtest_config["exchange"])
-
-    backtest_config_default = {
-        "debug_single_stock": None,
-        "debug_single_day": None,
-        "concurrency": -1,
-        "multiplier": 1.0,
-        "output_dir": "outputs_backtest/",
-        "generate_report": False,
-        "data_granularity": "1min",
-    }
-    backtest_config = merge_a_into_b(a=backtest_config, b=backtest_config_default)
-
-    return backtest_config
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+import os
+import platform
+import shutil
+import sys
+import tempfile
+from importlib import import_module
+
+import yaml
+
+
+DELETE_KEY = "_delete_"
+
+
+def merge_a_into_b(a: dict, b: dict) -> dict:
+    b = b.copy()
+    for k, v in a.items():
+        if isinstance(v, dict) and k in b:
+            v.pop(DELETE_KEY, False)
+            b[k] = merge_a_into_b(v, b[k])
+        else:
+            b[k] = v
+    return b
+
+
+def check_file_exist(filename: str, msg_tmpl: str = 'file "{}" does not exist') -> None:
+    if not os.path.isfile(filename):
+        raise FileNotFoundError(msg_tmpl.format(filename))
+
+
+def parse_backtest_config(path: str) -> dict:
+    abs_path = os.path.abspath(path)
+    check_file_exist(abs_path)
+
+    file_ext_name = os.path.splitext(abs_path)[1]
+    if file_ext_name not in (".py", ".json", ".yaml", ".yml"):
+        raise IOError("Only py/yml/yaml/json type are supported now!")
+
+    with tempfile.TemporaryDirectory() as tmp_config_dir:
+        with tempfile.NamedTemporaryFile(dir=tmp_config_dir, suffix=file_ext_name) as tmp_config_file:
+            if platform.system() == "Windows":
+                tmp_config_file.close()
+
+            tmp_config_name = os.path.basename(tmp_config_file.name)
+            shutil.copyfile(abs_path, tmp_config_file.name)
+
+            if abs_path.endswith(".py"):
+                tmp_module_name = os.path.splitext(tmp_config_name)[0]
+                sys.path.insert(0, tmp_config_dir)
+                module = import_module(tmp_module_name)
+                sys.path.pop(0)
+
+                config = {k: v for k, v in module.__dict__.items() if not k.startswith("__")}
+
+                del sys.modules[tmp_module_name]
+            else:
+                with open(tmp_config_file.name) as input_stream:
+                    config = yaml.safe_load(input_stream)
+
+    if "_base_" in config:
+        base_file_name = config.pop("_base_")
+        if not isinstance(base_file_name, list):
+            base_file_name = [base_file_name]
+
+        for f in base_file_name:
+            base_config = parse_backtest_config(os.path.join(os.path.dirname(abs_path), f))
+            config = merge_a_into_b(a=config, b=base_config)
+
+    return config
+
+
+def _convert_all_list_to_tuple(config: dict) -> dict:
+    for k, v in config.items():
+        if isinstance(v, list):
+            config[k] = tuple(v)
+        elif isinstance(v, dict):
+            config[k] = _convert_all_list_to_tuple(v)
+    return config
+
+
+def get_backtest_config_fromfile(path: str) -> dict:
+    backtest_config = parse_backtest_config(path)
+
+    exchange_config_default = {
+        "open_cost": 0.0005,
+        "close_cost": 0.0015,
+        "min_cost": 5.0,
+        "trade_unit": 100.0,
+        "cash_limit": None,
+    }
+    backtest_config["exchange"] = merge_a_into_b(a=backtest_config["exchange"], b=exchange_config_default)
+    backtest_config["exchange"] = _convert_all_list_to_tuple(backtest_config["exchange"])
+
+    backtest_config_default = {
+        "debug_single_stock": None,
+        "debug_single_day": None,
+        "concurrency": -1,
+        "multiplier": 1.0,
+        "output_dir": "outputs_backtest/",
+        "generate_report": False,
+        "data_granularity": "1min",
+    }
+    backtest_config = merge_a_into_b(a=backtest_config, b=backtest_config_default)
+
+    return backtest_config
```

## qlib/rl/contrib/train_onpolicy.py

 * *Ordering differences only*

```diff
@@ -1,268 +1,268 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-from __future__ import annotations
-
-import argparse
-import os
-import random
-import sys
-import warnings
-from pathlib import Path
-from typing import cast, List, Optional
-
-import numpy as np
-import pandas as pd
-import torch
-import yaml
-from qlib.backtest import Order
-from qlib.backtest.decision import OrderDir
-from qlib.constant import ONE_MIN
-from qlib.rl.data.native import load_handler_intraday_processed_data
-from qlib.rl.interpreter import ActionInterpreter, StateInterpreter
-from qlib.rl.order_execution import SingleAssetOrderExecutionSimple
-from qlib.rl.reward import Reward
-from qlib.rl.trainer import Checkpoint, backtest, train
-from qlib.rl.trainer.callbacks import Callback, EarlyStopping, MetricsWriter
-from qlib.rl.utils.log import CsvWriter
-from qlib.utils import init_instance_by_config
-from tianshou.policy import BasePolicy
-from torch.utils.data import Dataset
-
-
-def seed_everything(seed: int) -> None:
-    torch.manual_seed(seed)
-    torch.cuda.manual_seed_all(seed)
-    np.random.seed(seed)
-    random.seed(seed)
-    torch.backends.cudnn.deterministic = True
-
-
-def _read_orders(order_dir: Path) -> pd.DataFrame:
-    if os.path.isfile(order_dir):
-        return pd.read_pickle(order_dir)
-    else:
-        orders = []
-        for file in order_dir.iterdir():
-            order_data = pd.read_pickle(file)
-            orders.append(order_data)
-        return pd.concat(orders)
-
-
-class LazyLoadDataset(Dataset):
-    def __init__(
-        self,
-        data_dir: str,
-        order_file_path: Path,
-        default_start_time_index: int,
-        default_end_time_index: int,
-    ) -> None:
-        self._default_start_time_index = default_start_time_index
-        self._default_end_time_index = default_end_time_index
-
-        self._order_df = _read_orders(order_file_path).reset_index()
-        self._ticks_index: Optional[pd.DatetimeIndex] = None
-        self._data_dir = Path(data_dir)
-
-    def __len__(self) -> int:
-        return len(self._order_df)
-
-    def __getitem__(self, index: int) -> Order:
-        row = self._order_df.iloc[index]
-        date = pd.Timestamp(str(row["date"]))
-
-        if self._ticks_index is None:
-            # TODO: We only load ticks index once based on the assumption that ticks index of different dates
-            # TODO: in one experiment are all the same. If that assumption is not hold, we need to load ticks index
-            # TODO: of all dates.
-
-            data = load_handler_intraday_processed_data(
-                data_dir=self._data_dir,
-                stock_id=row["instrument"],
-                date=date,
-                feature_columns_today=[],
-                feature_columns_yesterday=[],
-                backtest=True,
-                index_only=True,
-            )
-            self._ticks_index = [t - date for t in data.today.index]
-
-        order = Order(
-            stock_id=row["instrument"],
-            amount=row["amount"],
-            direction=OrderDir(int(row["order_type"])),
-            start_time=date + self._ticks_index[self._default_start_time_index],
-            end_time=date + self._ticks_index[self._default_end_time_index - 1] + ONE_MIN,
-        )
-
-        return order
-
-
-def train_and_test(
-    env_config: dict,
-    simulator_config: dict,
-    trainer_config: dict,
-    data_config: dict,
-    state_interpreter: StateInterpreter,
-    action_interpreter: ActionInterpreter,
-    policy: BasePolicy,
-    reward: Reward,
-    run_training: bool,
-    run_backtest: bool,
-) -> None:
-    order_root_path = Path(data_config["source"]["order_dir"])
-
-    data_granularity = simulator_config.get("data_granularity", 1)
-
-    def _simulator_factory_simple(order: Order) -> SingleAssetOrderExecutionSimple:
-        return SingleAssetOrderExecutionSimple(
-            order=order,
-            data_dir=data_config["source"]["feature_root_dir"],
-            feature_columns_today=data_config["source"]["feature_columns_today"],
-            feature_columns_yesterday=data_config["source"]["feature_columns_yesterday"],
-            data_granularity=data_granularity,
-            ticks_per_step=simulator_config["time_per_step"],
-            vol_threshold=simulator_config["vol_limit"],
-        )
-
-    assert data_config["source"]["default_start_time_index"] % data_granularity == 0
-    assert data_config["source"]["default_end_time_index"] % data_granularity == 0
-
-    if run_training:
-        train_dataset, valid_dataset = [
-            LazyLoadDataset(
-                data_dir=data_config["source"]["feature_root_dir"],
-                order_file_path=order_root_path / tag,
-                default_start_time_index=data_config["source"]["default_start_time_index"] // data_granularity,
-                default_end_time_index=data_config["source"]["default_end_time_index"] // data_granularity,
-            )
-            for tag in ("train", "valid")
-        ]
-
-        callbacks: List[Callback] = []
-        if "checkpoint_path" in trainer_config:
-            callbacks.append(MetricsWriter(dirpath=Path(trainer_config["checkpoint_path"])))
-            callbacks.append(
-                Checkpoint(
-                    dirpath=Path(trainer_config["checkpoint_path"]) / "checkpoints",
-                    every_n_iters=trainer_config.get("checkpoint_every_n_iters", 1),
-                    save_latest="copy",
-                ),
-            )
-        if "earlystop_patience" in trainer_config:
-            callbacks.append(
-                EarlyStopping(
-                    patience=trainer_config["earlystop_patience"],
-                    monitor="val/pa",
-                )
-            )
-
-        train(
-            simulator_fn=_simulator_factory_simple,
-            state_interpreter=state_interpreter,
-            action_interpreter=action_interpreter,
-            policy=policy,
-            reward=reward,
-            initial_states=cast(List[Order], train_dataset),
-            trainer_kwargs={
-                "max_iters": trainer_config["max_epoch"],
-                "finite_env_type": env_config["parallel_mode"],
-                "concurrency": env_config["concurrency"],
-                "val_every_n_iters": trainer_config.get("val_every_n_epoch", None),
-                "callbacks": callbacks,
-            },
-            vessel_kwargs={
-                "episode_per_iter": trainer_config["episode_per_collect"],
-                "update_kwargs": {
-                    "batch_size": trainer_config["batch_size"],
-                    "repeat": trainer_config["repeat_per_collect"],
-                },
-                "val_initial_states": valid_dataset,
-            },
-        )
-
-    if run_backtest:
-        test_dataset = LazyLoadDataset(
-            data_dir=data_config["source"]["feature_root_dir"],
-            order_file_path=order_root_path / "test",
-            default_start_time_index=data_config["source"]["default_start_time_index"] // data_granularity,
-            default_end_time_index=data_config["source"]["default_end_time_index"] // data_granularity,
-        )
-
-        backtest(
-            simulator_fn=_simulator_factory_simple,
-            state_interpreter=state_interpreter,
-            action_interpreter=action_interpreter,
-            initial_states=test_dataset,
-            policy=policy,
-            logger=CsvWriter(Path(trainer_config["checkpoint_path"])),
-            reward=reward,
-            finite_env_type=env_config["parallel_mode"],
-            concurrency=env_config["concurrency"],
-        )
-
-
-def main(config: dict, run_training: bool, run_backtest: bool) -> None:
-    if not run_training and not run_backtest:
-        warnings.warn("Skip the entire job since training and backtest are both skipped.")
-        return
-
-    if "seed" in config["runtime"]:
-        seed_everything(config["runtime"]["seed"])
-
-    for extra_module_path in config["env"].get("extra_module_paths", []):
-        sys.path.append(extra_module_path)
-
-    state_interpreter: StateInterpreter = init_instance_by_config(config["state_interpreter"])
-    action_interpreter: ActionInterpreter = init_instance_by_config(config["action_interpreter"])
-    reward: Reward = init_instance_by_config(config["reward"])
-
-    additional_policy_kwargs = {
-        "obs_space": state_interpreter.observation_space,
-        "action_space": action_interpreter.action_space,
-    }
-
-    # Create torch network
-    if "network" in config:
-        if "kwargs" not in config["network"]:
-            config["network"]["kwargs"] = {}
-        config["network"]["kwargs"].update({"obs_space": state_interpreter.observation_space})
-        additional_policy_kwargs["network"] = init_instance_by_config(config["network"])
-
-    # Create policy
-    if "kwargs" not in config["policy"]:
-        config["policy"]["kwargs"] = {}
-    config["policy"]["kwargs"].update(additional_policy_kwargs)
-    policy: BasePolicy = init_instance_by_config(config["policy"])
-
-    use_cuda = config["runtime"].get("use_cuda", False)
-    if use_cuda:
-        policy.cuda()
-
-    train_and_test(
-        env_config=config["env"],
-        simulator_config=config["simulator"],
-        data_config=config["data"],
-        trainer_config=config["trainer"],
-        action_interpreter=action_interpreter,
-        state_interpreter=state_interpreter,
-        policy=policy,
-        reward=reward,
-        run_training=run_training,
-        run_backtest=run_backtest,
-    )
-
-
-if __name__ == "__main__":
-    warnings.filterwarnings("ignore", category=DeprecationWarning)
-    warnings.filterwarnings("ignore", category=RuntimeWarning)
-
-    parser = argparse.ArgumentParser()
-    parser.add_argument("--config_path", type=str, required=True, help="Path to the config file")
-    parser.add_argument("--no_training", action="store_true", help="Skip training workflow.")
-    parser.add_argument("--run_backtest", action="store_true", help="Run backtest workflow.")
-    args = parser.parse_args()
-
-    with open(args.config_path, "r") as input_stream:
-        config = yaml.safe_load(input_stream)
-
-    main(config, run_training=not args.no_training, run_backtest=args.run_backtest)
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+from __future__ import annotations
+
+import argparse
+import os
+import random
+import sys
+import warnings
+from pathlib import Path
+from typing import cast, List, Optional
+
+import numpy as np
+import pandas as pd
+import torch
+import yaml
+from qlib.backtest import Order
+from qlib.backtest.decision import OrderDir
+from qlib.constant import ONE_MIN
+from qlib.rl.data.native import load_handler_intraday_processed_data
+from qlib.rl.interpreter import ActionInterpreter, StateInterpreter
+from qlib.rl.order_execution import SingleAssetOrderExecutionSimple
+from qlib.rl.reward import Reward
+from qlib.rl.trainer import Checkpoint, backtest, train
+from qlib.rl.trainer.callbacks import Callback, EarlyStopping, MetricsWriter
+from qlib.rl.utils.log import CsvWriter
+from qlib.utils import init_instance_by_config
+from tianshou.policy import BasePolicy
+from torch.utils.data import Dataset
+
+
+def seed_everything(seed: int) -> None:
+    torch.manual_seed(seed)
+    torch.cuda.manual_seed_all(seed)
+    np.random.seed(seed)
+    random.seed(seed)
+    torch.backends.cudnn.deterministic = True
+
+
+def _read_orders(order_dir: Path) -> pd.DataFrame:
+    if os.path.isfile(order_dir):
+        return pd.read_pickle(order_dir)
+    else:
+        orders = []
+        for file in order_dir.iterdir():
+            order_data = pd.read_pickle(file)
+            orders.append(order_data)
+        return pd.concat(orders)
+
+
+class LazyLoadDataset(Dataset):
+    def __init__(
+        self,
+        data_dir: str,
+        order_file_path: Path,
+        default_start_time_index: int,
+        default_end_time_index: int,
+    ) -> None:
+        self._default_start_time_index = default_start_time_index
+        self._default_end_time_index = default_end_time_index
+
+        self._order_df = _read_orders(order_file_path).reset_index()
+        self._ticks_index: Optional[pd.DatetimeIndex] = None
+        self._data_dir = Path(data_dir)
+
+    def __len__(self) -> int:
+        return len(self._order_df)
+
+    def __getitem__(self, index: int) -> Order:
+        row = self._order_df.iloc[index]
+        date = pd.Timestamp(str(row["date"]))
+
+        if self._ticks_index is None:
+            # TODO: We only load ticks index once based on the assumption that ticks index of different dates
+            # TODO: in one experiment are all the same. If that assumption is not hold, we need to load ticks index
+            # TODO: of all dates.
+
+            data = load_handler_intraday_processed_data(
+                data_dir=self._data_dir,
+                stock_id=row["instrument"],
+                date=date,
+                feature_columns_today=[],
+                feature_columns_yesterday=[],
+                backtest=True,
+                index_only=True,
+            )
+            self._ticks_index = [t - date for t in data.today.index]
+
+        order = Order(
+            stock_id=row["instrument"],
+            amount=row["amount"],
+            direction=OrderDir(int(row["order_type"])),
+            start_time=date + self._ticks_index[self._default_start_time_index],
+            end_time=date + self._ticks_index[self._default_end_time_index - 1] + ONE_MIN,
+        )
+
+        return order
+
+
+def train_and_test(
+    env_config: dict,
+    simulator_config: dict,
+    trainer_config: dict,
+    data_config: dict,
+    state_interpreter: StateInterpreter,
+    action_interpreter: ActionInterpreter,
+    policy: BasePolicy,
+    reward: Reward,
+    run_training: bool,
+    run_backtest: bool,
+) -> None:
+    order_root_path = Path(data_config["source"]["order_dir"])
+
+    data_granularity = simulator_config.get("data_granularity", 1)
+
+    def _simulator_factory_simple(order: Order) -> SingleAssetOrderExecutionSimple:
+        return SingleAssetOrderExecutionSimple(
+            order=order,
+            data_dir=data_config["source"]["feature_root_dir"],
+            feature_columns_today=data_config["source"]["feature_columns_today"],
+            feature_columns_yesterday=data_config["source"]["feature_columns_yesterday"],
+            data_granularity=data_granularity,
+            ticks_per_step=simulator_config["time_per_step"],
+            vol_threshold=simulator_config["vol_limit"],
+        )
+
+    assert data_config["source"]["default_start_time_index"] % data_granularity == 0
+    assert data_config["source"]["default_end_time_index"] % data_granularity == 0
+
+    if run_training:
+        train_dataset, valid_dataset = [
+            LazyLoadDataset(
+                data_dir=data_config["source"]["feature_root_dir"],
+                order_file_path=order_root_path / tag,
+                default_start_time_index=data_config["source"]["default_start_time_index"] // data_granularity,
+                default_end_time_index=data_config["source"]["default_end_time_index"] // data_granularity,
+            )
+            for tag in ("train", "valid")
+        ]
+
+        callbacks: List[Callback] = []
+        if "checkpoint_path" in trainer_config:
+            callbacks.append(MetricsWriter(dirpath=Path(trainer_config["checkpoint_path"])))
+            callbacks.append(
+                Checkpoint(
+                    dirpath=Path(trainer_config["checkpoint_path"]) / "checkpoints",
+                    every_n_iters=trainer_config.get("checkpoint_every_n_iters", 1),
+                    save_latest="copy",
+                ),
+            )
+        if "earlystop_patience" in trainer_config:
+            callbacks.append(
+                EarlyStopping(
+                    patience=trainer_config["earlystop_patience"],
+                    monitor="val/pa",
+                )
+            )
+
+        train(
+            simulator_fn=_simulator_factory_simple,
+            state_interpreter=state_interpreter,
+            action_interpreter=action_interpreter,
+            policy=policy,
+            reward=reward,
+            initial_states=cast(List[Order], train_dataset),
+            trainer_kwargs={
+                "max_iters": trainer_config["max_epoch"],
+                "finite_env_type": env_config["parallel_mode"],
+                "concurrency": env_config["concurrency"],
+                "val_every_n_iters": trainer_config.get("val_every_n_epoch", None),
+                "callbacks": callbacks,
+            },
+            vessel_kwargs={
+                "episode_per_iter": trainer_config["episode_per_collect"],
+                "update_kwargs": {
+                    "batch_size": trainer_config["batch_size"],
+                    "repeat": trainer_config["repeat_per_collect"],
+                },
+                "val_initial_states": valid_dataset,
+            },
+        )
+
+    if run_backtest:
+        test_dataset = LazyLoadDataset(
+            data_dir=data_config["source"]["feature_root_dir"],
+            order_file_path=order_root_path / "test",
+            default_start_time_index=data_config["source"]["default_start_time_index"] // data_granularity,
+            default_end_time_index=data_config["source"]["default_end_time_index"] // data_granularity,
+        )
+
+        backtest(
+            simulator_fn=_simulator_factory_simple,
+            state_interpreter=state_interpreter,
+            action_interpreter=action_interpreter,
+            initial_states=test_dataset,
+            policy=policy,
+            logger=CsvWriter(Path(trainer_config["checkpoint_path"])),
+            reward=reward,
+            finite_env_type=env_config["parallel_mode"],
+            concurrency=env_config["concurrency"],
+        )
+
+
+def main(config: dict, run_training: bool, run_backtest: bool) -> None:
+    if not run_training and not run_backtest:
+        warnings.warn("Skip the entire job since training and backtest are both skipped.")
+        return
+
+    if "seed" in config["runtime"]:
+        seed_everything(config["runtime"]["seed"])
+
+    for extra_module_path in config["env"].get("extra_module_paths", []):
+        sys.path.append(extra_module_path)
+
+    state_interpreter: StateInterpreter = init_instance_by_config(config["state_interpreter"])
+    action_interpreter: ActionInterpreter = init_instance_by_config(config["action_interpreter"])
+    reward: Reward = init_instance_by_config(config["reward"])
+
+    additional_policy_kwargs = {
+        "obs_space": state_interpreter.observation_space,
+        "action_space": action_interpreter.action_space,
+    }
+
+    # Create torch network
+    if "network" in config:
+        if "kwargs" not in config["network"]:
+            config["network"]["kwargs"] = {}
+        config["network"]["kwargs"].update({"obs_space": state_interpreter.observation_space})
+        additional_policy_kwargs["network"] = init_instance_by_config(config["network"])
+
+    # Create policy
+    if "kwargs" not in config["policy"]:
+        config["policy"]["kwargs"] = {}
+    config["policy"]["kwargs"].update(additional_policy_kwargs)
+    policy: BasePolicy = init_instance_by_config(config["policy"])
+
+    use_cuda = config["runtime"].get("use_cuda", False)
+    if use_cuda:
+        policy.cuda()
+
+    train_and_test(
+        env_config=config["env"],
+        simulator_config=config["simulator"],
+        data_config=config["data"],
+        trainer_config=config["trainer"],
+        action_interpreter=action_interpreter,
+        state_interpreter=state_interpreter,
+        policy=policy,
+        reward=reward,
+        run_training=run_training,
+        run_backtest=run_backtest,
+    )
+
+
+if __name__ == "__main__":
+    warnings.filterwarnings("ignore", category=DeprecationWarning)
+    warnings.filterwarnings("ignore", category=RuntimeWarning)
+
+    parser = argparse.ArgumentParser()
+    parser.add_argument("--config_path", type=str, required=True, help="Path to the config file")
+    parser.add_argument("--no_training", action="store_true", help="Skip training workflow.")
+    parser.add_argument("--run_backtest", action="store_true", help="Run backtest workflow.")
+    args = parser.parse_args()
+
+    with open(args.config_path, "r") as input_stream:
+        config = yaml.safe_load(input_stream)
+
+    main(config, run_training=not args.no_training, run_backtest=args.run_backtest)
```

## qlib/rl/contrib/utils.py

 * *Ordering differences only*

```diff
@@ -1,29 +1,29 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-from __future__ import annotations
-
-from pathlib import Path
-
-import pandas as pd
-
-
-def read_order_file(order_file: Path | pd.DataFrame) -> pd.DataFrame:
-    if isinstance(order_file, pd.DataFrame):
-        return order_file
-
-    order_file = Path(order_file)
-
-    if order_file.suffix == ".pkl":
-        order_df = pd.read_pickle(order_file).reset_index()
-    elif order_file.suffix == ".csv":
-        order_df = pd.read_csv(order_file)
-    else:
-        raise TypeError(f"Unsupported order file type: {order_file}")
-
-    if "date" in order_df.columns:
-        # legacy dataframe columns
-        order_df = order_df.rename(columns={"date": "datetime", "order_type": "direction"})
-    order_df["datetime"] = order_df["datetime"].astype(str)
-
-    return order_df
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+from __future__ import annotations
+
+from pathlib import Path
+
+import pandas as pd
+
+
+def read_order_file(order_file: Path | pd.DataFrame) -> pd.DataFrame:
+    if isinstance(order_file, pd.DataFrame):
+        return order_file
+
+    order_file = Path(order_file)
+
+    if order_file.suffix == ".pkl":
+        order_df = pd.read_pickle(order_file).reset_index()
+    elif order_file.suffix == ".csv":
+        order_df = pd.read_csv(order_file)
+    else:
+        raise TypeError(f"Unsupported order file type: {order_file}")
+
+    if "date" in order_df.columns:
+        # legacy dataframe columns
+        order_df = order_df.rename(columns={"date": "datetime", "order_type": "direction"})
+    order_df["datetime"] = order_df["datetime"].astype(str)
+
+    return order_df
```

## qlib/rl/data/__init__.py

 * *Ordering differences only*

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-"""Common utilities to handle ad-hoc-styled data.
-
-Most of these snippets comes from research project (paper code).
-Please take caution when using them in production.
-"""
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+"""Common utilities to handle ad-hoc-styled data.
+
+Most of these snippets comes from research project (paper code).
+Please take caution when using them in production.
+"""
```

## qlib/rl/data/base.py

 * *Ordering differences only*

```diff
@@ -1,65 +1,65 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-from __future__ import annotations
-
-from abc import abstractmethod
-
-import pandas as pd
-
-
-class BaseIntradayBacktestData:
-    """
-    Raw market data that is often used in backtesting (thus called BacktestData).
-
-    Base class for all types of backtest data. Currently, each type of simulator has its corresponding backtest
-    data type.
-    """
-
-    @abstractmethod
-    def __repr__(self) -> str:
-        raise NotImplementedError
-
-    @abstractmethod
-    def __len__(self) -> int:
-        raise NotImplementedError
-
-    @abstractmethod
-    def get_deal_price(self) -> pd.Series:
-        raise NotImplementedError
-
-    @abstractmethod
-    def get_volume(self) -> pd.Series:
-        raise NotImplementedError
-
-    @abstractmethod
-    def get_time_index(self) -> pd.DatetimeIndex:
-        raise NotImplementedError
-
-
-class BaseIntradayProcessedData:
-    """Processed market data after data cleanup and feature engineering.
-
-    It contains both processed data for "today" and "yesterday", as some algorithms
-    might use the market information of the previous day to assist decision making.
-    """
-
-    today: pd.DataFrame
-    """Processed data for "today".
-    Number of records must be ``time_length``, and columns must be ``feature_dim``."""
-
-    yesterday: pd.DataFrame
-    """Processed data for "yesterday".
-    Number of records must be ``time_length``, and columns must be ``feature_dim``."""
-
-
-class ProcessedDataProvider:
-    """Provider of processed data"""
-
-    def get_data(
-        self,
-        stock_id: str,
-        date: pd.Timestamp,
-        feature_dim: int,
-        time_index: pd.Index,
-    ) -> BaseIntradayProcessedData:
-        raise NotImplementedError
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+from __future__ import annotations
+
+from abc import abstractmethod
+
+import pandas as pd
+
+
+class BaseIntradayBacktestData:
+    """
+    Raw market data that is often used in backtesting (thus called BacktestData).
+
+    Base class for all types of backtest data. Currently, each type of simulator has its corresponding backtest
+    data type.
+    """
+
+    @abstractmethod
+    def __repr__(self) -> str:
+        raise NotImplementedError
+
+    @abstractmethod
+    def __len__(self) -> int:
+        raise NotImplementedError
+
+    @abstractmethod
+    def get_deal_price(self) -> pd.Series:
+        raise NotImplementedError
+
+    @abstractmethod
+    def get_volume(self) -> pd.Series:
+        raise NotImplementedError
+
+    @abstractmethod
+    def get_time_index(self) -> pd.DatetimeIndex:
+        raise NotImplementedError
+
+
+class BaseIntradayProcessedData:
+    """Processed market data after data cleanup and feature engineering.
+
+    It contains both processed data for "today" and "yesterday", as some algorithms
+    might use the market information of the previous day to assist decision making.
+    """
+
+    today: pd.DataFrame
+    """Processed data for "today".
+    Number of records must be ``time_length``, and columns must be ``feature_dim``."""
+
+    yesterday: pd.DataFrame
+    """Processed data for "yesterday".
+    Number of records must be ``time_length``, and columns must be ``feature_dim``."""
+
+
+class ProcessedDataProvider:
+    """Provider of processed data"""
+
+    def get_data(
+        self,
+        stock_id: str,
+        date: pd.Timestamp,
+        feature_dim: int,
+        time_index: pd.Index,
+    ) -> BaseIntradayProcessedData:
+        raise NotImplementedError
```

## qlib/rl/data/integration.py

 * *Ordering differences only*

```diff
@@ -1,82 +1,82 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-"""
-TODO: This file is used to integrate NeuTrader with Qlib to run the existing projects.
-TODO: The implementation here is kind of adhoc. It is better to design a more uniformed & general implementation.
-"""
-
-from __future__ import annotations
-
-from pathlib import Path
-
-import qlib
-from qlib.constant import REG_CN
-from qlib.contrib.ops.high_freq import BFillNan, Cut, Date, DayCumsum, DayLast, FFillNan, IsInf, IsNull, Select
-
-
-def init_qlib(qlib_config: dict) -> None:
-    """Initialize necessary resource to launch the workflow, including data direction, feature columns, etc..
-
-    Parameters
-    ----------
-    qlib_config:
-        Qlib configuration.
-
-        Example::
-
-            {
-                "provider_uri_day": DATA_ROOT_DIR / "qlib_1d",
-                "provider_uri_1min": DATA_ROOT_DIR / "qlib_1min",
-                "feature_root_dir": DATA_ROOT_DIR / "qlib_handler_stock",
-                "feature_columns_today": [
-                    "$open", "$high", "$low", "$close", "$vwap", "$bid", "$ask", "$volume",
-                    "$bidV", "$bidV1", "$bidV3", "$bidV5", "$askV", "$askV1", "$askV3", "$askV5",
-                ],
-                "feature_columns_yesterday": [
-                    "$open_1", "$high_1", "$low_1", "$close_1", "$vwap_1", "$bid_1", "$ask_1", "$volume_1",
-                    "$bidV_1", "$bidV1_1", "$bidV3_1", "$bidV5_1", "$askV_1", "$askV1_1", "$askV3_1", "$askV5_1",
-                ],
-            }
-    """
-
-    def _convert_to_path(path: str | Path) -> Path:
-        return path if isinstance(path, Path) else Path(path)
-
-    provider_uri_map = {}
-    for granularity in ["1min", "5min", "day"]:
-        if f"provider_uri_{granularity}" in qlib_config:
-            provider_uri_map[f"{granularity}"] = _convert_to_path(qlib_config[f"provider_uri_{granularity}"]).as_posix()
-
-    qlib.init(
-        region=REG_CN,
-        auto_mount=False,
-        custom_ops=[DayLast, FFillNan, BFillNan, Date, Select, IsNull, IsInf, Cut, DayCumsum],
-        expression_cache=None,
-        calendar_provider={
-            "class": "LocalCalendarProvider",
-            "module_path": "qlib.data.data",
-            "kwargs": {
-                "backend": {
-                    "class": "FileCalendarStorage",
-                    "module_path": "qlib.data.storage.file_storage",
-                    "kwargs": {"provider_uri_map": provider_uri_map},
-                },
-            },
-        },
-        feature_provider={
-            "class": "LocalFeatureProvider",
-            "module_path": "qlib.data.data",
-            "kwargs": {
-                "backend": {
-                    "class": "FileFeatureStorage",
-                    "module_path": "qlib.data.storage.file_storage",
-                    "kwargs": {"provider_uri_map": provider_uri_map},
-                },
-            },
-        },
-        provider_uri=provider_uri_map,
-        kernels=1,
-        redis_port=-1,
-        clear_mem_cache=False,  # init_qlib will be called for multiple times. Keep the cache for improving performance
-    )
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+"""
+TODO: This file is used to integrate NeuTrader with Qlib to run the existing projects.
+TODO: The implementation here is kind of adhoc. It is better to design a more uniformed & general implementation.
+"""
+
+from __future__ import annotations
+
+from pathlib import Path
+
+import qlib
+from qlib.constant import REG_CN
+from qlib.contrib.ops.high_freq import BFillNan, Cut, Date, DayCumsum, DayLast, FFillNan, IsInf, IsNull, Select
+
+
+def init_qlib(qlib_config: dict) -> None:
+    """Initialize necessary resource to launch the workflow, including data direction, feature columns, etc..
+
+    Parameters
+    ----------
+    qlib_config:
+        Qlib configuration.
+
+        Example::
+
+            {
+                "provider_uri_day": DATA_ROOT_DIR / "qlib_1d",
+                "provider_uri_1min": DATA_ROOT_DIR / "qlib_1min",
+                "feature_root_dir": DATA_ROOT_DIR / "qlib_handler_stock",
+                "feature_columns_today": [
+                    "$open", "$high", "$low", "$close", "$vwap", "$bid", "$ask", "$volume",
+                    "$bidV", "$bidV1", "$bidV3", "$bidV5", "$askV", "$askV1", "$askV3", "$askV5",
+                ],
+                "feature_columns_yesterday": [
+                    "$open_1", "$high_1", "$low_1", "$close_1", "$vwap_1", "$bid_1", "$ask_1", "$volume_1",
+                    "$bidV_1", "$bidV1_1", "$bidV3_1", "$bidV5_1", "$askV_1", "$askV1_1", "$askV3_1", "$askV5_1",
+                ],
+            }
+    """
+
+    def _convert_to_path(path: str | Path) -> Path:
+        return path if isinstance(path, Path) else Path(path)
+
+    provider_uri_map = {}
+    for granularity in ["1min", "5min", "day"]:
+        if f"provider_uri_{granularity}" in qlib_config:
+            provider_uri_map[f"{granularity}"] = _convert_to_path(qlib_config[f"provider_uri_{granularity}"]).as_posix()
+
+    qlib.init(
+        region=REG_CN,
+        auto_mount=False,
+        custom_ops=[DayLast, FFillNan, BFillNan, Date, Select, IsNull, IsInf, Cut, DayCumsum],
+        expression_cache=None,
+        calendar_provider={
+            "class": "LocalCalendarProvider",
+            "module_path": "qlib.data.data",
+            "kwargs": {
+                "backend": {
+                    "class": "FileCalendarStorage",
+                    "module_path": "qlib.data.storage.file_storage",
+                    "kwargs": {"provider_uri_map": provider_uri_map},
+                },
+            },
+        },
+        feature_provider={
+            "class": "LocalFeatureProvider",
+            "module_path": "qlib.data.data",
+            "kwargs": {
+                "backend": {
+                    "class": "FileFeatureStorage",
+                    "module_path": "qlib.data.storage.file_storage",
+                    "kwargs": {"provider_uri_map": provider_uri_map},
+                },
+            },
+        },
+        provider_uri=provider_uri_map,
+        kernels=1,
+        redis_port=-1,
+        clear_mem_cache=False,  # init_qlib will be called for multiple times. Keep the cache for improving performance
+    )
```

## qlib/rl/data/native.py

 * *Ordering differences only*

```diff
@@ -1,233 +1,233 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-from __future__ import annotations
-
-from pathlib import Path
-from typing import cast, List
-
-import cachetools
-import pandas as pd
-import pickle
-import os
-
-from qlib.backtest import Exchange, Order
-from qlib.backtest.decision import TradeRange, TradeRangeByTime
-from qlib.constant import EPS_T
-from .base import BaseIntradayBacktestData, BaseIntradayProcessedData, ProcessedDataProvider
-
-
-def get_ticks_slice(
-    ticks_index: pd.DatetimeIndex,
-    start: pd.Timestamp,
-    end: pd.Timestamp,
-    include_end: bool = False,
-) -> pd.DatetimeIndex:
-    if not include_end:
-        end = end - EPS_T
-    return ticks_index[ticks_index.slice_indexer(start, end)]
-
-
-class IntradayBacktestData(BaseIntradayBacktestData):
-    """Backtest data for Qlib simulator"""
-
-    def __init__(
-        self,
-        order: Order,
-        exchange: Exchange,
-        ticks_index: pd.DatetimeIndex,
-        ticks_for_order: pd.DatetimeIndex,
-    ) -> None:
-        self._order = order
-        self._exchange = exchange
-        self._start_time = ticks_for_order[0]
-        self._end_time = ticks_for_order[-1]
-        self.ticks_index = ticks_index
-        self.ticks_for_order = ticks_for_order
-
-        self._deal_price = cast(
-            pd.Series,
-            self._exchange.get_deal_price(
-                self._order.stock_id,
-                self._start_time,
-                self._end_time,
-                direction=self._order.direction,
-                method=None,
-            ),
-        )
-        self._volume = cast(
-            pd.Series,
-            self._exchange.get_volume(
-                self._order.stock_id,
-                self._start_time,
-                self._end_time,
-                method=None,
-            ),
-        )
-
-    def __repr__(self) -> str:
-        return (
-            f"Order: {self._order}, Exchange: {self._exchange}, "
-            f"Start time: {self._start_time}, End time: {self._end_time}"
-        )
-
-    def __len__(self) -> int:
-        return len(self._deal_price)
-
-    def get_deal_price(self) -> pd.Series:
-        return self._deal_price
-
-    def get_volume(self) -> pd.Series:
-        return self._volume
-
-    def get_time_index(self) -> pd.DatetimeIndex:
-        return pd.DatetimeIndex([e[1] for e in list(self._exchange.quote_df.index)])
-
-
-class DataframeIntradayBacktestData(BaseIntradayBacktestData):
-    """Backtest data from dataframe"""
-
-    def __init__(self, df: pd.DataFrame, price_column: str = "$close0", volume_column: str = "$volume0") -> None:
-        self.df = df
-        self.price_column = price_column
-        self.volume_column = volume_column
-
-    def __repr__(self) -> str:
-        with pd.option_context("memory_usage", False, "display.max_info_columns", 1, "display.large_repr", "info"):
-            return f"{self.__class__.__name__}({self.df})"
-
-    def __len__(self) -> int:
-        return len(self.df)
-
-    def get_deal_price(self) -> pd.Series:
-        return self.df[self.price_column]
-
-    def get_volume(self) -> pd.Series:
-        return self.df[self.volume_column]
-
-    def get_time_index(self) -> pd.DatetimeIndex:
-        return cast(pd.DatetimeIndex, self.df.index)
-
-
-@cachetools.cached(  # type: ignore
-    cache=cachetools.LRUCache(100),
-    key=lambda order, _, __: order.key_by_day,
-)
-def load_backtest_data(
-    order: Order,
-    trade_exchange: Exchange,
-    trade_range: TradeRange,
-) -> IntradayBacktestData:
-    ticks_index = pd.DatetimeIndex(trade_exchange.quote_df.reset_index()["datetime"])
-    ticks_index = ticks_index[order.start_time <= ticks_index]
-    ticks_index = ticks_index[ticks_index <= order.end_time]
-
-    if isinstance(trade_range, TradeRangeByTime):
-        ticks_for_order = get_ticks_slice(
-            ticks_index,
-            trade_range.start_time,
-            trade_range.end_time,
-            include_end=True,
-        )
-    else:
-        ticks_for_order = None  # FIXME: implement this logic
-
-    backtest_data = IntradayBacktestData(
-        order=order,
-        exchange=trade_exchange,
-        ticks_index=ticks_index,
-        ticks_for_order=ticks_for_order,
-    )
-    return backtest_data
-
-
-class HandlerIntradayProcessedData(BaseIntradayProcessedData):
-    """Subclass of IntradayProcessedData. Used to handle handler (bin format) style data."""
-
-    def __init__(
-        self,
-        data_dir: Path,
-        stock_id: str,
-        date: pd.Timestamp,
-        feature_columns_today: List[str],
-        feature_columns_yesterday: List[str],
-        backtest: bool = False,
-        index_only: bool = False,
-    ) -> None:
-        def _drop_stock_id(df: pd.DataFrame) -> pd.DataFrame:
-            df = df.reset_index()
-            if "instrument" in df.columns:
-                df = df.drop(columns=["instrument"])
-            return df.set_index(["datetime"])
-
-        path = os.path.join(data_dir, "backtest" if backtest else "feature", f"{stock_id}.pkl")
-        start_time, end_time = date.replace(hour=0, minute=0, second=0), date.replace(hour=23, minute=59, second=59)
-        with open(path, "rb") as fstream:
-            dataset = pickle.load(fstream)
-        data = dataset.handler.fetch(pd.IndexSlice[stock_id, start_time:end_time], level=None)
-
-        if index_only:
-            self.today = _drop_stock_id(data[[]])
-            self.yesterday = _drop_stock_id(data[[]])
-        else:
-            self.today = _drop_stock_id(data[feature_columns_today])
-            self.yesterday = _drop_stock_id(data[feature_columns_yesterday])
-
-    def __repr__(self) -> str:
-        with pd.option_context("memory_usage", False, "display.max_info_columns", 1, "display.large_repr", "info"):
-            return f"{self.__class__.__name__}({self.today}, {self.yesterday})"
-
-
-@cachetools.cached(  # type: ignore
-    cache=cachetools.LRUCache(100),  # 100 * 50K = 5MB
-    key=lambda data_dir, stock_id, date, feature_columns_today, feature_columns_yesterday, backtest, index_only: (
-        stock_id,
-        date,
-        backtest,
-        index_only,
-    ),
-)
-def load_handler_intraday_processed_data(
-    data_dir: Path,
-    stock_id: str,
-    date: pd.Timestamp,
-    feature_columns_today: List[str],
-    feature_columns_yesterday: List[str],
-    backtest: bool = False,
-    index_only: bool = False,
-) -> HandlerIntradayProcessedData:
-    return HandlerIntradayProcessedData(
-        data_dir, stock_id, date, feature_columns_today, feature_columns_yesterday, backtest, index_only
-    )
-
-
-class HandlerProcessedDataProvider(ProcessedDataProvider):
-    def __init__(
-        self,
-        data_dir: str,
-        feature_columns_today: List[str],
-        feature_columns_yesterday: List[str],
-        backtest: bool = False,
-    ) -> None:
-        super().__init__()
-
-        self.data_dir = Path(data_dir)
-        self.feature_columns_today = feature_columns_today
-        self.feature_columns_yesterday = feature_columns_yesterday
-        self.backtest = backtest
-
-    def get_data(
-        self,
-        stock_id: str,
-        date: pd.Timestamp,
-        feature_dim: int,
-        time_index: pd.Index,
-    ) -> BaseIntradayProcessedData:
-        return load_handler_intraday_processed_data(
-            self.data_dir,
-            stock_id,
-            date,
-            self.feature_columns_today,
-            self.feature_columns_yesterday,
-            backtest=self.backtest,
-            index_only=False,
-        )
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+from __future__ import annotations
+
+from pathlib import Path
+from typing import cast, List
+
+import cachetools
+import pandas as pd
+import pickle
+import os
+
+from qlib.backtest import Exchange, Order
+from qlib.backtest.decision import TradeRange, TradeRangeByTime
+from qlib.constant import EPS_T
+from .base import BaseIntradayBacktestData, BaseIntradayProcessedData, ProcessedDataProvider
+
+
+def get_ticks_slice(
+    ticks_index: pd.DatetimeIndex,
+    start: pd.Timestamp,
+    end: pd.Timestamp,
+    include_end: bool = False,
+) -> pd.DatetimeIndex:
+    if not include_end:
+        end = end - EPS_T
+    return ticks_index[ticks_index.slice_indexer(start, end)]
+
+
+class IntradayBacktestData(BaseIntradayBacktestData):
+    """Backtest data for Qlib simulator"""
+
+    def __init__(
+        self,
+        order: Order,
+        exchange: Exchange,
+        ticks_index: pd.DatetimeIndex,
+        ticks_for_order: pd.DatetimeIndex,
+    ) -> None:
+        self._order = order
+        self._exchange = exchange
+        self._start_time = ticks_for_order[0]
+        self._end_time = ticks_for_order[-1]
+        self.ticks_index = ticks_index
+        self.ticks_for_order = ticks_for_order
+
+        self._deal_price = cast(
+            pd.Series,
+            self._exchange.get_deal_price(
+                self._order.stock_id,
+                self._start_time,
+                self._end_time,
+                direction=self._order.direction,
+                method=None,
+            ),
+        )
+        self._volume = cast(
+            pd.Series,
+            self._exchange.get_volume(
+                self._order.stock_id,
+                self._start_time,
+                self._end_time,
+                method=None,
+            ),
+        )
+
+    def __repr__(self) -> str:
+        return (
+            f"Order: {self._order}, Exchange: {self._exchange}, "
+            f"Start time: {self._start_time}, End time: {self._end_time}"
+        )
+
+    def __len__(self) -> int:
+        return len(self._deal_price)
+
+    def get_deal_price(self) -> pd.Series:
+        return self._deal_price
+
+    def get_volume(self) -> pd.Series:
+        return self._volume
+
+    def get_time_index(self) -> pd.DatetimeIndex:
+        return pd.DatetimeIndex([e[1] for e in list(self._exchange.quote_df.index)])
+
+
+class DataframeIntradayBacktestData(BaseIntradayBacktestData):
+    """Backtest data from dataframe"""
+
+    def __init__(self, df: pd.DataFrame, price_column: str = "$close0", volume_column: str = "$volume0") -> None:
+        self.df = df
+        self.price_column = price_column
+        self.volume_column = volume_column
+
+    def __repr__(self) -> str:
+        with pd.option_context("memory_usage", False, "display.max_info_columns", 1, "display.large_repr", "info"):
+            return f"{self.__class__.__name__}({self.df})"
+
+    def __len__(self) -> int:
+        return len(self.df)
+
+    def get_deal_price(self) -> pd.Series:
+        return self.df[self.price_column]
+
+    def get_volume(self) -> pd.Series:
+        return self.df[self.volume_column]
+
+    def get_time_index(self) -> pd.DatetimeIndex:
+        return cast(pd.DatetimeIndex, self.df.index)
+
+
+@cachetools.cached(  # type: ignore
+    cache=cachetools.LRUCache(100),
+    key=lambda order, _, __: order.key_by_day,
+)
+def load_backtest_data(
+    order: Order,
+    trade_exchange: Exchange,
+    trade_range: TradeRange,
+) -> IntradayBacktestData:
+    ticks_index = pd.DatetimeIndex(trade_exchange.quote_df.reset_index()["datetime"])
+    ticks_index = ticks_index[order.start_time <= ticks_index]
+    ticks_index = ticks_index[ticks_index <= order.end_time]
+
+    if isinstance(trade_range, TradeRangeByTime):
+        ticks_for_order = get_ticks_slice(
+            ticks_index,
+            trade_range.start_time,
+            trade_range.end_time,
+            include_end=True,
+        )
+    else:
+        ticks_for_order = None  # FIXME: implement this logic
+
+    backtest_data = IntradayBacktestData(
+        order=order,
+        exchange=trade_exchange,
+        ticks_index=ticks_index,
+        ticks_for_order=ticks_for_order,
+    )
+    return backtest_data
+
+
+class HandlerIntradayProcessedData(BaseIntradayProcessedData):
+    """Subclass of IntradayProcessedData. Used to handle handler (bin format) style data."""
+
+    def __init__(
+        self,
+        data_dir: Path,
+        stock_id: str,
+        date: pd.Timestamp,
+        feature_columns_today: List[str],
+        feature_columns_yesterday: List[str],
+        backtest: bool = False,
+        index_only: bool = False,
+    ) -> None:
+        def _drop_stock_id(df: pd.DataFrame) -> pd.DataFrame:
+            df = df.reset_index()
+            if "instrument" in df.columns:
+                df = df.drop(columns=["instrument"])
+            return df.set_index(["datetime"])
+
+        path = os.path.join(data_dir, "backtest" if backtest else "feature", f"{stock_id}.pkl")
+        start_time, end_time = date.replace(hour=0, minute=0, second=0), date.replace(hour=23, minute=59, second=59)
+        with open(path, "rb") as fstream:
+            dataset = pickle.load(fstream)
+        data = dataset.handler.fetch(pd.IndexSlice[stock_id, start_time:end_time], level=None)
+
+        if index_only:
+            self.today = _drop_stock_id(data[[]])
+            self.yesterday = _drop_stock_id(data[[]])
+        else:
+            self.today = _drop_stock_id(data[feature_columns_today])
+            self.yesterday = _drop_stock_id(data[feature_columns_yesterday])
+
+    def __repr__(self) -> str:
+        with pd.option_context("memory_usage", False, "display.max_info_columns", 1, "display.large_repr", "info"):
+            return f"{self.__class__.__name__}({self.today}, {self.yesterday})"
+
+
+@cachetools.cached(  # type: ignore
+    cache=cachetools.LRUCache(100),  # 100 * 50K = 5MB
+    key=lambda data_dir, stock_id, date, feature_columns_today, feature_columns_yesterday, backtest, index_only: (
+        stock_id,
+        date,
+        backtest,
+        index_only,
+    ),
+)
+def load_handler_intraday_processed_data(
+    data_dir: Path,
+    stock_id: str,
+    date: pd.Timestamp,
+    feature_columns_today: List[str],
+    feature_columns_yesterday: List[str],
+    backtest: bool = False,
+    index_only: bool = False,
+) -> HandlerIntradayProcessedData:
+    return HandlerIntradayProcessedData(
+        data_dir, stock_id, date, feature_columns_today, feature_columns_yesterday, backtest, index_only
+    )
+
+
+class HandlerProcessedDataProvider(ProcessedDataProvider):
+    def __init__(
+        self,
+        data_dir: str,
+        feature_columns_today: List[str],
+        feature_columns_yesterday: List[str],
+        backtest: bool = False,
+    ) -> None:
+        super().__init__()
+
+        self.data_dir = Path(data_dir)
+        self.feature_columns_today = feature_columns_today
+        self.feature_columns_yesterday = feature_columns_yesterday
+        self.backtest = backtest
+
+    def get_data(
+        self,
+        stock_id: str,
+        date: pd.Timestamp,
+        feature_dim: int,
+        time_index: pd.Index,
+    ) -> BaseIntradayProcessedData:
+        return load_handler_intraday_processed_data(
+            self.data_dir,
+            stock_id,
+            date,
+            self.feature_columns_today,
+            self.feature_columns_yesterday,
+            backtest=self.backtest,
+            index_only=False,
+        )
```

## qlib/rl/data/pickle_styled.py

 * *Ordering differences only*

```diff
@@ -1,296 +1,296 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-"""This module contains utilities to read financial data from pickle-styled files.
-
-This is the format used in `OPD paper <https://seqml.github.io/opd/>`__. NOT the standard data format in qlib.
-
-The data here are all wrapped with ``@lru_cache``, which saves the expensive IO cost to repetitively read the data.
-We also encourage users to use ``get_xxx_yyy`` rather than ``XxxYyy`` (although they are the same thing),
-because ``get_xxx_yyy`` is cache-optimized.
-
-Note that these pickle files are dumped with Python 3.8. Python lower than 3.7 might not be able to load them.
-See `PEP 574 <https://peps.python.org/pep-0574/>`__ for details.
-
-This file shows resemblence to qlib.backtest.high_performance_ds. We might merge those two in future.
-"""
-
-# TODO: merge with qlib/backtest/high_performance_ds.py
-
-from __future__ import annotations
-
-from functools import lru_cache
-from pathlib import Path
-from typing import List, Sequence, cast
-
-import cachetools
-import numpy as np
-import pandas as pd
-from cachetools.keys import hashkey
-
-from qlib.backtest.decision import Order, OrderDir
-from qlib.rl.data.base import BaseIntradayBacktestData, BaseIntradayProcessedData, ProcessedDataProvider
-from qlib.typehint import Literal
-
-DealPriceType = Literal["bid_or_ask", "bid_or_ask_fill", "close"]
-"""Several ad-hoc deal price.
-``bid_or_ask``: If sell, use column ``$bid0``; if buy, use column ``$ask0``.
-``bid_or_ask_fill``: Based on ``bid_or_ask``. If price is 0, use another price (``$ask0`` / ``$bid0``) instead.
-``close``: Use close price (``$close0``) as deal price.
-"""
-
-
-def _infer_processed_data_column_names(shape: int) -> List[str]:
-    if shape == 16:
-        return [
-            "$open",
-            "$high",
-            "$low",
-            "$close",
-            "$vwap",
-            "$bid",
-            "$ask",
-            "$volume",
-            "$bidV",
-            "$bidV1",
-            "$bidV3",
-            "$bidV5",
-            "$askV",
-            "$askV1",
-            "$askV3",
-            "$askV5",
-        ]
-    if shape == 6:
-        return ["$high", "$low", "$open", "$close", "$vwap", "$volume"]
-    elif shape == 5:
-        return ["$high", "$low", "$open", "$close", "$volume"]
-    raise ValueError(f"Unrecognized data shape: {shape}")
-
-
-def _find_pickle(filename_without_suffix: Path) -> Path:
-    suffix_list = [".pkl", ".pkl.backtest"]
-    paths: List[Path] = []
-    for suffix in suffix_list:
-        path = filename_without_suffix.parent / (filename_without_suffix.name + suffix)
-        if path.exists():
-            paths.append(path)
-    if not paths:
-        raise FileNotFoundError(f"No file starting with '{filename_without_suffix}' found")
-    if len(paths) > 1:
-        raise ValueError(f"Multiple paths are found with prefix '{filename_without_suffix}': {paths}")
-    return paths[0]
-
-
-@lru_cache(maxsize=10)  # 10 * 40M = 400MB
-def _read_pickle(filename_without_suffix: Path) -> pd.DataFrame:
-    df = pd.read_pickle(_find_pickle(filename_without_suffix))
-    index_cols = df.index.names
-
-    df = df.reset_index()
-    for date_col_name in ["date", "datetime"]:
-        if date_col_name in df:
-            df[date_col_name] = pd.to_datetime(df[date_col_name])
-    df = df.set_index(index_cols)
-
-    return df
-
-
-class SimpleIntradayBacktestData(BaseIntradayBacktestData):
-    """Backtest data for simple simulator"""
-
-    def __init__(
-        self,
-        data_dir: Path | str,
-        stock_id: str,
-        date: pd.Timestamp,
-        deal_price: DealPriceType = "close",
-        order_dir: int | None = None,
-    ) -> None:
-        super(SimpleIntradayBacktestData, self).__init__()
-
-        backtest = _read_pickle((data_dir if isinstance(data_dir, Path) else Path(data_dir)) / stock_id)
-        backtest = backtest.loc[pd.IndexSlice[stock_id, :, date]]
-
-        # No longer need for pandas >= 1.4
-        # backtest = backtest.droplevel([0, 2])
-
-        self.data: pd.DataFrame = backtest
-        self.deal_price_type: DealPriceType = deal_price
-        self.order_dir = order_dir
-
-    def __repr__(self) -> str:
-        with pd.option_context("memory_usage", False, "display.max_info_columns", 1, "display.large_repr", "info"):
-            return f"{self.__class__.__name__}({self.data})"
-
-    def __len__(self) -> int:
-        return len(self.data)
-
-    def get_deal_price(self) -> pd.Series:
-        """Return a pandas series that can be indexed with time.
-        See :attribute:`DealPriceType` for details."""
-        if self.deal_price_type in ("bid_or_ask", "bid_or_ask_fill"):
-            if self.order_dir is None:
-                raise ValueError("Order direction cannot be none when deal_price_type is not close.")
-            if self.order_dir == OrderDir.SELL:
-                col = "$bid0"
-            else:  # BUY
-                col = "$ask0"
-        elif self.deal_price_type == "close":
-            col = "$close0"
-        else:
-            raise ValueError(f"Unsupported deal_price_type: {self.deal_price_type}")
-        price = self.data[col]
-
-        if self.deal_price_type == "bid_or_ask_fill":
-            if self.order_dir == OrderDir.SELL:
-                fill_col = "$ask0"
-            else:
-                fill_col = "$bid0"
-            price = price.replace(0, np.nan).fillna(self.data[fill_col])
-
-        return price
-
-    def get_volume(self) -> pd.Series:
-        """Return a volume series that can be indexed with time."""
-        return self.data["$volume0"]
-
-    def get_time_index(self) -> pd.DatetimeIndex:
-        return cast(pd.DatetimeIndex, self.data.index)
-
-
-class PickleIntradayProcessedData(BaseIntradayProcessedData):
-    """Subclass of IntradayProcessedData. Used to handle pickle-styled data."""
-
-    def __init__(
-        self,
-        data_dir: Path | str,
-        stock_id: str,
-        date: pd.Timestamp,
-        feature_dim: int,
-        time_index: pd.Index,
-    ) -> None:
-        proc = _read_pickle((data_dir if isinstance(data_dir, Path) else Path(data_dir)) / stock_id)
-
-        # We have to infer the names here because,
-        # unfortunately they are not included in the original data.
-        cnames = _infer_processed_data_column_names(feature_dim)
-
-        time_length: int = len(time_index)
-
-        try:
-            # new data format
-            proc = proc.loc[pd.IndexSlice[stock_id, :, date]]
-            assert len(proc) == time_length and len(proc.columns) == feature_dim * 2
-            proc_today = proc[cnames]
-            proc_yesterday = proc[[f"{c}_1" for c in cnames]].rename(columns=lambda c: c[:-2])
-        except (IndexError, KeyError):
-            # legacy data
-            proc = proc.loc[pd.IndexSlice[stock_id, date]]
-            assert time_length * feature_dim * 2 == len(proc)
-            proc_today = proc.to_numpy()[: time_length * feature_dim].reshape((time_length, feature_dim))
-            proc_yesterday = proc.to_numpy()[time_length * feature_dim :].reshape((time_length, feature_dim))
-            proc_today = pd.DataFrame(proc_today, index=time_index, columns=cnames)
-            proc_yesterday = pd.DataFrame(proc_yesterday, index=time_index, columns=cnames)
-
-        self.today: pd.DataFrame = proc_today
-        self.yesterday: pd.DataFrame = proc_yesterday
-        assert len(self.today.columns) == len(self.yesterday.columns) == feature_dim
-        assert len(self.today) == len(self.yesterday) == time_length
-
-    def __repr__(self) -> str:
-        with pd.option_context("memory_usage", False, "display.max_info_columns", 1, "display.large_repr", "info"):
-            return f"{self.__class__.__name__}({self.today}, {self.yesterday})"
-
-
-@lru_cache(maxsize=100)  # 100 * 50K = 5MB
-def load_simple_intraday_backtest_data(
-    data_dir: Path,
-    stock_id: str,
-    date: pd.Timestamp,
-    deal_price: DealPriceType = "close",
-    order_dir: int | None = None,
-) -> SimpleIntradayBacktestData:
-    return SimpleIntradayBacktestData(data_dir, stock_id, date, deal_price, order_dir)
-
-
-@cachetools.cached(  # type: ignore
-    cache=cachetools.LRUCache(100),  # 100 * 50K = 5MB
-    key=lambda data_dir, stock_id, date, feature_dim, time_index: hashkey(data_dir, stock_id, date),
-)
-def load_pickle_intraday_processed_data(
-    data_dir: Path,
-    stock_id: str,
-    date: pd.Timestamp,
-    feature_dim: int,
-    time_index: pd.Index,
-) -> BaseIntradayProcessedData:
-    return PickleIntradayProcessedData(data_dir, stock_id, date, feature_dim, time_index)
-
-
-class PickleProcessedDataProvider(ProcessedDataProvider):
-    def __init__(self, data_dir: Path) -> None:
-        super().__init__()
-
-        self._data_dir = data_dir
-
-    def get_data(
-        self,
-        stock_id: str,
-        date: pd.Timestamp,
-        feature_dim: int,
-        time_index: pd.Index,
-    ) -> BaseIntradayProcessedData:
-        return load_pickle_intraday_processed_data(
-            data_dir=self._data_dir,
-            stock_id=stock_id,
-            date=date,
-            feature_dim=feature_dim,
-            time_index=time_index,
-        )
-
-
-def load_orders(
-    order_path: Path,
-    start_time: pd.Timestamp = None,
-    end_time: pd.Timestamp = None,
-) -> Sequence[Order]:
-    """Load orders, and set start time and end time for the orders."""
-
-    start_time = start_time or pd.Timestamp("0:00:00")
-    end_time = end_time or pd.Timestamp("23:59:59")
-
-    if order_path.is_file():
-        order_df = pd.read_pickle(order_path)
-    else:
-        order_df = []
-        for file in order_path.iterdir():
-            order_data = pd.read_pickle(file)
-            order_df.append(order_data)
-        order_df = pd.concat(order_df)
-
-    order_df = order_df.reset_index()
-
-    # Legacy-style orders have "date" instead of "datetime"
-    if "date" in order_df.columns:
-        order_df = order_df.rename(columns={"date": "datetime"})
-
-    # Sometimes "date" are str rather than Timestamp
-    order_df["datetime"] = pd.to_datetime(order_df["datetime"])
-
-    orders: List[Order] = []
-
-    for _, row in order_df.iterrows():
-        # filter out orders with amount == 0
-        if row["amount"] <= 0:
-            continue
-        orders.append(
-            Order(
-                row["instrument"],
-                row["amount"],
-                OrderDir(int(row["order_type"])),
-                row["datetime"].replace(hour=start_time.hour, minute=start_time.minute, second=start_time.second),
-                row["datetime"].replace(hour=end_time.hour, minute=end_time.minute, second=end_time.second),
-            ),
-        )
-
-    return orders
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+"""This module contains utilities to read financial data from pickle-styled files.
+
+This is the format used in `OPD paper <https://seqml.github.io/opd/>`__. NOT the standard data format in qlib.
+
+The data here are all wrapped with ``@lru_cache``, which saves the expensive IO cost to repetitively read the data.
+We also encourage users to use ``get_xxx_yyy`` rather than ``XxxYyy`` (although they are the same thing),
+because ``get_xxx_yyy`` is cache-optimized.
+
+Note that these pickle files are dumped with Python 3.8. Python lower than 3.7 might not be able to load them.
+See `PEP 574 <https://peps.python.org/pep-0574/>`__ for details.
+
+This file shows resemblence to qlib.backtest.high_performance_ds. We might merge those two in future.
+"""
+
+# TODO: merge with qlib/backtest/high_performance_ds.py
+
+from __future__ import annotations
+
+from functools import lru_cache
+from pathlib import Path
+from typing import List, Sequence, cast
+
+import cachetools
+import numpy as np
+import pandas as pd
+from cachetools.keys import hashkey
+
+from qlib.backtest.decision import Order, OrderDir
+from qlib.rl.data.base import BaseIntradayBacktestData, BaseIntradayProcessedData, ProcessedDataProvider
+from qlib.typehint import Literal
+
+DealPriceType = Literal["bid_or_ask", "bid_or_ask_fill", "close"]
+"""Several ad-hoc deal price.
+``bid_or_ask``: If sell, use column ``$bid0``; if buy, use column ``$ask0``.
+``bid_or_ask_fill``: Based on ``bid_or_ask``. If price is 0, use another price (``$ask0`` / ``$bid0``) instead.
+``close``: Use close price (``$close0``) as deal price.
+"""
+
+
+def _infer_processed_data_column_names(shape: int) -> List[str]:
+    if shape == 16:
+        return [
+            "$open",
+            "$high",
+            "$low",
+            "$close",
+            "$vwap",
+            "$bid",
+            "$ask",
+            "$volume",
+            "$bidV",
+            "$bidV1",
+            "$bidV3",
+            "$bidV5",
+            "$askV",
+            "$askV1",
+            "$askV3",
+            "$askV5",
+        ]
+    if shape == 6:
+        return ["$high", "$low", "$open", "$close", "$vwap", "$volume"]
+    elif shape == 5:
+        return ["$high", "$low", "$open", "$close", "$volume"]
+    raise ValueError(f"Unrecognized data shape: {shape}")
+
+
+def _find_pickle(filename_without_suffix: Path) -> Path:
+    suffix_list = [".pkl", ".pkl.backtest"]
+    paths: List[Path] = []
+    for suffix in suffix_list:
+        path = filename_without_suffix.parent / (filename_without_suffix.name + suffix)
+        if path.exists():
+            paths.append(path)
+    if not paths:
+        raise FileNotFoundError(f"No file starting with '{filename_without_suffix}' found")
+    if len(paths) > 1:
+        raise ValueError(f"Multiple paths are found with prefix '{filename_without_suffix}': {paths}")
+    return paths[0]
+
+
+@lru_cache(maxsize=10)  # 10 * 40M = 400MB
+def _read_pickle(filename_without_suffix: Path) -> pd.DataFrame:
+    df = pd.read_pickle(_find_pickle(filename_without_suffix))
+    index_cols = df.index.names
+
+    df = df.reset_index()
+    for date_col_name in ["date", "datetime"]:
+        if date_col_name in df:
+            df[date_col_name] = pd.to_datetime(df[date_col_name])
+    df = df.set_index(index_cols)
+
+    return df
+
+
+class SimpleIntradayBacktestData(BaseIntradayBacktestData):
+    """Backtest data for simple simulator"""
+
+    def __init__(
+        self,
+        data_dir: Path | str,
+        stock_id: str,
+        date: pd.Timestamp,
+        deal_price: DealPriceType = "close",
+        order_dir: int | None = None,
+    ) -> None:
+        super(SimpleIntradayBacktestData, self).__init__()
+
+        backtest = _read_pickle((data_dir if isinstance(data_dir, Path) else Path(data_dir)) / stock_id)
+        backtest = backtest.loc[pd.IndexSlice[stock_id, :, date]]
+
+        # No longer need for pandas >= 1.4
+        # backtest = backtest.droplevel([0, 2])
+
+        self.data: pd.DataFrame = backtest
+        self.deal_price_type: DealPriceType = deal_price
+        self.order_dir = order_dir
+
+    def __repr__(self) -> str:
+        with pd.option_context("memory_usage", False, "display.max_info_columns", 1, "display.large_repr", "info"):
+            return f"{self.__class__.__name__}({self.data})"
+
+    def __len__(self) -> int:
+        return len(self.data)
+
+    def get_deal_price(self) -> pd.Series:
+        """Return a pandas series that can be indexed with time.
+        See :attribute:`DealPriceType` for details."""
+        if self.deal_price_type in ("bid_or_ask", "bid_or_ask_fill"):
+            if self.order_dir is None:
+                raise ValueError("Order direction cannot be none when deal_price_type is not close.")
+            if self.order_dir == OrderDir.SELL:
+                col = "$bid0"
+            else:  # BUY
+                col = "$ask0"
+        elif self.deal_price_type == "close":
+            col = "$close0"
+        else:
+            raise ValueError(f"Unsupported deal_price_type: {self.deal_price_type}")
+        price = self.data[col]
+
+        if self.deal_price_type == "bid_or_ask_fill":
+            if self.order_dir == OrderDir.SELL:
+                fill_col = "$ask0"
+            else:
+                fill_col = "$bid0"
+            price = price.replace(0, np.nan).fillna(self.data[fill_col])
+
+        return price
+
+    def get_volume(self) -> pd.Series:
+        """Return a volume series that can be indexed with time."""
+        return self.data["$volume0"]
+
+    def get_time_index(self) -> pd.DatetimeIndex:
+        return cast(pd.DatetimeIndex, self.data.index)
+
+
+class PickleIntradayProcessedData(BaseIntradayProcessedData):
+    """Subclass of IntradayProcessedData. Used to handle pickle-styled data."""
+
+    def __init__(
+        self,
+        data_dir: Path | str,
+        stock_id: str,
+        date: pd.Timestamp,
+        feature_dim: int,
+        time_index: pd.Index,
+    ) -> None:
+        proc = _read_pickle((data_dir if isinstance(data_dir, Path) else Path(data_dir)) / stock_id)
+
+        # We have to infer the names here because,
+        # unfortunately they are not included in the original data.
+        cnames = _infer_processed_data_column_names(feature_dim)
+
+        time_length: int = len(time_index)
+
+        try:
+            # new data format
+            proc = proc.loc[pd.IndexSlice[stock_id, :, date]]
+            assert len(proc) == time_length and len(proc.columns) == feature_dim * 2
+            proc_today = proc[cnames]
+            proc_yesterday = proc[[f"{c}_1" for c in cnames]].rename(columns=lambda c: c[:-2])
+        except (IndexError, KeyError):
+            # legacy data
+            proc = proc.loc[pd.IndexSlice[stock_id, date]]
+            assert time_length * feature_dim * 2 == len(proc)
+            proc_today = proc.to_numpy()[: time_length * feature_dim].reshape((time_length, feature_dim))
+            proc_yesterday = proc.to_numpy()[time_length * feature_dim :].reshape((time_length, feature_dim))
+            proc_today = pd.DataFrame(proc_today, index=time_index, columns=cnames)
+            proc_yesterday = pd.DataFrame(proc_yesterday, index=time_index, columns=cnames)
+
+        self.today: pd.DataFrame = proc_today
+        self.yesterday: pd.DataFrame = proc_yesterday
+        assert len(self.today.columns) == len(self.yesterday.columns) == feature_dim
+        assert len(self.today) == len(self.yesterday) == time_length
+
+    def __repr__(self) -> str:
+        with pd.option_context("memory_usage", False, "display.max_info_columns", 1, "display.large_repr", "info"):
+            return f"{self.__class__.__name__}({self.today}, {self.yesterday})"
+
+
+@lru_cache(maxsize=100)  # 100 * 50K = 5MB
+def load_simple_intraday_backtest_data(
+    data_dir: Path,
+    stock_id: str,
+    date: pd.Timestamp,
+    deal_price: DealPriceType = "close",
+    order_dir: int | None = None,
+) -> SimpleIntradayBacktestData:
+    return SimpleIntradayBacktestData(data_dir, stock_id, date, deal_price, order_dir)
+
+
+@cachetools.cached(  # type: ignore
+    cache=cachetools.LRUCache(100),  # 100 * 50K = 5MB
+    key=lambda data_dir, stock_id, date, feature_dim, time_index: hashkey(data_dir, stock_id, date),
+)
+def load_pickle_intraday_processed_data(
+    data_dir: Path,
+    stock_id: str,
+    date: pd.Timestamp,
+    feature_dim: int,
+    time_index: pd.Index,
+) -> BaseIntradayProcessedData:
+    return PickleIntradayProcessedData(data_dir, stock_id, date, feature_dim, time_index)
+
+
+class PickleProcessedDataProvider(ProcessedDataProvider):
+    def __init__(self, data_dir: Path) -> None:
+        super().__init__()
+
+        self._data_dir = data_dir
+
+    def get_data(
+        self,
+        stock_id: str,
+        date: pd.Timestamp,
+        feature_dim: int,
+        time_index: pd.Index,
+    ) -> BaseIntradayProcessedData:
+        return load_pickle_intraday_processed_data(
+            data_dir=self._data_dir,
+            stock_id=stock_id,
+            date=date,
+            feature_dim=feature_dim,
+            time_index=time_index,
+        )
+
+
+def load_orders(
+    order_path: Path,
+    start_time: pd.Timestamp = None,
+    end_time: pd.Timestamp = None,
+) -> Sequence[Order]:
+    """Load orders, and set start time and end time for the orders."""
+
+    start_time = start_time or pd.Timestamp("0:00:00")
+    end_time = end_time or pd.Timestamp("23:59:59")
+
+    if order_path.is_file():
+        order_df = pd.read_pickle(order_path)
+    else:
+        order_df = []
+        for file in order_path.iterdir():
+            order_data = pd.read_pickle(file)
+            order_df.append(order_data)
+        order_df = pd.concat(order_df)
+
+    order_df = order_df.reset_index()
+
+    # Legacy-style orders have "date" instead of "datetime"
+    if "date" in order_df.columns:
+        order_df = order_df.rename(columns={"date": "datetime"})
+
+    # Sometimes "date" are str rather than Timestamp
+    order_df["datetime"] = pd.to_datetime(order_df["datetime"])
+
+    orders: List[Order] = []
+
+    for _, row in order_df.iterrows():
+        # filter out orders with amount == 0
+        if row["amount"] <= 0:
+            continue
+        orders.append(
+            Order(
+                row["instrument"],
+                row["amount"],
+                OrderDir(int(row["order_type"])),
+                row["datetime"].replace(hour=start_time.hour, minute=start_time.minute, second=start_time.second),
+                row["datetime"].replace(hour=end_time.hour, minute=end_time.minute, second=end_time.second),
+            ),
+        )
+
+    return orders
```

## qlib/rl/order_execution/__init__.py

 * *Ordering differences only*

```diff
@@ -1,38 +1,38 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-"""
-Currently it supports single-asset order execution.
-Multi-asset is on the way.
-"""
-
-from .interpreter import (
-    FullHistoryStateInterpreter,
-    CurrentStepStateInterpreter,
-    CategoricalActionInterpreter,
-    TwapRelativeActionInterpreter,
-)
-from .network import Recurrent
-from .policy import AllOne, PPO
-from .reward import PAPenaltyReward
-from .simulator_simple import SingleAssetOrderExecutionSimple
-from .state import SAOEMetrics, SAOEState
-from .strategy import SAOEStateAdapter, SAOEStrategy, ProxySAOEStrategy, SAOEIntStrategy
-
-__all__ = [
-    "FullHistoryStateInterpreter",
-    "CurrentStepStateInterpreter",
-    "CategoricalActionInterpreter",
-    "TwapRelativeActionInterpreter",
-    "Recurrent",
-    "AllOne",
-    "PPO",
-    "PAPenaltyReward",
-    "SingleAssetOrderExecutionSimple",
-    "SAOEStateAdapter",
-    "SAOEMetrics",
-    "SAOEState",
-    "SAOEStrategy",
-    "ProxySAOEStrategy",
-    "SAOEIntStrategy",
-]
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+"""
+Currently it supports single-asset order execution.
+Multi-asset is on the way.
+"""
+
+from .interpreter import (
+    FullHistoryStateInterpreter,
+    CurrentStepStateInterpreter,
+    CategoricalActionInterpreter,
+    TwapRelativeActionInterpreter,
+)
+from .network import Recurrent
+from .policy import AllOne, PPO
+from .reward import PAPenaltyReward
+from .simulator_simple import SingleAssetOrderExecutionSimple
+from .state import SAOEMetrics, SAOEState
+from .strategy import SAOEStateAdapter, SAOEStrategy, ProxySAOEStrategy, SAOEIntStrategy
+
+__all__ = [
+    "FullHistoryStateInterpreter",
+    "CurrentStepStateInterpreter",
+    "CategoricalActionInterpreter",
+    "TwapRelativeActionInterpreter",
+    "Recurrent",
+    "AllOne",
+    "PPO",
+    "PAPenaltyReward",
+    "SingleAssetOrderExecutionSimple",
+    "SAOEStateAdapter",
+    "SAOEMetrics",
+    "SAOEState",
+    "SAOEStrategy",
+    "ProxySAOEStrategy",
+    "SAOEIntStrategy",
+]
```

## qlib/rl/order_execution/interpreter.py

 * *Ordering differences only*

```diff
@@ -1,257 +1,257 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-from __future__ import annotations
-
-import math
-from typing import Any, List, Optional, cast
-
-import numpy as np
-import pandas as pd
-from gym import spaces
-
-from qlib.constant import EPS
-from qlib.rl.data.base import ProcessedDataProvider
-from qlib.rl.interpreter import ActionInterpreter, StateInterpreter
-from qlib.rl.order_execution.state import SAOEState
-from qlib.typehint import TypedDict
-
-__all__ = [
-    "FullHistoryStateInterpreter",
-    "CurrentStepStateInterpreter",
-    "CategoricalActionInterpreter",
-    "TwapRelativeActionInterpreter",
-    "FullHistoryObs",
-]
-
-from qlib.utils import init_instance_by_config
-
-
-def canonicalize(value: int | float | np.ndarray | pd.DataFrame | dict) -> np.ndarray | dict:
-    """To 32-bit numeric types. Recursively."""
-    if isinstance(value, pd.DataFrame):
-        return value.to_numpy()
-    if isinstance(value, (float, np.floating)) or (isinstance(value, np.ndarray) and value.dtype.kind == "f"):
-        return np.array(value, dtype=np.float32)
-    elif isinstance(value, (int, bool, np.integer)) or (isinstance(value, np.ndarray) and value.dtype.kind == "i"):
-        return np.array(value, dtype=np.int32)
-    elif isinstance(value, dict):
-        return {k: canonicalize(v) for k, v in value.items()}
-    else:
-        return value
-
-
-class FullHistoryObs(TypedDict):
-    data_processed: Any
-    data_processed_prev: Any
-    acquiring: Any
-    cur_tick: Any
-    cur_step: Any
-    num_step: Any
-    target: Any
-    position: Any
-    position_history: Any
-
-
-class DummyStateInterpreter(StateInterpreter[SAOEState, dict]):
-    """Dummy interpreter for policies that do not need inputs (for example, AllOne)."""
-
-    def interpret(self, state: SAOEState) -> dict:
-        # TODO: A fake state, used to pass `check_nan_observation`. Find a better way in the future.
-        return {"DUMMY": _to_int32(1)}
-
-    @property
-    def observation_space(self) -> spaces.Dict:
-        return spaces.Dict({"DUMMY": spaces.Box(-np.inf, np.inf, shape=(), dtype=np.int32)})
-
-
-class FullHistoryStateInterpreter(StateInterpreter[SAOEState, FullHistoryObs]):
-    """The observation of all the history, including today (until this moment), and yesterday.
-
-    Parameters
-    ----------
-    max_step
-        Total number of steps (an upper-bound estimation). For example, 390min / 30min-per-step = 13 steps.
-    data_ticks
-        Equal to the total number of records. For example, in SAOE per minute,
-        the total ticks is the length of day in minutes.
-    data_dim
-        Number of dimensions in data.
-    processed_data_provider
-        Provider of the processed data.
-    """
-
-    def __init__(
-        self,
-        max_step: int,
-        data_ticks: int,
-        data_dim: int,
-        processed_data_provider: dict | ProcessedDataProvider,
-    ) -> None:
-        super().__init__()
-
-        self.max_step = max_step
-        self.data_ticks = data_ticks
-        self.data_dim = data_dim
-        self.processed_data_provider: ProcessedDataProvider = init_instance_by_config(
-            processed_data_provider,
-            accept_types=ProcessedDataProvider,
-        )
-
-    def interpret(self, state: SAOEState) -> FullHistoryObs:
-        processed = self.processed_data_provider.get_data(
-            stock_id=state.order.stock_id,
-            date=pd.Timestamp(state.order.start_time.date()),
-            feature_dim=self.data_dim,
-            time_index=state.ticks_index,
-        )
-
-        position_history = np.full(self.max_step + 1, 0.0, dtype=np.float32)
-        position_history[0] = state.order.amount
-        position_history[1 : len(state.history_steps) + 1] = state.history_steps["position"].to_numpy()
-
-        # The min, slice here are to make sure that indices fit into the range,
-        # even after the final step of the simulator (in the done step),
-        # to make network in policy happy.
-        return cast(
-            FullHistoryObs,
-            canonicalize(
-                {
-                    "data_processed": np.array(self._mask_future_info(processed.today, state.cur_time)),
-                    "data_processed_prev": np.array(processed.yesterday),
-                    "acquiring": _to_int32(state.order.direction == state.order.BUY),
-                    "cur_tick": _to_int32(min(int(np.sum(state.ticks_index < state.cur_time)), self.data_ticks - 1)),
-                    "cur_step": _to_int32(min(state.cur_step, self.max_step - 1)),
-                    "num_step": _to_int32(self.max_step),
-                    "target": _to_float32(state.order.amount),
-                    "position": _to_float32(state.position),
-                    "position_history": _to_float32(position_history[: self.max_step]),
-                },
-            ),
-        )
-
-    @property
-    def observation_space(self) -> spaces.Dict:
-        space = {
-            "data_processed": spaces.Box(-np.inf, np.inf, shape=(self.data_ticks, self.data_dim)),
-            "data_processed_prev": spaces.Box(-np.inf, np.inf, shape=(self.data_ticks, self.data_dim)),
-            "acquiring": spaces.Discrete(2),
-            "cur_tick": spaces.Box(0, self.data_ticks - 1, shape=(), dtype=np.int32),
-            "cur_step": spaces.Box(0, self.max_step - 1, shape=(), dtype=np.int32),
-            # TODO: support arbitrary length index
-            "num_step": spaces.Box(self.max_step, self.max_step, shape=(), dtype=np.int32),
-            "target": spaces.Box(-EPS, np.inf, shape=()),
-            "position": spaces.Box(-EPS, np.inf, shape=()),
-            "position_history": spaces.Box(-EPS, np.inf, shape=(self.max_step,)),
-        }
-        return spaces.Dict(space)
-
-    @staticmethod
-    def _mask_future_info(arr: pd.DataFrame, current: pd.Timestamp) -> pd.DataFrame:
-        arr = arr.copy(deep=True)
-        arr.loc[current:] = 0.0  # mask out data after this moment (inclusive)
-        return arr
-
-
-class CurrentStateObs(TypedDict):
-    acquiring: bool
-    cur_step: int
-    num_step: int
-    target: float
-    position: float
-
-
-class CurrentStepStateInterpreter(StateInterpreter[SAOEState, CurrentStateObs]):
-    """The observation of current step.
-
-    Used when policy only depends on the latest state, but not history.
-    The key list is not full. You can add more if more information is needed by your policy.
-    """
-
-    def __init__(self, max_step: int) -> None:
-        super().__init__()
-
-        self.max_step = max_step
-
-    @property
-    def observation_space(self) -> spaces.Dict:
-        space = {
-            "acquiring": spaces.Discrete(2),
-            "cur_step": spaces.Box(0, self.max_step - 1, shape=(), dtype=np.int32),
-            "num_step": spaces.Box(self.max_step, self.max_step, shape=(), dtype=np.int32),
-            "target": spaces.Box(-EPS, np.inf, shape=()),
-            "position": spaces.Box(-EPS, np.inf, shape=()),
-        }
-        return spaces.Dict(space)
-
-    def interpret(self, state: SAOEState) -> CurrentStateObs:
-        assert state.cur_step <= self.max_step
-        obs = CurrentStateObs(
-            acquiring=state.order.direction == state.order.BUY,
-            cur_step=state.cur_step,
-            num_step=self.max_step,
-            target=state.order.amount,
-            position=state.position,
-        )
-        return obs
-
-
-class CategoricalActionInterpreter(ActionInterpreter[SAOEState, int, float]):
-    """Convert a discrete policy action to a continuous action, then multiplied by ``order.amount``.
-
-    Parameters
-    ----------
-    values
-        It can be a list of length $L$: $[a_1, a_2, \\ldots, a_L]$.
-        Then when policy givens decision $x$, $a_x$ times order amount is the output.
-        It can also be an integer $n$, in which case the list of length $n+1$ is auto-generated,
-        i.e., $[0, 1/n, 2/n, \\ldots, n/n]$.
-    max_step
-        Total number of steps (an upper-bound estimation). For example, 390min / 30min-per-step = 13 steps.
-    """
-
-    def __init__(self, values: int | List[float], max_step: Optional[int] = None) -> None:
-        super().__init__()
-
-        if isinstance(values, int):
-            values = [i / values for i in range(0, values + 1)]
-        self.action_values = values
-        self.max_step = max_step
-
-    @property
-    def action_space(self) -> spaces.Discrete:
-        return spaces.Discrete(len(self.action_values))
-
-    def interpret(self, state: SAOEState, action: int) -> float:
-        assert 0 <= action < len(self.action_values)
-        if self.max_step is not None and state.cur_step >= self.max_step - 1:
-            return state.position
-        else:
-            return min(state.position, state.order.amount * self.action_values[action])
-
-
-class TwapRelativeActionInterpreter(ActionInterpreter[SAOEState, float, float]):
-    """Convert a continuous ratio to deal amount.
-
-    The ratio is relative to TWAP on the remainder of the day.
-    For example, there are 5 steps left, and the left position is 300.
-    With TWAP strategy, in each position, 60 should be traded.
-    When this interpreter receives action $a$, its output is $60 \\cdot a$.
-    """
-
-    @property
-    def action_space(self) -> spaces.Box:
-        return spaces.Box(0, np.inf, shape=(), dtype=np.float32)
-
-    def interpret(self, state: SAOEState, action: float) -> float:
-        estimated_total_steps = math.ceil(len(state.ticks_for_order) / state.ticks_per_step)
-        twap_volume = state.position / (estimated_total_steps - state.cur_step)
-        return min(state.position, twap_volume * action)
-
-
-def _to_int32(val):
-    return np.array(int(val), dtype=np.int32)
-
-
-def _to_float32(val):
-    return np.array(val, dtype=np.float32)
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+from __future__ import annotations
+
+import math
+from typing import Any, List, Optional, cast
+
+import numpy as np
+import pandas as pd
+from gym import spaces
+
+from qlib.constant import EPS
+from qlib.rl.data.base import ProcessedDataProvider
+from qlib.rl.interpreter import ActionInterpreter, StateInterpreter
+from qlib.rl.order_execution.state import SAOEState
+from qlib.typehint import TypedDict
+
+__all__ = [
+    "FullHistoryStateInterpreter",
+    "CurrentStepStateInterpreter",
+    "CategoricalActionInterpreter",
+    "TwapRelativeActionInterpreter",
+    "FullHistoryObs",
+]
+
+from qlib.utils import init_instance_by_config
+
+
+def canonicalize(value: int | float | np.ndarray | pd.DataFrame | dict) -> np.ndarray | dict:
+    """To 32-bit numeric types. Recursively."""
+    if isinstance(value, pd.DataFrame):
+        return value.to_numpy()
+    if isinstance(value, (float, np.floating)) or (isinstance(value, np.ndarray) and value.dtype.kind == "f"):
+        return np.array(value, dtype=np.float32)
+    elif isinstance(value, (int, bool, np.integer)) or (isinstance(value, np.ndarray) and value.dtype.kind == "i"):
+        return np.array(value, dtype=np.int32)
+    elif isinstance(value, dict):
+        return {k: canonicalize(v) for k, v in value.items()}
+    else:
+        return value
+
+
+class FullHistoryObs(TypedDict):
+    data_processed: Any
+    data_processed_prev: Any
+    acquiring: Any
+    cur_tick: Any
+    cur_step: Any
+    num_step: Any
+    target: Any
+    position: Any
+    position_history: Any
+
+
+class DummyStateInterpreter(StateInterpreter[SAOEState, dict]):
+    """Dummy interpreter for policies that do not need inputs (for example, AllOne)."""
+
+    def interpret(self, state: SAOEState) -> dict:
+        # TODO: A fake state, used to pass `check_nan_observation`. Find a better way in the future.
+        return {"DUMMY": _to_int32(1)}
+
+    @property
+    def observation_space(self) -> spaces.Dict:
+        return spaces.Dict({"DUMMY": spaces.Box(-np.inf, np.inf, shape=(), dtype=np.int32)})
+
+
+class FullHistoryStateInterpreter(StateInterpreter[SAOEState, FullHistoryObs]):
+    """The observation of all the history, including today (until this moment), and yesterday.
+
+    Parameters
+    ----------
+    max_step
+        Total number of steps (an upper-bound estimation). For example, 390min / 30min-per-step = 13 steps.
+    data_ticks
+        Equal to the total number of records. For example, in SAOE per minute,
+        the total ticks is the length of day in minutes.
+    data_dim
+        Number of dimensions in data.
+    processed_data_provider
+        Provider of the processed data.
+    """
+
+    def __init__(
+        self,
+        max_step: int,
+        data_ticks: int,
+        data_dim: int,
+        processed_data_provider: dict | ProcessedDataProvider,
+    ) -> None:
+        super().__init__()
+
+        self.max_step = max_step
+        self.data_ticks = data_ticks
+        self.data_dim = data_dim
+        self.processed_data_provider: ProcessedDataProvider = init_instance_by_config(
+            processed_data_provider,
+            accept_types=ProcessedDataProvider,
+        )
+
+    def interpret(self, state: SAOEState) -> FullHistoryObs:
+        processed = self.processed_data_provider.get_data(
+            stock_id=state.order.stock_id,
+            date=pd.Timestamp(state.order.start_time.date()),
+            feature_dim=self.data_dim,
+            time_index=state.ticks_index,
+        )
+
+        position_history = np.full(self.max_step + 1, 0.0, dtype=np.float32)
+        position_history[0] = state.order.amount
+        position_history[1 : len(state.history_steps) + 1] = state.history_steps["position"].to_numpy()
+
+        # The min, slice here are to make sure that indices fit into the range,
+        # even after the final step of the simulator (in the done step),
+        # to make network in policy happy.
+        return cast(
+            FullHistoryObs,
+            canonicalize(
+                {
+                    "data_processed": np.array(self._mask_future_info(processed.today, state.cur_time)),
+                    "data_processed_prev": np.array(processed.yesterday),
+                    "acquiring": _to_int32(state.order.direction == state.order.BUY),
+                    "cur_tick": _to_int32(min(int(np.sum(state.ticks_index < state.cur_time)), self.data_ticks - 1)),
+                    "cur_step": _to_int32(min(state.cur_step, self.max_step - 1)),
+                    "num_step": _to_int32(self.max_step),
+                    "target": _to_float32(state.order.amount),
+                    "position": _to_float32(state.position),
+                    "position_history": _to_float32(position_history[: self.max_step]),
+                },
+            ),
+        )
+
+    @property
+    def observation_space(self) -> spaces.Dict:
+        space = {
+            "data_processed": spaces.Box(-np.inf, np.inf, shape=(self.data_ticks, self.data_dim)),
+            "data_processed_prev": spaces.Box(-np.inf, np.inf, shape=(self.data_ticks, self.data_dim)),
+            "acquiring": spaces.Discrete(2),
+            "cur_tick": spaces.Box(0, self.data_ticks - 1, shape=(), dtype=np.int32),
+            "cur_step": spaces.Box(0, self.max_step - 1, shape=(), dtype=np.int32),
+            # TODO: support arbitrary length index
+            "num_step": spaces.Box(self.max_step, self.max_step, shape=(), dtype=np.int32),
+            "target": spaces.Box(-EPS, np.inf, shape=()),
+            "position": spaces.Box(-EPS, np.inf, shape=()),
+            "position_history": spaces.Box(-EPS, np.inf, shape=(self.max_step,)),
+        }
+        return spaces.Dict(space)
+
+    @staticmethod
+    def _mask_future_info(arr: pd.DataFrame, current: pd.Timestamp) -> pd.DataFrame:
+        arr = arr.copy(deep=True)
+        arr.loc[current:] = 0.0  # mask out data after this moment (inclusive)
+        return arr
+
+
+class CurrentStateObs(TypedDict):
+    acquiring: bool
+    cur_step: int
+    num_step: int
+    target: float
+    position: float
+
+
+class CurrentStepStateInterpreter(StateInterpreter[SAOEState, CurrentStateObs]):
+    """The observation of current step.
+
+    Used when policy only depends on the latest state, but not history.
+    The key list is not full. You can add more if more information is needed by your policy.
+    """
+
+    def __init__(self, max_step: int) -> None:
+        super().__init__()
+
+        self.max_step = max_step
+
+    @property
+    def observation_space(self) -> spaces.Dict:
+        space = {
+            "acquiring": spaces.Discrete(2),
+            "cur_step": spaces.Box(0, self.max_step - 1, shape=(), dtype=np.int32),
+            "num_step": spaces.Box(self.max_step, self.max_step, shape=(), dtype=np.int32),
+            "target": spaces.Box(-EPS, np.inf, shape=()),
+            "position": spaces.Box(-EPS, np.inf, shape=()),
+        }
+        return spaces.Dict(space)
+
+    def interpret(self, state: SAOEState) -> CurrentStateObs:
+        assert state.cur_step <= self.max_step
+        obs = CurrentStateObs(
+            acquiring=state.order.direction == state.order.BUY,
+            cur_step=state.cur_step,
+            num_step=self.max_step,
+            target=state.order.amount,
+            position=state.position,
+        )
+        return obs
+
+
+class CategoricalActionInterpreter(ActionInterpreter[SAOEState, int, float]):
+    """Convert a discrete policy action to a continuous action, then multiplied by ``order.amount``.
+
+    Parameters
+    ----------
+    values
+        It can be a list of length $L$: $[a_1, a_2, \\ldots, a_L]$.
+        Then when policy givens decision $x$, $a_x$ times order amount is the output.
+        It can also be an integer $n$, in which case the list of length $n+1$ is auto-generated,
+        i.e., $[0, 1/n, 2/n, \\ldots, n/n]$.
+    max_step
+        Total number of steps (an upper-bound estimation). For example, 390min / 30min-per-step = 13 steps.
+    """
+
+    def __init__(self, values: int | List[float], max_step: Optional[int] = None) -> None:
+        super().__init__()
+
+        if isinstance(values, int):
+            values = [i / values for i in range(0, values + 1)]
+        self.action_values = values
+        self.max_step = max_step
+
+    @property
+    def action_space(self) -> spaces.Discrete:
+        return spaces.Discrete(len(self.action_values))
+
+    def interpret(self, state: SAOEState, action: int) -> float:
+        assert 0 <= action < len(self.action_values)
+        if self.max_step is not None and state.cur_step >= self.max_step - 1:
+            return state.position
+        else:
+            return min(state.position, state.order.amount * self.action_values[action])
+
+
+class TwapRelativeActionInterpreter(ActionInterpreter[SAOEState, float, float]):
+    """Convert a continuous ratio to deal amount.
+
+    The ratio is relative to TWAP on the remainder of the day.
+    For example, there are 5 steps left, and the left position is 300.
+    With TWAP strategy, in each position, 60 should be traded.
+    When this interpreter receives action $a$, its output is $60 \\cdot a$.
+    """
+
+    @property
+    def action_space(self) -> spaces.Box:
+        return spaces.Box(0, np.inf, shape=(), dtype=np.float32)
+
+    def interpret(self, state: SAOEState, action: float) -> float:
+        estimated_total_steps = math.ceil(len(state.ticks_for_order) / state.ticks_per_step)
+        twap_volume = state.position / (estimated_total_steps - state.cur_step)
+        return min(state.position, twap_volume * action)
+
+
+def _to_int32(val):
+    return np.array(int(val), dtype=np.int32)
+
+
+def _to_float32(val):
+    return np.array(val, dtype=np.float32)
```

## qlib/rl/order_execution/network.py

 * *Ordering differences only*

```diff
@@ -1,140 +1,140 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-from __future__ import annotations
-
-from typing import List, Tuple, cast
-
-import torch
-import torch.nn as nn
-from tianshou.data import Batch
-
-from qlib.typehint import Literal
-
-from .interpreter import FullHistoryObs
-
-__all__ = ["Recurrent"]
-
-
-class Recurrent(nn.Module):
-    """The network architecture proposed in `OPD <https://seqml.github.io/opd/opd_aaai21_supplement.pdf>`_.
-
-    At every time step the input of policy network is divided into two parts,
-    the public variables and the private variables. which are handled by ``raw_rnn``
-    and ``pri_rnn`` in this network, respectively.
-
-    One minor difference is that, in this implementation, we don't assume the direction to be fixed.
-    Thus, another ``dire_fc`` is added to produce an extra direction-related feature.
-    """
-
-    def __init__(
-        self,
-        obs_space: FullHistoryObs,
-        hidden_dim: int = 64,
-        output_dim: int = 32,
-        rnn_type: Literal["rnn", "lstm", "gru"] = "gru",
-        rnn_num_layers: int = 1,
-    ) -> None:
-        super().__init__()
-
-        self.hidden_dim = hidden_dim
-        self.output_dim = output_dim
-        self.num_sources = 3
-
-        rnn_classes = {"rnn": nn.RNN, "lstm": nn.LSTM, "gru": nn.GRU}
-
-        self.rnn_class = rnn_classes[rnn_type]
-        self.rnn_layers = rnn_num_layers
-
-        self.raw_rnn = self.rnn_class(hidden_dim, hidden_dim, batch_first=True, num_layers=self.rnn_layers)
-        self.prev_rnn = self.rnn_class(hidden_dim, hidden_dim, batch_first=True, num_layers=self.rnn_layers)
-        self.pri_rnn = self.rnn_class(hidden_dim, hidden_dim, batch_first=True, num_layers=self.rnn_layers)
-
-        self.raw_fc = nn.Sequential(nn.Linear(obs_space["data_processed"].shape[-1], hidden_dim), nn.ReLU())
-        self.pri_fc = nn.Sequential(nn.Linear(2, hidden_dim), nn.ReLU())
-        self.dire_fc = nn.Sequential(nn.Linear(2, hidden_dim), nn.ReLU(), nn.Linear(hidden_dim, hidden_dim), nn.ReLU())
-
-        self._init_extra_branches()
-
-        self.fc = nn.Sequential(
-            nn.Linear(hidden_dim * self.num_sources, hidden_dim),
-            nn.ReLU(),
-            nn.Linear(hidden_dim, output_dim),
-            nn.ReLU(),
-        )
-
-    def _init_extra_branches(self) -> None:
-        pass
-
-    def _source_features(self, obs: FullHistoryObs, device: torch.device) -> Tuple[List[torch.Tensor], torch.Tensor]:
-        bs, _, data_dim = obs["data_processed"].size()
-        data = torch.cat((torch.zeros(bs, 1, data_dim, device=device), obs["data_processed"]), 1)
-        cur_step = obs["cur_step"].long()
-        cur_tick = obs["cur_tick"].long()
-        bs_indices = torch.arange(bs, device=device)
-
-        position = obs["position_history"] / obs["target"].unsqueeze(-1)  # [bs, num_step]
-        steps = (
-            torch.arange(position.size(-1), device=device).unsqueeze(0).repeat(bs, 1).float()
-            / obs["num_step"].unsqueeze(-1).float()
-        )  # [bs, num_step]
-        priv = torch.stack((position.float(), steps), -1)
-
-        data_in = self.raw_fc(data)
-        data_out, _ = self.raw_rnn(data_in)
-        # as it is padded with zero in front, this should be last minute
-        data_out_slice = data_out[bs_indices, cur_tick]
-
-        priv_in = self.pri_fc(priv)
-        priv_out = self.pri_rnn(priv_in)[0]
-        priv_out = priv_out[bs_indices, cur_step]
-
-        sources = [data_out_slice, priv_out]
-
-        dir_out = self.dire_fc(torch.stack((obs["acquiring"], 1 - obs["acquiring"]), -1).float())
-        sources.append(dir_out)
-
-        return sources, data_out
-
-    def forward(self, batch: Batch) -> torch.Tensor:
-        """
-        Input should be a dict (at least) containing:
-
-        - data_processed: [N, T, C]
-        - cur_step: [N]  (int)
-        - cur_time: [N]  (int)
-        - position_history: [N, S]  (S is number of steps)
-        - target: [N]
-        - num_step: [N]  (int)
-        - acquiring: [N]  (0 or 1)
-        """
-
-        inp = cast(FullHistoryObs, batch)
-        device = inp["data_processed"].device
-
-        sources, _ = self._source_features(inp, device)
-        assert len(sources) == self.num_sources
-
-        out = torch.cat(sources, -1)
-        return self.fc(out)
-
-
-class Attention(nn.Module):
-    def __init__(self, in_dim, out_dim):
-        super().__init__()
-        self.q_net = nn.Linear(in_dim, out_dim)
-        self.k_net = nn.Linear(in_dim, out_dim)
-        self.v_net = nn.Linear(in_dim, out_dim)
-
-    def forward(self, Q, K, V):
-        q = self.q_net(Q)
-        k = self.k_net(K)
-        v = self.v_net(V)
-
-        attn = torch.einsum("ijk,ilk->ijl", q, k)
-        attn = attn.to(Q.device)
-        attn_prob = torch.softmax(attn, dim=-1)
-
-        attn_vec = torch.einsum("ijk,ikl->ijl", attn_prob, v)
-
-        return attn_vec
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+from __future__ import annotations
+
+from typing import List, Tuple, cast
+
+import torch
+import torch.nn as nn
+from tianshou.data import Batch
+
+from qlib.typehint import Literal
+
+from .interpreter import FullHistoryObs
+
+__all__ = ["Recurrent"]
+
+
+class Recurrent(nn.Module):
+    """The network architecture proposed in `OPD <https://seqml.github.io/opd/opd_aaai21_supplement.pdf>`_.
+
+    At every time step the input of policy network is divided into two parts,
+    the public variables and the private variables. which are handled by ``raw_rnn``
+    and ``pri_rnn`` in this network, respectively.
+
+    One minor difference is that, in this implementation, we don't assume the direction to be fixed.
+    Thus, another ``dire_fc`` is added to produce an extra direction-related feature.
+    """
+
+    def __init__(
+        self,
+        obs_space: FullHistoryObs,
+        hidden_dim: int = 64,
+        output_dim: int = 32,
+        rnn_type: Literal["rnn", "lstm", "gru"] = "gru",
+        rnn_num_layers: int = 1,
+    ) -> None:
+        super().__init__()
+
+        self.hidden_dim = hidden_dim
+        self.output_dim = output_dim
+        self.num_sources = 3
+
+        rnn_classes = {"rnn": nn.RNN, "lstm": nn.LSTM, "gru": nn.GRU}
+
+        self.rnn_class = rnn_classes[rnn_type]
+        self.rnn_layers = rnn_num_layers
+
+        self.raw_rnn = self.rnn_class(hidden_dim, hidden_dim, batch_first=True, num_layers=self.rnn_layers)
+        self.prev_rnn = self.rnn_class(hidden_dim, hidden_dim, batch_first=True, num_layers=self.rnn_layers)
+        self.pri_rnn = self.rnn_class(hidden_dim, hidden_dim, batch_first=True, num_layers=self.rnn_layers)
+
+        self.raw_fc = nn.Sequential(nn.Linear(obs_space["data_processed"].shape[-1], hidden_dim), nn.ReLU())
+        self.pri_fc = nn.Sequential(nn.Linear(2, hidden_dim), nn.ReLU())
+        self.dire_fc = nn.Sequential(nn.Linear(2, hidden_dim), nn.ReLU(), nn.Linear(hidden_dim, hidden_dim), nn.ReLU())
+
+        self._init_extra_branches()
+
+        self.fc = nn.Sequential(
+            nn.Linear(hidden_dim * self.num_sources, hidden_dim),
+            nn.ReLU(),
+            nn.Linear(hidden_dim, output_dim),
+            nn.ReLU(),
+        )
+
+    def _init_extra_branches(self) -> None:
+        pass
+
+    def _source_features(self, obs: FullHistoryObs, device: torch.device) -> Tuple[List[torch.Tensor], torch.Tensor]:
+        bs, _, data_dim = obs["data_processed"].size()
+        data = torch.cat((torch.zeros(bs, 1, data_dim, device=device), obs["data_processed"]), 1)
+        cur_step = obs["cur_step"].long()
+        cur_tick = obs["cur_tick"].long()
+        bs_indices = torch.arange(bs, device=device)
+
+        position = obs["position_history"] / obs["target"].unsqueeze(-1)  # [bs, num_step]
+        steps = (
+            torch.arange(position.size(-1), device=device).unsqueeze(0).repeat(bs, 1).float()
+            / obs["num_step"].unsqueeze(-1).float()
+        )  # [bs, num_step]
+        priv = torch.stack((position.float(), steps), -1)
+
+        data_in = self.raw_fc(data)
+        data_out, _ = self.raw_rnn(data_in)
+        # as it is padded with zero in front, this should be last minute
+        data_out_slice = data_out[bs_indices, cur_tick]
+
+        priv_in = self.pri_fc(priv)
+        priv_out = self.pri_rnn(priv_in)[0]
+        priv_out = priv_out[bs_indices, cur_step]
+
+        sources = [data_out_slice, priv_out]
+
+        dir_out = self.dire_fc(torch.stack((obs["acquiring"], 1 - obs["acquiring"]), -1).float())
+        sources.append(dir_out)
+
+        return sources, data_out
+
+    def forward(self, batch: Batch) -> torch.Tensor:
+        """
+        Input should be a dict (at least) containing:
+
+        - data_processed: [N, T, C]
+        - cur_step: [N]  (int)
+        - cur_time: [N]  (int)
+        - position_history: [N, S]  (S is number of steps)
+        - target: [N]
+        - num_step: [N]  (int)
+        - acquiring: [N]  (0 or 1)
+        """
+
+        inp = cast(FullHistoryObs, batch)
+        device = inp["data_processed"].device
+
+        sources, _ = self._source_features(inp, device)
+        assert len(sources) == self.num_sources
+
+        out = torch.cat(sources, -1)
+        return self.fc(out)
+
+
+class Attention(nn.Module):
+    def __init__(self, in_dim, out_dim):
+        super().__init__()
+        self.q_net = nn.Linear(in_dim, out_dim)
+        self.k_net = nn.Linear(in_dim, out_dim)
+        self.v_net = nn.Linear(in_dim, out_dim)
+
+    def forward(self, Q, K, V):
+        q = self.q_net(Q)
+        k = self.k_net(K)
+        v = self.v_net(V)
+
+        attn = torch.einsum("ijk,ilk->ijl", q, k)
+        attn = attn.to(Q.device)
+        attn_prob = torch.softmax(attn, dim=-1)
+
+        attn_vec = torch.einsum("ijk,ikl->ijl", attn_prob, v)
+
+        return attn_vec
```

## qlib/rl/order_execution/policy.py

 * *Ordering differences only*

```diff
@@ -1,237 +1,237 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-from __future__ import annotations
-
-from pathlib import Path
-from typing import Any, Dict, Generator, Iterable, Optional, OrderedDict, Tuple, cast
-
-import gym
-import numpy as np
-import torch
-import torch.nn as nn
-from gym.spaces import Discrete
-from tianshou.data import Batch, ReplayBuffer, to_torch
-from tianshou.policy import BasePolicy, PPOPolicy, DQNPolicy
-
-from qlib.rl.trainer.trainer import Trainer
-
-__all__ = ["AllOne", "PPO", "DQN"]
-
-
-# baselines #
-
-
-class NonLearnablePolicy(BasePolicy):
-    """Tianshou's BasePolicy with empty ``learn`` and ``process_fn``.
-
-    This could be moved outside in future.
-    """
-
-    def __init__(self, obs_space: gym.Space, action_space: gym.Space) -> None:
-        super().__init__()
-
-    def learn(self, batch: Batch, **kwargs: Any) -> Dict[str, Any]:
-        return {}
-
-    def process_fn(
-        self,
-        batch: Batch,
-        buffer: ReplayBuffer,
-        indices: np.ndarray,
-    ) -> Batch:
-        return Batch({})
-
-
-class AllOne(NonLearnablePolicy):
-    """Forward returns a batch full of 1.
-
-    Useful when implementing some baselines (e.g., TWAP).
-    """
-
-    def __init__(self, obs_space: gym.Space, action_space: gym.Space, fill_value: float | int = 1.0) -> None:
-        super().__init__(obs_space, action_space)
-
-        self.fill_value = fill_value
-
-    def forward(
-        self,
-        batch: Batch,
-        state: dict | Batch | np.ndarray = None,
-        **kwargs: Any,
-    ) -> Batch:
-        return Batch(act=np.full(len(batch), self.fill_value), state=state)
-
-
-# ppo #
-
-
-class PPOActor(nn.Module):
-    def __init__(self, extractor: nn.Module, action_dim: int) -> None:
-        super().__init__()
-        self.extractor = extractor
-        self.layer_out = nn.Sequential(nn.Linear(cast(int, extractor.output_dim), action_dim), nn.Softmax(dim=-1))
-
-    def forward(
-        self,
-        obs: torch.Tensor,
-        state: torch.Tensor = None,
-        info: dict = {},
-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:
-        feature = self.extractor(to_torch(obs, device=auto_device(self)))
-        out = self.layer_out(feature)
-        return out, state
-
-
-class PPOCritic(nn.Module):
-    def __init__(self, extractor: nn.Module) -> None:
-        super().__init__()
-        self.extractor = extractor
-        self.value_out = nn.Linear(cast(int, extractor.output_dim), 1)
-
-    def forward(
-        self,
-        obs: torch.Tensor,
-        state: torch.Tensor = None,
-        info: dict = {},
-    ) -> torch.Tensor:
-        feature = self.extractor(to_torch(obs, device=auto_device(self)))
-        return self.value_out(feature).squeeze(dim=-1)
-
-
-class PPO(PPOPolicy):
-    """A wrapper of tianshou PPOPolicy.
-
-    Differences:
-
-    - Auto-create actor and critic network. Supports discrete action space only.
-    - Dedup common parameters between actor network and critic network
-      (not sure whether this is included in latest tianshou or not).
-    - Support a ``weight_file`` that supports loading checkpoint.
-    - Some parameters' default values are different from original.
-    """
-
-    def __init__(
-        self,
-        network: nn.Module,
-        obs_space: gym.Space,
-        action_space: gym.Space,
-        lr: float,
-        weight_decay: float = 0.0,
-        discount_factor: float = 1.0,
-        max_grad_norm: float = 100.0,
-        reward_normalization: bool = True,
-        eps_clip: float = 0.3,
-        value_clip: bool = True,
-        vf_coef: float = 1.0,
-        gae_lambda: float = 1.0,
-        max_batch_size: int = 256,
-        deterministic_eval: bool = True,
-        weight_file: Optional[Path] = None,
-    ) -> None:
-        assert isinstance(action_space, Discrete)
-        actor = PPOActor(network, action_space.n)
-        critic = PPOCritic(network)
-        optimizer = torch.optim.Adam(
-            chain_dedup(actor.parameters(), critic.parameters()),
-            lr=lr,
-            weight_decay=weight_decay,
-        )
-        super().__init__(
-            actor,
-            critic,
-            optimizer,
-            torch.distributions.Categorical,
-            discount_factor=discount_factor,
-            max_grad_norm=max_grad_norm,
-            reward_normalization=reward_normalization,
-            eps_clip=eps_clip,
-            value_clip=value_clip,
-            vf_coef=vf_coef,
-            gae_lambda=gae_lambda,
-            max_batchsize=max_batch_size,
-            deterministic_eval=deterministic_eval,
-            observation_space=obs_space,
-            action_space=action_space,
-        )
-        if weight_file is not None:
-            set_weight(self, Trainer.get_policy_state_dict(weight_file))
-
-
-DQNModel = PPOActor  # Reuse PPOActor.
-
-
-class DQN(DQNPolicy):
-    """A wrapper of tianshou DQNPolicy.
-
-    Differences:
-
-    - Auto-create model network. Supports discrete action space only.
-    - Support a ``weight_file`` that supports loading checkpoint.
-    """
-
-    def __init__(
-        self,
-        network: nn.Module,
-        obs_space: gym.Space,
-        action_space: gym.Space,
-        lr: float,
-        weight_decay: float = 0.0,
-        discount_factor: float = 0.99,
-        estimation_step: int = 1,
-        target_update_freq: int = 0,
-        reward_normalization: bool = False,
-        is_double: bool = True,
-        clip_loss_grad: bool = False,
-        weight_file: Optional[Path] = None,
-    ) -> None:
-        assert isinstance(action_space, Discrete)
-
-        model = DQNModel(network, action_space.n)
-        optimizer = torch.optim.Adam(
-            model.parameters(),
-            lr=lr,
-            weight_decay=weight_decay,
-        )
-
-        super().__init__(
-            model,
-            optimizer,
-            discount_factor=discount_factor,
-            estimation_step=estimation_step,
-            target_update_freq=target_update_freq,
-            reward_normalization=reward_normalization,
-            is_double=is_double,
-            clip_loss_grad=clip_loss_grad,
-        )
-        if weight_file is not None:
-            set_weight(self, Trainer.get_policy_state_dict(weight_file))
-
-
-# utilities: these should be put in a separate (common) file. #
-
-
-def auto_device(module: nn.Module) -> torch.device:
-    for param in module.parameters():
-        return param.device
-    return torch.device("cpu")  # fallback to cpu
-
-
-def set_weight(policy: nn.Module, loaded_weight: OrderedDict) -> None:
-    try:
-        policy.load_state_dict(loaded_weight)
-    except RuntimeError:
-        # try again by loading the converted weight
-        # https://github.com/thu-ml/tianshou/issues/468
-        for k in list(loaded_weight):
-            loaded_weight["_actor_critic." + k] = loaded_weight[k]
-        policy.load_state_dict(loaded_weight)
-
-
-def chain_dedup(*iterables: Iterable) -> Generator[Any, None, None]:
-    seen = set()
-    for iterable in iterables:
-        for i in iterable:
-            if i not in seen:
-                seen.add(i)
-                yield i
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+from __future__ import annotations
+
+from pathlib import Path
+from typing import Any, Dict, Generator, Iterable, Optional, OrderedDict, Tuple, cast
+
+import gym
+import numpy as np
+import torch
+import torch.nn as nn
+from gym.spaces import Discrete
+from tianshou.data import Batch, ReplayBuffer, to_torch
+from tianshou.policy import BasePolicy, PPOPolicy, DQNPolicy
+
+from qlib.rl.trainer.trainer import Trainer
+
+__all__ = ["AllOne", "PPO", "DQN"]
+
+
+# baselines #
+
+
+class NonLearnablePolicy(BasePolicy):
+    """Tianshou's BasePolicy with empty ``learn`` and ``process_fn``.
+
+    This could be moved outside in future.
+    """
+
+    def __init__(self, obs_space: gym.Space, action_space: gym.Space) -> None:
+        super().__init__()
+
+    def learn(self, batch: Batch, **kwargs: Any) -> Dict[str, Any]:
+        return {}
+
+    def process_fn(
+        self,
+        batch: Batch,
+        buffer: ReplayBuffer,
+        indices: np.ndarray,
+    ) -> Batch:
+        return Batch({})
+
+
+class AllOne(NonLearnablePolicy):
+    """Forward returns a batch full of 1.
+
+    Useful when implementing some baselines (e.g., TWAP).
+    """
+
+    def __init__(self, obs_space: gym.Space, action_space: gym.Space, fill_value: float | int = 1.0) -> None:
+        super().__init__(obs_space, action_space)
+
+        self.fill_value = fill_value
+
+    def forward(
+        self,
+        batch: Batch,
+        state: dict | Batch | np.ndarray = None,
+        **kwargs: Any,
+    ) -> Batch:
+        return Batch(act=np.full(len(batch), self.fill_value), state=state)
+
+
+# ppo #
+
+
+class PPOActor(nn.Module):
+    def __init__(self, extractor: nn.Module, action_dim: int) -> None:
+        super().__init__()
+        self.extractor = extractor
+        self.layer_out = nn.Sequential(nn.Linear(cast(int, extractor.output_dim), action_dim), nn.Softmax(dim=-1))
+
+    def forward(
+        self,
+        obs: torch.Tensor,
+        state: torch.Tensor = None,
+        info: dict = {},
+    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:
+        feature = self.extractor(to_torch(obs, device=auto_device(self)))
+        out = self.layer_out(feature)
+        return out, state
+
+
+class PPOCritic(nn.Module):
+    def __init__(self, extractor: nn.Module) -> None:
+        super().__init__()
+        self.extractor = extractor
+        self.value_out = nn.Linear(cast(int, extractor.output_dim), 1)
+
+    def forward(
+        self,
+        obs: torch.Tensor,
+        state: torch.Tensor = None,
+        info: dict = {},
+    ) -> torch.Tensor:
+        feature = self.extractor(to_torch(obs, device=auto_device(self)))
+        return self.value_out(feature).squeeze(dim=-1)
+
+
+class PPO(PPOPolicy):
+    """A wrapper of tianshou PPOPolicy.
+
+    Differences:
+
+    - Auto-create actor and critic network. Supports discrete action space only.
+    - Dedup common parameters between actor network and critic network
+      (not sure whether this is included in latest tianshou or not).
+    - Support a ``weight_file`` that supports loading checkpoint.
+    - Some parameters' default values are different from original.
+    """
+
+    def __init__(
+        self,
+        network: nn.Module,
+        obs_space: gym.Space,
+        action_space: gym.Space,
+        lr: float,
+        weight_decay: float = 0.0,
+        discount_factor: float = 1.0,
+        max_grad_norm: float = 100.0,
+        reward_normalization: bool = True,
+        eps_clip: float = 0.3,
+        value_clip: bool = True,
+        vf_coef: float = 1.0,
+        gae_lambda: float = 1.0,
+        max_batch_size: int = 256,
+        deterministic_eval: bool = True,
+        weight_file: Optional[Path] = None,
+    ) -> None:
+        assert isinstance(action_space, Discrete)
+        actor = PPOActor(network, action_space.n)
+        critic = PPOCritic(network)
+        optimizer = torch.optim.Adam(
+            chain_dedup(actor.parameters(), critic.parameters()),
+            lr=lr,
+            weight_decay=weight_decay,
+        )
+        super().__init__(
+            actor,
+            critic,
+            optimizer,
+            torch.distributions.Categorical,
+            discount_factor=discount_factor,
+            max_grad_norm=max_grad_norm,
+            reward_normalization=reward_normalization,
+            eps_clip=eps_clip,
+            value_clip=value_clip,
+            vf_coef=vf_coef,
+            gae_lambda=gae_lambda,
+            max_batchsize=max_batch_size,
+            deterministic_eval=deterministic_eval,
+            observation_space=obs_space,
+            action_space=action_space,
+        )
+        if weight_file is not None:
+            set_weight(self, Trainer.get_policy_state_dict(weight_file))
+
+
+DQNModel = PPOActor  # Reuse PPOActor.
+
+
+class DQN(DQNPolicy):
+    """A wrapper of tianshou DQNPolicy.
+
+    Differences:
+
+    - Auto-create model network. Supports discrete action space only.
+    - Support a ``weight_file`` that supports loading checkpoint.
+    """
+
+    def __init__(
+        self,
+        network: nn.Module,
+        obs_space: gym.Space,
+        action_space: gym.Space,
+        lr: float,
+        weight_decay: float = 0.0,
+        discount_factor: float = 0.99,
+        estimation_step: int = 1,
+        target_update_freq: int = 0,
+        reward_normalization: bool = False,
+        is_double: bool = True,
+        clip_loss_grad: bool = False,
+        weight_file: Optional[Path] = None,
+    ) -> None:
+        assert isinstance(action_space, Discrete)
+
+        model = DQNModel(network, action_space.n)
+        optimizer = torch.optim.Adam(
+            model.parameters(),
+            lr=lr,
+            weight_decay=weight_decay,
+        )
+
+        super().__init__(
+            model,
+            optimizer,
+            discount_factor=discount_factor,
+            estimation_step=estimation_step,
+            target_update_freq=target_update_freq,
+            reward_normalization=reward_normalization,
+            is_double=is_double,
+            clip_loss_grad=clip_loss_grad,
+        )
+        if weight_file is not None:
+            set_weight(self, Trainer.get_policy_state_dict(weight_file))
+
+
+# utilities: these should be put in a separate (common) file. #
+
+
+def auto_device(module: nn.Module) -> torch.device:
+    for param in module.parameters():
+        return param.device
+    return torch.device("cpu")  # fallback to cpu
+
+
+def set_weight(policy: nn.Module, loaded_weight: OrderedDict) -> None:
+    try:
+        policy.load_state_dict(loaded_weight)
+    except RuntimeError:
+        # try again by loading the converted weight
+        # https://github.com/thu-ml/tianshou/issues/468
+        for k in list(loaded_weight):
+            loaded_weight["_actor_critic." + k] = loaded_weight[k]
+        policy.load_state_dict(loaded_weight)
+
+
+def chain_dedup(*iterables: Iterable) -> Generator[Any, None, None]:
+    seen = set()
+    for iterable in iterables:
+        for i in iterable:
+            if i not in seen:
+                seen.add(i)
+                yield i
```

## qlib/rl/order_execution/reward.py

 * *Ordering differences only*

```diff
@@ -1,99 +1,99 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-from __future__ import annotations
-
-from typing import cast
-
-import numpy as np
-
-from qlib.backtest.decision import OrderDir
-from qlib.rl.order_execution.state import SAOEMetrics, SAOEState
-from qlib.rl.reward import Reward
-
-__all__ = ["PAPenaltyReward"]
-
-
-class PAPenaltyReward(Reward[SAOEState]):
-    """Encourage higher PAs, but penalize stacking all the amounts within a very short time.
-    Formally, for each time step, the reward is :math:`(PA_t * vol_t / target - vol_t^2 * penalty)`.
-
-    Parameters
-    ----------
-    penalty
-        The penalty for large volume in a short time.
-    scale
-        The weight used to scale up or down the reward.
-    """
-
-    def __init__(self, penalty: float = 100.0, scale: float = 1.0) -> None:
-        self.penalty = penalty
-        self.scale = scale
-
-    def reward(self, simulator_state: SAOEState) -> float:
-        whole_order = simulator_state.order.amount
-        assert whole_order > 0
-        last_step = cast(SAOEMetrics, simulator_state.history_steps.reset_index().iloc[-1].to_dict())
-        pa = last_step["pa"] * last_step["amount"] / whole_order
-
-        # Inspect the "break-down" of the latest step: trading amount at every tick
-        last_step_breakdown = simulator_state.history_exec.loc[last_step["datetime"] :]
-        penalty = -self.penalty * ((last_step_breakdown["amount"] / whole_order) ** 2).sum()
-
-        reward = pa + penalty
-
-        # Throw error in case of NaN
-        assert not (np.isnan(reward) or np.isinf(reward)), f"Invalid reward for simulator state: {simulator_state}"
-
-        self.log("reward/pa", pa)
-        self.log("reward/penalty", penalty)
-        return reward * self.scale
-
-
-class PPOReward(Reward[SAOEState]):
-    """Reward proposed by paper "An End-to-End Optimal Trade Execution Framework based on Proximal Policy Optimization".
-
-    Parameters
-    ----------
-    max_step
-        Maximum number of steps.
-    start_time_index
-        First time index that allowed to trade.
-    end_time_index
-        Last time index that allowed to trade.
-    """
-
-    def __init__(self, max_step: int, start_time_index: int = 0, end_time_index: int = 239) -> None:
-        self.max_step = max_step
-        self.start_time_index = start_time_index
-        self.end_time_index = end_time_index
-
-    def reward(self, simulator_state: SAOEState) -> float:
-        if simulator_state.cur_step == self.max_step - 1 or simulator_state.position < 1e-6:
-            if simulator_state.history_exec["deal_amount"].sum() == 0.0:
-                vwap_price = cast(
-                    float,
-                    np.average(simulator_state.history_exec["market_price"]),
-                )
-            else:
-                vwap_price = cast(
-                    float,
-                    np.average(
-                        simulator_state.history_exec["market_price"],
-                        weights=simulator_state.history_exec["deal_amount"],
-                    ),
-                )
-            twap_price = simulator_state.backtest_data.get_deal_price().mean()
-
-            if simulator_state.order.direction == OrderDir.SELL:
-                ratio = vwap_price / twap_price if twap_price != 0 else 1.0
-            else:
-                ratio = twap_price / vwap_price if vwap_price != 0 else 1.0
-            if ratio < 1.0:
-                return -1.0
-            elif ratio < 1.1:
-                return 0.0
-            else:
-                return 1.0
-        else:
-            return 0.0
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+from __future__ import annotations
+
+from typing import cast
+
+import numpy as np
+
+from qlib.backtest.decision import OrderDir
+from qlib.rl.order_execution.state import SAOEMetrics, SAOEState
+from qlib.rl.reward import Reward
+
+__all__ = ["PAPenaltyReward"]
+
+
+class PAPenaltyReward(Reward[SAOEState]):
+    """Encourage higher PAs, but penalize stacking all the amounts within a very short time.
+    Formally, for each time step, the reward is :math:`(PA_t * vol_t / target - vol_t^2 * penalty)`.
+
+    Parameters
+    ----------
+    penalty
+        The penalty for large volume in a short time.
+    scale
+        The weight used to scale up or down the reward.
+    """
+
+    def __init__(self, penalty: float = 100.0, scale: float = 1.0) -> None:
+        self.penalty = penalty
+        self.scale = scale
+
+    def reward(self, simulator_state: SAOEState) -> float:
+        whole_order = simulator_state.order.amount
+        assert whole_order > 0
+        last_step = cast(SAOEMetrics, simulator_state.history_steps.reset_index().iloc[-1].to_dict())
+        pa = last_step["pa"] * last_step["amount"] / whole_order
+
+        # Inspect the "break-down" of the latest step: trading amount at every tick
+        last_step_breakdown = simulator_state.history_exec.loc[last_step["datetime"] :]
+        penalty = -self.penalty * ((last_step_breakdown["amount"] / whole_order) ** 2).sum()
+
+        reward = pa + penalty
+
+        # Throw error in case of NaN
+        assert not (np.isnan(reward) or np.isinf(reward)), f"Invalid reward for simulator state: {simulator_state}"
+
+        self.log("reward/pa", pa)
+        self.log("reward/penalty", penalty)
+        return reward * self.scale
+
+
+class PPOReward(Reward[SAOEState]):
+    """Reward proposed by paper "An End-to-End Optimal Trade Execution Framework based on Proximal Policy Optimization".
+
+    Parameters
+    ----------
+    max_step
+        Maximum number of steps.
+    start_time_index
+        First time index that allowed to trade.
+    end_time_index
+        Last time index that allowed to trade.
+    """
+
+    def __init__(self, max_step: int, start_time_index: int = 0, end_time_index: int = 239) -> None:
+        self.max_step = max_step
+        self.start_time_index = start_time_index
+        self.end_time_index = end_time_index
+
+    def reward(self, simulator_state: SAOEState) -> float:
+        if simulator_state.cur_step == self.max_step - 1 or simulator_state.position < 1e-6:
+            if simulator_state.history_exec["deal_amount"].sum() == 0.0:
+                vwap_price = cast(
+                    float,
+                    np.average(simulator_state.history_exec["market_price"]),
+                )
+            else:
+                vwap_price = cast(
+                    float,
+                    np.average(
+                        simulator_state.history_exec["market_price"],
+                        weights=simulator_state.history_exec["deal_amount"],
+                    ),
+                )
+            twap_price = simulator_state.backtest_data.get_deal_price().mean()
+
+            if simulator_state.order.direction == OrderDir.SELL:
+                ratio = vwap_price / twap_price if twap_price != 0 else 1.0
+            else:
+                ratio = twap_price / vwap_price if vwap_price != 0 else 1.0
+            if ratio < 1.0:
+                return -1.0
+            elif ratio < 1.1:
+                return 0.0
+            else:
+                return 1.0
+        else:
+            return 0.0
```

## qlib/rl/order_execution/simulator_qlib.py

 * *Ordering differences only*

```diff
@@ -1,141 +1,141 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-from __future__ import annotations
-
-from typing import Generator, List, Optional
-
-import pandas as pd
-
-from qlib.backtest import collect_data_loop, get_strategy_executor
-from qlib.backtest.decision import BaseTradeDecision, Order, TradeRangeByTime
-from qlib.backtest.executor import NestedExecutor
-from qlib.rl.data.integration import init_qlib
-from qlib.rl.simulator import Simulator
-from .state import SAOEState
-from .strategy import SAOEStateAdapter, SAOEStrategy
-
-
-class SingleAssetOrderExecution(Simulator[Order, SAOEState, float]):
-    """Single-asset order execution (SAOE) simulator which is implemented based on Qlib backtest tools.
-
-    Parameters
-    ----------
-    order
-        The seed to start an SAOE simulator is an order.
-    executor_config
-        Executor configuration
-    exchange_config
-        Exchange configuration
-    qlib_config
-        Configuration used to initialize Qlib. If it is None, Qlib will not be initialized.
-    cash_limit:
-        Cash limit.
-    """
-
-    def __init__(
-        self,
-        order: Order,
-        executor_config: dict,
-        exchange_config: dict,
-        qlib_config: dict | None = None,
-        cash_limit: float | None = None,
-    ) -> None:
-        super().__init__(initial=order)
-
-        assert order.start_time.date() == order.end_time.date(), "Start date and end date must be the same."
-
-        strategy_config = {
-            "class": "SingleOrderStrategy",
-            "module_path": "qlib.rl.strategy.single_order",
-            "kwargs": {
-                "order": order,
-                "trade_range": TradeRangeByTime(order.start_time.time(), order.end_time.time()),
-            },
-        }
-
-        self._collect_data_loop: Optional[Generator] = None
-        self.reset(order, strategy_config, executor_config, exchange_config, qlib_config, cash_limit)
-
-    def reset(
-        self,
-        order: Order,
-        strategy_config: dict,
-        executor_config: dict,
-        exchange_config: dict,
-        qlib_config: dict | None = None,
-        cash_limit: Optional[float] = None,
-    ) -> None:
-        if qlib_config is not None:
-            init_qlib(qlib_config)
-
-        strategy, self._executor = get_strategy_executor(
-            start_time=order.date,
-            end_time=order.date + pd.DateOffset(1),
-            strategy=strategy_config,
-            executor=executor_config,
-            benchmark=order.stock_id,
-            account=cash_limit if cash_limit is not None else int(1e12),
-            exchange_kwargs=exchange_config,
-            pos_type="Position" if cash_limit is not None else "InfPosition",
-        )
-
-        assert isinstance(self._executor, NestedExecutor)
-
-        self.report_dict: dict = {}
-        self.decisions: List[BaseTradeDecision] = []
-        self._collect_data_loop = collect_data_loop(
-            start_time=order.date,
-            end_time=order.date,
-            trade_strategy=strategy,
-            trade_executor=self._executor,
-            return_value=self.report_dict,
-        )
-        assert isinstance(self._collect_data_loop, Generator)
-
-        self.step(action=None)
-
-        self._order = order
-
-    def _get_adapter(self) -> SAOEStateAdapter:
-        return self._last_yielded_saoe_strategy.adapter_dict[self._order.key_by_day]
-
-    @property
-    def twap_price(self) -> float:
-        return self._get_adapter().twap_price
-
-    def _iter_strategy(self, action: Optional[float] = None) -> SAOEStrategy:
-        """Iterate the _collect_data_loop until we get the next yield SAOEStrategy."""
-        assert self._collect_data_loop is not None
-
-        obj = next(self._collect_data_loop) if action is None else self._collect_data_loop.send(action)
-        while not isinstance(obj, SAOEStrategy):
-            if isinstance(obj, BaseTradeDecision):
-                self.decisions.append(obj)
-            obj = next(self._collect_data_loop) if action is None else self._collect_data_loop.send(action)
-        assert isinstance(obj, SAOEStrategy)
-        return obj
-
-    def step(self, action: Optional[float]) -> None:
-        """Execute one step or SAOE.
-
-        Parameters
-        ----------
-        action (float):
-            The amount you wish to deal. The simulator doesn't guarantee all the amount to be successfully dealt.
-        """
-
-        assert not self.done(), "Simulator has already done!"
-
-        try:
-            self._last_yielded_saoe_strategy = self._iter_strategy(action=action)
-        except StopIteration:
-            pass
-
-        assert self._executor is not None
-
-    def get_state(self) -> SAOEState:
-        return self._get_adapter().saoe_state
-
-    def done(self) -> bool:
-        return self._executor.finished()
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+from __future__ import annotations
+
+from typing import Generator, List, Optional
+
+import pandas as pd
+
+from qlib.backtest import collect_data_loop, get_strategy_executor
+from qlib.backtest.decision import BaseTradeDecision, Order, TradeRangeByTime
+from qlib.backtest.executor import NestedExecutor
+from qlib.rl.data.integration import init_qlib
+from qlib.rl.simulator import Simulator
+from .state import SAOEState
+from .strategy import SAOEStateAdapter, SAOEStrategy
+
+
+class SingleAssetOrderExecution(Simulator[Order, SAOEState, float]):
+    """Single-asset order execution (SAOE) simulator which is implemented based on Qlib backtest tools.
+
+    Parameters
+    ----------
+    order
+        The seed to start an SAOE simulator is an order.
+    executor_config
+        Executor configuration
+    exchange_config
+        Exchange configuration
+    qlib_config
+        Configuration used to initialize Qlib. If it is None, Qlib will not be initialized.
+    cash_limit:
+        Cash limit.
+    """
+
+    def __init__(
+        self,
+        order: Order,
+        executor_config: dict,
+        exchange_config: dict,
+        qlib_config: dict | None = None,
+        cash_limit: float | None = None,
+    ) -> None:
+        super().__init__(initial=order)
+
+        assert order.start_time.date() == order.end_time.date(), "Start date and end date must be the same."
+
+        strategy_config = {
+            "class": "SingleOrderStrategy",
+            "module_path": "qlib.rl.strategy.single_order",
+            "kwargs": {
+                "order": order,
+                "trade_range": TradeRangeByTime(order.start_time.time(), order.end_time.time()),
+            },
+        }
+
+        self._collect_data_loop: Optional[Generator] = None
+        self.reset(order, strategy_config, executor_config, exchange_config, qlib_config, cash_limit)
+
+    def reset(
+        self,
+        order: Order,
+        strategy_config: dict,
+        executor_config: dict,
+        exchange_config: dict,
+        qlib_config: dict | None = None,
+        cash_limit: Optional[float] = None,
+    ) -> None:
+        if qlib_config is not None:
+            init_qlib(qlib_config)
+
+        strategy, self._executor = get_strategy_executor(
+            start_time=order.date,
+            end_time=order.date + pd.DateOffset(1),
+            strategy=strategy_config,
+            executor=executor_config,
+            benchmark=order.stock_id,
+            account=cash_limit if cash_limit is not None else int(1e12),
+            exchange_kwargs=exchange_config,
+            pos_type="Position" if cash_limit is not None else "InfPosition",
+        )
+
+        assert isinstance(self._executor, NestedExecutor)
+
+        self.report_dict: dict = {}
+        self.decisions: List[BaseTradeDecision] = []
+        self._collect_data_loop = collect_data_loop(
+            start_time=order.date,
+            end_time=order.date,
+            trade_strategy=strategy,
+            trade_executor=self._executor,
+            return_value=self.report_dict,
+        )
+        assert isinstance(self._collect_data_loop, Generator)
+
+        self.step(action=None)
+
+        self._order = order
+
+    def _get_adapter(self) -> SAOEStateAdapter:
+        return self._last_yielded_saoe_strategy.adapter_dict[self._order.key_by_day]
+
+    @property
+    def twap_price(self) -> float:
+        return self._get_adapter().twap_price
+
+    def _iter_strategy(self, action: Optional[float] = None) -> SAOEStrategy:
+        """Iterate the _collect_data_loop until we get the next yield SAOEStrategy."""
+        assert self._collect_data_loop is not None
+
+        obj = next(self._collect_data_loop) if action is None else self._collect_data_loop.send(action)
+        while not isinstance(obj, SAOEStrategy):
+            if isinstance(obj, BaseTradeDecision):
+                self.decisions.append(obj)
+            obj = next(self._collect_data_loop) if action is None else self._collect_data_loop.send(action)
+        assert isinstance(obj, SAOEStrategy)
+        return obj
+
+    def step(self, action: Optional[float]) -> None:
+        """Execute one step or SAOE.
+
+        Parameters
+        ----------
+        action (float):
+            The amount you wish to deal. The simulator doesn't guarantee all the amount to be successfully dealt.
+        """
+
+        assert not self.done(), "Simulator has already done!"
+
+        try:
+            self._last_yielded_saoe_strategy = self._iter_strategy(action=action)
+        except StopIteration:
+            pass
+
+        assert self._executor is not None
+
+    def get_state(self) -> SAOEState:
+        return self._get_adapter().saoe_state
+
+    def done(self) -> bool:
+        return self._executor.finished()
```

## qlib/rl/order_execution/simulator_simple.py

 * *Ordering differences only*

```diff
@@ -1,362 +1,362 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-from __future__ import annotations
-
-from typing import Any, cast, List, Optional
-
-import numpy as np
-import pandas as pd
-
-from pathlib import Path
-from qlib.backtest.decision import Order, OrderDir
-from qlib.constant import EPS, EPS_T, float_or_ndarray
-from qlib.rl.data.base import BaseIntradayBacktestData
-from qlib.rl.data.native import DataframeIntradayBacktestData, load_handler_intraday_processed_data
-from qlib.rl.data.pickle_styled import load_simple_intraday_backtest_data
-from qlib.rl.simulator import Simulator
-from qlib.rl.utils import LogLevel
-from .state import SAOEMetrics, SAOEState
-
-__all__ = ["SingleAssetOrderExecutionSimple"]
-
-
-class SingleAssetOrderExecutionSimple(Simulator[Order, SAOEState, float]):
-    """Single-asset order execution (SAOE) simulator.
-
-    As there's no "calendar" in the simple simulator, ticks are used to trade.
-    A tick is a record (a line) in the pickle-styled data file.
-    Each tick is considered as a individual trading opportunity.
-    If such fine granularity is not needed, use ``ticks_per_step`` to
-    lengthen the ticks for each step.
-
-    In each step, the traded amount are "equally" separated to each tick,
-    then bounded by volume maximum execution volume (i.e., ``vol_threshold``),
-    and if it's the last step, try to ensure all the amount to be executed.
-
-    Parameters
-    ----------
-    order
-        The seed to start an SAOE simulator is an order.
-    data_dir
-        Path to load backtest data.
-    feature_columns_today
-        Columns of today's feature.
-    feature_columns_yesterday
-        Columns of yesterday's feature.
-    data_granularity
-        Number of ticks between consecutive data entries.
-    ticks_per_step
-        How many ticks per step.
-    vol_threshold
-        Maximum execution volume (divided by market execution volume).
-    """
-
-    history_exec: pd.DataFrame
-    """All execution history at every possible time ticks. See :class:`SAOEMetrics` for available columns.
-    Index is ``datetime``.
-    """
-
-    history_steps: pd.DataFrame
-    """Positions at each step. The position before first step is also recorded.
-    See :class:`SAOEMetrics` for available columns.
-    Index is ``datetime``, which is the **starting** time of each step."""
-
-    metrics: Optional[SAOEMetrics]
-    """Metrics. Only available when done."""
-
-    twap_price: float
-    """This price is used to compute price advantage.
-    It"s defined as the average price in the period from order"s start time to end time."""
-
-    ticks_index: pd.DatetimeIndex
-    """All available ticks for the day (not restricted to order)."""
-
-    ticks_for_order: pd.DatetimeIndex
-    """Ticks that is available for trading (sliced by order)."""
-
-    def __init__(
-        self,
-        order: Order,
-        data_dir: Path,
-        feature_columns_today: List[str] = [],
-        feature_columns_yesterday: List[str] = [],
-        data_granularity: int = 1,
-        ticks_per_step: int = 30,
-        vol_threshold: Optional[float] = None,
-    ) -> None:
-        super().__init__(initial=order)
-
-        assert ticks_per_step % data_granularity == 0
-
-        self.order = order
-        self.data_dir = data_dir
-        self.feature_columns_today = feature_columns_today
-        self.feature_columns_yesterday = feature_columns_yesterday
-        self.ticks_per_step: int = ticks_per_step // data_granularity
-        self.vol_threshold = vol_threshold
-
-        self.backtest_data = self.get_backtest_data()
-        self.ticks_index = self.backtest_data.get_time_index()
-
-        # Get time index available for trading
-        self.ticks_for_order = self._get_ticks_slice(self.order.start_time, self.order.end_time)
-
-        self.cur_time = self.ticks_for_order[0]
-        self.cur_step = 0
-        # NOTE: astype(float) is necessary in some systems.
-        # this will align the precision with `.to_numpy()` in `_split_exec_vol`
-        self.twap_price = float(self.backtest_data.get_deal_price().loc[self.ticks_for_order].astype(float).mean())
-
-        self.position = order.amount
-
-        metric_keys = list(SAOEMetrics.__annotations__.keys())  # pylint: disable=no-member
-        # NOTE: can empty dataframe contain index?
-        self.history_exec = pd.DataFrame(columns=metric_keys).set_index("datetime")
-        self.history_steps = pd.DataFrame(columns=metric_keys).set_index("datetime")
-        self.metrics = None
-
-        self.market_price: Optional[np.ndarray] = None
-        self.market_vol: Optional[np.ndarray] = None
-        self.market_vol_limit: Optional[np.ndarray] = None
-
-    def get_backtest_data(self) -> BaseIntradayBacktestData:
-        try:
-            data = load_handler_intraday_processed_data(
-                data_dir=self.data_dir,
-                stock_id=self.order.stock_id,
-                date=pd.Timestamp(self.order.start_time.date()),
-                feature_columns_today=self.feature_columns_today,
-                feature_columns_yesterday=self.feature_columns_yesterday,
-                backtest=True,
-                index_only=False,
-            )
-            return DataframeIntradayBacktestData(data.today)
-        except (AttributeError, FileNotFoundError):
-            # TODO: For compatibility with older versions of test scripts (tests/rl/test_saoe_simple.py)
-            # TODO: In the future, we should modify the data format used by the test script,
-            # TODO: and then delete this branch.
-            return load_simple_intraday_backtest_data(
-                self.data_dir / "backtest",
-                self.order.stock_id,
-                pd.Timestamp(self.order.start_time.date()),
-                "close",
-                self.order.direction,
-            )
-
-    def step(self, amount: float) -> None:
-        """Execute one step or SAOE.
-
-        Parameters
-        ----------
-        amount
-            The amount you wish to deal. The simulator doesn't guarantee all the amount to be successfully dealt.
-        """
-
-        assert not self.done()
-
-        self.market_price = self.market_vol = None  # avoid misuse
-        exec_vol = self._split_exec_vol(amount)
-        assert self.market_price is not None
-        assert self.market_vol is not None
-
-        ticks_position = self.position - np.cumsum(exec_vol)
-
-        self.position -= exec_vol.sum()
-        if abs(self.position) < 1e-6:
-            self.position = 0.0
-        if self.position < -EPS or (exec_vol < -EPS).any():
-            raise ValueError(f"Execution volume is invalid: {exec_vol} (position = {self.position})")
-
-        # Get time index available for this step
-        time_index = self._get_ticks_slice(self.cur_time, self._next_time())
-
-        self.history_exec = self._dataframe_append(
-            self.history_exec,
-            SAOEMetrics(
-                # It should have the same keys with SAOEMetrics,
-                # but the values do not necessarily have the annotated type.
-                # Some values could be vectorized (e.g., exec_vol).
-                stock_id=self.order.stock_id,
-                datetime=time_index,
-                direction=self.order.direction,
-                market_volume=self.market_vol,
-                market_price=self.market_price,
-                amount=exec_vol,
-                inner_amount=exec_vol,
-                deal_amount=exec_vol,
-                trade_price=self.market_price,
-                trade_value=self.market_price * exec_vol,
-                position=ticks_position,
-                ffr=exec_vol / self.order.amount,
-                pa=price_advantage(self.market_price, self.twap_price, self.order.direction),
-            ),
-        )
-
-        self.history_steps = self._dataframe_append(
-            self.history_steps,
-            [self._metrics_collect(self.cur_time, self.market_vol, self.market_price, amount, exec_vol)],
-        )
-
-        if self.done():
-            if self.env is not None:
-                self.env.logger.add_any("history_steps", self.history_steps, loglevel=LogLevel.DEBUG)
-                self.env.logger.add_any("history_exec", self.history_exec, loglevel=LogLevel.DEBUG)
-
-            self.metrics = self._metrics_collect(
-                self.ticks_index[0],  # start time
-                self.history_exec["market_volume"],
-                self.history_exec["market_price"],
-                self.history_steps["amount"].sum(),
-                self.history_exec["deal_amount"],
-            )
-
-            # NOTE (yuge): It looks to me that it's the "correct" decision to
-            # put all the logs here, because only components like simulators themselves
-            # have the knowledge about what could appear in the logs, and what's the format.
-            # But I admit it's not necessarily the most convenient way.
-            # I'll rethink about it when we have the second environment
-            # Maybe some APIs like self.logger.enable_auto_log() ?
-
-            if self.env is not None:
-                for key, value in self.metrics.items():
-                    if isinstance(value, float):
-                        self.env.logger.add_scalar(key, value)
-                    else:
-                        self.env.logger.add_any(key, value)
-
-        self.cur_time = self._next_time()
-        self.cur_step += 1
-
-    def get_state(self) -> SAOEState:
-        return SAOEState(
-            order=self.order,
-            cur_time=self.cur_time,
-            cur_step=self.cur_step,
-            position=self.position,
-            history_exec=self.history_exec,
-            history_steps=self.history_steps,
-            metrics=self.metrics,
-            backtest_data=self.backtest_data,
-            ticks_per_step=self.ticks_per_step,
-            ticks_index=self.ticks_index,
-            ticks_for_order=self.ticks_for_order,
-        )
-
-    def done(self) -> bool:
-        return self.position < EPS or self.cur_time >= self.order.end_time
-
-    def _next_time(self) -> pd.Timestamp:
-        """The "current time" (``cur_time``) for next step."""
-        # Look for next time on time index
-        current_loc = self.ticks_index.get_loc(self.cur_time)
-        next_loc = current_loc + self.ticks_per_step
-
-        # Calibrate the next location to multiple of ticks_per_step.
-        # This is to make sure that:
-        # as long as ticks_per_step is a multiple of something, each step won't cross morning and afternoon.
-        next_loc = next_loc - next_loc % self.ticks_per_step
-
-        if next_loc < len(self.ticks_index) and self.ticks_index[next_loc] < self.order.end_time:
-            return self.ticks_index[next_loc]
-        else:
-            return self.order.end_time
-
-    def _cur_duration(self) -> pd.Timedelta:
-        """The "duration" of this step (step that is about to happen)."""
-        return self._next_time() - self.cur_time
-
-    def _split_exec_vol(self, exec_vol_sum: float) -> np.ndarray:
-        """
-        Split the volume in each step into minutes, considering possible constraints.
-        This follows TWAP strategy.
-        """
-        next_time = self._next_time()
-
-        # get the backtest data for next interval
-        self.market_vol = self.backtest_data.get_volume().loc[self.cur_time : next_time - EPS_T].to_numpy()
-        self.market_price = self.backtest_data.get_deal_price().loc[self.cur_time : next_time - EPS_T].to_numpy()
-
-        assert self.market_vol is not None and self.market_price is not None
-
-        # split the volume equally into each minute
-        exec_vol = np.repeat(exec_vol_sum / len(self.market_price), len(self.market_price))
-
-        # apply the volume threshold
-        market_vol_limit = self.vol_threshold * self.market_vol if self.vol_threshold is not None else np.inf
-        exec_vol = np.minimum(exec_vol, market_vol_limit)  # type: ignore
-
-        # Complete all the order amount at the last moment.
-        if next_time >= self.order.end_time:
-            exec_vol[-1] += self.position - exec_vol.sum()
-            exec_vol = np.minimum(exec_vol, market_vol_limit)  # type: ignore
-
-        return exec_vol
-
-    def _metrics_collect(
-        self,
-        datetime: pd.Timestamp,
-        market_vol: np.ndarray,
-        market_price: np.ndarray,
-        amount: float,  # intended to trade such amount
-        exec_vol: np.ndarray,
-    ) -> SAOEMetrics:
-        assert len(market_vol) == len(market_price) == len(exec_vol)
-
-        if np.abs(np.sum(exec_vol)) < EPS:
-            exec_avg_price = 0.0
-        else:
-            exec_avg_price = cast(float, np.average(market_price, weights=exec_vol))  # could be nan
-            if hasattr(exec_avg_price, "item"):  # could be numpy scalar
-                exec_avg_price = exec_avg_price.item()  # type: ignore
-
-        return SAOEMetrics(
-            stock_id=self.order.stock_id,
-            datetime=datetime,
-            direction=self.order.direction,
-            market_volume=market_vol.sum(),
-            market_price=market_price.mean(),
-            amount=amount,
-            inner_amount=exec_vol.sum(),
-            deal_amount=exec_vol.sum(),  # in this simulator, there's no other restrictions
-            trade_price=exec_avg_price,
-            trade_value=float(np.sum(market_price * exec_vol)),
-            position=self.position,
-            ffr=float(exec_vol.sum() / self.order.amount),
-            pa=price_advantage(exec_avg_price, self.twap_price, self.order.direction),
-        )
-
-    def _get_ticks_slice(self, start: pd.Timestamp, end: pd.Timestamp, include_end: bool = False) -> pd.DatetimeIndex:
-        if not include_end:
-            end = end - EPS_T
-        return self.ticks_index[self.ticks_index.slice_indexer(start, end)]
-
-    @staticmethod
-    def _dataframe_append(df: pd.DataFrame, other: Any) -> pd.DataFrame:
-        # dataframe.append is deprecated
-        other_df = pd.DataFrame(other).set_index("datetime")
-        other_df.index.name = "datetime"
-        return pd.concat([df, other_df], axis=0)
-
-
-def price_advantage(
-    exec_price: float_or_ndarray,
-    baseline_price: float,
-    direction: OrderDir | int,
-) -> float_or_ndarray:
-    if baseline_price == 0:  # something is wrong with data. Should be nan here
-        if isinstance(exec_price, float):
-            return 0.0
-        else:
-            return np.zeros_like(exec_price)
-    if direction == OrderDir.BUY:
-        res = (1 - exec_price / baseline_price) * 10000
-    elif direction == OrderDir.SELL:
-        res = (exec_price / baseline_price - 1) * 10000
-    else:
-        raise ValueError(f"Unexpected order direction: {direction}")
-    res_wo_nan: np.ndarray = np.nan_to_num(res, nan=0.0)
-    if res_wo_nan.size == 1:
-        return res_wo_nan.item()
-    else:
-        return cast(float_or_ndarray, res_wo_nan)
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+from __future__ import annotations
+
+from typing import Any, cast, List, Optional
+
+import numpy as np
+import pandas as pd
+
+from pathlib import Path
+from qlib.backtest.decision import Order, OrderDir
+from qlib.constant import EPS, EPS_T, float_or_ndarray
+from qlib.rl.data.base import BaseIntradayBacktestData
+from qlib.rl.data.native import DataframeIntradayBacktestData, load_handler_intraday_processed_data
+from qlib.rl.data.pickle_styled import load_simple_intraday_backtest_data
+from qlib.rl.simulator import Simulator
+from qlib.rl.utils import LogLevel
+from .state import SAOEMetrics, SAOEState
+
+__all__ = ["SingleAssetOrderExecutionSimple"]
+
+
+class SingleAssetOrderExecutionSimple(Simulator[Order, SAOEState, float]):
+    """Single-asset order execution (SAOE) simulator.
+
+    As there's no "calendar" in the simple simulator, ticks are used to trade.
+    A tick is a record (a line) in the pickle-styled data file.
+    Each tick is considered as a individual trading opportunity.
+    If such fine granularity is not needed, use ``ticks_per_step`` to
+    lengthen the ticks for each step.
+
+    In each step, the traded amount are "equally" separated to each tick,
+    then bounded by volume maximum execution volume (i.e., ``vol_threshold``),
+    and if it's the last step, try to ensure all the amount to be executed.
+
+    Parameters
+    ----------
+    order
+        The seed to start an SAOE simulator is an order.
+    data_dir
+        Path to load backtest data.
+    feature_columns_today
+        Columns of today's feature.
+    feature_columns_yesterday
+        Columns of yesterday's feature.
+    data_granularity
+        Number of ticks between consecutive data entries.
+    ticks_per_step
+        How many ticks per step.
+    vol_threshold
+        Maximum execution volume (divided by market execution volume).
+    """
+
+    history_exec: pd.DataFrame
+    """All execution history at every possible time ticks. See :class:`SAOEMetrics` for available columns.
+    Index is ``datetime``.
+    """
+
+    history_steps: pd.DataFrame
+    """Positions at each step. The position before first step is also recorded.
+    See :class:`SAOEMetrics` for available columns.
+    Index is ``datetime``, which is the **starting** time of each step."""
+
+    metrics: Optional[SAOEMetrics]
+    """Metrics. Only available when done."""
+
+    twap_price: float
+    """This price is used to compute price advantage.
+    It"s defined as the average price in the period from order"s start time to end time."""
+
+    ticks_index: pd.DatetimeIndex
+    """All available ticks for the day (not restricted to order)."""
+
+    ticks_for_order: pd.DatetimeIndex
+    """Ticks that is available for trading (sliced by order)."""
+
+    def __init__(
+        self,
+        order: Order,
+        data_dir: Path,
+        feature_columns_today: List[str] = [],
+        feature_columns_yesterday: List[str] = [],
+        data_granularity: int = 1,
+        ticks_per_step: int = 30,
+        vol_threshold: Optional[float] = None,
+    ) -> None:
+        super().__init__(initial=order)
+
+        assert ticks_per_step % data_granularity == 0
+
+        self.order = order
+        self.data_dir = data_dir
+        self.feature_columns_today = feature_columns_today
+        self.feature_columns_yesterday = feature_columns_yesterday
+        self.ticks_per_step: int = ticks_per_step // data_granularity
+        self.vol_threshold = vol_threshold
+
+        self.backtest_data = self.get_backtest_data()
+        self.ticks_index = self.backtest_data.get_time_index()
+
+        # Get time index available for trading
+        self.ticks_for_order = self._get_ticks_slice(self.order.start_time, self.order.end_time)
+
+        self.cur_time = self.ticks_for_order[0]
+        self.cur_step = 0
+        # NOTE: astype(float) is necessary in some systems.
+        # this will align the precision with `.to_numpy()` in `_split_exec_vol`
+        self.twap_price = float(self.backtest_data.get_deal_price().loc[self.ticks_for_order].astype(float).mean())
+
+        self.position = order.amount
+
+        metric_keys = list(SAOEMetrics.__annotations__.keys())  # pylint: disable=no-member
+        # NOTE: can empty dataframe contain index?
+        self.history_exec = pd.DataFrame(columns=metric_keys).set_index("datetime")
+        self.history_steps = pd.DataFrame(columns=metric_keys).set_index("datetime")
+        self.metrics = None
+
+        self.market_price: Optional[np.ndarray] = None
+        self.market_vol: Optional[np.ndarray] = None
+        self.market_vol_limit: Optional[np.ndarray] = None
+
+    def get_backtest_data(self) -> BaseIntradayBacktestData:
+        try:
+            data = load_handler_intraday_processed_data(
+                data_dir=self.data_dir,
+                stock_id=self.order.stock_id,
+                date=pd.Timestamp(self.order.start_time.date()),
+                feature_columns_today=self.feature_columns_today,
+                feature_columns_yesterday=self.feature_columns_yesterday,
+                backtest=True,
+                index_only=False,
+            )
+            return DataframeIntradayBacktestData(data.today)
+        except (AttributeError, FileNotFoundError):
+            # TODO: For compatibility with older versions of test scripts (tests/rl/test_saoe_simple.py)
+            # TODO: In the future, we should modify the data format used by the test script,
+            # TODO: and then delete this branch.
+            return load_simple_intraday_backtest_data(
+                self.data_dir / "backtest",
+                self.order.stock_id,
+                pd.Timestamp(self.order.start_time.date()),
+                "close",
+                self.order.direction,
+            )
+
+    def step(self, amount: float) -> None:
+        """Execute one step or SAOE.
+
+        Parameters
+        ----------
+        amount
+            The amount you wish to deal. The simulator doesn't guarantee all the amount to be successfully dealt.
+        """
+
+        assert not self.done()
+
+        self.market_price = self.market_vol = None  # avoid misuse
+        exec_vol = self._split_exec_vol(amount)
+        assert self.market_price is not None
+        assert self.market_vol is not None
+
+        ticks_position = self.position - np.cumsum(exec_vol)
+
+        self.position -= exec_vol.sum()
+        if abs(self.position) < 1e-6:
+            self.position = 0.0
+        if self.position < -EPS or (exec_vol < -EPS).any():
+            raise ValueError(f"Execution volume is invalid: {exec_vol} (position = {self.position})")
+
+        # Get time index available for this step
+        time_index = self._get_ticks_slice(self.cur_time, self._next_time())
+
+        self.history_exec = self._dataframe_append(
+            self.history_exec,
+            SAOEMetrics(
+                # It should have the same keys with SAOEMetrics,
+                # but the values do not necessarily have the annotated type.
+                # Some values could be vectorized (e.g., exec_vol).
+                stock_id=self.order.stock_id,
+                datetime=time_index,
+                direction=self.order.direction,
+                market_volume=self.market_vol,
+                market_price=self.market_price,
+                amount=exec_vol,
+                inner_amount=exec_vol,
+                deal_amount=exec_vol,
+                trade_price=self.market_price,
+                trade_value=self.market_price * exec_vol,
+                position=ticks_position,
+                ffr=exec_vol / self.order.amount,
+                pa=price_advantage(self.market_price, self.twap_price, self.order.direction),
+            ),
+        )
+
+        self.history_steps = self._dataframe_append(
+            self.history_steps,
+            [self._metrics_collect(self.cur_time, self.market_vol, self.market_price, amount, exec_vol)],
+        )
+
+        if self.done():
+            if self.env is not None:
+                self.env.logger.add_any("history_steps", self.history_steps, loglevel=LogLevel.DEBUG)
+                self.env.logger.add_any("history_exec", self.history_exec, loglevel=LogLevel.DEBUG)
+
+            self.metrics = self._metrics_collect(
+                self.ticks_index[0],  # start time
+                self.history_exec["market_volume"],
+                self.history_exec["market_price"],
+                self.history_steps["amount"].sum(),
+                self.history_exec["deal_amount"],
+            )
+
+            # NOTE (yuge): It looks to me that it's the "correct" decision to
+            # put all the logs here, because only components like simulators themselves
+            # have the knowledge about what could appear in the logs, and what's the format.
+            # But I admit it's not necessarily the most convenient way.
+            # I'll rethink about it when we have the second environment
+            # Maybe some APIs like self.logger.enable_auto_log() ?
+
+            if self.env is not None:
+                for key, value in self.metrics.items():
+                    if isinstance(value, float):
+                        self.env.logger.add_scalar(key, value)
+                    else:
+                        self.env.logger.add_any(key, value)
+
+        self.cur_time = self._next_time()
+        self.cur_step += 1
+
+    def get_state(self) -> SAOEState:
+        return SAOEState(
+            order=self.order,
+            cur_time=self.cur_time,
+            cur_step=self.cur_step,
+            position=self.position,
+            history_exec=self.history_exec,
+            history_steps=self.history_steps,
+            metrics=self.metrics,
+            backtest_data=self.backtest_data,
+            ticks_per_step=self.ticks_per_step,
+            ticks_index=self.ticks_index,
+            ticks_for_order=self.ticks_for_order,
+        )
+
+    def done(self) -> bool:
+        return self.position < EPS or self.cur_time >= self.order.end_time
+
+    def _next_time(self) -> pd.Timestamp:
+        """The "current time" (``cur_time``) for next step."""
+        # Look for next time on time index
+        current_loc = self.ticks_index.get_loc(self.cur_time)
+        next_loc = current_loc + self.ticks_per_step
+
+        # Calibrate the next location to multiple of ticks_per_step.
+        # This is to make sure that:
+        # as long as ticks_per_step is a multiple of something, each step won't cross morning and afternoon.
+        next_loc = next_loc - next_loc % self.ticks_per_step
+
+        if next_loc < len(self.ticks_index) and self.ticks_index[next_loc] < self.order.end_time:
+            return self.ticks_index[next_loc]
+        else:
+            return self.order.end_time
+
+    def _cur_duration(self) -> pd.Timedelta:
+        """The "duration" of this step (step that is about to happen)."""
+        return self._next_time() - self.cur_time
+
+    def _split_exec_vol(self, exec_vol_sum: float) -> np.ndarray:
+        """
+        Split the volume in each step into minutes, considering possible constraints.
+        This follows TWAP strategy.
+        """
+        next_time = self._next_time()
+
+        # get the backtest data for next interval
+        self.market_vol = self.backtest_data.get_volume().loc[self.cur_time : next_time - EPS_T].to_numpy()
+        self.market_price = self.backtest_data.get_deal_price().loc[self.cur_time : next_time - EPS_T].to_numpy()
+
+        assert self.market_vol is not None and self.market_price is not None
+
+        # split the volume equally into each minute
+        exec_vol = np.repeat(exec_vol_sum / len(self.market_price), len(self.market_price))
+
+        # apply the volume threshold
+        market_vol_limit = self.vol_threshold * self.market_vol if self.vol_threshold is not None else np.inf
+        exec_vol = np.minimum(exec_vol, market_vol_limit)  # type: ignore
+
+        # Complete all the order amount at the last moment.
+        if next_time >= self.order.end_time:
+            exec_vol[-1] += self.position - exec_vol.sum()
+            exec_vol = np.minimum(exec_vol, market_vol_limit)  # type: ignore
+
+        return exec_vol
+
+    def _metrics_collect(
+        self,
+        datetime: pd.Timestamp,
+        market_vol: np.ndarray,
+        market_price: np.ndarray,
+        amount: float,  # intended to trade such amount
+        exec_vol: np.ndarray,
+    ) -> SAOEMetrics:
+        assert len(market_vol) == len(market_price) == len(exec_vol)
+
+        if np.abs(np.sum(exec_vol)) < EPS:
+            exec_avg_price = 0.0
+        else:
+            exec_avg_price = cast(float, np.average(market_price, weights=exec_vol))  # could be nan
+            if hasattr(exec_avg_price, "item"):  # could be numpy scalar
+                exec_avg_price = exec_avg_price.item()  # type: ignore
+
+        return SAOEMetrics(
+            stock_id=self.order.stock_id,
+            datetime=datetime,
+            direction=self.order.direction,
+            market_volume=market_vol.sum(),
+            market_price=market_price.mean(),
+            amount=amount,
+            inner_amount=exec_vol.sum(),
+            deal_amount=exec_vol.sum(),  # in this simulator, there's no other restrictions
+            trade_price=exec_avg_price,
+            trade_value=float(np.sum(market_price * exec_vol)),
+            position=self.position,
+            ffr=float(exec_vol.sum() / self.order.amount),
+            pa=price_advantage(exec_avg_price, self.twap_price, self.order.direction),
+        )
+
+    def _get_ticks_slice(self, start: pd.Timestamp, end: pd.Timestamp, include_end: bool = False) -> pd.DatetimeIndex:
+        if not include_end:
+            end = end - EPS_T
+        return self.ticks_index[self.ticks_index.slice_indexer(start, end)]
+
+    @staticmethod
+    def _dataframe_append(df: pd.DataFrame, other: Any) -> pd.DataFrame:
+        # dataframe.append is deprecated
+        other_df = pd.DataFrame(other).set_index("datetime")
+        other_df.index.name = "datetime"
+        return pd.concat([df, other_df], axis=0)
+
+
+def price_advantage(
+    exec_price: float_or_ndarray,
+    baseline_price: float,
+    direction: OrderDir | int,
+) -> float_or_ndarray:
+    if baseline_price == 0:  # something is wrong with data. Should be nan here
+        if isinstance(exec_price, float):
+            return 0.0
+        else:
+            return np.zeros_like(exec_price)
+    if direction == OrderDir.BUY:
+        res = (1 - exec_price / baseline_price) * 10000
+    elif direction == OrderDir.SELL:
+        res = (exec_price / baseline_price - 1) * 10000
+    else:
+        raise ValueError(f"Unexpected order direction: {direction}")
+    res_wo_nan: np.ndarray = np.nan_to_num(res, nan=0.0)
+    if res_wo_nan.size == 1:
+        return res_wo_nan.item()
+    else:
+        return cast(float_or_ndarray, res_wo_nan)
```

## qlib/rl/order_execution/state.py

 * *Ordering differences only*

```diff
@@ -1,101 +1,101 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-from __future__ import annotations
-
-import typing
-from typing import NamedTuple, Optional
-
-import numpy as np
-import pandas as pd
-from qlib.backtest import Order
-from qlib.typehint import TypedDict
-
-if typing.TYPE_CHECKING:
-    from qlib.rl.data.base import BaseIntradayBacktestData
-
-
-class SAOEMetrics(TypedDict):
-    """Metrics for SAOE accumulated for a "period".
-    It could be accumulated for a day, or a period of time (e.g., 30min), or calculated separately for every minute.
-
-    Warnings
-    --------
-    The type hints are for single elements. In lots of times, they can be vectorized.
-    For example, ``market_volume`` could be a list of float (or ndarray) rather tahn a single float.
-    """
-
-    stock_id: str
-    """Stock ID of this record."""
-    datetime: pd.Timestamp | pd.DatetimeIndex
-    """Datetime of this record (this is index in the dataframe)."""
-    direction: int
-    """Direction of the order. 0 for sell, 1 for buy."""
-
-    # Market information.
-    market_volume: np.ndarray | float
-    """(total) market volume traded in the period."""
-    market_price: np.ndarray | float
-    """Deal price. If it's a period of time, this is the average market deal price."""
-
-    # Strategy records.
-
-    amount: np.ndarray | float
-    """Total amount (volume) strategy intends to trade."""
-    inner_amount: np.ndarray | float
-    """Total amount that the lower-level strategy intends to trade
-    (might be larger than amount, e.g., to ensure ffr)."""
-
-    deal_amount: np.ndarray | float
-    """Amount that successfully takes effect (must be less than inner_amount)."""
-    trade_price: np.ndarray | float
-    """The average deal price for this strategy."""
-    trade_value: np.ndarray | float
-    """Total worth of trading. In the simple simulation, trade_value = deal_amount * price."""
-    position: np.ndarray | float
-    """Position left after this "period"."""
-
-    # Accumulated metrics
-
-    ffr: np.ndarray | float
-    """Completed how much percent of the daily order."""
-
-    pa: np.ndarray | float
-    """Price advantage compared to baseline (i.e., trade with baseline market price).
-    The baseline is trade price when using TWAP strategy to execute this order.
-    Please note that there could be data leak here).
-    Unit is BP (basis point, 1/10000)."""
-
-
-class SAOEState(NamedTuple):
-    """Data structure holding a state for SAOE simulator."""
-
-    order: Order
-    """The order we are dealing with."""
-    cur_time: pd.Timestamp
-    """Current time, e.g., 9:30."""
-    cur_step: int
-    """Current step, e.g., 0."""
-    position: float
-    """Current remaining volume to execute."""
-    history_exec: pd.DataFrame
-    """See :attr:`SingleAssetOrderExecution.history_exec`."""
-    history_steps: pd.DataFrame
-    """See :attr:`SingleAssetOrderExecution.history_steps`."""
-
-    metrics: Optional[SAOEMetrics]
-    """Daily metric, only available when the trading is in "done" state."""
-
-    backtest_data: BaseIntradayBacktestData
-    """Backtest data is included in the state.
-    Actually, only the time index of this data is needed, at this moment.
-    I include the full data so that algorithms (e.g., VWAP) that relies on the raw data can be implemented.
-    Interpreter can use this as they wish, but they should be careful not to leak future data.
-    """
-
-    ticks_per_step: int
-    """How many ticks for each step."""
-    ticks_index: pd.DatetimeIndex
-    """Trading ticks in all day, NOT sliced by order (defined in data). e.g., [9:30, 9:31, ..., 14:59]."""
-    ticks_for_order: pd.DatetimeIndex
-    """Trading ticks sliced by order, e.g., [9:45, 9:46, ..., 14:44]."""
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+from __future__ import annotations
+
+import typing
+from typing import NamedTuple, Optional
+
+import numpy as np
+import pandas as pd
+from qlib.backtest import Order
+from qlib.typehint import TypedDict
+
+if typing.TYPE_CHECKING:
+    from qlib.rl.data.base import BaseIntradayBacktestData
+
+
+class SAOEMetrics(TypedDict):
+    """Metrics for SAOE accumulated for a "period".
+    It could be accumulated for a day, or a period of time (e.g., 30min), or calculated separately for every minute.
+
+    Warnings
+    --------
+    The type hints are for single elements. In lots of times, they can be vectorized.
+    For example, ``market_volume`` could be a list of float (or ndarray) rather tahn a single float.
+    """
+
+    stock_id: str
+    """Stock ID of this record."""
+    datetime: pd.Timestamp | pd.DatetimeIndex
+    """Datetime of this record (this is index in the dataframe)."""
+    direction: int
+    """Direction of the order. 0 for sell, 1 for buy."""
+
+    # Market information.
+    market_volume: np.ndarray | float
+    """(total) market volume traded in the period."""
+    market_price: np.ndarray | float
+    """Deal price. If it's a period of time, this is the average market deal price."""
+
+    # Strategy records.
+
+    amount: np.ndarray | float
+    """Total amount (volume) strategy intends to trade."""
+    inner_amount: np.ndarray | float
+    """Total amount that the lower-level strategy intends to trade
+    (might be larger than amount, e.g., to ensure ffr)."""
+
+    deal_amount: np.ndarray | float
+    """Amount that successfully takes effect (must be less than inner_amount)."""
+    trade_price: np.ndarray | float
+    """The average deal price for this strategy."""
+    trade_value: np.ndarray | float
+    """Total worth of trading. In the simple simulation, trade_value = deal_amount * price."""
+    position: np.ndarray | float
+    """Position left after this "period"."""
+
+    # Accumulated metrics
+
+    ffr: np.ndarray | float
+    """Completed how much percent of the daily order."""
+
+    pa: np.ndarray | float
+    """Price advantage compared to baseline (i.e., trade with baseline market price).
+    The baseline is trade price when using TWAP strategy to execute this order.
+    Please note that there could be data leak here).
+    Unit is BP (basis point, 1/10000)."""
+
+
+class SAOEState(NamedTuple):
+    """Data structure holding a state for SAOE simulator."""
+
+    order: Order
+    """The order we are dealing with."""
+    cur_time: pd.Timestamp
+    """Current time, e.g., 9:30."""
+    cur_step: int
+    """Current step, e.g., 0."""
+    position: float
+    """Current remaining volume to execute."""
+    history_exec: pd.DataFrame
+    """See :attr:`SingleAssetOrderExecution.history_exec`."""
+    history_steps: pd.DataFrame
+    """See :attr:`SingleAssetOrderExecution.history_steps`."""
+
+    metrics: Optional[SAOEMetrics]
+    """Daily metric, only available when the trading is in "done" state."""
+
+    backtest_data: BaseIntradayBacktestData
+    """Backtest data is included in the state.
+    Actually, only the time index of this data is needed, at this moment.
+    I include the full data so that algorithms (e.g., VWAP) that relies on the raw data can be implemented.
+    Interpreter can use this as they wish, but they should be careful not to leak future data.
+    """
+
+    ticks_per_step: int
+    """How many ticks for each step."""
+    ticks_index: pd.DatetimeIndex
+    """Trading ticks in all day, NOT sliced by order (defined in data). e.g., [9:30, 9:31, ..., 14:59]."""
+    ticks_for_order: pd.DatetimeIndex
+    """Trading ticks sliced by order, e.g., [9:45, 9:46, ..., 14:44]."""
```

## qlib/rl/order_execution/strategy.py

 * *Ordering differences only*

```diff
@@ -1,551 +1,551 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-from __future__ import annotations
-
-import collections
-from types import GeneratorType
-from typing import Any, Callable, cast, Dict, Generator, List, Optional, Tuple, Union
-
-import warnings
-import numpy as np
-import pandas as pd
-import torch
-from tianshou.data import Batch
-from tianshou.policy import BasePolicy
-
-from qlib.backtest import CommonInfrastructure, Order
-from qlib.backtest.decision import BaseTradeDecision, TradeDecisionWithDetails, TradeDecisionWO, TradeRange
-from qlib.backtest.exchange import Exchange
-from qlib.backtest.executor import BaseExecutor
-from qlib.backtest.utils import LevelInfrastructure, get_start_end_idx
-from qlib.constant import EPS, ONE_MIN, REG_CN
-from qlib.rl.data.native import IntradayBacktestData, load_backtest_data
-from qlib.rl.interpreter import ActionInterpreter, StateInterpreter
-from qlib.rl.order_execution.state import SAOEMetrics, SAOEState
-from qlib.rl.order_execution.utils import dataframe_append, price_advantage
-from qlib.strategy.base import RLStrategy
-from qlib.utils import init_instance_by_config
-from qlib.utils.index_data import IndexData
-from qlib.utils.time import get_day_min_idx_range
-
-
-def _get_all_timestamps(
-    start: pd.Timestamp,
-    end: pd.Timestamp,
-    granularity: pd.Timedelta = ONE_MIN,
-    include_end: bool = True,
-) -> pd.DatetimeIndex:
-    ret = []
-    while start <= end:
-        ret.append(start)
-        start += granularity
-
-    if ret[-1] > end:
-        ret.pop()
-    if ret[-1] == end and not include_end:
-        ret.pop()
-    return pd.DatetimeIndex(ret)
-
-
-def fill_missing_data(
-    original_data: np.ndarray,
-    fill_method: Callable = np.nanmedian,
-) -> np.ndarray:
-    """Fill missing data.
-
-    Parameters
-    ----------
-    original_data
-        Original data without missing values.
-    fill_method
-        Method used to fill the missing data.
-
-    Returns
-    -------
-        The filled data.
-    """
-    return np.nan_to_num(original_data, nan=fill_method(original_data))
-
-
-class SAOEStateAdapter:
-    """
-    Maintain states of the environment. SAOEStateAdapter accepts execution results and update its internal state
-    according to the execution results with additional information acquired from executors & exchange. For example,
-    it gets the dealt order amount from execution results, and get the corresponding market price / volume from
-    exchange.
-
-    Example usage::
-
-        adapter = SAOEStateAdapter(...)
-        adapter.update(...)
-        state = adapter.saoe_state
-    """
-
-    def __init__(
-        self,
-        order: Order,
-        trade_decision: BaseTradeDecision,
-        executor: BaseExecutor,
-        exchange: Exchange,
-        ticks_per_step: int,
-        backtest_data: IntradayBacktestData,
-        data_granularity: int = 1,
-    ) -> None:
-        self.position = order.amount
-        self.order = order
-        self.executor = executor
-        self.exchange = exchange
-        self.backtest_data = backtest_data
-        self.start_idx, _ = get_start_end_idx(self.executor.trade_calendar, trade_decision)
-
-        self.twap_price = self.backtest_data.get_deal_price().mean()
-
-        metric_keys = list(SAOEMetrics.__annotations__.keys())  # pylint: disable=no-member
-        self.history_exec = pd.DataFrame(columns=metric_keys).set_index("datetime")
-        self.history_steps = pd.DataFrame(columns=metric_keys).set_index("datetime")
-        self.metrics: Optional[SAOEMetrics] = None
-
-        self.cur_time = max(backtest_data.ticks_for_order[0], order.start_time)
-        self.ticks_per_step = ticks_per_step
-        self.data_granularity = data_granularity
-        assert self.ticks_per_step % self.data_granularity == 0
-
-    def _next_time(self) -> pd.Timestamp:
-        current_loc = self.backtest_data.ticks_index.get_loc(self.cur_time)
-        next_loc = current_loc + (self.ticks_per_step // self.data_granularity)
-        next_loc = next_loc - next_loc % (self.ticks_per_step // self.data_granularity)
-        if (
-            next_loc < len(self.backtest_data.ticks_index)
-            and self.backtest_data.ticks_index[next_loc] < self.order.end_time
-        ):
-            return self.backtest_data.ticks_index[next_loc]
-        else:
-            return self.order.end_time
-
-    def update(
-        self,
-        execute_result: list,
-        last_step_range: Tuple[int, int],
-    ) -> None:
-        last_step_size = last_step_range[1] - last_step_range[0] + 1
-        start_time = self.backtest_data.ticks_index[last_step_range[0]]
-        end_time = self.backtest_data.ticks_index[last_step_range[1]]
-
-        exec_vol = np.zeros(last_step_size)
-        for order, _, __, ___ in execute_result:
-            idx, _ = get_day_min_idx_range(order.start_time, order.end_time, f"{self.data_granularity}min", REG_CN)
-            exec_vol[idx - last_step_range[0]] = order.deal_amount
-
-        if exec_vol.sum() > self.position and exec_vol.sum() > 0.0:
-            if exec_vol.sum() > self.position + 1.0:
-                warnings.warn(
-                    f"Sum of execution volume is {exec_vol.sum()} which is larger than "
-                    f"position + 1.0 = {self.position} + 1.0 = {self.position + 1.0}. "
-                    f"All execution volume is scaled down linearly to ensure that their sum does not position."
-                )
-            exec_vol *= self.position / (exec_vol.sum())
-
-        market_volume = cast(
-            IndexData,
-            self.exchange.get_volume(
-                self.order.stock_id,
-                pd.Timestamp(start_time),
-                pd.Timestamp(end_time),
-                method=None,
-            ),
-        )
-        market_price = cast(
-            IndexData,
-            self.exchange.get_deal_price(
-                self.order.stock_id,
-                pd.Timestamp(start_time),
-                pd.Timestamp(end_time),
-                method=None,
-                direction=self.order.direction,
-            ),
-        )
-        market_price = fill_missing_data(np.array(market_price, dtype=float).reshape(-1))
-        market_volume = fill_missing_data(np.array(market_volume, dtype=float).reshape(-1))
-
-        assert market_price.shape == market_volume.shape == exec_vol.shape
-
-        # Get data from the current level executor's indicator
-        current_trade_account = self.executor.trade_account
-        current_df = current_trade_account.get_trade_indicator().generate_trade_indicators_dataframe()
-        self.history_exec = dataframe_append(
-            self.history_exec,
-            self._collect_multi_order_metric(
-                order=self.order,
-                datetime=_get_all_timestamps(
-                    start_time, end_time, include_end=True, granularity=ONE_MIN * self.data_granularity
-                ),
-                market_vol=market_volume,
-                market_price=market_price,
-                exec_vol=exec_vol,
-                pa=current_df.iloc[-1]["pa"],
-            ),
-        )
-
-        self.history_steps = dataframe_append(
-            self.history_steps,
-            [
-                self._collect_single_order_metric(
-                    self.order,
-                    self.cur_time,
-                    market_volume,
-                    market_price,
-                    exec_vol.sum(),
-                    exec_vol,
-                ),
-            ],
-        )
-
-        # Do this at the end
-        self.position -= exec_vol.sum()
-
-        self.cur_time = self._next_time()
-
-    def generate_metrics_after_done(self) -> None:
-        """Generate metrics once the upper level execution is done"""
-
-        self.metrics = self._collect_single_order_metric(
-            self.order,
-            self.backtest_data.ticks_index[0],  # start time
-            self.history_exec["market_volume"],
-            self.history_exec["market_price"],
-            self.history_steps["amount"].sum(),
-            self.history_exec["deal_amount"],
-        )
-
-    def _collect_multi_order_metric(
-        self,
-        order: Order,
-        datetime: pd.DatetimeIndex,
-        market_vol: np.ndarray,
-        market_price: np.ndarray,
-        exec_vol: np.ndarray,
-        pa: float,
-    ) -> SAOEMetrics:
-        return SAOEMetrics(
-            # It should have the same keys with SAOEMetrics,
-            # but the values do not necessarily have the annotated type.
-            # Some values could be vectorized (e.g., exec_vol).
-            stock_id=order.stock_id,
-            datetime=datetime,
-            direction=order.direction,
-            market_volume=market_vol,
-            market_price=market_price,
-            amount=exec_vol,
-            inner_amount=exec_vol,
-            deal_amount=exec_vol,
-            trade_price=market_price,
-            trade_value=market_price * exec_vol,
-            position=self.position - np.cumsum(exec_vol),
-            ffr=exec_vol / order.amount,
-            pa=pa,
-        )
-
-    def _collect_single_order_metric(
-        self,
-        order: Order,
-        datetime: pd.Timestamp,
-        market_vol: np.ndarray,
-        market_price: np.ndarray,
-        amount: float,  # intended to trade such amount
-        exec_vol: np.ndarray,
-    ) -> SAOEMetrics:
-        assert len(market_vol) == len(market_price) == len(exec_vol)
-
-        if np.abs(np.sum(exec_vol)) < EPS:
-            exec_avg_price = 0.0
-        else:
-            exec_avg_price = cast(float, np.average(market_price, weights=exec_vol))  # could be nan
-            if hasattr(exec_avg_price, "item"):  # could be numpy scalar
-                exec_avg_price = exec_avg_price.item()  # type: ignore
-
-        exec_sum = exec_vol.sum()
-        return SAOEMetrics(
-            stock_id=order.stock_id,
-            datetime=datetime,
-            direction=order.direction,
-            market_volume=market_vol.sum(),
-            market_price=market_price.mean() if len(market_price) > 0 else np.nan,
-            amount=amount,
-            inner_amount=exec_sum,
-            deal_amount=exec_sum,  # in this simulator, there's no other restrictions
-            trade_price=exec_avg_price,
-            trade_value=float(np.sum(market_price * exec_vol)),
-            position=self.position - exec_sum,
-            ffr=float(exec_sum / order.amount),
-            pa=price_advantage(exec_avg_price, self.twap_price, order.direction),
-        )
-
-    @property
-    def saoe_state(self) -> SAOEState:
-        return SAOEState(
-            order=self.order,
-            cur_time=self.cur_time,
-            cur_step=self.executor.trade_calendar.get_trade_step() - self.start_idx,
-            position=self.position,
-            history_exec=self.history_exec,
-            history_steps=self.history_steps,
-            metrics=self.metrics,
-            backtest_data=self.backtest_data,
-            ticks_per_step=self.ticks_per_step,
-            ticks_index=self.backtest_data.ticks_index,
-            ticks_for_order=self.backtest_data.ticks_for_order,
-        )
-
-
-class SAOEStrategy(RLStrategy):
-    """RL-based strategies that use SAOEState as state."""
-
-    def __init__(
-        self,
-        policy: BasePolicy,
-        outer_trade_decision: BaseTradeDecision | None = None,
-        level_infra: LevelInfrastructure | None = None,
-        common_infra: CommonInfrastructure | None = None,
-        data_granularity: int = 1,
-        **kwargs: Any,
-    ) -> None:
-        super(SAOEStrategy, self).__init__(
-            policy=policy,
-            outer_trade_decision=outer_trade_decision,
-            level_infra=level_infra,
-            common_infra=common_infra,
-            **kwargs,
-        )
-
-        self._data_granularity = data_granularity
-        self.adapter_dict: Dict[tuple, SAOEStateAdapter] = {}
-        self._last_step_range = (0, 0)
-
-    def _create_qlib_backtest_adapter(
-        self,
-        order: Order,
-        trade_decision: BaseTradeDecision,
-        trade_range: TradeRange,
-    ) -> SAOEStateAdapter:
-        backtest_data = load_backtest_data(order, self.trade_exchange, trade_range)
-
-        return SAOEStateAdapter(
-            order=order,
-            trade_decision=trade_decision,
-            executor=self.executor,
-            exchange=self.trade_exchange,
-            ticks_per_step=int(pd.Timedelta(self.trade_calendar.get_freq()) / ONE_MIN),
-            backtest_data=backtest_data,
-            data_granularity=self._data_granularity,
-        )
-
-    def reset(self, outer_trade_decision: BaseTradeDecision | None = None, **kwargs: Any) -> None:
-        super(SAOEStrategy, self).reset(outer_trade_decision=outer_trade_decision, **kwargs)
-
-        self.adapter_dict = {}
-        self._last_step_range = (0, 0)
-
-        if outer_trade_decision is not None and not outer_trade_decision.empty():
-            trade_range = outer_trade_decision.trade_range
-            assert trade_range is not None
-
-            self.adapter_dict = {}
-            for decision in outer_trade_decision.get_decision():
-                order = cast(Order, decision)
-                self.adapter_dict[order.key_by_day] = self._create_qlib_backtest_adapter(
-                    order, outer_trade_decision, trade_range
-                )
-
-    def get_saoe_state_by_order(self, order: Order) -> SAOEState:
-        return self.adapter_dict[order.key_by_day].saoe_state
-
-    def post_upper_level_exe_step(self) -> None:
-        for adapter in self.adapter_dict.values():
-            adapter.generate_metrics_after_done()
-
-    def post_exe_step(self, execute_result: Optional[list]) -> None:
-        last_step_length = self._last_step_range[1] - self._last_step_range[0]
-        if last_step_length <= 0:
-            assert not execute_result
-            return
-
-        results = collections.defaultdict(list)
-        if execute_result is not None:
-            for e in execute_result:
-                results[e[0].key_by_day].append(e)
-
-        for key, adapter in self.adapter_dict.items():
-            adapter.update(results[key], self._last_step_range)
-
-    def generate_trade_decision(
-        self,
-        execute_result: list | None = None,
-    ) -> Union[BaseTradeDecision, Generator[Any, Any, BaseTradeDecision]]:
-        """
-        For SAOEStrategy, we need to update the `self._last_step_range` every time a decision is generated.
-        This operation should be invisible to developers, so we implement it in `generate_trade_decision()`
-        The concrete logic to generate decisions should be implemented in `_generate_trade_decision()`.
-        In other words, all subclass of `SAOEStrategy` should overwrite `_generate_trade_decision()` instead of
-        `generate_trade_decision()`.
-        """
-        self._last_step_range = self.get_data_cal_avail_range(rtype="step")
-
-        decision = self._generate_trade_decision(execute_result)
-        if isinstance(decision, GeneratorType):
-            decision = yield from decision
-
-        return decision
-
-    def _generate_trade_decision(
-        self,
-        execute_result: list | None = None,
-    ) -> Union[BaseTradeDecision, Generator[Any, Any, BaseTradeDecision]]:
-        raise NotImplementedError
-
-
-class ProxySAOEStrategy(SAOEStrategy):
-    """Proxy strategy that uses SAOEState. It is called a 'proxy' strategy because it does not make any decisions
-    by itself. Instead, when the strategy is required to generate a decision, it will yield the environment's
-    information and let the outside agents to make the decision. Please refer to `_generate_trade_decision` for
-    more details.
-    """
-
-    def __init__(
-        self,
-        outer_trade_decision: BaseTradeDecision | None = None,
-        level_infra: LevelInfrastructure | None = None,
-        common_infra: CommonInfrastructure | None = None,
-        **kwargs: Any,
-    ) -> None:
-        super().__init__(None, outer_trade_decision, level_infra, common_infra, **kwargs)
-
-    def _generate_trade_decision(self, execute_result: list | None = None) -> Generator[Any, Any, BaseTradeDecision]:
-        # Once the following line is executed, this ProxySAOEStrategy (self) will be yielded to the outside
-        # of the entire executor, and the execution will be suspended. When the execution is resumed by `send()`,
-        # the item will be captured by `exec_vol`. The outside policy could communicate with the inner
-        # level strategy through this way.
-        exec_vol = yield self
-
-        oh = self.trade_exchange.get_order_helper()
-        order = oh.create(self._order.stock_id, exec_vol, self._order.direction)
-
-        return TradeDecisionWO([order], self)
-
-    def reset(self, outer_trade_decision: BaseTradeDecision | None = None, **kwargs: Any) -> None:
-        super().reset(outer_trade_decision=outer_trade_decision, **kwargs)
-
-        assert isinstance(outer_trade_decision, TradeDecisionWO)
-        if outer_trade_decision is not None:
-            order_list = outer_trade_decision.order_list
-            assert len(order_list) == 1
-            self._order = order_list[0]
-
-
-class SAOEIntStrategy(SAOEStrategy):
-    """(SAOE)state based strategy with (Int)preters."""
-
-    def __init__(
-        self,
-        policy: dict | BasePolicy,
-        state_interpreter: dict | StateInterpreter,
-        action_interpreter: dict | ActionInterpreter,
-        network: dict | torch.nn.Module | None = None,
-        outer_trade_decision: BaseTradeDecision | None = None,
-        level_infra: LevelInfrastructure | None = None,
-        common_infra: CommonInfrastructure | None = None,
-        **kwargs: Any,
-    ) -> None:
-        super(SAOEIntStrategy, self).__init__(
-            policy=policy,
-            outer_trade_decision=outer_trade_decision,
-            level_infra=level_infra,
-            common_infra=common_infra,
-            **kwargs,
-        )
-
-        self._state_interpreter: StateInterpreter = init_instance_by_config(
-            state_interpreter,
-            accept_types=StateInterpreter,
-        )
-        self._action_interpreter: ActionInterpreter = init_instance_by_config(
-            action_interpreter,
-            accept_types=ActionInterpreter,
-        )
-
-        if isinstance(policy, dict):
-            assert network is not None
-
-            if isinstance(network, dict):
-                network["kwargs"].update(
-                    {
-                        "obs_space": self._state_interpreter.observation_space,
-                    }
-                )
-                network_inst = init_instance_by_config(network)
-            else:
-                network_inst = network
-
-            policy["kwargs"].update(
-                {
-                    "obs_space": self._state_interpreter.observation_space,
-                    "action_space": self._action_interpreter.action_space,
-                    "network": network_inst,
-                }
-            )
-            self._policy = init_instance_by_config(policy)
-        elif isinstance(policy, BasePolicy):
-            self._policy = policy
-        else:
-            raise ValueError(f"Unsupported policy type: {type(policy)}.")
-
-        if self._policy is not None:
-            self._policy.eval()
-
-    def reset(self, outer_trade_decision: BaseTradeDecision | None = None, **kwargs: Any) -> None:
-        super().reset(outer_trade_decision=outer_trade_decision, **kwargs)
-
-    def _generate_trade_details(self, act: np.ndarray, exec_vols: List[float]) -> pd.DataFrame:
-        assert hasattr(self.outer_trade_decision, "order_list")
-
-        trade_details = []
-        for a, v, o in zip(act, exec_vols, getattr(self.outer_trade_decision, "order_list")):
-            trade_details.append(
-                {
-                    "instrument": o.stock_id,
-                    "datetime": self.trade_calendar.get_step_time()[0],
-                    "freq": self.trade_calendar.get_freq(),
-                    "rl_exec_vol": v,
-                }
-            )
-            if a is not None:
-                trade_details[-1]["rl_action"] = a
-        return pd.DataFrame.from_records(trade_details)
-
-    def _generate_trade_decision(self, execute_result: list | None = None) -> BaseTradeDecision:
-        states = []
-        obs_batch = []
-        for decision in self.outer_trade_decision.get_decision():
-            order = cast(Order, decision)
-            state = self.get_saoe_state_by_order(order)
-
-            states.append(state)
-            obs_batch.append({"obs": self._state_interpreter.interpret(state)})
-
-        with torch.no_grad():
-            policy_out = self._policy(Batch(obs_batch))
-        act = policy_out.act.numpy() if torch.is_tensor(policy_out.act) else policy_out.act
-        exec_vols = [self._action_interpreter.interpret(s, a) for s, a in zip(states, act)]
-
-        oh = self.trade_exchange.get_order_helper()
-        order_list = []
-        for decision, exec_vol in zip(self.outer_trade_decision.get_decision(), exec_vols):
-            if exec_vol != 0:
-                order = cast(Order, decision)
-                order_list.append(oh.create(order.stock_id, exec_vol, order.direction))
-
-        return TradeDecisionWithDetails(
-            order_list=order_list,
-            strategy=self,
-            details=self._generate_trade_details(act, exec_vols),
-        )
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+from __future__ import annotations
+
+import collections
+from types import GeneratorType
+from typing import Any, Callable, cast, Dict, Generator, List, Optional, Tuple, Union
+
+import warnings
+import numpy as np
+import pandas as pd
+import torch
+from tianshou.data import Batch
+from tianshou.policy import BasePolicy
+
+from qlib.backtest import CommonInfrastructure, Order
+from qlib.backtest.decision import BaseTradeDecision, TradeDecisionWithDetails, TradeDecisionWO, TradeRange
+from qlib.backtest.exchange import Exchange
+from qlib.backtest.executor import BaseExecutor
+from qlib.backtest.utils import LevelInfrastructure, get_start_end_idx
+from qlib.constant import EPS, ONE_MIN, REG_CN
+from qlib.rl.data.native import IntradayBacktestData, load_backtest_data
+from qlib.rl.interpreter import ActionInterpreter, StateInterpreter
+from qlib.rl.order_execution.state import SAOEMetrics, SAOEState
+from qlib.rl.order_execution.utils import dataframe_append, price_advantage
+from qlib.strategy.base import RLStrategy
+from qlib.utils import init_instance_by_config
+from qlib.utils.index_data import IndexData
+from qlib.utils.time import get_day_min_idx_range
+
+
+def _get_all_timestamps(
+    start: pd.Timestamp,
+    end: pd.Timestamp,
+    granularity: pd.Timedelta = ONE_MIN,
+    include_end: bool = True,
+) -> pd.DatetimeIndex:
+    ret = []
+    while start <= end:
+        ret.append(start)
+        start += granularity
+
+    if ret[-1] > end:
+        ret.pop()
+    if ret[-1] == end and not include_end:
+        ret.pop()
+    return pd.DatetimeIndex(ret)
+
+
+def fill_missing_data(
+    original_data: np.ndarray,
+    fill_method: Callable = np.nanmedian,
+) -> np.ndarray:
+    """Fill missing data.
+
+    Parameters
+    ----------
+    original_data
+        Original data without missing values.
+    fill_method
+        Method used to fill the missing data.
+
+    Returns
+    -------
+        The filled data.
+    """
+    return np.nan_to_num(original_data, nan=fill_method(original_data))
+
+
+class SAOEStateAdapter:
+    """
+    Maintain states of the environment. SAOEStateAdapter accepts execution results and update its internal state
+    according to the execution results with additional information acquired from executors & exchange. For example,
+    it gets the dealt order amount from execution results, and get the corresponding market price / volume from
+    exchange.
+
+    Example usage::
+
+        adapter = SAOEStateAdapter(...)
+        adapter.update(...)
+        state = adapter.saoe_state
+    """
+
+    def __init__(
+        self,
+        order: Order,
+        trade_decision: BaseTradeDecision,
+        executor: BaseExecutor,
+        exchange: Exchange,
+        ticks_per_step: int,
+        backtest_data: IntradayBacktestData,
+        data_granularity: int = 1,
+    ) -> None:
+        self.position = order.amount
+        self.order = order
+        self.executor = executor
+        self.exchange = exchange
+        self.backtest_data = backtest_data
+        self.start_idx, _ = get_start_end_idx(self.executor.trade_calendar, trade_decision)
+
+        self.twap_price = self.backtest_data.get_deal_price().mean()
+
+        metric_keys = list(SAOEMetrics.__annotations__.keys())  # pylint: disable=no-member
+        self.history_exec = pd.DataFrame(columns=metric_keys).set_index("datetime")
+        self.history_steps = pd.DataFrame(columns=metric_keys).set_index("datetime")
+        self.metrics: Optional[SAOEMetrics] = None
+
+        self.cur_time = max(backtest_data.ticks_for_order[0], order.start_time)
+        self.ticks_per_step = ticks_per_step
+        self.data_granularity = data_granularity
+        assert self.ticks_per_step % self.data_granularity == 0
+
+    def _next_time(self) -> pd.Timestamp:
+        current_loc = self.backtest_data.ticks_index.get_loc(self.cur_time)
+        next_loc = current_loc + (self.ticks_per_step // self.data_granularity)
+        next_loc = next_loc - next_loc % (self.ticks_per_step // self.data_granularity)
+        if (
+            next_loc < len(self.backtest_data.ticks_index)
+            and self.backtest_data.ticks_index[next_loc] < self.order.end_time
+        ):
+            return self.backtest_data.ticks_index[next_loc]
+        else:
+            return self.order.end_time
+
+    def update(
+        self,
+        execute_result: list,
+        last_step_range: Tuple[int, int],
+    ) -> None:
+        last_step_size = last_step_range[1] - last_step_range[0] + 1
+        start_time = self.backtest_data.ticks_index[last_step_range[0]]
+        end_time = self.backtest_data.ticks_index[last_step_range[1]]
+
+        exec_vol = np.zeros(last_step_size)
+        for order, _, __, ___ in execute_result:
+            idx, _ = get_day_min_idx_range(order.start_time, order.end_time, f"{self.data_granularity}min", REG_CN)
+            exec_vol[idx - last_step_range[0]] = order.deal_amount
+
+        if exec_vol.sum() > self.position and exec_vol.sum() > 0.0:
+            if exec_vol.sum() > self.position + 1.0:
+                warnings.warn(
+                    f"Sum of execution volume is {exec_vol.sum()} which is larger than "
+                    f"position + 1.0 = {self.position} + 1.0 = {self.position + 1.0}. "
+                    f"All execution volume is scaled down linearly to ensure that their sum does not position."
+                )
+            exec_vol *= self.position / (exec_vol.sum())
+
+        market_volume = cast(
+            IndexData,
+            self.exchange.get_volume(
+                self.order.stock_id,
+                pd.Timestamp(start_time),
+                pd.Timestamp(end_time),
+                method=None,
+            ),
+        )
+        market_price = cast(
+            IndexData,
+            self.exchange.get_deal_price(
+                self.order.stock_id,
+                pd.Timestamp(start_time),
+                pd.Timestamp(end_time),
+                method=None,
+                direction=self.order.direction,
+            ),
+        )
+        market_price = fill_missing_data(np.array(market_price, dtype=float).reshape(-1))
+        market_volume = fill_missing_data(np.array(market_volume, dtype=float).reshape(-1))
+
+        assert market_price.shape == market_volume.shape == exec_vol.shape
+
+        # Get data from the current level executor's indicator
+        current_trade_account = self.executor.trade_account
+        current_df = current_trade_account.get_trade_indicator().generate_trade_indicators_dataframe()
+        self.history_exec = dataframe_append(
+            self.history_exec,
+            self._collect_multi_order_metric(
+                order=self.order,
+                datetime=_get_all_timestamps(
+                    start_time, end_time, include_end=True, granularity=ONE_MIN * self.data_granularity
+                ),
+                market_vol=market_volume,
+                market_price=market_price,
+                exec_vol=exec_vol,
+                pa=current_df.iloc[-1]["pa"],
+            ),
+        )
+
+        self.history_steps = dataframe_append(
+            self.history_steps,
+            [
+                self._collect_single_order_metric(
+                    self.order,
+                    self.cur_time,
+                    market_volume,
+                    market_price,
+                    exec_vol.sum(),
+                    exec_vol,
+                ),
+            ],
+        )
+
+        # Do this at the end
+        self.position -= exec_vol.sum()
+
+        self.cur_time = self._next_time()
+
+    def generate_metrics_after_done(self) -> None:
+        """Generate metrics once the upper level execution is done"""
+
+        self.metrics = self._collect_single_order_metric(
+            self.order,
+            self.backtest_data.ticks_index[0],  # start time
+            self.history_exec["market_volume"],
+            self.history_exec["market_price"],
+            self.history_steps["amount"].sum(),
+            self.history_exec["deal_amount"],
+        )
+
+    def _collect_multi_order_metric(
+        self,
+        order: Order,
+        datetime: pd.DatetimeIndex,
+        market_vol: np.ndarray,
+        market_price: np.ndarray,
+        exec_vol: np.ndarray,
+        pa: float,
+    ) -> SAOEMetrics:
+        return SAOEMetrics(
+            # It should have the same keys with SAOEMetrics,
+            # but the values do not necessarily have the annotated type.
+            # Some values could be vectorized (e.g., exec_vol).
+            stock_id=order.stock_id,
+            datetime=datetime,
+            direction=order.direction,
+            market_volume=market_vol,
+            market_price=market_price,
+            amount=exec_vol,
+            inner_amount=exec_vol,
+            deal_amount=exec_vol,
+            trade_price=market_price,
+            trade_value=market_price * exec_vol,
+            position=self.position - np.cumsum(exec_vol),
+            ffr=exec_vol / order.amount,
+            pa=pa,
+        )
+
+    def _collect_single_order_metric(
+        self,
+        order: Order,
+        datetime: pd.Timestamp,
+        market_vol: np.ndarray,
+        market_price: np.ndarray,
+        amount: float,  # intended to trade such amount
+        exec_vol: np.ndarray,
+    ) -> SAOEMetrics:
+        assert len(market_vol) == len(market_price) == len(exec_vol)
+
+        if np.abs(np.sum(exec_vol)) < EPS:
+            exec_avg_price = 0.0
+        else:
+            exec_avg_price = cast(float, np.average(market_price, weights=exec_vol))  # could be nan
+            if hasattr(exec_avg_price, "item"):  # could be numpy scalar
+                exec_avg_price = exec_avg_price.item()  # type: ignore
+
+        exec_sum = exec_vol.sum()
+        return SAOEMetrics(
+            stock_id=order.stock_id,
+            datetime=datetime,
+            direction=order.direction,
+            market_volume=market_vol.sum(),
+            market_price=market_price.mean() if len(market_price) > 0 else np.nan,
+            amount=amount,
+            inner_amount=exec_sum,
+            deal_amount=exec_sum,  # in this simulator, there's no other restrictions
+            trade_price=exec_avg_price,
+            trade_value=float(np.sum(market_price * exec_vol)),
+            position=self.position - exec_sum,
+            ffr=float(exec_sum / order.amount),
+            pa=price_advantage(exec_avg_price, self.twap_price, order.direction),
+        )
+
+    @property
+    def saoe_state(self) -> SAOEState:
+        return SAOEState(
+            order=self.order,
+            cur_time=self.cur_time,
+            cur_step=self.executor.trade_calendar.get_trade_step() - self.start_idx,
+            position=self.position,
+            history_exec=self.history_exec,
+            history_steps=self.history_steps,
+            metrics=self.metrics,
+            backtest_data=self.backtest_data,
+            ticks_per_step=self.ticks_per_step,
+            ticks_index=self.backtest_data.ticks_index,
+            ticks_for_order=self.backtest_data.ticks_for_order,
+        )
+
+
+class SAOEStrategy(RLStrategy):
+    """RL-based strategies that use SAOEState as state."""
+
+    def __init__(
+        self,
+        policy: BasePolicy,
+        outer_trade_decision: BaseTradeDecision | None = None,
+        level_infra: LevelInfrastructure | None = None,
+        common_infra: CommonInfrastructure | None = None,
+        data_granularity: int = 1,
+        **kwargs: Any,
+    ) -> None:
+        super(SAOEStrategy, self).__init__(
+            policy=policy,
+            outer_trade_decision=outer_trade_decision,
+            level_infra=level_infra,
+            common_infra=common_infra,
+            **kwargs,
+        )
+
+        self._data_granularity = data_granularity
+        self.adapter_dict: Dict[tuple, SAOEStateAdapter] = {}
+        self._last_step_range = (0, 0)
+
+    def _create_qlib_backtest_adapter(
+        self,
+        order: Order,
+        trade_decision: BaseTradeDecision,
+        trade_range: TradeRange,
+    ) -> SAOEStateAdapter:
+        backtest_data = load_backtest_data(order, self.trade_exchange, trade_range)
+
+        return SAOEStateAdapter(
+            order=order,
+            trade_decision=trade_decision,
+            executor=self.executor,
+            exchange=self.trade_exchange,
+            ticks_per_step=int(pd.Timedelta(self.trade_calendar.get_freq()) / ONE_MIN),
+            backtest_data=backtest_data,
+            data_granularity=self._data_granularity,
+        )
+
+    def reset(self, outer_trade_decision: BaseTradeDecision | None = None, **kwargs: Any) -> None:
+        super(SAOEStrategy, self).reset(outer_trade_decision=outer_trade_decision, **kwargs)
+
+        self.adapter_dict = {}
+        self._last_step_range = (0, 0)
+
+        if outer_trade_decision is not None and not outer_trade_decision.empty():
+            trade_range = outer_trade_decision.trade_range
+            assert trade_range is not None
+
+            self.adapter_dict = {}
+            for decision in outer_trade_decision.get_decision():
+                order = cast(Order, decision)
+                self.adapter_dict[order.key_by_day] = self._create_qlib_backtest_adapter(
+                    order, outer_trade_decision, trade_range
+                )
+
+    def get_saoe_state_by_order(self, order: Order) -> SAOEState:
+        return self.adapter_dict[order.key_by_day].saoe_state
+
+    def post_upper_level_exe_step(self) -> None:
+        for adapter in self.adapter_dict.values():
+            adapter.generate_metrics_after_done()
+
+    def post_exe_step(self, execute_result: Optional[list]) -> None:
+        last_step_length = self._last_step_range[1] - self._last_step_range[0]
+        if last_step_length <= 0:
+            assert not execute_result
+            return
+
+        results = collections.defaultdict(list)
+        if execute_result is not None:
+            for e in execute_result:
+                results[e[0].key_by_day].append(e)
+
+        for key, adapter in self.adapter_dict.items():
+            adapter.update(results[key], self._last_step_range)
+
+    def generate_trade_decision(
+        self,
+        execute_result: list | None = None,
+    ) -> Union[BaseTradeDecision, Generator[Any, Any, BaseTradeDecision]]:
+        """
+        For SAOEStrategy, we need to update the `self._last_step_range` every time a decision is generated.
+        This operation should be invisible to developers, so we implement it in `generate_trade_decision()`
+        The concrete logic to generate decisions should be implemented in `_generate_trade_decision()`.
+        In other words, all subclass of `SAOEStrategy` should overwrite `_generate_trade_decision()` instead of
+        `generate_trade_decision()`.
+        """
+        self._last_step_range = self.get_data_cal_avail_range(rtype="step")
+
+        decision = self._generate_trade_decision(execute_result)
+        if isinstance(decision, GeneratorType):
+            decision = yield from decision
+
+        return decision
+
+    def _generate_trade_decision(
+        self,
+        execute_result: list | None = None,
+    ) -> Union[BaseTradeDecision, Generator[Any, Any, BaseTradeDecision]]:
+        raise NotImplementedError
+
+
+class ProxySAOEStrategy(SAOEStrategy):
+    """Proxy strategy that uses SAOEState. It is called a 'proxy' strategy because it does not make any decisions
+    by itself. Instead, when the strategy is required to generate a decision, it will yield the environment's
+    information and let the outside agents to make the decision. Please refer to `_generate_trade_decision` for
+    more details.
+    """
+
+    def __init__(
+        self,
+        outer_trade_decision: BaseTradeDecision | None = None,
+        level_infra: LevelInfrastructure | None = None,
+        common_infra: CommonInfrastructure | None = None,
+        **kwargs: Any,
+    ) -> None:
+        super().__init__(None, outer_trade_decision, level_infra, common_infra, **kwargs)
+
+    def _generate_trade_decision(self, execute_result: list | None = None) -> Generator[Any, Any, BaseTradeDecision]:
+        # Once the following line is executed, this ProxySAOEStrategy (self) will be yielded to the outside
+        # of the entire executor, and the execution will be suspended. When the execution is resumed by `send()`,
+        # the item will be captured by `exec_vol`. The outside policy could communicate with the inner
+        # level strategy through this way.
+        exec_vol = yield self
+
+        oh = self.trade_exchange.get_order_helper()
+        order = oh.create(self._order.stock_id, exec_vol, self._order.direction)
+
+        return TradeDecisionWO([order], self)
+
+    def reset(self, outer_trade_decision: BaseTradeDecision | None = None, **kwargs: Any) -> None:
+        super().reset(outer_trade_decision=outer_trade_decision, **kwargs)
+
+        assert isinstance(outer_trade_decision, TradeDecisionWO)
+        if outer_trade_decision is not None:
+            order_list = outer_trade_decision.order_list
+            assert len(order_list) == 1
+            self._order = order_list[0]
+
+
+class SAOEIntStrategy(SAOEStrategy):
+    """(SAOE)state based strategy with (Int)preters."""
+
+    def __init__(
+        self,
+        policy: dict | BasePolicy,
+        state_interpreter: dict | StateInterpreter,
+        action_interpreter: dict | ActionInterpreter,
+        network: dict | torch.nn.Module | None = None,
+        outer_trade_decision: BaseTradeDecision | None = None,
+        level_infra: LevelInfrastructure | None = None,
+        common_infra: CommonInfrastructure | None = None,
+        **kwargs: Any,
+    ) -> None:
+        super(SAOEIntStrategy, self).__init__(
+            policy=policy,
+            outer_trade_decision=outer_trade_decision,
+            level_infra=level_infra,
+            common_infra=common_infra,
+            **kwargs,
+        )
+
+        self._state_interpreter: StateInterpreter = init_instance_by_config(
+            state_interpreter,
+            accept_types=StateInterpreter,
+        )
+        self._action_interpreter: ActionInterpreter = init_instance_by_config(
+            action_interpreter,
+            accept_types=ActionInterpreter,
+        )
+
+        if isinstance(policy, dict):
+            assert network is not None
+
+            if isinstance(network, dict):
+                network["kwargs"].update(
+                    {
+                        "obs_space": self._state_interpreter.observation_space,
+                    }
+                )
+                network_inst = init_instance_by_config(network)
+            else:
+                network_inst = network
+
+            policy["kwargs"].update(
+                {
+                    "obs_space": self._state_interpreter.observation_space,
+                    "action_space": self._action_interpreter.action_space,
+                    "network": network_inst,
+                }
+            )
+            self._policy = init_instance_by_config(policy)
+        elif isinstance(policy, BasePolicy):
+            self._policy = policy
+        else:
+            raise ValueError(f"Unsupported policy type: {type(policy)}.")
+
+        if self._policy is not None:
+            self._policy.eval()
+
+    def reset(self, outer_trade_decision: BaseTradeDecision | None = None, **kwargs: Any) -> None:
+        super().reset(outer_trade_decision=outer_trade_decision, **kwargs)
+
+    def _generate_trade_details(self, act: np.ndarray, exec_vols: List[float]) -> pd.DataFrame:
+        assert hasattr(self.outer_trade_decision, "order_list")
+
+        trade_details = []
+        for a, v, o in zip(act, exec_vols, getattr(self.outer_trade_decision, "order_list")):
+            trade_details.append(
+                {
+                    "instrument": o.stock_id,
+                    "datetime": self.trade_calendar.get_step_time()[0],
+                    "freq": self.trade_calendar.get_freq(),
+                    "rl_exec_vol": v,
+                }
+            )
+            if a is not None:
+                trade_details[-1]["rl_action"] = a
+        return pd.DataFrame.from_records(trade_details)
+
+    def _generate_trade_decision(self, execute_result: list | None = None) -> BaseTradeDecision:
+        states = []
+        obs_batch = []
+        for decision in self.outer_trade_decision.get_decision():
+            order = cast(Order, decision)
+            state = self.get_saoe_state_by_order(order)
+
+            states.append(state)
+            obs_batch.append({"obs": self._state_interpreter.interpret(state)})
+
+        with torch.no_grad():
+            policy_out = self._policy(Batch(obs_batch))
+        act = policy_out.act.numpy() if torch.is_tensor(policy_out.act) else policy_out.act
+        exec_vols = [self._action_interpreter.interpret(s, a) for s, a in zip(states, act)]
+
+        oh = self.trade_exchange.get_order_helper()
+        order_list = []
+        for decision, exec_vol in zip(self.outer_trade_decision.get_decision(), exec_vols):
+            if exec_vol != 0:
+                order = cast(Order, decision)
+                order_list.append(oh.create(order.stock_id, exec_vol, order.direction))
+
+        return TradeDecisionWithDetails(
+            order_list=order_list,
+            strategy=self,
+            details=self._generate_trade_details(act, exec_vols),
+        )
```

## qlib/rl/order_execution/utils.py

 * *Ordering differences only*

```diff
@@ -1,52 +1,52 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-from __future__ import annotations
-
-from typing import Any, cast
-
-import numpy as np
-import pandas as pd
-
-from qlib.backtest.decision import OrderDir
-from qlib.backtest.executor import BaseExecutor, NestedExecutor, SimulatorExecutor
-from qlib.constant import float_or_ndarray
-
-
-def dataframe_append(df: pd.DataFrame, other: Any) -> pd.DataFrame:
-    # dataframe.append is deprecated
-    other_df = pd.DataFrame(other).set_index("datetime")
-    other_df.index.name = "datetime"
-
-    res = pd.concat([df, other_df], axis=0)
-    return res
-
-
-def price_advantage(
-    exec_price: float_or_ndarray,
-    baseline_price: float,
-    direction: OrderDir | int,
-) -> float_or_ndarray:
-    if baseline_price == 0:  # something is wrong with data. Should be nan here
-        if isinstance(exec_price, float):
-            return 0.0
-        else:
-            return np.zeros_like(exec_price)
-    if direction == OrderDir.BUY:
-        res = (1 - exec_price / baseline_price) * 10000
-    elif direction == OrderDir.SELL:
-        res = (exec_price / baseline_price - 1) * 10000
-    else:
-        raise ValueError(f"Unexpected order direction: {direction}")
-    res_wo_nan: np.ndarray = np.nan_to_num(res, nan=0.0)
-    if res_wo_nan.size == 1:
-        return res_wo_nan.item()
-    else:
-        return cast(float_or_ndarray, res_wo_nan)
-
-
-def get_simulator_executor(executor: BaseExecutor) -> SimulatorExecutor:
-    while isinstance(executor, NestedExecutor):
-        executor = executor.inner_executor
-    assert isinstance(executor, SimulatorExecutor)
-    return executor
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+from __future__ import annotations
+
+from typing import Any, cast
+
+import numpy as np
+import pandas as pd
+
+from qlib.backtest.decision import OrderDir
+from qlib.backtest.executor import BaseExecutor, NestedExecutor, SimulatorExecutor
+from qlib.constant import float_or_ndarray
+
+
+def dataframe_append(df: pd.DataFrame, other: Any) -> pd.DataFrame:
+    # dataframe.append is deprecated
+    other_df = pd.DataFrame(other).set_index("datetime")
+    other_df.index.name = "datetime"
+
+    res = pd.concat([df, other_df], axis=0)
+    return res
+
+
+def price_advantage(
+    exec_price: float_or_ndarray,
+    baseline_price: float,
+    direction: OrderDir | int,
+) -> float_or_ndarray:
+    if baseline_price == 0:  # something is wrong with data. Should be nan here
+        if isinstance(exec_price, float):
+            return 0.0
+        else:
+            return np.zeros_like(exec_price)
+    if direction == OrderDir.BUY:
+        res = (1 - exec_price / baseline_price) * 10000
+    elif direction == OrderDir.SELL:
+        res = (exec_price / baseline_price - 1) * 10000
+    else:
+        raise ValueError(f"Unexpected order direction: {direction}")
+    res_wo_nan: np.ndarray = np.nan_to_num(res, nan=0.0)
+    if res_wo_nan.size == 1:
+        return res_wo_nan.item()
+    else:
+        return cast(float_or_ndarray, res_wo_nan)
+
+
+def get_simulator_executor(executor: BaseExecutor) -> SimulatorExecutor:
+    while isinstance(executor, NestedExecutor):
+        executor = executor.inner_executor
+    assert isinstance(executor, SimulatorExecutor)
+    return executor
```

## qlib/rl/strategy/__init__.py

 * *Ordering differences only*

```diff
@@ -1,5 +1,5 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-from .single_order import SingleOrderStrategy
-
-__all__ = ["SingleOrderStrategy"]
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+from .single_order import SingleOrderStrategy
+
+__all__ = ["SingleOrderStrategy"]
```

## qlib/rl/strategy/single_order.py

 * *Ordering differences only*

```diff
@@ -1,33 +1,33 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-from __future__ import annotations
-
-from qlib.backtest import Order
-from qlib.backtest.decision import OrderHelper, TradeDecisionWO, TradeRange
-from qlib.strategy.base import BaseStrategy
-
-
-class SingleOrderStrategy(BaseStrategy):
-    """Strategy used to generate a trade decision with exactly one order."""
-
-    def __init__(
-        self,
-        order: Order,
-        trade_range: TradeRange | None = None,
-    ) -> None:
-        super().__init__()
-
-        self._order = order
-        self._trade_range = trade_range
-
-    def generate_trade_decision(self, execute_result: list | None = None) -> TradeDecisionWO:
-        oh: OrderHelper = self.common_infra.get("trade_exchange").get_order_helper()
-        order_list = [
-            oh.create(
-                code=self._order.stock_id,
-                amount=self._order.amount,
-                direction=self._order.direction,
-            ),
-        ]
-        return TradeDecisionWO(order_list, self, self._trade_range)
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+from __future__ import annotations
+
+from qlib.backtest import Order
+from qlib.backtest.decision import OrderHelper, TradeDecisionWO, TradeRange
+from qlib.strategy.base import BaseStrategy
+
+
+class SingleOrderStrategy(BaseStrategy):
+    """Strategy used to generate a trade decision with exactly one order."""
+
+    def __init__(
+        self,
+        order: Order,
+        trade_range: TradeRange | None = None,
+    ) -> None:
+        super().__init__()
+
+        self._order = order
+        self._trade_range = trade_range
+
+    def generate_trade_decision(self, execute_result: list | None = None) -> TradeDecisionWO:
+        oh: OrderHelper = self.common_infra.get("trade_exchange").get_order_helper()
+        order_list = [
+            oh.create(
+                code=self._order.stock_id,
+                amount=self._order.amount,
+                direction=self._order.direction,
+            ),
+        ]
+        return TradeDecisionWO(order_list, self, self._trade_range)
```

## qlib/rl/trainer/__init__.py

 * *Ordering differences only*

```diff
@@ -1,20 +1,20 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-"""Train, test, inference utilities."""
-
-from .api import backtest, train
-from .callbacks import Checkpoint, EarlyStopping, MetricsWriter
-from .trainer import Trainer
-from .vessel import TrainingVessel, TrainingVesselBase
-
-__all__ = [
-    "Trainer",
-    "TrainingVessel",
-    "TrainingVesselBase",
-    "Checkpoint",
-    "EarlyStopping",
-    "MetricsWriter",
-    "train",
-    "backtest",
-]
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+"""Train, test, inference utilities."""
+
+from .api import backtest, train
+from .callbacks import Checkpoint, EarlyStopping, MetricsWriter
+from .trainer import Trainer
+from .vessel import TrainingVessel, TrainingVesselBase
+
+__all__ = [
+    "Trainer",
+    "TrainingVessel",
+    "TrainingVesselBase",
+    "Checkpoint",
+    "EarlyStopping",
+    "MetricsWriter",
+    "train",
+    "backtest",
+]
```

## qlib/rl/trainer/api.py

 * *Ordering differences only*

```diff
@@ -1,118 +1,118 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-from __future__ import annotations
-
-from typing import Any, Callable, Dict, List, Sequence, cast
-
-from tianshou.policy import BasePolicy
-
-from qlib.rl.interpreter import ActionInterpreter, StateInterpreter
-from qlib.rl.reward import Reward
-from qlib.rl.simulator import InitialStateType, Simulator
-from qlib.rl.utils import FiniteEnvType, LogWriter
-
-from .trainer import Trainer
-from .vessel import TrainingVessel
-
-
-def train(
-    simulator_fn: Callable[[InitialStateType], Simulator],
-    state_interpreter: StateInterpreter,
-    action_interpreter: ActionInterpreter,
-    initial_states: Sequence[InitialStateType],
-    policy: BasePolicy,
-    reward: Reward,
-    vessel_kwargs: Dict[str, Any],
-    trainer_kwargs: Dict[str, Any],
-) -> None:
-    """Train a policy with the parallelism provided by RL framework.
-
-    Experimental API. Parameters might change shortly.
-
-    Parameters
-    ----------
-    simulator_fn
-        Callable receiving initial seed, returning a simulator.
-    state_interpreter
-        Interprets the state of simulators.
-    action_interpreter
-        Interprets the policy actions.
-    initial_states
-        Initial states to iterate over. Every state will be run exactly once.
-    policy
-        Policy to train against.
-    reward
-        Reward function.
-    vessel_kwargs
-        Keyword arguments passed to :class:`TrainingVessel`, like ``episode_per_iter``.
-    trainer_kwargs
-        Keyword arguments passed to :class:`Trainer`, like ``finite_env_type``, ``concurrency``.
-    """
-
-    vessel = TrainingVessel(
-        simulator_fn=simulator_fn,
-        state_interpreter=state_interpreter,
-        action_interpreter=action_interpreter,
-        policy=policy,
-        train_initial_states=initial_states,
-        reward=reward,  # ignore none
-        **vessel_kwargs,
-    )
-    trainer = Trainer(**trainer_kwargs)
-    trainer.fit(vessel)
-
-
-def backtest(
-    simulator_fn: Callable[[InitialStateType], Simulator],
-    state_interpreter: StateInterpreter,
-    action_interpreter: ActionInterpreter,
-    initial_states: Sequence[InitialStateType],
-    policy: BasePolicy,
-    logger: LogWriter | List[LogWriter],
-    reward: Reward | None = None,
-    finite_env_type: FiniteEnvType = "subproc",
-    concurrency: int = 2,
-) -> None:
-    """Backtest with the parallelism provided by RL framework.
-
-    Experimental API. Parameters might change shortly.
-
-    Parameters
-    ----------
-    simulator_fn
-        Callable receiving initial seed, returning a simulator.
-    state_interpreter
-        Interprets the state of simulators.
-    action_interpreter
-        Interprets the policy actions.
-    initial_states
-        Initial states to iterate over. Every state will be run exactly once.
-    policy
-        Policy to test against.
-    logger
-        Logger to record the backtest results. Logger must be present because
-        without logger, all information will be lost.
-    reward
-        Optional reward function. For backtest, this is for testing the rewards
-        and logging them only.
-    finite_env_type
-        Type of finite env implementation.
-    concurrency
-        Parallel workers.
-    """
-
-    vessel = TrainingVessel(
-        simulator_fn=simulator_fn,
-        state_interpreter=state_interpreter,
-        action_interpreter=action_interpreter,
-        policy=policy,
-        test_initial_states=initial_states,
-        reward=cast(Reward, reward),  # ignore none
-    )
-    trainer = Trainer(
-        finite_env_type=finite_env_type,
-        concurrency=concurrency,
-        loggers=logger,
-    )
-    trainer.test(vessel)
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+from __future__ import annotations
+
+from typing import Any, Callable, Dict, List, Sequence, cast
+
+from tianshou.policy import BasePolicy
+
+from qlib.rl.interpreter import ActionInterpreter, StateInterpreter
+from qlib.rl.reward import Reward
+from qlib.rl.simulator import InitialStateType, Simulator
+from qlib.rl.utils import FiniteEnvType, LogWriter
+
+from .trainer import Trainer
+from .vessel import TrainingVessel
+
+
+def train(
+    simulator_fn: Callable[[InitialStateType], Simulator],
+    state_interpreter: StateInterpreter,
+    action_interpreter: ActionInterpreter,
+    initial_states: Sequence[InitialStateType],
+    policy: BasePolicy,
+    reward: Reward,
+    vessel_kwargs: Dict[str, Any],
+    trainer_kwargs: Dict[str, Any],
+) -> None:
+    """Train a policy with the parallelism provided by RL framework.
+
+    Experimental API. Parameters might change shortly.
+
+    Parameters
+    ----------
+    simulator_fn
+        Callable receiving initial seed, returning a simulator.
+    state_interpreter
+        Interprets the state of simulators.
+    action_interpreter
+        Interprets the policy actions.
+    initial_states
+        Initial states to iterate over. Every state will be run exactly once.
+    policy
+        Policy to train against.
+    reward
+        Reward function.
+    vessel_kwargs
+        Keyword arguments passed to :class:`TrainingVessel`, like ``episode_per_iter``.
+    trainer_kwargs
+        Keyword arguments passed to :class:`Trainer`, like ``finite_env_type``, ``concurrency``.
+    """
+
+    vessel = TrainingVessel(
+        simulator_fn=simulator_fn,
+        state_interpreter=state_interpreter,
+        action_interpreter=action_interpreter,
+        policy=policy,
+        train_initial_states=initial_states,
+        reward=reward,  # ignore none
+        **vessel_kwargs,
+    )
+    trainer = Trainer(**trainer_kwargs)
+    trainer.fit(vessel)
+
+
+def backtest(
+    simulator_fn: Callable[[InitialStateType], Simulator],
+    state_interpreter: StateInterpreter,
+    action_interpreter: ActionInterpreter,
+    initial_states: Sequence[InitialStateType],
+    policy: BasePolicy,
+    logger: LogWriter | List[LogWriter],
+    reward: Reward | None = None,
+    finite_env_type: FiniteEnvType = "subproc",
+    concurrency: int = 2,
+) -> None:
+    """Backtest with the parallelism provided by RL framework.
+
+    Experimental API. Parameters might change shortly.
+
+    Parameters
+    ----------
+    simulator_fn
+        Callable receiving initial seed, returning a simulator.
+    state_interpreter
+        Interprets the state of simulators.
+    action_interpreter
+        Interprets the policy actions.
+    initial_states
+        Initial states to iterate over. Every state will be run exactly once.
+    policy
+        Policy to test against.
+    logger
+        Logger to record the backtest results. Logger must be present because
+        without logger, all information will be lost.
+    reward
+        Optional reward function. For backtest, this is for testing the rewards
+        and logging them only.
+    finite_env_type
+        Type of finite env implementation.
+    concurrency
+        Parallel workers.
+    """
+
+    vessel = TrainingVessel(
+        simulator_fn=simulator_fn,
+        state_interpreter=state_interpreter,
+        action_interpreter=action_interpreter,
+        policy=policy,
+        test_initial_states=initial_states,
+        reward=cast(Reward, reward),  # ignore none
+    )
+    trainer = Trainer(
+        finite_env_type=finite_env_type,
+        concurrency=concurrency,
+        loggers=logger,
+    )
+    trainer.test(vessel)
```

## qlib/rl/trainer/callbacks.py

 * *Ordering differences only*

```diff
@@ -1,291 +1,291 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-"""Callbacks to insert customized recipes during the training.
-Mimicks the hooks of Keras / PyTorch-Lightning, but tailored for the context of RL.
-"""
-
-from __future__ import annotations
-
-import copy
-import os
-import shutil
-import time
-from datetime import datetime
-from pathlib import Path
-from typing import Any, List, TYPE_CHECKING
-
-import numpy as np
-import pandas as pd
-import torch
-
-from qlib.log import get_module_logger
-from qlib.typehint import Literal
-
-if TYPE_CHECKING:
-    from .trainer import Trainer
-    from .vessel import TrainingVesselBase
-
-_logger = get_module_logger(__name__)
-
-
-class Callback:
-    """Base class of all callbacks."""
-
-    def on_fit_start(self, trainer: Trainer, vessel: TrainingVesselBase) -> None:
-        """Called before the whole fit process begins."""
-
-    def on_fit_end(self, trainer: Trainer, vessel: TrainingVesselBase) -> None:
-        """Called after the whole fit process ends."""
-
-    def on_train_start(self, trainer: Trainer, vessel: TrainingVesselBase) -> None:
-        """Called when each collect for training begins."""
-
-    def on_train_end(self, trainer: Trainer, vessel: TrainingVesselBase) -> None:
-        """Called when the training ends.
-        To access all outputs produced during training, cache the data in either trainer and vessel,
-        and post-process them in this hook.
-        """
-
-    def on_validate_start(self, trainer: Trainer, vessel: TrainingVesselBase) -> None:
-        """Called when every run for validation begins."""
-
-    def on_validate_end(self, trainer: Trainer, vessel: TrainingVesselBase) -> None:
-        """Called when the validation ends."""
-
-    def on_test_start(self, trainer: Trainer, vessel: TrainingVesselBase) -> None:
-        """Called when every run of testing begins."""
-
-    def on_test_end(self, trainer: Trainer, vessel: TrainingVesselBase) -> None:
-        """Called when the testing ends."""
-
-    def on_iter_start(self, trainer: Trainer, vessel: TrainingVesselBase) -> None:
-        """Called when every iteration (i.e., collect) starts."""
-
-    def on_iter_end(self, trainer: Trainer, vessel: TrainingVesselBase) -> None:
-        """Called upon every end of iteration.
-        This is called **after** the bump of ``current_iter``,
-        when the previous iteration is considered complete.
-        """
-
-    def state_dict(self) -> Any:
-        """Get a state dict of the callback for pause and resume."""
-
-    def load_state_dict(self, state_dict: Any) -> None:
-        """Resume the callback from a saved state dict."""
-
-
-class EarlyStopping(Callback):
-    """Stop training when a monitored metric has stopped improving.
-
-    The earlystopping callback will be triggered each time validation ends.
-    It will examine the metrics produced in validation,
-    and get the metric with name ``monitor` (``monitor`` is ``reward`` by default),
-    to check whether it's no longer increasing / decreasing.
-    It takes ``min_delta`` and ``patience`` if applicable.
-    If it's found to be not increasing / decreasing any more.
-    ``trainer.should_stop`` will be set to true,
-    and the training terminates.
-
-    Implementation reference: https://github.com/keras-team/keras/blob/v2.9.0/keras/callbacks.py#L1744-L1893
-    """
-
-    def __init__(
-        self,
-        monitor: str = "reward",
-        min_delta: float = 0.0,
-        patience: int = 0,
-        mode: Literal["min", "max"] = "max",
-        baseline: float | None = None,
-        restore_best_weights: bool = False,
-    ):
-        super().__init__()
-
-        self.monitor = monitor
-        self.patience = patience
-        self.baseline = baseline
-        self.min_delta = abs(min_delta)
-        self.restore_best_weights = restore_best_weights
-        self.best_weights: Any | None = None
-
-        if mode not in ["min", "max"]:
-            raise ValueError("Unsupported earlystopping mode: " + mode)
-
-        if mode == "min":
-            self.monitor_op = np.less
-        elif mode == "max":
-            self.monitor_op = np.greater
-
-        if self.monitor_op == np.greater:
-            self.min_delta *= 1
-        else:
-            self.min_delta *= -1
-
-    def state_dict(self) -> dict:
-        return {"wait": self.wait, "best": self.best, "best_weights": self.best_weights, "best_iter": self.best_iter}
-
-    def load_state_dict(self, state_dict: dict) -> None:
-        self.wait = state_dict["wait"]
-        self.best = state_dict["best"]
-        self.best_weights = state_dict["best_weights"]
-        self.best_iter = state_dict["best_iter"]
-
-    def on_fit_start(self, trainer: Trainer, vessel: TrainingVesselBase) -> None:
-        # Allow instances to be re-used
-        self.wait = 0
-        self.best = np.inf if self.monitor_op == np.less else -np.inf
-        self.best_weights = None
-        self.best_iter = 0
-
-    def on_validate_end(self, trainer: Trainer, vessel: TrainingVesselBase) -> None:
-        current = self.get_monitor_value(trainer)
-        if current is None:
-            return
-        if self.restore_best_weights and self.best_weights is None:
-            # Restore the weights after first iteration if no progress is ever made.
-            self.best_weights = copy.deepcopy(vessel.state_dict())
-
-        self.wait += 1
-        if self._is_improvement(current, self.best):
-            self.best = current
-            self.best_iter = trainer.current_iter
-            if self.restore_best_weights:
-                self.best_weights = copy.deepcopy(vessel.state_dict())
-            # Only restart wait if we beat both the baseline and our previous best.
-            if self.baseline is None or self._is_improvement(current, self.baseline):
-                self.wait = 0
-
-        msg = (
-            f"#{trainer.current_iter} current reward: {current:.4f}, best reward: {self.best:.4f} in #{self.best_iter}"
-        )
-        _logger.info(msg)
-
-        # Only check after the first epoch.
-        if self.wait >= self.patience and trainer.current_iter > 0:
-            trainer.should_stop = True
-            _logger.info(f"On iteration %d: early stopping", trainer.current_iter + 1)
-            if self.restore_best_weights and self.best_weights is not None:
-                _logger.info("Restoring model weights from the end of the best iteration: %d", self.best_iter + 1)
-                vessel.load_state_dict(self.best_weights)
-
-    def get_monitor_value(self, trainer: Trainer) -> Any:
-        monitor_value = trainer.metrics.get(self.monitor)
-        if monitor_value is None:
-            _logger.warning(
-                "Early stopping conditioned on metric `%s` which is not available. Available metrics are: %s",
-                self.monitor,
-                ",".join(list(trainer.metrics.keys())),
-            )
-        return monitor_value
-
-    def _is_improvement(self, monitor_value, reference_value):
-        return self.monitor_op(monitor_value - self.min_delta, reference_value)
-
-
-class MetricsWriter(Callback):
-    """Dump training metrics to file."""
-
-    def __init__(self, dirpath: Path) -> None:
-        self.dirpath = dirpath
-        self.dirpath.mkdir(exist_ok=True, parents=True)
-        self.train_records: List[dict] = []
-        self.valid_records: List[dict] = []
-
-    def on_train_end(self, trainer: Trainer, vessel: TrainingVesselBase) -> None:
-        self.train_records.append({k: v for k, v in trainer.metrics.items() if not k.startswith("val/")})
-        pd.DataFrame.from_records(self.train_records).to_csv(self.dirpath / "train_result.csv", index=True)
-
-    def on_validate_end(self, trainer: Trainer, vessel: TrainingVesselBase) -> None:
-        self.valid_records.append({k: v for k, v in trainer.metrics.items() if k.startswith("val/")})
-        pd.DataFrame.from_records(self.valid_records).to_csv(self.dirpath / "validation_result.csv", index=True)
-
-
-class Checkpoint(Callback):
-    """Save checkpoints periodically for persistence and recovery.
-
-    Reference: https://github.com/PyTorchLightning/pytorch-lightning/blob/bfa8b7be/pytorch_lightning/callbacks/model_checkpoint.py
-
-    Parameters
-    ----------
-    dirpath
-        Directory to save the checkpoint file.
-    filename
-        Checkpoint filename. Can contain named formatting options to be auto-filled.
-        For example: ``{iter:03d}-{reward:.2f}.pth``.
-        Supported argument names are:
-
-        - iter (int)
-        - metrics in ``trainer.metrics``
-        - time string, in the format of ``%Y%m%d%H%M%S``
-    save_latest
-        Save the latest checkpoint in ``latest.pth``.
-        If ``link``, ``latest.pth`` will be created as a softlink.
-        If ``copy``, ``latest.pth`` will be stored as an individual copy.
-        Set to none to disable this.
-    every_n_iters
-        Checkpoints are saved at the end of every n iterations of training,
-        after validation if applicable.
-    time_interval
-        Maximum time (seconds) before checkpoints save again.
-    save_on_fit_end
-        Save one last checkpoint at the end to fit.
-        Do nothing if a checkpoint is already saved there.
-    """
-
-    def __init__(
-        self,
-        dirpath: Path,
-        filename: str = "{iter:03d}.pth",
-        save_latest: Literal["link", "copy"] | None = "link",
-        every_n_iters: int | None = None,
-        time_interval: int | None = None,
-        save_on_fit_end: bool = True,
-    ):
-        self.dirpath = Path(dirpath)
-        self.filename = filename
-        self.save_latest = save_latest
-        self.every_n_iters = every_n_iters
-        self.time_interval = time_interval
-        self.save_on_fit_end = save_on_fit_end
-
-        self._last_checkpoint_name: str | None = None
-        self._last_checkpoint_iter: int | None = None
-        self._last_checkpoint_time: float | None = None
-
-    def on_fit_end(self, trainer: Trainer, vessel: TrainingVesselBase) -> None:
-        if self.save_on_fit_end and (trainer.current_iter != self._last_checkpoint_iter):
-            self._save_checkpoint(trainer)
-
-    def on_iter_end(self, trainer: Trainer, vessel: TrainingVesselBase) -> None:
-        should_save_ckpt = False
-        if self.every_n_iters is not None and (trainer.current_iter + 1) % self.every_n_iters == 0:
-            should_save_ckpt = True
-        if self.time_interval is not None and (
-            self._last_checkpoint_time is None or (time.time() - self._last_checkpoint_time) >= self.time_interval
-        ):
-            should_save_ckpt = True
-        if should_save_ckpt:
-            self._save_checkpoint(trainer)
-
-    def _save_checkpoint(self, trainer: Trainer) -> None:
-        self.dirpath.mkdir(exist_ok=True, parents=True)
-        self._last_checkpoint_name = self._new_checkpoint_name(trainer)
-        self._last_checkpoint_iter = trainer.current_iter
-        self._last_checkpoint_time = time.time()
-        torch.save(trainer.state_dict(), self.dirpath / self._last_checkpoint_name)
-
-        latest_pth = self.dirpath / "latest.pth"
-
-        # Remove first before saving
-        if self.save_latest and (latest_pth.exists() or os.path.islink(latest_pth)):
-            latest_pth.unlink()
-
-        if self.save_latest == "link":
-            latest_pth.symlink_to(self.dirpath / self._last_checkpoint_name)
-        elif self.save_latest == "copy":
-            shutil.copyfile(self.dirpath / self._last_checkpoint_name, latest_pth)
-
-    def _new_checkpoint_name(self, trainer: Trainer) -> str:
-        return self.filename.format(
-            iter=trainer.current_iter, time=datetime.now().strftime("%Y%m%d%H%M%S"), **trainer.metrics
-        )
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+"""Callbacks to insert customized recipes during the training.
+Mimicks the hooks of Keras / PyTorch-Lightning, but tailored for the context of RL.
+"""
+
+from __future__ import annotations
+
+import copy
+import os
+import shutil
+import time
+from datetime import datetime
+from pathlib import Path
+from typing import Any, List, TYPE_CHECKING
+
+import numpy as np
+import pandas as pd
+import torch
+
+from qlib.log import get_module_logger
+from qlib.typehint import Literal
+
+if TYPE_CHECKING:
+    from .trainer import Trainer
+    from .vessel import TrainingVesselBase
+
+_logger = get_module_logger(__name__)
+
+
+class Callback:
+    """Base class of all callbacks."""
+
+    def on_fit_start(self, trainer: Trainer, vessel: TrainingVesselBase) -> None:
+        """Called before the whole fit process begins."""
+
+    def on_fit_end(self, trainer: Trainer, vessel: TrainingVesselBase) -> None:
+        """Called after the whole fit process ends."""
+
+    def on_train_start(self, trainer: Trainer, vessel: TrainingVesselBase) -> None:
+        """Called when each collect for training begins."""
+
+    def on_train_end(self, trainer: Trainer, vessel: TrainingVesselBase) -> None:
+        """Called when the training ends.
+        To access all outputs produced during training, cache the data in either trainer and vessel,
+        and post-process them in this hook.
+        """
+
+    def on_validate_start(self, trainer: Trainer, vessel: TrainingVesselBase) -> None:
+        """Called when every run for validation begins."""
+
+    def on_validate_end(self, trainer: Trainer, vessel: TrainingVesselBase) -> None:
+        """Called when the validation ends."""
+
+    def on_test_start(self, trainer: Trainer, vessel: TrainingVesselBase) -> None:
+        """Called when every run of testing begins."""
+
+    def on_test_end(self, trainer: Trainer, vessel: TrainingVesselBase) -> None:
+        """Called when the testing ends."""
+
+    def on_iter_start(self, trainer: Trainer, vessel: TrainingVesselBase) -> None:
+        """Called when every iteration (i.e., collect) starts."""
+
+    def on_iter_end(self, trainer: Trainer, vessel: TrainingVesselBase) -> None:
+        """Called upon every end of iteration.
+        This is called **after** the bump of ``current_iter``,
+        when the previous iteration is considered complete.
+        """
+
+    def state_dict(self) -> Any:
+        """Get a state dict of the callback for pause and resume."""
+
+    def load_state_dict(self, state_dict: Any) -> None:
+        """Resume the callback from a saved state dict."""
+
+
+class EarlyStopping(Callback):
+    """Stop training when a monitored metric has stopped improving.
+
+    The earlystopping callback will be triggered each time validation ends.
+    It will examine the metrics produced in validation,
+    and get the metric with name ``monitor` (``monitor`` is ``reward`` by default),
+    to check whether it's no longer increasing / decreasing.
+    It takes ``min_delta`` and ``patience`` if applicable.
+    If it's found to be not increasing / decreasing any more.
+    ``trainer.should_stop`` will be set to true,
+    and the training terminates.
+
+    Implementation reference: https://github.com/keras-team/keras/blob/v2.9.0/keras/callbacks.py#L1744-L1893
+    """
+
+    def __init__(
+        self,
+        monitor: str = "reward",
+        min_delta: float = 0.0,
+        patience: int = 0,
+        mode: Literal["min", "max"] = "max",
+        baseline: float | None = None,
+        restore_best_weights: bool = False,
+    ):
+        super().__init__()
+
+        self.monitor = monitor
+        self.patience = patience
+        self.baseline = baseline
+        self.min_delta = abs(min_delta)
+        self.restore_best_weights = restore_best_weights
+        self.best_weights: Any | None = None
+
+        if mode not in ["min", "max"]:
+            raise ValueError("Unsupported earlystopping mode: " + mode)
+
+        if mode == "min":
+            self.monitor_op = np.less
+        elif mode == "max":
+            self.monitor_op = np.greater
+
+        if self.monitor_op == np.greater:
+            self.min_delta *= 1
+        else:
+            self.min_delta *= -1
+
+    def state_dict(self) -> dict:
+        return {"wait": self.wait, "best": self.best, "best_weights": self.best_weights, "best_iter": self.best_iter}
+
+    def load_state_dict(self, state_dict: dict) -> None:
+        self.wait = state_dict["wait"]
+        self.best = state_dict["best"]
+        self.best_weights = state_dict["best_weights"]
+        self.best_iter = state_dict["best_iter"]
+
+    def on_fit_start(self, trainer: Trainer, vessel: TrainingVesselBase) -> None:
+        # Allow instances to be re-used
+        self.wait = 0
+        self.best = np.inf if self.monitor_op == np.less else -np.inf
+        self.best_weights = None
+        self.best_iter = 0
+
+    def on_validate_end(self, trainer: Trainer, vessel: TrainingVesselBase) -> None:
+        current = self.get_monitor_value(trainer)
+        if current is None:
+            return
+        if self.restore_best_weights and self.best_weights is None:
+            # Restore the weights after first iteration if no progress is ever made.
+            self.best_weights = copy.deepcopy(vessel.state_dict())
+
+        self.wait += 1
+        if self._is_improvement(current, self.best):
+            self.best = current
+            self.best_iter = trainer.current_iter
+            if self.restore_best_weights:
+                self.best_weights = copy.deepcopy(vessel.state_dict())
+            # Only restart wait if we beat both the baseline and our previous best.
+            if self.baseline is None or self._is_improvement(current, self.baseline):
+                self.wait = 0
+
+        msg = (
+            f"#{trainer.current_iter} current reward: {current:.4f}, best reward: {self.best:.4f} in #{self.best_iter}"
+        )
+        _logger.info(msg)
+
+        # Only check after the first epoch.
+        if self.wait >= self.patience and trainer.current_iter > 0:
+            trainer.should_stop = True
+            _logger.info(f"On iteration %d: early stopping", trainer.current_iter + 1)
+            if self.restore_best_weights and self.best_weights is not None:
+                _logger.info("Restoring model weights from the end of the best iteration: %d", self.best_iter + 1)
+                vessel.load_state_dict(self.best_weights)
+
+    def get_monitor_value(self, trainer: Trainer) -> Any:
+        monitor_value = trainer.metrics.get(self.monitor)
+        if monitor_value is None:
+            _logger.warning(
+                "Early stopping conditioned on metric `%s` which is not available. Available metrics are: %s",
+                self.monitor,
+                ",".join(list(trainer.metrics.keys())),
+            )
+        return monitor_value
+
+    def _is_improvement(self, monitor_value, reference_value):
+        return self.monitor_op(monitor_value - self.min_delta, reference_value)
+
+
+class MetricsWriter(Callback):
+    """Dump training metrics to file."""
+
+    def __init__(self, dirpath: Path) -> None:
+        self.dirpath = dirpath
+        self.dirpath.mkdir(exist_ok=True, parents=True)
+        self.train_records: List[dict] = []
+        self.valid_records: List[dict] = []
+
+    def on_train_end(self, trainer: Trainer, vessel: TrainingVesselBase) -> None:
+        self.train_records.append({k: v for k, v in trainer.metrics.items() if not k.startswith("val/")})
+        pd.DataFrame.from_records(self.train_records).to_csv(self.dirpath / "train_result.csv", index=True)
+
+    def on_validate_end(self, trainer: Trainer, vessel: TrainingVesselBase) -> None:
+        self.valid_records.append({k: v for k, v in trainer.metrics.items() if k.startswith("val/")})
+        pd.DataFrame.from_records(self.valid_records).to_csv(self.dirpath / "validation_result.csv", index=True)
+
+
+class Checkpoint(Callback):
+    """Save checkpoints periodically for persistence and recovery.
+
+    Reference: https://github.com/PyTorchLightning/pytorch-lightning/blob/bfa8b7be/pytorch_lightning/callbacks/model_checkpoint.py
+
+    Parameters
+    ----------
+    dirpath
+        Directory to save the checkpoint file.
+    filename
+        Checkpoint filename. Can contain named formatting options to be auto-filled.
+        For example: ``{iter:03d}-{reward:.2f}.pth``.
+        Supported argument names are:
+
+        - iter (int)
+        - metrics in ``trainer.metrics``
+        - time string, in the format of ``%Y%m%d%H%M%S``
+    save_latest
+        Save the latest checkpoint in ``latest.pth``.
+        If ``link``, ``latest.pth`` will be created as a softlink.
+        If ``copy``, ``latest.pth`` will be stored as an individual copy.
+        Set to none to disable this.
+    every_n_iters
+        Checkpoints are saved at the end of every n iterations of training,
+        after validation if applicable.
+    time_interval
+        Maximum time (seconds) before checkpoints save again.
+    save_on_fit_end
+        Save one last checkpoint at the end to fit.
+        Do nothing if a checkpoint is already saved there.
+    """
+
+    def __init__(
+        self,
+        dirpath: Path,
+        filename: str = "{iter:03d}.pth",
+        save_latest: Literal["link", "copy"] | None = "link",
+        every_n_iters: int | None = None,
+        time_interval: int | None = None,
+        save_on_fit_end: bool = True,
+    ):
+        self.dirpath = Path(dirpath)
+        self.filename = filename
+        self.save_latest = save_latest
+        self.every_n_iters = every_n_iters
+        self.time_interval = time_interval
+        self.save_on_fit_end = save_on_fit_end
+
+        self._last_checkpoint_name: str | None = None
+        self._last_checkpoint_iter: int | None = None
+        self._last_checkpoint_time: float | None = None
+
+    def on_fit_end(self, trainer: Trainer, vessel: TrainingVesselBase) -> None:
+        if self.save_on_fit_end and (trainer.current_iter != self._last_checkpoint_iter):
+            self._save_checkpoint(trainer)
+
+    def on_iter_end(self, trainer: Trainer, vessel: TrainingVesselBase) -> None:
+        should_save_ckpt = False
+        if self.every_n_iters is not None and (trainer.current_iter + 1) % self.every_n_iters == 0:
+            should_save_ckpt = True
+        if self.time_interval is not None and (
+            self._last_checkpoint_time is None or (time.time() - self._last_checkpoint_time) >= self.time_interval
+        ):
+            should_save_ckpt = True
+        if should_save_ckpt:
+            self._save_checkpoint(trainer)
+
+    def _save_checkpoint(self, trainer: Trainer) -> None:
+        self.dirpath.mkdir(exist_ok=True, parents=True)
+        self._last_checkpoint_name = self._new_checkpoint_name(trainer)
+        self._last_checkpoint_iter = trainer.current_iter
+        self._last_checkpoint_time = time.time()
+        torch.save(trainer.state_dict(), self.dirpath / self._last_checkpoint_name)
+
+        latest_pth = self.dirpath / "latest.pth"
+
+        # Remove first before saving
+        if self.save_latest and (latest_pth.exists() or os.path.islink(latest_pth)):
+            latest_pth.unlink()
+
+        if self.save_latest == "link":
+            latest_pth.symlink_to(self.dirpath / self._last_checkpoint_name)
+        elif self.save_latest == "copy":
+            shutil.copyfile(self.dirpath / self._last_checkpoint_name, latest_pth)
+
+    def _new_checkpoint_name(self, trainer: Trainer) -> str:
+        return self.filename.format(
+            iter=trainer.current_iter, time=datetime.now().strftime("%Y%m%d%H%M%S"), **trainer.metrics
+        )
```

## qlib/rl/trainer/trainer.py

 * *Ordering differences only*

```diff
@@ -1,355 +1,355 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-from __future__ import annotations
-
-import collections
-import copy
-from contextlib import AbstractContextManager, contextmanager
-from datetime import datetime
-from pathlib import Path
-from typing import Any, Dict, Iterable, List, OrderedDict, Sequence, TypeVar, cast
-
-import torch
-
-from qlib.log import get_module_logger
-from qlib.rl.simulator import InitialStateType
-from qlib.rl.utils import EnvWrapper, FiniteEnvType, LogBuffer, LogCollector, LogLevel, LogWriter, vectorize_env
-from qlib.rl.utils.finite_env import FiniteVectorEnv
-from qlib.typehint import Literal
-
-from .callbacks import Callback
-from .vessel import TrainingVesselBase
-
-_logger = get_module_logger(__name__)
-
-
-T = TypeVar("T")
-
-
-class Trainer:
-    """
-    Utility to train a policy on a particular task.
-
-    Different from traditional DL trainer, the iteration of this trainer is "collect",
-    rather than "epoch", or "mini-batch".
-    In each collect, :class:`Collector` collects a number of policy-env interactions, and accumulates
-    them into a replay buffer. This buffer is used as the "data" to train the policy.
-    At the end of each collect, the policy is *updated* several times.
-
-    The API has some resemblence with `PyTorch Lightning <https://pytorch-lightning.readthedocs.io/>`__,
-    but it's essentially different because this trainer is built for RL applications, and thus
-    most configurations are under RL context.
-    We are still looking for ways to incorporate existing trainer libraries, because it looks like
-    big efforts to build a trainer as powerful as those libraries, and also, that's not our primary goal.
-
-    It's essentially different
-    `tianshou's built-in trainers <https://tianshou.readthedocs.io/en/master/api/tianshou.trainer.html>`__,
-    as it's far much more complicated than that.
-
-    Parameters
-    ----------
-    max_iters
-        Maximum iterations before stopping.
-    val_every_n_iters
-        Perform validation every n iterations (i.e., training collects).
-    logger
-        Logger to record the backtest results. Logger must be present because
-        without logger, all information will be lost.
-    finite_env_type
-        Type of finite env implementation.
-    concurrency
-        Parallel workers.
-    fast_dev_run
-        Create a subset for debugging.
-        How this is implemented depends on the implementation of training vessel.
-        For :class:`~qlib.rl.vessel.TrainingVessel`, if greater than zero,
-        a random subset sized ``fast_dev_run`` will be used
-        instead of ``train_initial_states`` and ``val_initial_states``.
-    """
-
-    should_stop: bool
-    """Set to stop the training."""
-
-    metrics: dict
-    """Numeric metrics of produced in train/val/test.
-    In the middle of training / validation, metrics will be of the latest episode.
-    When each iteration of training / validation finishes, metrics will be the aggregation
-    of all episodes encountered in this iteration.
-
-    Cleared on every new iteration of training.
-
-    In fit, validation metrics will be prefixed with ``val/``.
-    """
-
-    current_iter: int
-    """Current iteration (collect) of training."""
-
-    loggers: List[LogWriter]
-    """A list of log writers."""
-
-    def __init__(
-        self,
-        *,
-        max_iters: int | None = None,
-        val_every_n_iters: int | None = None,
-        loggers: LogWriter | List[LogWriter] | None = None,
-        callbacks: List[Callback] | None = None,
-        finite_env_type: FiniteEnvType = "subproc",
-        concurrency: int = 2,
-        fast_dev_run: int | None = None,
-    ):
-        self.max_iters = max_iters
-        self.val_every_n_iters = val_every_n_iters
-
-        if isinstance(loggers, list):
-            self.loggers = loggers
-        elif isinstance(loggers, LogWriter):
-            self.loggers = [loggers]
-        else:
-            self.loggers = []
-
-        self.loggers.append(LogBuffer(self._metrics_callback, loglevel=self._min_loglevel()))
-
-        self.callbacks: List[Callback] = callbacks if callbacks is not None else []
-        self.finite_env_type = finite_env_type
-        self.concurrency = concurrency
-        self.fast_dev_run = fast_dev_run
-
-        self.current_stage: Literal["train", "val", "test"] = "train"
-
-        self.vessel: TrainingVesselBase = cast(TrainingVesselBase, None)
-
-    def initialize(self):
-        """Initialize the whole training process.
-
-        The states here should be synchronized with state_dict.
-        """
-        self.should_stop = False
-        self.current_iter = 0
-        self.current_episode = 0
-        self.current_stage = "train"
-
-    def initialize_iter(self):
-        """Initialize one iteration / collect."""
-        self.metrics = {}
-
-    def state_dict(self) -> dict:
-        """Putting every states of current training into a dict, at best effort.
-
-        It doesn't try to handle all the possible kinds of states in the middle of one training collect.
-        For most cases at the end of each iteration, things should be usually correct.
-
-        Note that it's also intended behavior that replay buffer data in the collector will be lost.
-        """
-        return {
-            "vessel": self.vessel.state_dict(),
-            "callbacks": {name: callback.state_dict() for name, callback in self.named_callbacks().items()},
-            "loggers": {name: logger.state_dict() for name, logger in self.named_loggers().items()},
-            "should_stop": self.should_stop,
-            "current_iter": self.current_iter,
-            "current_episode": self.current_episode,
-            "current_stage": self.current_stage,
-            "metrics": self.metrics,
-        }
-
-    @staticmethod
-    def get_policy_state_dict(ckpt_path: Path) -> OrderedDict:
-        state_dict = torch.load(ckpt_path, map_location="cpu")
-        if "vessel" in state_dict:
-            state_dict = state_dict["vessel"]["policy"]
-        return state_dict
-
-    def load_state_dict(self, state_dict: dict) -> None:
-        """Load all states into current trainer."""
-        self.vessel.load_state_dict(state_dict["vessel"])
-        for name, callback in self.named_callbacks().items():
-            callback.load_state_dict(state_dict["callbacks"][name])
-        for name, logger in self.named_loggers().items():
-            logger.load_state_dict(state_dict["loggers"][name])
-        self.should_stop = state_dict["should_stop"]
-        self.current_iter = state_dict["current_iter"]
-        self.current_episode = state_dict["current_episode"]
-        self.current_stage = state_dict["current_stage"]
-        self.metrics = state_dict["metrics"]
-
-    def named_callbacks(self) -> Dict[str, Callback]:
-        """Retrieve a collection of callbacks where each one has a name.
-        Useful when saving checkpoints.
-        """
-        return _named_collection(self.callbacks)
-
-    def named_loggers(self) -> Dict[str, LogWriter]:
-        """Retrieve a collection of loggers where each one has a name.
-        Useful when saving checkpoints.
-        """
-        return _named_collection(self.loggers)
-
-    def fit(self, vessel: TrainingVesselBase, ckpt_path: Path | None = None) -> None:
-        """Train the RL policy upon the defined simulator.
-
-        Parameters
-        ----------
-        vessel
-            A bundle of all elements used in training.
-        ckpt_path
-            Load a pre-trained / paused training checkpoint.
-        """
-        self.vessel = vessel
-        vessel.assign_trainer(self)
-
-        if ckpt_path is not None:
-            _logger.info("Resuming states from %s", str(ckpt_path))
-            self.load_state_dict(torch.load(ckpt_path))
-        else:
-            self.initialize()
-
-        self._call_callback_hooks("on_fit_start")
-
-        while not self.should_stop:
-            msg = f"\n{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\tTrain iteration {self.current_iter + 1}/{self.max_iters}"
-            _logger.info(msg)
-
-            self.initialize_iter()
-
-            self._call_callback_hooks("on_iter_start")
-
-            self.current_stage = "train"
-            self._call_callback_hooks("on_train_start")
-
-            # TODO
-            # Add a feature that supports reloading the training environment every few iterations.
-            with _wrap_context(vessel.train_seed_iterator()) as iterator:
-                vector_env = self.venv_from_iterator(iterator)
-                self.vessel.train(vector_env)
-                del vector_env  # FIXME: Explicitly delete this object to avoid memory leak.
-
-            self._call_callback_hooks("on_train_end")
-
-            if self.val_every_n_iters is not None and (self.current_iter + 1) % self.val_every_n_iters == 0:
-                # Implementation of validation loop
-                self.current_stage = "val"
-                self._call_callback_hooks("on_validate_start")
-                with _wrap_context(vessel.val_seed_iterator()) as iterator:
-                    vector_env = self.venv_from_iterator(iterator)
-                    self.vessel.validate(vector_env)
-                    del vector_env  # FIXME: Explicitly delete this object to avoid memory leak.
-
-                self._call_callback_hooks("on_validate_end")
-
-            # This iteration is considered complete.
-            # Bumping the current iteration counter.
-            self.current_iter += 1
-
-            if self.max_iters is not None and self.current_iter >= self.max_iters:
-                self.should_stop = True
-
-            self._call_callback_hooks("on_iter_end")
-
-        self._call_callback_hooks("on_fit_end")
-
-    def test(self, vessel: TrainingVesselBase) -> None:
-        """Test the RL policy against the simulator.
-
-        The simulator will be fed with data generated in ``test_seed_iterator``.
-
-        Parameters
-        ----------
-        vessel
-            A bundle of all related elements.
-        """
-        self.vessel = vessel
-        vessel.assign_trainer(self)
-
-        self.initialize_iter()
-
-        self.current_stage = "test"
-        self._call_callback_hooks("on_test_start")
-        with _wrap_context(vessel.test_seed_iterator()) as iterator:
-            vector_env = self.venv_from_iterator(iterator)
-            self.vessel.test(vector_env)
-            del vector_env  # FIXME: Explicitly delete this object to avoid memory leak.
-        self._call_callback_hooks("on_test_end")
-
-    def venv_from_iterator(self, iterator: Iterable[InitialStateType]) -> FiniteVectorEnv:
-        """Create a vectorized environment from iterator and the training vessel."""
-
-        def env_factory():
-            # FIXME: state_interpreter and action_interpreter are stateful (having a weakref of env),
-            # and could be thread unsafe.
-            # I'm not sure whether it's a design flaw.
-            # I'll rethink about this when designing the trainer.
-
-            if self.finite_env_type == "dummy":
-                # We could only experience the "threading-unsafe" problem in dummy.
-                state = copy.deepcopy(self.vessel.state_interpreter)
-                action = copy.deepcopy(self.vessel.action_interpreter)
-                rew = copy.deepcopy(self.vessel.reward)
-            else:
-                state = self.vessel.state_interpreter
-                action = self.vessel.action_interpreter
-                rew = self.vessel.reward
-
-            return EnvWrapper(
-                self.vessel.simulator_fn,
-                state,
-                action,
-                iterator,
-                rew,
-                logger=LogCollector(min_loglevel=self._min_loglevel()),
-            )
-
-        return vectorize_env(
-            env_factory,
-            self.finite_env_type,
-            self.concurrency,
-            self.loggers,
-        )
-
-    def _metrics_callback(self, on_episode: bool, on_collect: bool, log_buffer: LogBuffer) -> None:
-        if on_episode:
-            # Update the global counter.
-            self.current_episode = log_buffer.global_episode
-            metrics = log_buffer.episode_metrics()
-        elif on_collect:
-            # Update the latest metrics.
-            metrics = log_buffer.collect_metrics()
-        if self.current_stage == "val":
-            metrics = {"val/" + name: value for name, value in metrics.items()}
-        self.metrics.update(metrics)
-
-    def _call_callback_hooks(self, hook_name: str, *args: Any, **kwargs: Any) -> None:
-        for callback in self.callbacks:
-            fn = getattr(callback, hook_name)
-            fn(self, self.vessel, *args, **kwargs)
-
-    def _min_loglevel(self):
-        if not self.loggers:
-            return LogLevel.PERIODIC
-        else:
-            # To save bandwidth
-            return min(lg.loglevel for lg in self.loggers)
-
-
-@contextmanager
-def _wrap_context(obj):
-    """Make any object a (possibly dummy) context manager."""
-
-    if isinstance(obj, AbstractContextManager):
-        # obj has __enter__ and __exit__
-        with obj as ctx:
-            yield ctx
-    else:
-        yield obj
-
-
-def _named_collection(seq: Sequence[T]) -> Dict[str, T]:
-    """Convert a list into a dict, where each item is named with its type."""
-    res = {}
-    retry_cnt: collections.Counter = collections.Counter()
-    for item in seq:
-        typename = type(item).__name__.lower()
-        key = typename if retry_cnt[typename] == 0 else f"{typename}{retry_cnt[typename]}"
-        retry_cnt[typename] += 1
-        res[key] = item
-    return res
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+from __future__ import annotations
+
+import collections
+import copy
+from contextlib import AbstractContextManager, contextmanager
+from datetime import datetime
+from pathlib import Path
+from typing import Any, Dict, Iterable, List, OrderedDict, Sequence, TypeVar, cast
+
+import torch
+
+from qlib.log import get_module_logger
+from qlib.rl.simulator import InitialStateType
+from qlib.rl.utils import EnvWrapper, FiniteEnvType, LogBuffer, LogCollector, LogLevel, LogWriter, vectorize_env
+from qlib.rl.utils.finite_env import FiniteVectorEnv
+from qlib.typehint import Literal
+
+from .callbacks import Callback
+from .vessel import TrainingVesselBase
+
+_logger = get_module_logger(__name__)
+
+
+T = TypeVar("T")
+
+
+class Trainer:
+    """
+    Utility to train a policy on a particular task.
+
+    Different from traditional DL trainer, the iteration of this trainer is "collect",
+    rather than "epoch", or "mini-batch".
+    In each collect, :class:`Collector` collects a number of policy-env interactions, and accumulates
+    them into a replay buffer. This buffer is used as the "data" to train the policy.
+    At the end of each collect, the policy is *updated* several times.
+
+    The API has some resemblence with `PyTorch Lightning <https://pytorch-lightning.readthedocs.io/>`__,
+    but it's essentially different because this trainer is built for RL applications, and thus
+    most configurations are under RL context.
+    We are still looking for ways to incorporate existing trainer libraries, because it looks like
+    big efforts to build a trainer as powerful as those libraries, and also, that's not our primary goal.
+
+    It's essentially different
+    `tianshou's built-in trainers <https://tianshou.readthedocs.io/en/master/api/tianshou.trainer.html>`__,
+    as it's far much more complicated than that.
+
+    Parameters
+    ----------
+    max_iters
+        Maximum iterations before stopping.
+    val_every_n_iters
+        Perform validation every n iterations (i.e., training collects).
+    logger
+        Logger to record the backtest results. Logger must be present because
+        without logger, all information will be lost.
+    finite_env_type
+        Type of finite env implementation.
+    concurrency
+        Parallel workers.
+    fast_dev_run
+        Create a subset for debugging.
+        How this is implemented depends on the implementation of training vessel.
+        For :class:`~qlib.rl.vessel.TrainingVessel`, if greater than zero,
+        a random subset sized ``fast_dev_run`` will be used
+        instead of ``train_initial_states`` and ``val_initial_states``.
+    """
+
+    should_stop: bool
+    """Set to stop the training."""
+
+    metrics: dict
+    """Numeric metrics of produced in train/val/test.
+    In the middle of training / validation, metrics will be of the latest episode.
+    When each iteration of training / validation finishes, metrics will be the aggregation
+    of all episodes encountered in this iteration.
+
+    Cleared on every new iteration of training.
+
+    In fit, validation metrics will be prefixed with ``val/``.
+    """
+
+    current_iter: int
+    """Current iteration (collect) of training."""
+
+    loggers: List[LogWriter]
+    """A list of log writers."""
+
+    def __init__(
+        self,
+        *,
+        max_iters: int | None = None,
+        val_every_n_iters: int | None = None,
+        loggers: LogWriter | List[LogWriter] | None = None,
+        callbacks: List[Callback] | None = None,
+        finite_env_type: FiniteEnvType = "subproc",
+        concurrency: int = 2,
+        fast_dev_run: int | None = None,
+    ):
+        self.max_iters = max_iters
+        self.val_every_n_iters = val_every_n_iters
+
+        if isinstance(loggers, list):
+            self.loggers = loggers
+        elif isinstance(loggers, LogWriter):
+            self.loggers = [loggers]
+        else:
+            self.loggers = []
+
+        self.loggers.append(LogBuffer(self._metrics_callback, loglevel=self._min_loglevel()))
+
+        self.callbacks: List[Callback] = callbacks if callbacks is not None else []
+        self.finite_env_type = finite_env_type
+        self.concurrency = concurrency
+        self.fast_dev_run = fast_dev_run
+
+        self.current_stage: Literal["train", "val", "test"] = "train"
+
+        self.vessel: TrainingVesselBase = cast(TrainingVesselBase, None)
+
+    def initialize(self):
+        """Initialize the whole training process.
+
+        The states here should be synchronized with state_dict.
+        """
+        self.should_stop = False
+        self.current_iter = 0
+        self.current_episode = 0
+        self.current_stage = "train"
+
+    def initialize_iter(self):
+        """Initialize one iteration / collect."""
+        self.metrics = {}
+
+    def state_dict(self) -> dict:
+        """Putting every states of current training into a dict, at best effort.
+
+        It doesn't try to handle all the possible kinds of states in the middle of one training collect.
+        For most cases at the end of each iteration, things should be usually correct.
+
+        Note that it's also intended behavior that replay buffer data in the collector will be lost.
+        """
+        return {
+            "vessel": self.vessel.state_dict(),
+            "callbacks": {name: callback.state_dict() for name, callback in self.named_callbacks().items()},
+            "loggers": {name: logger.state_dict() for name, logger in self.named_loggers().items()},
+            "should_stop": self.should_stop,
+            "current_iter": self.current_iter,
+            "current_episode": self.current_episode,
+            "current_stage": self.current_stage,
+            "metrics": self.metrics,
+        }
+
+    @staticmethod
+    def get_policy_state_dict(ckpt_path: Path) -> OrderedDict:
+        state_dict = torch.load(ckpt_path, map_location="cpu")
+        if "vessel" in state_dict:
+            state_dict = state_dict["vessel"]["policy"]
+        return state_dict
+
+    def load_state_dict(self, state_dict: dict) -> None:
+        """Load all states into current trainer."""
+        self.vessel.load_state_dict(state_dict["vessel"])
+        for name, callback in self.named_callbacks().items():
+            callback.load_state_dict(state_dict["callbacks"][name])
+        for name, logger in self.named_loggers().items():
+            logger.load_state_dict(state_dict["loggers"][name])
+        self.should_stop = state_dict["should_stop"]
+        self.current_iter = state_dict["current_iter"]
+        self.current_episode = state_dict["current_episode"]
+        self.current_stage = state_dict["current_stage"]
+        self.metrics = state_dict["metrics"]
+
+    def named_callbacks(self) -> Dict[str, Callback]:
+        """Retrieve a collection of callbacks where each one has a name.
+        Useful when saving checkpoints.
+        """
+        return _named_collection(self.callbacks)
+
+    def named_loggers(self) -> Dict[str, LogWriter]:
+        """Retrieve a collection of loggers where each one has a name.
+        Useful when saving checkpoints.
+        """
+        return _named_collection(self.loggers)
+
+    def fit(self, vessel: TrainingVesselBase, ckpt_path: Path | None = None) -> None:
+        """Train the RL policy upon the defined simulator.
+
+        Parameters
+        ----------
+        vessel
+            A bundle of all elements used in training.
+        ckpt_path
+            Load a pre-trained / paused training checkpoint.
+        """
+        self.vessel = vessel
+        vessel.assign_trainer(self)
+
+        if ckpt_path is not None:
+            _logger.info("Resuming states from %s", str(ckpt_path))
+            self.load_state_dict(torch.load(ckpt_path))
+        else:
+            self.initialize()
+
+        self._call_callback_hooks("on_fit_start")
+
+        while not self.should_stop:
+            msg = f"\n{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\tTrain iteration {self.current_iter + 1}/{self.max_iters}"
+            _logger.info(msg)
+
+            self.initialize_iter()
+
+            self._call_callback_hooks("on_iter_start")
+
+            self.current_stage = "train"
+            self._call_callback_hooks("on_train_start")
+
+            # TODO
+            # Add a feature that supports reloading the training environment every few iterations.
+            with _wrap_context(vessel.train_seed_iterator()) as iterator:
+                vector_env = self.venv_from_iterator(iterator)
+                self.vessel.train(vector_env)
+                del vector_env  # FIXME: Explicitly delete this object to avoid memory leak.
+
+            self._call_callback_hooks("on_train_end")
+
+            if self.val_every_n_iters is not None and (self.current_iter + 1) % self.val_every_n_iters == 0:
+                # Implementation of validation loop
+                self.current_stage = "val"
+                self._call_callback_hooks("on_validate_start")
+                with _wrap_context(vessel.val_seed_iterator()) as iterator:
+                    vector_env = self.venv_from_iterator(iterator)
+                    self.vessel.validate(vector_env)
+                    del vector_env  # FIXME: Explicitly delete this object to avoid memory leak.
+
+                self._call_callback_hooks("on_validate_end")
+
+            # This iteration is considered complete.
+            # Bumping the current iteration counter.
+            self.current_iter += 1
+
+            if self.max_iters is not None and self.current_iter >= self.max_iters:
+                self.should_stop = True
+
+            self._call_callback_hooks("on_iter_end")
+
+        self._call_callback_hooks("on_fit_end")
+
+    def test(self, vessel: TrainingVesselBase) -> None:
+        """Test the RL policy against the simulator.
+
+        The simulator will be fed with data generated in ``test_seed_iterator``.
+
+        Parameters
+        ----------
+        vessel
+            A bundle of all related elements.
+        """
+        self.vessel = vessel
+        vessel.assign_trainer(self)
+
+        self.initialize_iter()
+
+        self.current_stage = "test"
+        self._call_callback_hooks("on_test_start")
+        with _wrap_context(vessel.test_seed_iterator()) as iterator:
+            vector_env = self.venv_from_iterator(iterator)
+            self.vessel.test(vector_env)
+            del vector_env  # FIXME: Explicitly delete this object to avoid memory leak.
+        self._call_callback_hooks("on_test_end")
+
+    def venv_from_iterator(self, iterator: Iterable[InitialStateType]) -> FiniteVectorEnv:
+        """Create a vectorized environment from iterator and the training vessel."""
+
+        def env_factory():
+            # FIXME: state_interpreter and action_interpreter are stateful (having a weakref of env),
+            # and could be thread unsafe.
+            # I'm not sure whether it's a design flaw.
+            # I'll rethink about this when designing the trainer.
+
+            if self.finite_env_type == "dummy":
+                # We could only experience the "threading-unsafe" problem in dummy.
+                state = copy.deepcopy(self.vessel.state_interpreter)
+                action = copy.deepcopy(self.vessel.action_interpreter)
+                rew = copy.deepcopy(self.vessel.reward)
+            else:
+                state = self.vessel.state_interpreter
+                action = self.vessel.action_interpreter
+                rew = self.vessel.reward
+
+            return EnvWrapper(
+                self.vessel.simulator_fn,
+                state,
+                action,
+                iterator,
+                rew,
+                logger=LogCollector(min_loglevel=self._min_loglevel()),
+            )
+
+        return vectorize_env(
+            env_factory,
+            self.finite_env_type,
+            self.concurrency,
+            self.loggers,
+        )
+
+    def _metrics_callback(self, on_episode: bool, on_collect: bool, log_buffer: LogBuffer) -> None:
+        if on_episode:
+            # Update the global counter.
+            self.current_episode = log_buffer.global_episode
+            metrics = log_buffer.episode_metrics()
+        elif on_collect:
+            # Update the latest metrics.
+            metrics = log_buffer.collect_metrics()
+        if self.current_stage == "val":
+            metrics = {"val/" + name: value for name, value in metrics.items()}
+        self.metrics.update(metrics)
+
+    def _call_callback_hooks(self, hook_name: str, *args: Any, **kwargs: Any) -> None:
+        for callback in self.callbacks:
+            fn = getattr(callback, hook_name)
+            fn(self, self.vessel, *args, **kwargs)
+
+    def _min_loglevel(self):
+        if not self.loggers:
+            return LogLevel.PERIODIC
+        else:
+            # To save bandwidth
+            return min(lg.loglevel for lg in self.loggers)
+
+
+@contextmanager
+def _wrap_context(obj):
+    """Make any object a (possibly dummy) context manager."""
+
+    if isinstance(obj, AbstractContextManager):
+        # obj has __enter__ and __exit__
+        with obj as ctx:
+            yield ctx
+    else:
+        yield obj
+
+
+def _named_collection(seq: Sequence[T]) -> Dict[str, T]:
+    """Convert a list into a dict, where each item is named with its type."""
+    res = {}
+    retry_cnt: collections.Counter = collections.Counter()
+    for item in seq:
+        typename = type(item).__name__.lower()
+        key = typename if retry_cnt[typename] == 0 else f"{typename}{retry_cnt[typename]}"
+        retry_cnt[typename] += 1
+        res[key] = item
+    return res
```

## qlib/rl/trainer/vessel.py

 * *Ordering differences only*

```diff
@@ -1,216 +1,216 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-from __future__ import annotations
-
-import weakref
-from typing import TYPE_CHECKING, Any, Callable, ContextManager, Dict, Generic, Iterable, Sequence, TypeVar, cast
-
-import numpy as np
-from tianshou.data import Collector, VectorReplayBuffer
-from tianshou.env import BaseVectorEnv
-from tianshou.policy import BasePolicy
-
-from qlib.constant import INF
-from qlib.log import get_module_logger
-from qlib.rl.interpreter import ActionInterpreter, ActType, ObsType, PolicyActType, StateInterpreter, StateType
-from qlib.rl.reward import Reward
-from qlib.rl.simulator import InitialStateType, Simulator
-from qlib.rl.utils import DataQueue
-from qlib.rl.utils.finite_env import FiniteVectorEnv
-
-if TYPE_CHECKING:
-    from .trainer import Trainer
-
-
-T = TypeVar("T")
-_logger = get_module_logger(__name__)
-
-
-class SeedIteratorNotAvailable(BaseException):
-    pass
-
-
-class TrainingVesselBase(Generic[InitialStateType, StateType, ActType, ObsType, PolicyActType]):
-    """A ship that contains simulator, interpreter, and policy, will be sent to trainer.
-    This class controls algorithm-related parts of training, while trainer is responsible for runtime part.
-
-    The ship also defines the most important logic of the core training part,
-    and (optionally) some callbacks to insert customized logics at specific events.
-    """
-
-    simulator_fn: Callable[[InitialStateType], Simulator[InitialStateType, StateType, ActType]]
-    state_interpreter: StateInterpreter[StateType, ObsType]
-    action_interpreter: ActionInterpreter[StateType, PolicyActType, ActType]
-    policy: BasePolicy
-    reward: Reward
-    trainer: Trainer
-
-    def assign_trainer(self, trainer: Trainer) -> None:
-        self.trainer = weakref.proxy(trainer)  # type: ignore
-
-    def train_seed_iterator(self) -> ContextManager[Iterable[InitialStateType]] | Iterable[InitialStateType]:
-        """Override this to create a seed iterator for training.
-        If the iterable is a context manager, the whole training will be invoked in the with-block,
-        and the iterator will be automatically closed after the training is done."""
-        raise SeedIteratorNotAvailable("Seed iterator for training is not available.")
-
-    def val_seed_iterator(self) -> ContextManager[Iterable[InitialStateType]] | Iterable[InitialStateType]:
-        """Override this to create a seed iterator for validation."""
-        raise SeedIteratorNotAvailable("Seed iterator for validation is not available.")
-
-    def test_seed_iterator(self) -> ContextManager[Iterable[InitialStateType]] | Iterable[InitialStateType]:
-        """Override this to create a seed iterator for testing."""
-        raise SeedIteratorNotAvailable("Seed iterator for testing is not available.")
-
-    def train(self, vector_env: BaseVectorEnv) -> Dict[str, Any]:
-        """Implement this to train one iteration. In RL, one iteration usually refers to one collect."""
-        raise NotImplementedError()
-
-    def validate(self, vector_env: FiniteVectorEnv) -> Dict[str, Any]:
-        """Implement this to validate the policy once."""
-        raise NotImplementedError()
-
-    def test(self, vector_env: FiniteVectorEnv) -> Dict[str, Any]:
-        """Implement this to evaluate the policy on test environment once."""
-        raise NotImplementedError()
-
-    def log(self, name: str, value: Any) -> None:
-        # FIXME: this is a workaround to make the log at least show somewhere.
-        # Need a refactor in logger to formalize this.
-        if isinstance(value, (np.ndarray, list)):
-            value = np.mean(value)
-        _logger.info(f"[Iter {self.trainer.current_iter + 1}] {name} = {value}")
-
-    def log_dict(self, data: Dict[str, Any]) -> None:
-        for name, value in data.items():
-            self.log(name, value)
-
-    def state_dict(self) -> Dict:
-        """Return a checkpoint of current vessel state."""
-        return {"policy": self.policy.state_dict()}
-
-    def load_state_dict(self, state_dict: Dict) -> None:
-        """Restore a checkpoint from a previously saved state dict."""
-        self.policy.load_state_dict(state_dict["policy"])
-
-
-class TrainingVessel(TrainingVesselBase):
-    """The default implementation of training vessel.
-
-    ``__init__`` accepts a sequence of initial states so that iterator can be created.
-    ``train``, ``validate``, ``test`` each do one collect (and also update in train).
-    By default, the train initial states will be repeated infinitely during training,
-    and collector will control the number of episodes for each iteration.
-    In validation and testing, the val / test initial states will be used exactly once.
-
-    Extra hyper-parameters (only used in train) include:
-
-    - ``buffer_size``: Size of replay buffer.
-    - ``episode_per_iter``: Episodes per collect at training. Can be overridden by fast dev run.
-    - ``update_kwargs``: Keyword arguments appearing in ``policy.update``.
-      For example, ``dict(repeat=10, batch_size=64)``.
-    """
-
-    def __init__(
-        self,
-        *,
-        simulator_fn: Callable[[InitialStateType], Simulator[InitialStateType, StateType, ActType]],
-        state_interpreter: StateInterpreter[StateType, ObsType],
-        action_interpreter: ActionInterpreter[StateType, PolicyActType, ActType],
-        policy: BasePolicy,
-        reward: Reward,
-        train_initial_states: Sequence[InitialStateType] | None = None,
-        val_initial_states: Sequence[InitialStateType] | None = None,
-        test_initial_states: Sequence[InitialStateType] | None = None,
-        buffer_size: int = 20000,
-        episode_per_iter: int = 1000,
-        update_kwargs: Dict[str, Any] = cast(Dict[str, Any], None),
-    ):
-        self.simulator_fn = simulator_fn  # type: ignore
-        self.state_interpreter = state_interpreter
-        self.action_interpreter = action_interpreter
-        self.policy = policy
-        self.reward = reward
-        self.train_initial_states = train_initial_states
-        self.val_initial_states = val_initial_states
-        self.test_initial_states = test_initial_states
-        self.buffer_size = buffer_size
-        self.episode_per_iter = episode_per_iter
-        self.update_kwargs = update_kwargs or {}
-
-    def train_seed_iterator(self) -> ContextManager[Iterable[InitialStateType]] | Iterable[InitialStateType]:
-        if self.train_initial_states is not None:
-            _logger.info("Training initial states collection size: %d", len(self.train_initial_states))
-            # Implement fast_dev_run here.
-            train_initial_states = self._random_subset("train", self.train_initial_states, self.trainer.fast_dev_run)
-            return DataQueue(train_initial_states, repeat=-1, shuffle=True)
-        return super().train_seed_iterator()
-
-    def val_seed_iterator(self) -> ContextManager[Iterable[InitialStateType]] | Iterable[InitialStateType]:
-        if self.val_initial_states is not None:
-            _logger.info("Validation initial states collection size: %d", len(self.val_initial_states))
-            val_initial_states = self._random_subset("val", self.val_initial_states, self.trainer.fast_dev_run)
-            return DataQueue(val_initial_states, repeat=1)
-        return super().val_seed_iterator()
-
-    def test_seed_iterator(self) -> ContextManager[Iterable[InitialStateType]] | Iterable[InitialStateType]:
-        if self.test_initial_states is not None:
-            _logger.info("Testing initial states collection size: %d", len(self.test_initial_states))
-            test_initial_states = self._random_subset("test", self.test_initial_states, self.trainer.fast_dev_run)
-            return DataQueue(test_initial_states, repeat=1)
-        return super().test_seed_iterator()
-
-    def train(self, vector_env: FiniteVectorEnv) -> Dict[str, Any]:
-        """Create a collector and collects ``episode_per_iter`` episodes.
-        Update the policy on the collected replay buffer.
-        """
-        self.policy.train()
-
-        with vector_env.collector_guard():
-            collector = Collector(self.policy, vector_env, VectorReplayBuffer(self.buffer_size, len(vector_env)))
-
-            # Number of episodes collected in each training iteration can be overridden by fast dev run.
-            if self.trainer.fast_dev_run is not None:
-                episodes = self.trainer.fast_dev_run
-            else:
-                episodes = self.episode_per_iter
-
-            col_result = collector.collect(n_episode=episodes)
-            update_result = self.policy.update(sample_size=0, buffer=collector.buffer, **self.update_kwargs)
-            res = {**col_result, **update_result}
-            self.log_dict(res)
-            return res
-
-    def validate(self, vector_env: FiniteVectorEnv) -> Dict[str, Any]:
-        self.policy.eval()
-
-        with vector_env.collector_guard():
-            test_collector = Collector(self.policy, vector_env)
-            res = test_collector.collect(n_step=INF * len(vector_env))
-            self.log_dict(res)
-            return res
-
-    def test(self, vector_env: FiniteVectorEnv) -> Dict[str, Any]:
-        self.policy.eval()
-
-        with vector_env.collector_guard():
-            test_collector = Collector(self.policy, vector_env)
-            res = test_collector.collect(n_step=INF * len(vector_env))
-            self.log_dict(res)
-            return res
-
-    @staticmethod
-    def _random_subset(name: str, collection: Sequence[T], size: int | None) -> Sequence[T]:
-        if size is None:
-            # Size = None -> original collection
-            return collection
-        order = np.random.permutation(len(collection))
-        res = [collection[o] for o in order[:size]]
-        _logger.info(
-            "Fast running in development mode. Cut %s initial states from %d to %d.",
-            name,
-            len(collection),
-            len(res),
-        )
-        return res
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+from __future__ import annotations
+
+import weakref
+from typing import TYPE_CHECKING, Any, Callable, ContextManager, Dict, Generic, Iterable, Sequence, TypeVar, cast
+
+import numpy as np
+from tianshou.data import Collector, VectorReplayBuffer
+from tianshou.env import BaseVectorEnv
+from tianshou.policy import BasePolicy
+
+from qlib.constant import INF
+from qlib.log import get_module_logger
+from qlib.rl.interpreter import ActionInterpreter, ActType, ObsType, PolicyActType, StateInterpreter, StateType
+from qlib.rl.reward import Reward
+from qlib.rl.simulator import InitialStateType, Simulator
+from qlib.rl.utils import DataQueue
+from qlib.rl.utils.finite_env import FiniteVectorEnv
+
+if TYPE_CHECKING:
+    from .trainer import Trainer
+
+
+T = TypeVar("T")
+_logger = get_module_logger(__name__)
+
+
+class SeedIteratorNotAvailable(BaseException):
+    pass
+
+
+class TrainingVesselBase(Generic[InitialStateType, StateType, ActType, ObsType, PolicyActType]):
+    """A ship that contains simulator, interpreter, and policy, will be sent to trainer.
+    This class controls algorithm-related parts of training, while trainer is responsible for runtime part.
+
+    The ship also defines the most important logic of the core training part,
+    and (optionally) some callbacks to insert customized logics at specific events.
+    """
+
+    simulator_fn: Callable[[InitialStateType], Simulator[InitialStateType, StateType, ActType]]
+    state_interpreter: StateInterpreter[StateType, ObsType]
+    action_interpreter: ActionInterpreter[StateType, PolicyActType, ActType]
+    policy: BasePolicy
+    reward: Reward
+    trainer: Trainer
+
+    def assign_trainer(self, trainer: Trainer) -> None:
+        self.trainer = weakref.proxy(trainer)  # type: ignore
+
+    def train_seed_iterator(self) -> ContextManager[Iterable[InitialStateType]] | Iterable[InitialStateType]:
+        """Override this to create a seed iterator for training.
+        If the iterable is a context manager, the whole training will be invoked in the with-block,
+        and the iterator will be automatically closed after the training is done."""
+        raise SeedIteratorNotAvailable("Seed iterator for training is not available.")
+
+    def val_seed_iterator(self) -> ContextManager[Iterable[InitialStateType]] | Iterable[InitialStateType]:
+        """Override this to create a seed iterator for validation."""
+        raise SeedIteratorNotAvailable("Seed iterator for validation is not available.")
+
+    def test_seed_iterator(self) -> ContextManager[Iterable[InitialStateType]] | Iterable[InitialStateType]:
+        """Override this to create a seed iterator for testing."""
+        raise SeedIteratorNotAvailable("Seed iterator for testing is not available.")
+
+    def train(self, vector_env: BaseVectorEnv) -> Dict[str, Any]:
+        """Implement this to train one iteration. In RL, one iteration usually refers to one collect."""
+        raise NotImplementedError()
+
+    def validate(self, vector_env: FiniteVectorEnv) -> Dict[str, Any]:
+        """Implement this to validate the policy once."""
+        raise NotImplementedError()
+
+    def test(self, vector_env: FiniteVectorEnv) -> Dict[str, Any]:
+        """Implement this to evaluate the policy on test environment once."""
+        raise NotImplementedError()
+
+    def log(self, name: str, value: Any) -> None:
+        # FIXME: this is a workaround to make the log at least show somewhere.
+        # Need a refactor in logger to formalize this.
+        if isinstance(value, (np.ndarray, list)):
+            value = np.mean(value)
+        _logger.info(f"[Iter {self.trainer.current_iter + 1}] {name} = {value}")
+
+    def log_dict(self, data: Dict[str, Any]) -> None:
+        for name, value in data.items():
+            self.log(name, value)
+
+    def state_dict(self) -> Dict:
+        """Return a checkpoint of current vessel state."""
+        return {"policy": self.policy.state_dict()}
+
+    def load_state_dict(self, state_dict: Dict) -> None:
+        """Restore a checkpoint from a previously saved state dict."""
+        self.policy.load_state_dict(state_dict["policy"])
+
+
+class TrainingVessel(TrainingVesselBase):
+    """The default implementation of training vessel.
+
+    ``__init__`` accepts a sequence of initial states so that iterator can be created.
+    ``train``, ``validate``, ``test`` each do one collect (and also update in train).
+    By default, the train initial states will be repeated infinitely during training,
+    and collector will control the number of episodes for each iteration.
+    In validation and testing, the val / test initial states will be used exactly once.
+
+    Extra hyper-parameters (only used in train) include:
+
+    - ``buffer_size``: Size of replay buffer.
+    - ``episode_per_iter``: Episodes per collect at training. Can be overridden by fast dev run.
+    - ``update_kwargs``: Keyword arguments appearing in ``policy.update``.
+      For example, ``dict(repeat=10, batch_size=64)``.
+    """
+
+    def __init__(
+        self,
+        *,
+        simulator_fn: Callable[[InitialStateType], Simulator[InitialStateType, StateType, ActType]],
+        state_interpreter: StateInterpreter[StateType, ObsType],
+        action_interpreter: ActionInterpreter[StateType, PolicyActType, ActType],
+        policy: BasePolicy,
+        reward: Reward,
+        train_initial_states: Sequence[InitialStateType] | None = None,
+        val_initial_states: Sequence[InitialStateType] | None = None,
+        test_initial_states: Sequence[InitialStateType] | None = None,
+        buffer_size: int = 20000,
+        episode_per_iter: int = 1000,
+        update_kwargs: Dict[str, Any] = cast(Dict[str, Any], None),
+    ):
+        self.simulator_fn = simulator_fn  # type: ignore
+        self.state_interpreter = state_interpreter
+        self.action_interpreter = action_interpreter
+        self.policy = policy
+        self.reward = reward
+        self.train_initial_states = train_initial_states
+        self.val_initial_states = val_initial_states
+        self.test_initial_states = test_initial_states
+        self.buffer_size = buffer_size
+        self.episode_per_iter = episode_per_iter
+        self.update_kwargs = update_kwargs or {}
+
+    def train_seed_iterator(self) -> ContextManager[Iterable[InitialStateType]] | Iterable[InitialStateType]:
+        if self.train_initial_states is not None:
+            _logger.info("Training initial states collection size: %d", len(self.train_initial_states))
+            # Implement fast_dev_run here.
+            train_initial_states = self._random_subset("train", self.train_initial_states, self.trainer.fast_dev_run)
+            return DataQueue(train_initial_states, repeat=-1, shuffle=True)
+        return super().train_seed_iterator()
+
+    def val_seed_iterator(self) -> ContextManager[Iterable[InitialStateType]] | Iterable[InitialStateType]:
+        if self.val_initial_states is not None:
+            _logger.info("Validation initial states collection size: %d", len(self.val_initial_states))
+            val_initial_states = self._random_subset("val", self.val_initial_states, self.trainer.fast_dev_run)
+            return DataQueue(val_initial_states, repeat=1)
+        return super().val_seed_iterator()
+
+    def test_seed_iterator(self) -> ContextManager[Iterable[InitialStateType]] | Iterable[InitialStateType]:
+        if self.test_initial_states is not None:
+            _logger.info("Testing initial states collection size: %d", len(self.test_initial_states))
+            test_initial_states = self._random_subset("test", self.test_initial_states, self.trainer.fast_dev_run)
+            return DataQueue(test_initial_states, repeat=1)
+        return super().test_seed_iterator()
+
+    def train(self, vector_env: FiniteVectorEnv) -> Dict[str, Any]:
+        """Create a collector and collects ``episode_per_iter`` episodes.
+        Update the policy on the collected replay buffer.
+        """
+        self.policy.train()
+
+        with vector_env.collector_guard():
+            collector = Collector(self.policy, vector_env, VectorReplayBuffer(self.buffer_size, len(vector_env)))
+
+            # Number of episodes collected in each training iteration can be overridden by fast dev run.
+            if self.trainer.fast_dev_run is not None:
+                episodes = self.trainer.fast_dev_run
+            else:
+                episodes = self.episode_per_iter
+
+            col_result = collector.collect(n_episode=episodes)
+            update_result = self.policy.update(sample_size=0, buffer=collector.buffer, **self.update_kwargs)
+            res = {**col_result, **update_result}
+            self.log_dict(res)
+            return res
+
+    def validate(self, vector_env: FiniteVectorEnv) -> Dict[str, Any]:
+        self.policy.eval()
+
+        with vector_env.collector_guard():
+            test_collector = Collector(self.policy, vector_env)
+            res = test_collector.collect(n_step=INF * len(vector_env))
+            self.log_dict(res)
+            return res
+
+    def test(self, vector_env: FiniteVectorEnv) -> Dict[str, Any]:
+        self.policy.eval()
+
+        with vector_env.collector_guard():
+            test_collector = Collector(self.policy, vector_env)
+            res = test_collector.collect(n_step=INF * len(vector_env))
+            self.log_dict(res)
+            return res
+
+    @staticmethod
+    def _random_subset(name: str, collection: Sequence[T], size: int | None) -> Sequence[T]:
+        if size is None:
+            # Size = None -> original collection
+            return collection
+        order = np.random.permutation(len(collection))
+        res = [collection[o] for o in order[:size]]
+        _logger.info(
+            "Fast running in development mode. Cut %s initial states from %d to %d.",
+            name,
+            len(collection),
+            len(res),
+        )
+        return res
```

## qlib/rl/utils/__init__.py

 * *Ordering differences only*

```diff
@@ -1,21 +1,21 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-from .data_queue import DataQueue
-from .env_wrapper import EnvWrapper, EnvWrapperStatus
-from .finite_env import FiniteEnvType, vectorize_env
-from .log import ConsoleWriter, CsvWriter, LogBuffer, LogCollector, LogLevel, LogWriter
-
-__all__ = [
-    "LogLevel",
-    "DataQueue",
-    "EnvWrapper",
-    "FiniteEnvType",
-    "LogCollector",
-    "LogWriter",
-    "vectorize_env",
-    "ConsoleWriter",
-    "CsvWriter",
-    "EnvWrapperStatus",
-    "LogBuffer",
-]
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+from .data_queue import DataQueue
+from .env_wrapper import EnvWrapper, EnvWrapperStatus
+from .finite_env import FiniteEnvType, vectorize_env
+from .log import ConsoleWriter, CsvWriter, LogBuffer, LogCollector, LogLevel, LogWriter
+
+__all__ = [
+    "LogLevel",
+    "DataQueue",
+    "EnvWrapper",
+    "FiniteEnvType",
+    "LogCollector",
+    "LogWriter",
+    "vectorize_env",
+    "ConsoleWriter",
+    "CsvWriter",
+    "EnvWrapperStatus",
+    "LogBuffer",
+]
```

## qlib/rl/utils/data_queue.py

 * *Ordering differences only*

```diff
@@ -1,188 +1,188 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-from __future__ import annotations
-
-import multiprocessing
-from multiprocessing.sharedctypes import Synchronized
-import os
-import threading
-import time
-import warnings
-from queue import Empty
-from typing import Any, Generator, Generic, Sequence, TypeVar, cast
-
-from qlib.log import get_module_logger
-
-_logger = get_module_logger(__name__)
-
-T = TypeVar("T")
-
-__all__ = ["DataQueue"]
-
-
-class DataQueue(Generic[T]):
-    """Main process (producer) produces data and stores them in a queue.
-    Sub-processes (consumers) can retrieve the data-points from the queue.
-    Data-points are generated via reading items from ``dataset``.
-
-    :class:`DataQueue` is ephemeral. You must create a new DataQueue
-    when the ``repeat`` is exhausted.
-
-    See the documents of :class:`qlib.rl.utils.FiniteVectorEnv` for more background.
-
-    Parameters
-    ----------
-    dataset
-        The dataset to read data from. Must implement ``__len__`` and ``__getitem__``.
-    repeat
-        Iterate over the data-points for how many times. Use ``-1`` to iterate forever.
-    shuffle
-        If ``shuffle`` is true, the items will be read in random order.
-    producer_num_workers
-        Concurrent workers for data-loading.
-    queue_maxsize
-        Maximum items to put into queue before it jams.
-
-    Examples
-    --------
-    >>> data_queue = DataQueue(my_dataset)
-    >>> with data_queue:
-    ...     ...
-
-    In worker:
-
-    >>> for data in data_queue:
-    ...     print(data)
-    """
-
-    def __init__(
-        self,
-        dataset: Sequence[T],
-        repeat: int = 1,
-        shuffle: bool = True,
-        producer_num_workers: int = 0,
-        queue_maxsize: int = 0,
-    ) -> None:
-        if queue_maxsize == 0:
-            if os.cpu_count() is not None:
-                queue_maxsize = cast(int, os.cpu_count())
-                _logger.info(f"Automatically set data queue maxsize to {queue_maxsize} to avoid overwhelming.")
-            else:
-                queue_maxsize = 1
-                _logger.warning(f"CPU count not available. Setting queue maxsize to 1.")
-
-        self.dataset: Sequence[T] = dataset
-        self.repeat: int = repeat
-        self.shuffle: bool = shuffle
-        self.producer_num_workers: int = producer_num_workers
-
-        self._activated: bool = False
-        self._queue: multiprocessing.Queue = multiprocessing.Queue(maxsize=queue_maxsize)
-        # Mypy 0.981 brought '"SynchronizedBase[Any]" has no attribute "value"  [attr-defined]' bug.
-        # Therefore, add this type casting to pass Mypy checking.
-        self._done = cast(Synchronized, multiprocessing.Value("i", 0))
-
-    def __enter__(self) -> DataQueue:
-        self.activate()
-        return self
-
-    def __exit__(self, exc_type, exc_val, exc_tb):
-        self.cleanup()
-
-    def cleanup(self) -> None:
-        with self._done.get_lock():
-            self._done.value += 1
-        for repeat in range(500):
-            if repeat >= 1:
-                warnings.warn(f"After {repeat} cleanup, the queue is still not empty.", category=RuntimeWarning)
-            while not self._queue.empty():
-                try:
-                    self._queue.get(block=False)
-                except Empty:
-                    pass
-            # Sometimes when the queue gets emptied, more data have already been sent,
-            # and they are on the way into the queue.
-            # If these data didn't get consumed, it will jam the queue and make the process hang.
-            # We wait a second here for potential data arriving, and check again (for ``repeat`` times).
-            time.sleep(1.0)
-            if self._queue.empty():
-                break
-        _logger.debug(f"Remaining items in queue collection done. Empty: {self._queue.empty()}")
-
-    def get(self, block: bool = True) -> Any:
-        if not hasattr(self, "_first_get"):
-            self._first_get = True
-        if self._first_get:
-            timeout = 5.0
-            self._first_get = False
-        else:
-            timeout = 0.5
-        while True:
-            try:
-                return self._queue.get(block=block, timeout=timeout)
-            except Empty:
-                if self._done.value:
-                    raise StopIteration  # pylint: disable=raise-missing-from
-
-    def put(self, obj: Any, block: bool = True, timeout: int | None = None) -> None:
-        self._queue.put(obj, block=block, timeout=timeout)
-
-    def mark_as_done(self) -> None:
-        with self._done.get_lock():
-            self._done.value = 1
-
-    def done(self) -> int:
-        return self._done.value
-
-    def activate(self) -> DataQueue:
-        if self._activated:
-            raise ValueError("DataQueue can not activate twice.")
-        thread = threading.Thread(target=self._producer, daemon=True)
-        thread.start()
-        self._activated = True
-        return self
-
-    def __del__(self) -> None:
-        _logger.debug(f"__del__ of {__name__}.DataQueue")
-        self.cleanup()
-
-    def __iter__(self) -> Generator[Any, None, None]:
-        if not self._activated:
-            raise ValueError(
-                "Need to call activate() to launch a daemon worker "
-                "to produce data into data queue before using it. "
-                "You probably have forgotten to use the DataQueue in a with block.",
-            )
-        return self._consumer()
-
-    def _consumer(self) -> Generator[Any, None, None]:
-        while True:
-            try:
-                yield self.get()
-            except StopIteration:
-                _logger.debug("Data consumer timed-out from get.")
-                return
-
-    def _producer(self) -> None:
-        # pytorch dataloader is used here only because we need its sampler and multi-processing
-        from torch.utils.data import DataLoader, Dataset  # pylint: disable=import-outside-toplevel
-
-        try:
-            dataloader = DataLoader(
-                cast(Dataset[T], self.dataset),
-                batch_size=None,
-                num_workers=self.producer_num_workers,
-                shuffle=self.shuffle,
-                collate_fn=lambda t: t,  # identity collate fn
-            )
-            repeat = 10**18 if self.repeat == -1 else self.repeat
-            for _rep in range(repeat):
-                for data in dataloader:
-                    if self._done.value:
-                        # Already done.
-                        return
-                    self._queue.put(data)
-                _logger.debug(f"Dataloader loop done. Repeat {_rep}.")
-        finally:
-            self.mark_as_done()
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+from __future__ import annotations
+
+import multiprocessing
+from multiprocessing.sharedctypes import Synchronized
+import os
+import threading
+import time
+import warnings
+from queue import Empty
+from typing import Any, Generator, Generic, Sequence, TypeVar, cast
+
+from qlib.log import get_module_logger
+
+_logger = get_module_logger(__name__)
+
+T = TypeVar("T")
+
+__all__ = ["DataQueue"]
+
+
+class DataQueue(Generic[T]):
+    """Main process (producer) produces data and stores them in a queue.
+    Sub-processes (consumers) can retrieve the data-points from the queue.
+    Data-points are generated via reading items from ``dataset``.
+
+    :class:`DataQueue` is ephemeral. You must create a new DataQueue
+    when the ``repeat`` is exhausted.
+
+    See the documents of :class:`qlib.rl.utils.FiniteVectorEnv` for more background.
+
+    Parameters
+    ----------
+    dataset
+        The dataset to read data from. Must implement ``__len__`` and ``__getitem__``.
+    repeat
+        Iterate over the data-points for how many times. Use ``-1`` to iterate forever.
+    shuffle
+        If ``shuffle`` is true, the items will be read in random order.
+    producer_num_workers
+        Concurrent workers for data-loading.
+    queue_maxsize
+        Maximum items to put into queue before it jams.
+
+    Examples
+    --------
+    >>> data_queue = DataQueue(my_dataset)
+    >>> with data_queue:
+    ...     ...
+
+    In worker:
+
+    >>> for data in data_queue:
+    ...     print(data)
+    """
+
+    def __init__(
+        self,
+        dataset: Sequence[T],
+        repeat: int = 1,
+        shuffle: bool = True,
+        producer_num_workers: int = 0,
+        queue_maxsize: int = 0,
+    ) -> None:
+        if queue_maxsize == 0:
+            if os.cpu_count() is not None:
+                queue_maxsize = cast(int, os.cpu_count())
+                _logger.info(f"Automatically set data queue maxsize to {queue_maxsize} to avoid overwhelming.")
+            else:
+                queue_maxsize = 1
+                _logger.warning(f"CPU count not available. Setting queue maxsize to 1.")
+
+        self.dataset: Sequence[T] = dataset
+        self.repeat: int = repeat
+        self.shuffle: bool = shuffle
+        self.producer_num_workers: int = producer_num_workers
+
+        self._activated: bool = False
+        self._queue: multiprocessing.Queue = multiprocessing.Queue(maxsize=queue_maxsize)
+        # Mypy 0.981 brought '"SynchronizedBase[Any]" has no attribute "value"  [attr-defined]' bug.
+        # Therefore, add this type casting to pass Mypy checking.
+        self._done = cast(Synchronized, multiprocessing.Value("i", 0))
+
+    def __enter__(self) -> DataQueue:
+        self.activate()
+        return self
+
+    def __exit__(self, exc_type, exc_val, exc_tb):
+        self.cleanup()
+
+    def cleanup(self) -> None:
+        with self._done.get_lock():
+            self._done.value += 1
+        for repeat in range(500):
+            if repeat >= 1:
+                warnings.warn(f"After {repeat} cleanup, the queue is still not empty.", category=RuntimeWarning)
+            while not self._queue.empty():
+                try:
+                    self._queue.get(block=False)
+                except Empty:
+                    pass
+            # Sometimes when the queue gets emptied, more data have already been sent,
+            # and they are on the way into the queue.
+            # If these data didn't get consumed, it will jam the queue and make the process hang.
+            # We wait a second here for potential data arriving, and check again (for ``repeat`` times).
+            time.sleep(1.0)
+            if self._queue.empty():
+                break
+        _logger.debug(f"Remaining items in queue collection done. Empty: {self._queue.empty()}")
+
+    def get(self, block: bool = True) -> Any:
+        if not hasattr(self, "_first_get"):
+            self._first_get = True
+        if self._first_get:
+            timeout = 5.0
+            self._first_get = False
+        else:
+            timeout = 0.5
+        while True:
+            try:
+                return self._queue.get(block=block, timeout=timeout)
+            except Empty:
+                if self._done.value:
+                    raise StopIteration  # pylint: disable=raise-missing-from
+
+    def put(self, obj: Any, block: bool = True, timeout: int | None = None) -> None:
+        self._queue.put(obj, block=block, timeout=timeout)
+
+    def mark_as_done(self) -> None:
+        with self._done.get_lock():
+            self._done.value = 1
+
+    def done(self) -> int:
+        return self._done.value
+
+    def activate(self) -> DataQueue:
+        if self._activated:
+            raise ValueError("DataQueue can not activate twice.")
+        thread = threading.Thread(target=self._producer, daemon=True)
+        thread.start()
+        self._activated = True
+        return self
+
+    def __del__(self) -> None:
+        _logger.debug(f"__del__ of {__name__}.DataQueue")
+        self.cleanup()
+
+    def __iter__(self) -> Generator[Any, None, None]:
+        if not self._activated:
+            raise ValueError(
+                "Need to call activate() to launch a daemon worker "
+                "to produce data into data queue before using it. "
+                "You probably have forgotten to use the DataQueue in a with block.",
+            )
+        return self._consumer()
+
+    def _consumer(self) -> Generator[Any, None, None]:
+        while True:
+            try:
+                yield self.get()
+            except StopIteration:
+                _logger.debug("Data consumer timed-out from get.")
+                return
+
+    def _producer(self) -> None:
+        # pytorch dataloader is used here only because we need its sampler and multi-processing
+        from torch.utils.data import DataLoader, Dataset  # pylint: disable=import-outside-toplevel
+
+        try:
+            dataloader = DataLoader(
+                cast(Dataset[T], self.dataset),
+                batch_size=None,
+                num_workers=self.producer_num_workers,
+                shuffle=self.shuffle,
+                collate_fn=lambda t: t,  # identity collate fn
+            )
+            repeat = 10**18 if self.repeat == -1 else self.repeat
+            for _rep in range(repeat):
+                for data in dataloader:
+                    if self._done.value:
+                        # Already done.
+                        return
+                    self._queue.put(data)
+                _logger.debug(f"Dataloader loop done. Repeat {_rep}.")
+        finally:
+            self.mark_as_done()
```

## qlib/rl/utils/env_wrapper.py

 * *Ordering differences only*

```diff
@@ -1,250 +1,250 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-from __future__ import annotations
-
-import weakref
-from typing import Any, Callable, cast, Dict, Generic, Iterable, Iterator, Optional, Tuple
-
-import gym
-from gym import Space
-
-from qlib.rl.aux_info import AuxiliaryInfoCollector
-from qlib.rl.interpreter import ActionInterpreter, ObsType, PolicyActType, StateInterpreter
-from qlib.rl.reward import Reward
-from qlib.rl.simulator import ActType, InitialStateType, Simulator, StateType
-from qlib.typehint import TypedDict
-from .finite_env import generate_nan_observation
-from .log import LogCollector, LogLevel
-
-__all__ = ["InfoDict", "EnvWrapperStatus", "EnvWrapper"]
-
-# in this case, there won't be any seed for simulator
-SEED_INTERATOR_MISSING = "_missing_"
-
-
-class InfoDict(TypedDict):
-    """The type of dict that is used in the 4th return value of ``env.step()``."""
-
-    aux_info: dict
-    """Any information depends on auxiliary info collector."""
-    log: Dict[str, Any]
-    """Collected by LogCollector."""
-
-
-class EnvWrapperStatus(TypedDict):
-    """
-    This is the status data structure used in EnvWrapper.
-    The fields here are in the semantics of RL.
-    For example, ``obs`` means the observation fed into policy.
-    ``action`` means the raw action returned by policy.
-    """
-
-    cur_step: int
-    done: bool
-    initial_state: Optional[Any]
-    obs_history: list
-    action_history: list
-    reward_history: list
-
-
-class EnvWrapper(
-    gym.Env[ObsType, PolicyActType],
-    Generic[InitialStateType, StateType, ActType, ObsType, PolicyActType],
-):
-    """Qlib-based RL environment, subclassing ``gym.Env``.
-    A wrapper of components, including simulator, state-interpreter, action-interpreter, reward.
-
-    This is what the framework of simulator - interpreter - policy looks like in RL training.
-    All the components other than policy needs to be assembled into a single object called "environment".
-    The "environment" are replicated into multiple workers, and (at least in tianshou's implementation),
-    one single policy (agent) plays against a batch of environments.
-
-    Parameters
-    ----------
-    simulator_fn
-        A callable that is the simulator factory.
-        When ``seed_iterator`` is present, the factory should take one argument,
-        that is the seed (aka initial state).
-        Otherwise, it should take zero argument.
-    state_interpreter
-        State-observation converter.
-    action_interpreter
-        Policy-simulator action converter.
-    seed_iterator
-        An iterable of seed. With the help of :class:`qlib.rl.utils.DataQueue`,
-        environment workers in different processes can share one ``seed_iterator``.
-    reward_fn
-        A callable that accepts the StateType and returns a float (at least in single-agent case).
-    aux_info_collector
-        Collect auxiliary information. Could be useful in MARL.
-    logger
-        Log collector that collects the logs. The collected logs are sent back to main process,
-        via the return value of ``env.step()``.
-
-    Attributes
-    ----------
-    status : EnvWrapperStatus
-        Status indicator. All terms are in *RL language*.
-        It can be used if users care about data on the RL side.
-        Can be none when no trajectory is available.
-    """
-
-    simulator: Simulator[InitialStateType, StateType, ActType]
-    seed_iterator: str | Iterator[InitialStateType] | None
-
-    def __init__(
-        self,
-        simulator_fn: Callable[..., Simulator[InitialStateType, StateType, ActType]],
-        state_interpreter: StateInterpreter[StateType, ObsType],
-        action_interpreter: ActionInterpreter[StateType, PolicyActType, ActType],
-        seed_iterator: Optional[Iterable[InitialStateType]],
-        reward_fn: Reward | None = None,
-        aux_info_collector: AuxiliaryInfoCollector[StateType, Any] | None = None,
-        logger: LogCollector | None = None,
-    ) -> None:
-        # Assign weak reference to wrapper.
-        #
-        # Use weak reference here, because:
-        # 1. Logically, the other components should be able to live without an env_wrapper.
-        #    For example, they might live in a strategy_wrapper in future.
-        #    Therefore injecting a "hard" attribute called "env" is not appropripate.
-        # 2. When the environment gets destroyed, it gets destoryed.
-        #    We don't want it to silently live inside some interpreters.
-        # 3. Avoid circular reference.
-        # 4. When the components get serialized, we can throw away the env without any burden.
-        #    (though this part is not implemented yet)
-        for obj in [state_interpreter, action_interpreter, reward_fn, aux_info_collector]:
-            if obj is not None:
-                obj.env = weakref.proxy(self)  # type: ignore
-
-        self.simulator_fn = simulator_fn
-        self.state_interpreter = state_interpreter
-        self.action_interpreter = action_interpreter
-
-        if seed_iterator is None:
-            # In this case, there won't be any seed for simulator
-            # We can't set it to None because None actually means something else.
-            # If `seed_iterator` is None, it means that it's exhausted.
-            self.seed_iterator = SEED_INTERATOR_MISSING
-        else:
-            self.seed_iterator = iter(seed_iterator)
-        self.reward_fn = reward_fn
-
-        self.aux_info_collector = aux_info_collector
-        self.logger: LogCollector = logger or LogCollector()
-        self.status: EnvWrapperStatus = cast(EnvWrapperStatus, None)
-
-    @property
-    def action_space(self) -> Space:
-        return self.action_interpreter.action_space
-
-    @property
-    def observation_space(self) -> Space:
-        return self.state_interpreter.observation_space
-
-    def reset(self, **kwargs: Any) -> ObsType:
-        """
-        Try to get a state from state queue, and init the simulator with this state.
-        If the queue is exhausted, generate an invalid (nan) observation.
-        """
-
-        try:
-            if self.seed_iterator is None:
-                raise RuntimeError("You can trying to get a state from a dead environment wrapper.")
-
-            # TODO: simulator/observation might need seed to prefetch something
-            # as only seed has the ability to do the work beforehands
-
-            # NOTE: though logger is reset here, logs in this function won't work,
-            # because we can't send them outside.
-            # See https://github.com/thu-ml/tianshou/issues/605
-            self.logger.reset()
-
-            if self.seed_iterator is SEED_INTERATOR_MISSING:
-                # no initial state
-                initial_state = None
-                self.simulator = cast(Callable[[], Simulator], self.simulator_fn)()
-            else:
-                initial_state = next(cast(Iterator[InitialStateType], self.seed_iterator))
-                self.simulator = self.simulator_fn(initial_state)
-
-            self.status = EnvWrapperStatus(
-                cur_step=0,
-                done=False,
-                initial_state=initial_state,
-                obs_history=[],
-                action_history=[],
-                reward_history=[],
-            )
-
-            self.simulator.env = cast(EnvWrapper, weakref.proxy(self))
-
-            sim_state = self.simulator.get_state()
-            obs = self.state_interpreter(sim_state)
-
-            self.status["obs_history"].append(obs)
-
-            return obs
-
-        except StopIteration:
-            # The environment should be recycled because it's in a dead state.
-            self.seed_iterator = None
-            return generate_nan_observation(self.observation_space)
-
-    def step(self, policy_action: PolicyActType, **kwargs: Any) -> Tuple[ObsType, float, bool, InfoDict]:
-        """Environment step.
-
-        See the code along with comments to get a sequence of things happening here.
-        """
-
-        if self.seed_iterator is None:
-            raise RuntimeError("State queue is already exhausted, but the environment is still receiving action.")
-
-        # Clear the logged information from last step
-        self.logger.reset()
-
-        # Action is what we have got from policy
-        self.status["action_history"].append(policy_action)
-        action = self.action_interpreter(self.simulator.get_state(), policy_action)
-
-        # This update must be after action interpreter and before simulator.
-        self.status["cur_step"] += 1
-
-        # Use the converted action of update the simulator
-        self.simulator.step(action)
-
-        # Update "done" first, as this status might be used by reward_fn later
-        done = self.simulator.done()
-        self.status["done"] = done
-
-        # Get state and calculate observation
-        sim_state = self.simulator.get_state()
-        obs = self.state_interpreter(sim_state)
-        self.status["obs_history"].append(obs)
-
-        # Reward and extra info
-        if self.reward_fn is not None:
-            rew = self.reward_fn(sim_state)
-        else:
-            # No reward. Treated as 0.
-            rew = 0.0
-        self.status["reward_history"].append(rew)
-
-        if self.aux_info_collector is not None:
-            aux_info = self.aux_info_collector(sim_state)
-        else:
-            aux_info = {}
-
-        # Final logging stuff: RL-specific logs
-        if done:
-            self.logger.add_scalar("steps_per_episode", self.status["cur_step"])
-        self.logger.add_scalar("reward", rew)
-        self.logger.add_any("obs", obs, loglevel=LogLevel.DEBUG)
-        self.logger.add_any("policy_act", policy_action, loglevel=LogLevel.DEBUG)
-
-        info_dict = InfoDict(log=self.logger.logs(), aux_info=aux_info)
-        return obs, rew, done, info_dict
-
-    def render(self, mode: str = "human") -> None:
-        raise NotImplementedError("Render is not implemented in EnvWrapper.")
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+from __future__ import annotations
+
+import weakref
+from typing import Any, Callable, cast, Dict, Generic, Iterable, Iterator, Optional, Tuple
+
+import gym
+from gym import Space
+
+from qlib.rl.aux_info import AuxiliaryInfoCollector
+from qlib.rl.interpreter import ActionInterpreter, ObsType, PolicyActType, StateInterpreter
+from qlib.rl.reward import Reward
+from qlib.rl.simulator import ActType, InitialStateType, Simulator, StateType
+from qlib.typehint import TypedDict
+from .finite_env import generate_nan_observation
+from .log import LogCollector, LogLevel
+
+__all__ = ["InfoDict", "EnvWrapperStatus", "EnvWrapper"]
+
+# in this case, there won't be any seed for simulator
+SEED_INTERATOR_MISSING = "_missing_"
+
+
+class InfoDict(TypedDict):
+    """The type of dict that is used in the 4th return value of ``env.step()``."""
+
+    aux_info: dict
+    """Any information depends on auxiliary info collector."""
+    log: Dict[str, Any]
+    """Collected by LogCollector."""
+
+
+class EnvWrapperStatus(TypedDict):
+    """
+    This is the status data structure used in EnvWrapper.
+    The fields here are in the semantics of RL.
+    For example, ``obs`` means the observation fed into policy.
+    ``action`` means the raw action returned by policy.
+    """
+
+    cur_step: int
+    done: bool
+    initial_state: Optional[Any]
+    obs_history: list
+    action_history: list
+    reward_history: list
+
+
+class EnvWrapper(
+    gym.Env[ObsType, PolicyActType],
+    Generic[InitialStateType, StateType, ActType, ObsType, PolicyActType],
+):
+    """Qlib-based RL environment, subclassing ``gym.Env``.
+    A wrapper of components, including simulator, state-interpreter, action-interpreter, reward.
+
+    This is what the framework of simulator - interpreter - policy looks like in RL training.
+    All the components other than policy needs to be assembled into a single object called "environment".
+    The "environment" are replicated into multiple workers, and (at least in tianshou's implementation),
+    one single policy (agent) plays against a batch of environments.
+
+    Parameters
+    ----------
+    simulator_fn
+        A callable that is the simulator factory.
+        When ``seed_iterator`` is present, the factory should take one argument,
+        that is the seed (aka initial state).
+        Otherwise, it should take zero argument.
+    state_interpreter
+        State-observation converter.
+    action_interpreter
+        Policy-simulator action converter.
+    seed_iterator
+        An iterable of seed. With the help of :class:`qlib.rl.utils.DataQueue`,
+        environment workers in different processes can share one ``seed_iterator``.
+    reward_fn
+        A callable that accepts the StateType and returns a float (at least in single-agent case).
+    aux_info_collector
+        Collect auxiliary information. Could be useful in MARL.
+    logger
+        Log collector that collects the logs. The collected logs are sent back to main process,
+        via the return value of ``env.step()``.
+
+    Attributes
+    ----------
+    status : EnvWrapperStatus
+        Status indicator. All terms are in *RL language*.
+        It can be used if users care about data on the RL side.
+        Can be none when no trajectory is available.
+    """
+
+    simulator: Simulator[InitialStateType, StateType, ActType]
+    seed_iterator: str | Iterator[InitialStateType] | None
+
+    def __init__(
+        self,
+        simulator_fn: Callable[..., Simulator[InitialStateType, StateType, ActType]],
+        state_interpreter: StateInterpreter[StateType, ObsType],
+        action_interpreter: ActionInterpreter[StateType, PolicyActType, ActType],
+        seed_iterator: Optional[Iterable[InitialStateType]],
+        reward_fn: Reward | None = None,
+        aux_info_collector: AuxiliaryInfoCollector[StateType, Any] | None = None,
+        logger: LogCollector | None = None,
+    ) -> None:
+        # Assign weak reference to wrapper.
+        #
+        # Use weak reference here, because:
+        # 1. Logically, the other components should be able to live without an env_wrapper.
+        #    For example, they might live in a strategy_wrapper in future.
+        #    Therefore injecting a "hard" attribute called "env" is not appropripate.
+        # 2. When the environment gets destroyed, it gets destoryed.
+        #    We don't want it to silently live inside some interpreters.
+        # 3. Avoid circular reference.
+        # 4. When the components get serialized, we can throw away the env without any burden.
+        #    (though this part is not implemented yet)
+        for obj in [state_interpreter, action_interpreter, reward_fn, aux_info_collector]:
+            if obj is not None:
+                obj.env = weakref.proxy(self)  # type: ignore
+
+        self.simulator_fn = simulator_fn
+        self.state_interpreter = state_interpreter
+        self.action_interpreter = action_interpreter
+
+        if seed_iterator is None:
+            # In this case, there won't be any seed for simulator
+            # We can't set it to None because None actually means something else.
+            # If `seed_iterator` is None, it means that it's exhausted.
+            self.seed_iterator = SEED_INTERATOR_MISSING
+        else:
+            self.seed_iterator = iter(seed_iterator)
+        self.reward_fn = reward_fn
+
+        self.aux_info_collector = aux_info_collector
+        self.logger: LogCollector = logger or LogCollector()
+        self.status: EnvWrapperStatus = cast(EnvWrapperStatus, None)
+
+    @property
+    def action_space(self) -> Space:
+        return self.action_interpreter.action_space
+
+    @property
+    def observation_space(self) -> Space:
+        return self.state_interpreter.observation_space
+
+    def reset(self, **kwargs: Any) -> ObsType:
+        """
+        Try to get a state from state queue, and init the simulator with this state.
+        If the queue is exhausted, generate an invalid (nan) observation.
+        """
+
+        try:
+            if self.seed_iterator is None:
+                raise RuntimeError("You can trying to get a state from a dead environment wrapper.")
+
+            # TODO: simulator/observation might need seed to prefetch something
+            # as only seed has the ability to do the work beforehands
+
+            # NOTE: though logger is reset here, logs in this function won't work,
+            # because we can't send them outside.
+            # See https://github.com/thu-ml/tianshou/issues/605
+            self.logger.reset()
+
+            if self.seed_iterator is SEED_INTERATOR_MISSING:
+                # no initial state
+                initial_state = None
+                self.simulator = cast(Callable[[], Simulator], self.simulator_fn)()
+            else:
+                initial_state = next(cast(Iterator[InitialStateType], self.seed_iterator))
+                self.simulator = self.simulator_fn(initial_state)
+
+            self.status = EnvWrapperStatus(
+                cur_step=0,
+                done=False,
+                initial_state=initial_state,
+                obs_history=[],
+                action_history=[],
+                reward_history=[],
+            )
+
+            self.simulator.env = cast(EnvWrapper, weakref.proxy(self))
+
+            sim_state = self.simulator.get_state()
+            obs = self.state_interpreter(sim_state)
+
+            self.status["obs_history"].append(obs)
+
+            return obs
+
+        except StopIteration:
+            # The environment should be recycled because it's in a dead state.
+            self.seed_iterator = None
+            return generate_nan_observation(self.observation_space)
+
+    def step(self, policy_action: PolicyActType, **kwargs: Any) -> Tuple[ObsType, float, bool, InfoDict]:
+        """Environment step.
+
+        See the code along with comments to get a sequence of things happening here.
+        """
+
+        if self.seed_iterator is None:
+            raise RuntimeError("State queue is already exhausted, but the environment is still receiving action.")
+
+        # Clear the logged information from last step
+        self.logger.reset()
+
+        # Action is what we have got from policy
+        self.status["action_history"].append(policy_action)
+        action = self.action_interpreter(self.simulator.get_state(), policy_action)
+
+        # This update must be after action interpreter and before simulator.
+        self.status["cur_step"] += 1
+
+        # Use the converted action of update the simulator
+        self.simulator.step(action)
+
+        # Update "done" first, as this status might be used by reward_fn later
+        done = self.simulator.done()
+        self.status["done"] = done
+
+        # Get state and calculate observation
+        sim_state = self.simulator.get_state()
+        obs = self.state_interpreter(sim_state)
+        self.status["obs_history"].append(obs)
+
+        # Reward and extra info
+        if self.reward_fn is not None:
+            rew = self.reward_fn(sim_state)
+        else:
+            # No reward. Treated as 0.
+            rew = 0.0
+        self.status["reward_history"].append(rew)
+
+        if self.aux_info_collector is not None:
+            aux_info = self.aux_info_collector(sim_state)
+        else:
+            aux_info = {}
+
+        # Final logging stuff: RL-specific logs
+        if done:
+            self.logger.add_scalar("steps_per_episode", self.status["cur_step"])
+        self.logger.add_scalar("reward", rew)
+        self.logger.add_any("obs", obs, loglevel=LogLevel.DEBUG)
+        self.logger.add_any("policy_act", policy_action, loglevel=LogLevel.DEBUG)
+
+        info_dict = InfoDict(log=self.logger.logs(), aux_info=aux_info)
+        return obs, rew, done, info_dict
+
+    def render(self, mode: str = "human") -> None:
+        raise NotImplementedError("Render is not implemented in EnvWrapper.")
```

## qlib/rl/utils/finite_env.py

 * *Ordering differences only*

```diff
@@ -1,369 +1,369 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-"""
-This is to support finite env in vector env.
-See https://github.com/thu-ml/tianshou/issues/322 for details.
-"""
-
-from __future__ import annotations
-
-import copy
-import warnings
-from contextlib import contextmanager
-from typing import Any, Callable, Dict, Generator, List, Optional, Set, Tuple, Type, Union, cast
-
-import gym
-import numpy as np
-from tianshou.env import BaseVectorEnv, DummyVectorEnv, ShmemVectorEnv, SubprocVectorEnv
-
-from qlib.typehint import Literal
-
-from .log import LogWriter
-
-__all__ = [
-    "generate_nan_observation",
-    "check_nan_observation",
-    "FiniteVectorEnv",
-    "FiniteDummyVectorEnv",
-    "FiniteSubprocVectorEnv",
-    "FiniteShmemVectorEnv",
-    "FiniteEnvType",
-    "vectorize_env",
-]
-
-FiniteEnvType = Literal["dummy", "subproc", "shmem"]
-T = Union[dict, list, tuple, np.ndarray]
-
-
-def fill_invalid(obj: int | float | bool | T) -> T:
-    if isinstance(obj, (int, float, bool)):
-        return fill_invalid(np.array(obj))
-    if hasattr(obj, "dtype"):
-        if isinstance(obj, np.ndarray):
-            if np.issubdtype(obj.dtype, np.floating):
-                return np.full_like(obj, np.nan)
-            return np.full_like(obj, np.iinfo(obj.dtype).max)
-        # dealing with corner cases that numpy number is not supported by tianshou's sharray
-        return fill_invalid(np.array(obj))
-    elif isinstance(obj, dict):
-        return {k: fill_invalid(v) for k, v in obj.items()}
-    elif isinstance(obj, list):
-        return [fill_invalid(v) for v in obj]
-    elif isinstance(obj, tuple):
-        return tuple(fill_invalid(v) for v in obj)
-    raise ValueError(f"Unsupported value to fill with invalid: {obj}")
-
-
-def is_invalid(arr: int | float | bool | T) -> bool:
-    if isinstance(arr, np.ndarray):
-        if np.issubdtype(arr.dtype, np.floating):
-            return np.isnan(arr).all()
-        return cast(bool, cast(np.ndarray, np.iinfo(arr.dtype).max == arr).all())
-    if isinstance(arr, dict):
-        return all(is_invalid(o) for o in arr.values())
-    if isinstance(arr, (list, tuple)):
-        return all(is_invalid(o) for o in arr)
-    if isinstance(arr, (int, float, bool, np.number)):
-        return is_invalid(np.array(arr))
-    return True
-
-
-def generate_nan_observation(obs_space: gym.Space) -> Any:
-    """The NaN observation that indicates the environment receives no seed.
-
-    We assume that obs is complex and there must be something like float.
-    Otherwise this logic doesn't work.
-    """
-
-    sample = obs_space.sample()
-    sample = fill_invalid(sample)
-    return sample
-
-
-def check_nan_observation(obs: Any) -> bool:
-    """Check whether obs is generated by :func:`generate_nan_observation`."""
-    return is_invalid(obs)
-
-
-class FiniteVectorEnv(BaseVectorEnv):
-    """To allow the paralleled env workers consume a single DataQueue until it's exhausted.
-
-    See `tianshou issue #322 <https://github.com/thu-ml/tianshou/issues/322>`_.
-
-    The requirement is to make every possible seed (stored in :class:`qlib.rl.utils.DataQueue` in our case)
-    consumed by exactly one environment. This is not possible by tianshou's native VectorEnv and Collector,
-    because tianshou is unaware of this "exactly one" constraint, and might launch extra workers.
-
-    Consider a corner case, where concurrency is 2, but there is only one seed in DataQueue.
-    The reset of two workers must be both called according to the logic in collect.
-    The returned results of two workers are collected, regardless of what they are.
-    The problem is, one of the reset result must be invalid, or repeated,
-    because there's only one need in queue, and collector isn't aware of such situation.
-
-    Luckily, we can hack the vector env, and make a protocol between single env and vector env.
-    The single environment (should be :class:`qlib.rl.utils.EnvWrapper` in our case) is responsible for
-    reading from queue, and generate a special observation when the queue is exhausted. The special obs
-    is called "nan observation", because simply using none causes problems in shared-memory vector env.
-    :class:`FiniteVectorEnv` then read the observations from all workers, and select those non-nan
-    observation. It also maintains an ``_alive_env_ids`` to track which workers should never be
-    called again. When also the environments are exhausted, it will raise StopIteration exception.
-
-    The usage of this vector env in collector are two parts:
-
-    1. If the data queue is finite (usually when inference), collector should collect "infinity" number of
-       episodes, until the vector env exhausts by itself.
-    2. If the data queue is infinite (usually in training), collector can set number of episodes / steps.
-       In this case, data would be randomly ordered, and some repetitions wouldn't matter.
-
-    One extra function of this vector env is that it has a logger that explicitly collects logs
-    from child workers. See :class:`qlib.rl.utils.LogWriter`.
-    """
-
-    _logger: list[LogWriter]
-
-    def __init__(
-        self, logger: LogWriter | list[LogWriter] | None, env_fns: list[Callable[..., gym.Env]], **kwargs: Any
-    ) -> None:
-        super().__init__(env_fns, **kwargs)
-
-        if isinstance(logger, list):
-            self._logger = logger
-        elif isinstance(logger, LogWriter):
-            self._logger = [logger]
-        else:
-            self._logger = []
-        self._alive_env_ids: Set[int] = set()
-        self._reset_alive_envs()
-        self._default_obs = self._default_info = self._default_rew = None
-        self._zombie = False
-
-        self._collector_guarded: bool = False
-
-    def _reset_alive_envs(self) -> None:
-        if not self._alive_env_ids:
-            # starting or running out
-            self._alive_env_ids = set(range(self.env_num))
-
-    # to workaround with tianshou's buffer and batch
-    def _set_default_obs(self, obs: Any) -> None:
-        if obs is not None and self._default_obs is None:
-            self._default_obs = copy.deepcopy(obs)
-
-    def _set_default_info(self, info: Any) -> None:
-        if info is not None and self._default_info is None:
-            self._default_info = copy.deepcopy(info)
-
-    def _set_default_rew(self, rew: Any) -> None:
-        if rew is not None and self._default_rew is None:
-            self._default_rew = copy.deepcopy(rew)
-
-    def _get_default_obs(self) -> Any:
-        return copy.deepcopy(self._default_obs)
-
-    def _get_default_info(self) -> Any:
-        return copy.deepcopy(self._default_info)
-
-    def _get_default_rew(self) -> Any:
-        return copy.deepcopy(self._default_rew)
-
-    # END
-
-    @staticmethod
-    def _postproc_env_obs(obs: Any) -> Optional[Any]:
-        # reserved for shmem vector env to restore empty observation
-        if obs is None or check_nan_observation(obs):
-            return None
-        return obs
-
-    @contextmanager
-    def collector_guard(self) -> Generator[FiniteVectorEnv, None, None]:
-        """Guard the collector. Recommended to guard every collect.
-
-        This guard is for two purposes.
-
-        1. Catch and ignore the StopIteration exception, which is the stopping signal
-           thrown by FiniteEnv to let tianshou know that ``collector.collect()`` should exit.
-        2. Notify the loggers that the collect is ready / done what it's ready / done.
-
-        Examples
-        --------
-        >>> with finite_env.collector_guard():
-        ...     collector.collect(n_episode=INF)
-        """
-        self._collector_guarded = True
-
-        for logger in self._logger:
-            logger.on_env_all_ready()
-
-        try:
-            yield self
-        except StopIteration:
-            pass
-        finally:
-            self._collector_guarded = False
-
-        # At last trigger the loggers
-        for logger in self._logger:
-            logger.on_env_all_done()
-
-    def reset(
-        self,
-        id: int | List[int] | np.ndarray | None = None,
-    ) -> np.ndarray:
-        assert not self._zombie
-
-        # Check whether it's guarded by collector_guard()
-        if not self._collector_guarded:
-            warnings.warn(
-                "Collector is not guarded by FiniteEnv. "
-                "This may cause unexpected problems, like unexpected StopIteration exception, "
-                "or missing logs.",
-                RuntimeWarning,
-            )
-
-        wrapped_id = self._wrap_id(id)
-        self._reset_alive_envs()
-
-        # ask super to reset alive envs and remap to current index
-        request_id = [i for i in wrapped_id if i in self._alive_env_ids]
-        obs = [None] * len(wrapped_id)
-        id2idx = {i: k for k, i in enumerate(wrapped_id)}
-        if request_id:
-            for i, o in zip(request_id, super().reset(request_id)):
-                obs[id2idx[i]] = self._postproc_env_obs(o)
-
-        for i, o in zip(wrapped_id, obs):
-            if o is None and i in self._alive_env_ids:
-                self._alive_env_ids.remove(i)
-
-        # logging
-        for i, o in zip(wrapped_id, obs):
-            if i in self._alive_env_ids:
-                for logger in self._logger:
-                    logger.on_env_reset(i, obs)
-
-        # fill empty observation with default(fake) observation
-        for o in obs:
-            self._set_default_obs(o)
-        for i, o in enumerate(obs):
-            if o is None:
-                obs[i] = self._get_default_obs()
-
-        if not self._alive_env_ids:
-            # comment this line so that the env becomes indispensable
-            # self.reset()
-            self._zombie = True
-            raise StopIteration
-
-        return np.stack(obs)
-
-    def step(
-        self,
-        action: np.ndarray,
-        id: int | List[int] | np.ndarray | None = None,
-    ) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
-        assert not self._zombie
-        wrapped_id = self._wrap_id(id)
-        id2idx = {i: k for k, i in enumerate(wrapped_id)}
-        request_id = list(filter(lambda i: i in self._alive_env_ids, wrapped_id))
-        result = [[None, None, False, None] for _ in range(len(wrapped_id))]
-
-        # ask super to step alive envs and remap to current index
-        if request_id:
-            valid_act = np.stack([action[id2idx[i]] for i in request_id])
-            for i, r in zip(request_id, zip(*super().step(valid_act, request_id))):
-                result[id2idx[i]] = list(r)
-                result[id2idx[i]][0] = self._postproc_env_obs(result[id2idx[i]][0])
-
-        # logging
-        for i, r in zip(wrapped_id, result):
-            if i in self._alive_env_ids:
-                for logger in self._logger:
-                    logger.on_env_step(i, *r)
-
-        # fill empty observation/info with default(fake)
-        for _, r, ___, i in result:
-            self._set_default_info(i)
-            self._set_default_rew(r)
-        for i, r in enumerate(result):
-            if r[0] is None:
-                result[i][0] = self._get_default_obs()
-            if r[1] is None:
-                result[i][1] = self._get_default_rew()
-            if r[3] is None:
-                result[i][3] = self._get_default_info()
-
-        ret = list(map(np.stack, zip(*result)))
-        return cast(Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray], ret)
-
-
-class FiniteDummyVectorEnv(FiniteVectorEnv, DummyVectorEnv):
-    pass
-
-
-class FiniteSubprocVectorEnv(FiniteVectorEnv, SubprocVectorEnv):
-    pass
-
-
-class FiniteShmemVectorEnv(FiniteVectorEnv, ShmemVectorEnv):
-    pass
-
-
-def vectorize_env(
-    env_factory: Callable[..., gym.Env],
-    env_type: FiniteEnvType,
-    concurrency: int,
-    logger: LogWriter | List[LogWriter],
-) -> FiniteVectorEnv:
-    """Helper function to create a vector env. Can be used to replace usual VectorEnv.
-
-    For example, once you wrote: ::
-
-        DummyVectorEnv([lambda: gym.make(task) for _ in range(env_num)])
-
-    Now you can replace it with: ::
-
-        finite_env_factory(lambda: gym.make(task), "dummy", env_num, my_logger)
-
-    By doing such replacement, you have two additional features enabled (compared to normal VectorEnv):
-
-    1. The vector env will check for NaN observation and kill the worker when its found.
-       See :class:`FiniteVectorEnv` for why we need this.
-    2. A logger to explicit collect logs from environment workers.
-
-    Parameters
-    ----------
-    env_factory
-        Callable to instantiate one single ``gym.Env``.
-        All concurrent workers will have the same ``env_factory``.
-    env_type
-        dummy or subproc or shmem. Corresponding to
-        `parallelism in tianshou <https://tianshou.readthedocs.io/en/master/api/tianshou.env.html#vectorenv>`_.
-    concurrency
-        Concurrent environment workers.
-    logger
-        Log writers.
-
-    Warnings
-    --------
-    Please do not use lambda expression here for ``env_factory`` as it may create incorrectly-shared instances.
-
-    Don't do: ::
-
-        vectorize_env(lambda: EnvWrapper(...), ...)
-
-    Please do: ::
-
-        def env_factory(): ...
-        vectorize_env(env_factory, ...)
-    """
-    env_type_cls_mapping: Dict[str, Type[FiniteVectorEnv]] = {
-        "dummy": FiniteDummyVectorEnv,
-        "subproc": FiniteSubprocVectorEnv,
-        "shmem": FiniteShmemVectorEnv,
-    }
-
-    finite_env_cls = env_type_cls_mapping[env_type]
-
-    return finite_env_cls(logger, [env_factory for _ in range(concurrency)])
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+"""
+This is to support finite env in vector env.
+See https://github.com/thu-ml/tianshou/issues/322 for details.
+"""
+
+from __future__ import annotations
+
+import copy
+import warnings
+from contextlib import contextmanager
+from typing import Any, Callable, Dict, Generator, List, Optional, Set, Tuple, Type, Union, cast
+
+import gym
+import numpy as np
+from tianshou.env import BaseVectorEnv, DummyVectorEnv, ShmemVectorEnv, SubprocVectorEnv
+
+from qlib.typehint import Literal
+
+from .log import LogWriter
+
+__all__ = [
+    "generate_nan_observation",
+    "check_nan_observation",
+    "FiniteVectorEnv",
+    "FiniteDummyVectorEnv",
+    "FiniteSubprocVectorEnv",
+    "FiniteShmemVectorEnv",
+    "FiniteEnvType",
+    "vectorize_env",
+]
+
+FiniteEnvType = Literal["dummy", "subproc", "shmem"]
+T = Union[dict, list, tuple, np.ndarray]
+
+
+def fill_invalid(obj: int | float | bool | T) -> T:
+    if isinstance(obj, (int, float, bool)):
+        return fill_invalid(np.array(obj))
+    if hasattr(obj, "dtype"):
+        if isinstance(obj, np.ndarray):
+            if np.issubdtype(obj.dtype, np.floating):
+                return np.full_like(obj, np.nan)
+            return np.full_like(obj, np.iinfo(obj.dtype).max)
+        # dealing with corner cases that numpy number is not supported by tianshou's sharray
+        return fill_invalid(np.array(obj))
+    elif isinstance(obj, dict):
+        return {k: fill_invalid(v) for k, v in obj.items()}
+    elif isinstance(obj, list):
+        return [fill_invalid(v) for v in obj]
+    elif isinstance(obj, tuple):
+        return tuple(fill_invalid(v) for v in obj)
+    raise ValueError(f"Unsupported value to fill with invalid: {obj}")
+
+
+def is_invalid(arr: int | float | bool | T) -> bool:
+    if isinstance(arr, np.ndarray):
+        if np.issubdtype(arr.dtype, np.floating):
+            return np.isnan(arr).all()
+        return cast(bool, cast(np.ndarray, np.iinfo(arr.dtype).max == arr).all())
+    if isinstance(arr, dict):
+        return all(is_invalid(o) for o in arr.values())
+    if isinstance(arr, (list, tuple)):
+        return all(is_invalid(o) for o in arr)
+    if isinstance(arr, (int, float, bool, np.number)):
+        return is_invalid(np.array(arr))
+    return True
+
+
+def generate_nan_observation(obs_space: gym.Space) -> Any:
+    """The NaN observation that indicates the environment receives no seed.
+
+    We assume that obs is complex and there must be something like float.
+    Otherwise this logic doesn't work.
+    """
+
+    sample = obs_space.sample()
+    sample = fill_invalid(sample)
+    return sample
+
+
+def check_nan_observation(obs: Any) -> bool:
+    """Check whether obs is generated by :func:`generate_nan_observation`."""
+    return is_invalid(obs)
+
+
+class FiniteVectorEnv(BaseVectorEnv):
+    """To allow the paralleled env workers consume a single DataQueue until it's exhausted.
+
+    See `tianshou issue #322 <https://github.com/thu-ml/tianshou/issues/322>`_.
+
+    The requirement is to make every possible seed (stored in :class:`qlib.rl.utils.DataQueue` in our case)
+    consumed by exactly one environment. This is not possible by tianshou's native VectorEnv and Collector,
+    because tianshou is unaware of this "exactly one" constraint, and might launch extra workers.
+
+    Consider a corner case, where concurrency is 2, but there is only one seed in DataQueue.
+    The reset of two workers must be both called according to the logic in collect.
+    The returned results of two workers are collected, regardless of what they are.
+    The problem is, one of the reset result must be invalid, or repeated,
+    because there's only one need in queue, and collector isn't aware of such situation.
+
+    Luckily, we can hack the vector env, and make a protocol between single env and vector env.
+    The single environment (should be :class:`qlib.rl.utils.EnvWrapper` in our case) is responsible for
+    reading from queue, and generate a special observation when the queue is exhausted. The special obs
+    is called "nan observation", because simply using none causes problems in shared-memory vector env.
+    :class:`FiniteVectorEnv` then read the observations from all workers, and select those non-nan
+    observation. It also maintains an ``_alive_env_ids`` to track which workers should never be
+    called again. When also the environments are exhausted, it will raise StopIteration exception.
+
+    The usage of this vector env in collector are two parts:
+
+    1. If the data queue is finite (usually when inference), collector should collect "infinity" number of
+       episodes, until the vector env exhausts by itself.
+    2. If the data queue is infinite (usually in training), collector can set number of episodes / steps.
+       In this case, data would be randomly ordered, and some repetitions wouldn't matter.
+
+    One extra function of this vector env is that it has a logger that explicitly collects logs
+    from child workers. See :class:`qlib.rl.utils.LogWriter`.
+    """
+
+    _logger: list[LogWriter]
+
+    def __init__(
+        self, logger: LogWriter | list[LogWriter] | None, env_fns: list[Callable[..., gym.Env]], **kwargs: Any
+    ) -> None:
+        super().__init__(env_fns, **kwargs)
+
+        if isinstance(logger, list):
+            self._logger = logger
+        elif isinstance(logger, LogWriter):
+            self._logger = [logger]
+        else:
+            self._logger = []
+        self._alive_env_ids: Set[int] = set()
+        self._reset_alive_envs()
+        self._default_obs = self._default_info = self._default_rew = None
+        self._zombie = False
+
+        self._collector_guarded: bool = False
+
+    def _reset_alive_envs(self) -> None:
+        if not self._alive_env_ids:
+            # starting or running out
+            self._alive_env_ids = set(range(self.env_num))
+
+    # to workaround with tianshou's buffer and batch
+    def _set_default_obs(self, obs: Any) -> None:
+        if obs is not None and self._default_obs is None:
+            self._default_obs = copy.deepcopy(obs)
+
+    def _set_default_info(self, info: Any) -> None:
+        if info is not None and self._default_info is None:
+            self._default_info = copy.deepcopy(info)
+
+    def _set_default_rew(self, rew: Any) -> None:
+        if rew is not None and self._default_rew is None:
+            self._default_rew = copy.deepcopy(rew)
+
+    def _get_default_obs(self) -> Any:
+        return copy.deepcopy(self._default_obs)
+
+    def _get_default_info(self) -> Any:
+        return copy.deepcopy(self._default_info)
+
+    def _get_default_rew(self) -> Any:
+        return copy.deepcopy(self._default_rew)
+
+    # END
+
+    @staticmethod
+    def _postproc_env_obs(obs: Any) -> Optional[Any]:
+        # reserved for shmem vector env to restore empty observation
+        if obs is None or check_nan_observation(obs):
+            return None
+        return obs
+
+    @contextmanager
+    def collector_guard(self) -> Generator[FiniteVectorEnv, None, None]:
+        """Guard the collector. Recommended to guard every collect.
+
+        This guard is for two purposes.
+
+        1. Catch and ignore the StopIteration exception, which is the stopping signal
+           thrown by FiniteEnv to let tianshou know that ``collector.collect()`` should exit.
+        2. Notify the loggers that the collect is ready / done what it's ready / done.
+
+        Examples
+        --------
+        >>> with finite_env.collector_guard():
+        ...     collector.collect(n_episode=INF)
+        """
+        self._collector_guarded = True
+
+        for logger in self._logger:
+            logger.on_env_all_ready()
+
+        try:
+            yield self
+        except StopIteration:
+            pass
+        finally:
+            self._collector_guarded = False
+
+        # At last trigger the loggers
+        for logger in self._logger:
+            logger.on_env_all_done()
+
+    def reset(
+        self,
+        id: int | List[int] | np.ndarray | None = None,
+    ) -> np.ndarray:
+        assert not self._zombie
+
+        # Check whether it's guarded by collector_guard()
+        if not self._collector_guarded:
+            warnings.warn(
+                "Collector is not guarded by FiniteEnv. "
+                "This may cause unexpected problems, like unexpected StopIteration exception, "
+                "or missing logs.",
+                RuntimeWarning,
+            )
+
+        wrapped_id = self._wrap_id(id)
+        self._reset_alive_envs()
+
+        # ask super to reset alive envs and remap to current index
+        request_id = [i for i in wrapped_id if i in self._alive_env_ids]
+        obs = [None] * len(wrapped_id)
+        id2idx = {i: k for k, i in enumerate(wrapped_id)}
+        if request_id:
+            for i, o in zip(request_id, super().reset(request_id)):
+                obs[id2idx[i]] = self._postproc_env_obs(o)
+
+        for i, o in zip(wrapped_id, obs):
+            if o is None and i in self._alive_env_ids:
+                self._alive_env_ids.remove(i)
+
+        # logging
+        for i, o in zip(wrapped_id, obs):
+            if i in self._alive_env_ids:
+                for logger in self._logger:
+                    logger.on_env_reset(i, obs)
+
+        # fill empty observation with default(fake) observation
+        for o in obs:
+            self._set_default_obs(o)
+        for i, o in enumerate(obs):
+            if o is None:
+                obs[i] = self._get_default_obs()
+
+        if not self._alive_env_ids:
+            # comment this line so that the env becomes indispensable
+            # self.reset()
+            self._zombie = True
+            raise StopIteration
+
+        return np.stack(obs)
+
+    def step(
+        self,
+        action: np.ndarray,
+        id: int | List[int] | np.ndarray | None = None,
+    ) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
+        assert not self._zombie
+        wrapped_id = self._wrap_id(id)
+        id2idx = {i: k for k, i in enumerate(wrapped_id)}
+        request_id = list(filter(lambda i: i in self._alive_env_ids, wrapped_id))
+        result = [[None, None, False, None] for _ in range(len(wrapped_id))]
+
+        # ask super to step alive envs and remap to current index
+        if request_id:
+            valid_act = np.stack([action[id2idx[i]] for i in request_id])
+            for i, r in zip(request_id, zip(*super().step(valid_act, request_id))):
+                result[id2idx[i]] = list(r)
+                result[id2idx[i]][0] = self._postproc_env_obs(result[id2idx[i]][0])
+
+        # logging
+        for i, r in zip(wrapped_id, result):
+            if i in self._alive_env_ids:
+                for logger in self._logger:
+                    logger.on_env_step(i, *r)
+
+        # fill empty observation/info with default(fake)
+        for _, r, ___, i in result:
+            self._set_default_info(i)
+            self._set_default_rew(r)
+        for i, r in enumerate(result):
+            if r[0] is None:
+                result[i][0] = self._get_default_obs()
+            if r[1] is None:
+                result[i][1] = self._get_default_rew()
+            if r[3] is None:
+                result[i][3] = self._get_default_info()
+
+        ret = list(map(np.stack, zip(*result)))
+        return cast(Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray], ret)
+
+
+class FiniteDummyVectorEnv(FiniteVectorEnv, DummyVectorEnv):
+    pass
+
+
+class FiniteSubprocVectorEnv(FiniteVectorEnv, SubprocVectorEnv):
+    pass
+
+
+class FiniteShmemVectorEnv(FiniteVectorEnv, ShmemVectorEnv):
+    pass
+
+
+def vectorize_env(
+    env_factory: Callable[..., gym.Env],
+    env_type: FiniteEnvType,
+    concurrency: int,
+    logger: LogWriter | List[LogWriter],
+) -> FiniteVectorEnv:
+    """Helper function to create a vector env. Can be used to replace usual VectorEnv.
+
+    For example, once you wrote: ::
+
+        DummyVectorEnv([lambda: gym.make(task) for _ in range(env_num)])
+
+    Now you can replace it with: ::
+
+        finite_env_factory(lambda: gym.make(task), "dummy", env_num, my_logger)
+
+    By doing such replacement, you have two additional features enabled (compared to normal VectorEnv):
+
+    1. The vector env will check for NaN observation and kill the worker when its found.
+       See :class:`FiniteVectorEnv` for why we need this.
+    2. A logger to explicit collect logs from environment workers.
+
+    Parameters
+    ----------
+    env_factory
+        Callable to instantiate one single ``gym.Env``.
+        All concurrent workers will have the same ``env_factory``.
+    env_type
+        dummy or subproc or shmem. Corresponding to
+        `parallelism in tianshou <https://tianshou.readthedocs.io/en/master/api/tianshou.env.html#vectorenv>`_.
+    concurrency
+        Concurrent environment workers.
+    logger
+        Log writers.
+
+    Warnings
+    --------
+    Please do not use lambda expression here for ``env_factory`` as it may create incorrectly-shared instances.
+
+    Don't do: ::
+
+        vectorize_env(lambda: EnvWrapper(...), ...)
+
+    Please do: ::
+
+        def env_factory(): ...
+        vectorize_env(env_factory, ...)
+    """
+    env_type_cls_mapping: Dict[str, Type[FiniteVectorEnv]] = {
+        "dummy": FiniteDummyVectorEnv,
+        "subproc": FiniteSubprocVectorEnv,
+        "shmem": FiniteShmemVectorEnv,
+    }
+
+    finite_env_cls = env_type_cls_mapping[env_type]
+
+    return finite_env_cls(logger, [env_factory for _ in range(concurrency)])
```

## qlib/rl/utils/log.py

 * *Ordering differences only*

```diff
@@ -1,523 +1,523 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-"""Distributed logger for RL.
-
-:class:`LogCollector` runs in every environment workers. It collects log info from simulator states,
-and add them (as a dict) to auxiliary info returned for each step.
-
-:class:`LogWriter` runs in the central worker. It decodes the dict collected by :class:`LogCollector`
-in each worker, and writes them to console, log files, or tensorboard...
-
-The two modules communicate by the "log" field in "info" returned by ``env.step()``.
-"""
-
-# NOTE: This file contains many hardcoded / ad-hoc rules.
-# Refactoring it will be one of the future tasks.
-
-from __future__ import annotations
-
-import logging
-from collections import defaultdict
-from enum import IntEnum
-from pathlib import Path
-from typing import TYPE_CHECKING, Any, Callable, Dict, Generic, List, Sequence, Set, Tuple, TypeVar
-
-import numpy as np
-import pandas as pd
-
-from qlib.log import get_module_logger
-
-if TYPE_CHECKING:
-    from .env_wrapper import InfoDict
-
-
-__all__ = ["LogCollector", "LogWriter", "LogLevel", "LogBuffer", "ConsoleWriter", "CsvWriter"]
-
-ObsType = TypeVar("ObsType")
-ActType = TypeVar("ActType")
-
-
-class LogLevel(IntEnum):
-    """Log-levels for RL training.
-    The behavior of handling each log level depends on the implementation of :class:`LogWriter`.
-    """
-
-    DEBUG = 10
-    """If you only want to see the metric in debug mode."""
-    PERIODIC = 20
-    """If you want to see the metric periodically."""
-    # FIXME: I haven't given much thought about this. Let's hold it for one iteration.
-
-    INFO = 30
-    """Important log messages."""
-    CRITICAL = 40
-    """LogWriter should always handle CRITICAL messages"""
-
-
-class LogCollector:
-    """Logs are first collected in each environment worker,
-    and then aggregated to stream at the central thread in vector env.
-
-    In :class:`LogCollector`, every metric is added to a dict, which needs to be ``reset()`` at each step.
-    The dict is sent via the ``info`` in ``env.step()``, and decoded by the :class:`LogWriter` at vector env.
-
-    ``min_loglevel`` is for optimization purposes: to avoid too much traffic on networks / in pipe.
-    """
-
-    _logged: Dict[str, Tuple[int, Any]]
-    _min_loglevel: int
-
-    def __init__(self, min_loglevel: int | LogLevel = LogLevel.PERIODIC) -> None:
-        self._min_loglevel = int(min_loglevel)
-
-    def reset(self) -> None:
-        """Clear all collected contents."""
-        self._logged = {}
-
-    def _add_metric(self, name: str, metric: Any, loglevel: int | LogLevel) -> None:
-        if name in self._logged:
-            raise ValueError(f"A metric with {name} is already added. Please change a name or reset the log collector.")
-        self._logged[name] = (int(loglevel), metric)
-
-    def add_string(self, name: str, string: str, loglevel: int | LogLevel = LogLevel.PERIODIC) -> None:
-        """Add a string with name into logged contents."""
-        if loglevel < self._min_loglevel:
-            return
-        if not isinstance(string, str):
-            raise TypeError(f"{string} is not a string.")
-        self._add_metric(name, string, loglevel)
-
-    def add_scalar(self, name: str, scalar: Any, loglevel: int | LogLevel = LogLevel.PERIODIC) -> None:
-        """Add a scalar with name into logged contents.
-        Scalar will be converted into a float.
-        """
-        if loglevel < self._min_loglevel:
-            return
-
-        if hasattr(scalar, "item"):
-            # could be single-item number
-            scalar = scalar.item()
-        if not isinstance(scalar, (float, int)):
-            raise TypeError(f"{scalar} is not and can not be converted into float or integer.")
-        scalar = float(scalar)
-        self._add_metric(name, scalar, loglevel)
-
-    def add_array(
-        self,
-        name: str,
-        array: np.ndarray | pd.DataFrame | pd.Series,
-        loglevel: int | LogLevel = LogLevel.PERIODIC,
-    ) -> None:
-        """Add an array with name into logging."""
-        if loglevel < self._min_loglevel:
-            return
-
-        if not isinstance(array, (np.ndarray, pd.DataFrame, pd.Series)):
-            raise TypeError(f"{array} is not one of ndarray, DataFrame and Series.")
-        self._add_metric(name, array, loglevel)
-
-    def add_any(self, name: str, obj: Any, loglevel: int | LogLevel = LogLevel.PERIODIC) -> None:
-        """Log something with any type.
-
-        As it's an "any" object, the only LogWriter accepting it is pickle.
-        Therefore, pickle must be able to serialize it.
-        """
-        if loglevel < self._min_loglevel:
-            return
-
-        # FIXME: detect and rescue object that could be scalar or array
-
-        self._add_metric(name, obj, loglevel)
-
-    def logs(self) -> Dict[str, np.ndarray]:
-        return {key: np.asanyarray(value, dtype="object") for key, value in self._logged.items()}
-
-
-class LogWriter(Generic[ObsType, ActType]):
-    """Base class for log writers, triggered at every reset and step by finite env.
-
-    What to do with a specific log depends on the implementation of subclassing :class:`LogWriter`.
-    The general principle is that, it should handle logs above its loglevel (inclusive),
-    and discard logs that are not acceptable. For instance, console loggers obviously can't handle an image.
-    """
-
-    episode_count: int
-    """Counter of episodes."""
-
-    step_count: int
-    """Counter of steps."""
-
-    global_step: int
-    """Counter of steps. Won"t be cleared in ``clear``."""
-
-    global_episode: int
-    """Counter of episodes. Won"t be cleared in ``clear``."""
-
-    active_env_ids: Set[int]
-    """Active environment ids in vector env."""
-
-    episode_lengths: Dict[int, int]
-    """Map from environment id to episode length."""
-
-    episode_rewards: Dict[int, List[float]]
-    """Map from environment id to episode total reward."""
-
-    episode_logs: Dict[int, list]
-    """Map from environment id to episode logs."""
-
-    def __init__(self, loglevel: int | LogLevel = LogLevel.PERIODIC) -> None:
-        self.loglevel = loglevel
-
-        self.global_step = 0
-        self.global_episode = 0
-
-        # Information, logs of one episode is stored here.
-        # This assumes that episode is not too long to fit into the memory.
-        self.episode_lengths = dict()
-        self.episode_rewards = dict()
-        self.episode_logs = dict()
-
-        self.clear()
-
-    def clear(self):
-        """Clear all the metrics for a fresh start.
-        To make the logger instance reusable.
-        """
-        self.episode_count = self.step_count = 0
-        self.active_env_ids = set()
-
-    def state_dict(self) -> dict:
-        """Save the states of the logger to a dict."""
-        return {
-            "episode_count": self.episode_count,
-            "step_count": self.step_count,
-            "global_step": self.global_step,
-            "global_episode": self.global_episode,
-            "active_env_ids": self.active_env_ids,
-            "episode_lengths": self.episode_lengths,
-            "episode_rewards": self.episode_rewards,
-            "episode_logs": self.episode_logs,
-        }
-
-    def load_state_dict(self, state_dict: dict) -> None:
-        """Load the states of current logger from a dict."""
-        self.episode_count = state_dict["episode_count"]
-        self.step_count = state_dict["step_count"]
-        self.global_step = state_dict["global_step"]
-        self.global_episode = state_dict["global_episode"]
-
-        # These are runtime infos.
-        # Though they are loaded, I don't think it really helps.
-        self.active_env_ids = state_dict["active_env_ids"]
-        self.episode_lengths = state_dict["episode_lengths"]
-        self.episode_rewards = state_dict["episode_rewards"]
-        self.episode_logs = state_dict["episode_logs"]
-
-    @staticmethod
-    def aggregation(array: Sequence[Any], name: str | None = None) -> Any:
-        """Aggregation function from step-wise to episode-wise.
-
-        If it's a sequence of float, take the mean.
-        Otherwise, take the first element.
-
-        If a name is specified and,
-
-        - if it's ``reward``, the reduction will be sum.
-        """
-        assert len(array) > 0, "The aggregated array must be not empty."
-        if all(isinstance(v, float) for v in array):
-            if name == "reward":
-                return np.sum(array)
-            return np.mean(array)
-        else:
-            return array[0]
-
-    def log_episode(self, length: int, rewards: List[float], contents: List[Dict[str, Any]]) -> None:
-        """This is triggered at the end of each trajectory.
-
-        Parameters
-        ----------
-        length
-            Length of this trajectory.
-        rewards
-            A list of rewards at each step of this episode.
-        contents
-            Logged contents for every step.
-        """
-
-    def log_step(self, reward: float, contents: Dict[str, Any]) -> None:
-        """This is triggered at each step.
-
-        Parameters
-        ----------
-        reward
-            Reward for this step.
-        contents
-            Logged contents for this step.
-        """
-
-    def on_env_step(self, env_id: int, obs: ObsType, rew: float, done: bool, info: InfoDict) -> None:
-        """Callback for finite env, on each step."""
-
-        # Update counter
-        self.global_step += 1
-        self.step_count += 1
-
-        self.active_env_ids.add(env_id)
-        self.episode_lengths[env_id] += 1
-        # TODO: reward can be a list of list for MARL
-        self.episode_rewards[env_id].append(rew)
-
-        values: Dict[str, Any] = {}
-
-        for key, (loglevel, value) in info["log"].items():
-            if loglevel >= self.loglevel:  # FIXME: this is actually incorrect (see last FIXME)
-                values[key] = value
-        self.episode_logs[env_id].append(values)
-
-        self.log_step(rew, values)
-
-        if done:
-            # Update counter
-            self.global_episode += 1
-            self.episode_count += 1
-
-            self.log_episode(self.episode_lengths[env_id], self.episode_rewards[env_id], self.episode_logs[env_id])
-
-    def on_env_reset(self, env_id: int, _: ObsType) -> None:
-        """Callback for finite env.
-
-        Reset episode statistics. Nothing task-specific is logged here because of
-        `a limitation of tianshou <https://github.com/thu-ml/tianshou/issues/605>`__.
-        """
-        self.episode_lengths[env_id] = 0
-        self.episode_rewards[env_id] = []
-        self.episode_logs[env_id] = []
-
-    def on_env_all_ready(self) -> None:
-        """When all environments are ready to run.
-        Usually, loggers should be reset here.
-        """
-        self.clear()
-
-    def on_env_all_done(self) -> None:
-        """All done. Time for cleanup."""
-
-
-class LogBuffer(LogWriter):
-    """Keep all numbers in memory.
-
-    Objects that can't be aggregated like strings, tensors, images can't be stored in the buffer.
-    To persist them, please use :class:`PickleWriter`.
-
-    Every time, Log buffer receives a new metric, the callback is triggered,
-    which is useful when tracking metrics inside a trainer.
-
-    Parameters
-    ----------
-    callback
-        A callback receiving three arguments:
-
-        - on_episode: Whether it's called at the end of an episode
-        - on_collect: Whether it's called at the end of a collect
-        - log_buffer: the :class:`LogBbuffer` object
-
-        No return value is expected.
-    """
-
-    # FIXME: needs a metric count
-
-    def __init__(self, callback: Callable[[bool, bool, LogBuffer], None], loglevel: int | LogLevel = LogLevel.PERIODIC):
-        super().__init__(loglevel)
-        self.callback = callback
-
-    def state_dict(self) -> dict:
-        return {
-            **super().state_dict(),
-            "latest_metrics": self._latest_metrics,
-            "aggregated_metrics": self._aggregated_metrics,
-        }
-
-    def load_state_dict(self, state_dict: dict) -> None:
-        self._latest_metrics = state_dict["latest_metrics"]
-        self._aggregated_metrics = state_dict["aggregated_metrics"]
-        return super().load_state_dict(state_dict)
-
-    def clear(self):
-        super().clear()
-        self._latest_metrics: dict[str, float] | None = None
-        self._aggregated_metrics: dict[str, float] = defaultdict(float)
-
-    def log_episode(self, length: int, rewards: list[float], contents: list[dict[str, Any]]) -> None:
-        # FIXME Dup of ConsoleWriter
-        episode_wise_contents: dict[str, list] = defaultdict(list)
-        for step_contents in contents:
-            for name, value in step_contents.items():
-                # FIXME This could be false-negative for some numpy types
-                if isinstance(value, float):
-                    episode_wise_contents[name].append(value)
-
-        logs: dict[str, float] = {}
-        for name, values in episode_wise_contents.items():
-            logs[name] = self.aggregation(values, name)  # type: ignore
-            self._aggregated_metrics[name] += logs[name]
-
-        self._latest_metrics = logs
-
-        self.callback(True, False, self)
-
-    def on_env_all_done(self) -> None:
-        # This happens when collect exits
-        self.callback(False, True, self)
-
-    def episode_metrics(self) -> dict[str, float]:
-        """Retrieve the numeric metrics of the latest episode."""
-        if self._latest_metrics is None:
-            raise ValueError("No episode metrics available yet.")
-        return self._latest_metrics
-
-    def collect_metrics(self) -> dict[str, float]:
-        """Retrieve the aggregated metrics of the latest collect."""
-        return {name: value / self.episode_count for name, value in self._aggregated_metrics.items()}
-
-
-class ConsoleWriter(LogWriter):
-    """Write log messages to console periodically.
-
-    It tracks an average meter for each metric, which is the average value since last ``clear()`` till now.
-    The display format for each metric is ``<name> <latest_value> (<average_value>)``.
-
-    Non-single-number metrics are auto skipped.
-    """
-
-    prefix: str
-    """Prefix can be set via ``writer.prefix``."""
-
-    def __init__(
-        self,
-        log_every_n_episode: int = 20,
-        total_episodes: int | None = None,
-        float_format: str = ":.4f",
-        counter_format: str = ":4d",
-        loglevel: int | LogLevel = LogLevel.PERIODIC,
-    ) -> None:
-        super().__init__(loglevel)
-        # TODO: support log_every_n_step
-        self.log_every_n_episode = log_every_n_episode
-        self.total_episodes = total_episodes
-
-        self.counter_format = counter_format
-        self.float_format = float_format
-
-        self.prefix = ""
-
-        self.console_logger = get_module_logger(__name__, level=logging.INFO)
-
-    # FIXME: save & reload
-
-    def clear(self) -> None:
-        super().clear()
-        # Clear average meters
-        self.metric_counts: Dict[str, int] = defaultdict(int)
-        self.metric_sums: Dict[str, float] = defaultdict(float)
-
-    def log_episode(self, length: int, rewards: List[float], contents: List[Dict[str, Any]]) -> None:
-        # Aggregate step-wise to episode-wise
-        episode_wise_contents: Dict[str, list] = defaultdict(list)
-
-        for step_contents in contents:
-            for name, value in step_contents.items():
-                if isinstance(value, float):
-                    episode_wise_contents[name].append(value)
-
-        # Generate log contents and track them in average-meter.
-        # This should be done at every step, regardless of periodic or not.
-        logs: Dict[str, float] = {}
-        for name, values in episode_wise_contents.items():
-            logs[name] = self.aggregation(values, name)  # type: ignore
-
-        for name, value in logs.items():
-            self.metric_counts[name] += 1
-            self.metric_sums[name] += value
-
-        if self.episode_count % self.log_every_n_episode == 0 or self.episode_count == self.total_episodes:
-            # Only log periodically or at the end
-            self.console_logger.info(self.generate_log_message(logs))
-
-    def generate_log_message(self, logs: Dict[str, float]) -> str:
-        if self.prefix:
-            msg_prefix = self.prefix + " "
-        else:
-            msg_prefix = ""
-        if self.total_episodes is None:
-            msg_prefix += "[Step {" + self.counter_format + "}]"
-        else:
-            msg_prefix += "[{" + self.counter_format + "}/" + str(self.total_episodes) + "]"
-        msg_prefix = msg_prefix.format(self.episode_count)
-
-        msg = ""
-        for name, value in logs.items():
-            # Double-space as delimiter
-            format_template = r"  {} {" + self.float_format + "} ({" + self.float_format + "})"
-            msg += format_template.format(name, value, self.metric_sums[name] / self.metric_counts[name])
-
-        msg = msg_prefix + " " + msg
-
-        return msg
-
-
-class CsvWriter(LogWriter):
-    """Dump all episode metrics to a ``result.csv``.
-
-    This is not the correct implementation. It's only used for first iteration.
-    """
-
-    SUPPORTED_TYPES = (float, str, pd.Timestamp)
-
-    all_records: List[Dict[str, Any]]
-
-    # FIXME: save & reload
-
-    def __init__(self, output_dir: Path, loglevel: int | LogLevel = LogLevel.PERIODIC) -> None:
-        super().__init__(loglevel)
-        self.output_dir = output_dir
-        self.output_dir.mkdir(exist_ok=True)
-
-    def clear(self) -> None:
-        super().clear()
-        self.all_records = []
-
-    def log_episode(self, length: int, rewards: List[float], contents: List[Dict[str, Any]]) -> None:
-        # FIXME Same as ConsoleLogger, needs a refactor to eliminate code-dup
-        episode_wise_contents: Dict[str, list] = defaultdict(list)
-
-        for step_contents in contents:
-            for name, value in step_contents.items():
-                if isinstance(value, self.SUPPORTED_TYPES):
-                    episode_wise_contents[name].append(value)
-
-        logs: Dict[str, float] = {}
-        for name, values in episode_wise_contents.items():
-            logs[name] = self.aggregation(values, name)  # type: ignore
-
-        self.all_records.append(logs)
-
-    def on_env_all_done(self) -> None:
-        # FIXME: this is temporary
-        pd.DataFrame.from_records(self.all_records).to_csv(self.output_dir / "result.csv", index=False)
-
-
-# The following are not implemented yet.
-
-
-class PickleWriter(LogWriter):
-    """Dump logs to pickle files."""
-
-
-class TensorboardWriter(LogWriter):
-    """Write logs to event files that can be visualized with tensorboard."""
-
-
-class MlflowWriter(LogWriter):
-    """Add logs to mlflow."""
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+"""Distributed logger for RL.
+
+:class:`LogCollector` runs in every environment workers. It collects log info from simulator states,
+and add them (as a dict) to auxiliary info returned for each step.
+
+:class:`LogWriter` runs in the central worker. It decodes the dict collected by :class:`LogCollector`
+in each worker, and writes them to console, log files, or tensorboard...
+
+The two modules communicate by the "log" field in "info" returned by ``env.step()``.
+"""
+
+# NOTE: This file contains many hardcoded / ad-hoc rules.
+# Refactoring it will be one of the future tasks.
+
+from __future__ import annotations
+
+import logging
+from collections import defaultdict
+from enum import IntEnum
+from pathlib import Path
+from typing import TYPE_CHECKING, Any, Callable, Dict, Generic, List, Sequence, Set, Tuple, TypeVar
+
+import numpy as np
+import pandas as pd
+
+from qlib.log import get_module_logger
+
+if TYPE_CHECKING:
+    from .env_wrapper import InfoDict
+
+
+__all__ = ["LogCollector", "LogWriter", "LogLevel", "LogBuffer", "ConsoleWriter", "CsvWriter"]
+
+ObsType = TypeVar("ObsType")
+ActType = TypeVar("ActType")
+
+
+class LogLevel(IntEnum):
+    """Log-levels for RL training.
+    The behavior of handling each log level depends on the implementation of :class:`LogWriter`.
+    """
+
+    DEBUG = 10
+    """If you only want to see the metric in debug mode."""
+    PERIODIC = 20
+    """If you want to see the metric periodically."""
+    # FIXME: I haven't given much thought about this. Let's hold it for one iteration.
+
+    INFO = 30
+    """Important log messages."""
+    CRITICAL = 40
+    """LogWriter should always handle CRITICAL messages"""
+
+
+class LogCollector:
+    """Logs are first collected in each environment worker,
+    and then aggregated to stream at the central thread in vector env.
+
+    In :class:`LogCollector`, every metric is added to a dict, which needs to be ``reset()`` at each step.
+    The dict is sent via the ``info`` in ``env.step()``, and decoded by the :class:`LogWriter` at vector env.
+
+    ``min_loglevel`` is for optimization purposes: to avoid too much traffic on networks / in pipe.
+    """
+
+    _logged: Dict[str, Tuple[int, Any]]
+    _min_loglevel: int
+
+    def __init__(self, min_loglevel: int | LogLevel = LogLevel.PERIODIC) -> None:
+        self._min_loglevel = int(min_loglevel)
+
+    def reset(self) -> None:
+        """Clear all collected contents."""
+        self._logged = {}
+
+    def _add_metric(self, name: str, metric: Any, loglevel: int | LogLevel) -> None:
+        if name in self._logged:
+            raise ValueError(f"A metric with {name} is already added. Please change a name or reset the log collector.")
+        self._logged[name] = (int(loglevel), metric)
+
+    def add_string(self, name: str, string: str, loglevel: int | LogLevel = LogLevel.PERIODIC) -> None:
+        """Add a string with name into logged contents."""
+        if loglevel < self._min_loglevel:
+            return
+        if not isinstance(string, str):
+            raise TypeError(f"{string} is not a string.")
+        self._add_metric(name, string, loglevel)
+
+    def add_scalar(self, name: str, scalar: Any, loglevel: int | LogLevel = LogLevel.PERIODIC) -> None:
+        """Add a scalar with name into logged contents.
+        Scalar will be converted into a float.
+        """
+        if loglevel < self._min_loglevel:
+            return
+
+        if hasattr(scalar, "item"):
+            # could be single-item number
+            scalar = scalar.item()
+        if not isinstance(scalar, (float, int)):
+            raise TypeError(f"{scalar} is not and can not be converted into float or integer.")
+        scalar = float(scalar)
+        self._add_metric(name, scalar, loglevel)
+
+    def add_array(
+        self,
+        name: str,
+        array: np.ndarray | pd.DataFrame | pd.Series,
+        loglevel: int | LogLevel = LogLevel.PERIODIC,
+    ) -> None:
+        """Add an array with name into logging."""
+        if loglevel < self._min_loglevel:
+            return
+
+        if not isinstance(array, (np.ndarray, pd.DataFrame, pd.Series)):
+            raise TypeError(f"{array} is not one of ndarray, DataFrame and Series.")
+        self._add_metric(name, array, loglevel)
+
+    def add_any(self, name: str, obj: Any, loglevel: int | LogLevel = LogLevel.PERIODIC) -> None:
+        """Log something with any type.
+
+        As it's an "any" object, the only LogWriter accepting it is pickle.
+        Therefore, pickle must be able to serialize it.
+        """
+        if loglevel < self._min_loglevel:
+            return
+
+        # FIXME: detect and rescue object that could be scalar or array
+
+        self._add_metric(name, obj, loglevel)
+
+    def logs(self) -> Dict[str, np.ndarray]:
+        return {key: np.asanyarray(value, dtype="object") for key, value in self._logged.items()}
+
+
+class LogWriter(Generic[ObsType, ActType]):
+    """Base class for log writers, triggered at every reset and step by finite env.
+
+    What to do with a specific log depends on the implementation of subclassing :class:`LogWriter`.
+    The general principle is that, it should handle logs above its loglevel (inclusive),
+    and discard logs that are not acceptable. For instance, console loggers obviously can't handle an image.
+    """
+
+    episode_count: int
+    """Counter of episodes."""
+
+    step_count: int
+    """Counter of steps."""
+
+    global_step: int
+    """Counter of steps. Won"t be cleared in ``clear``."""
+
+    global_episode: int
+    """Counter of episodes. Won"t be cleared in ``clear``."""
+
+    active_env_ids: Set[int]
+    """Active environment ids in vector env."""
+
+    episode_lengths: Dict[int, int]
+    """Map from environment id to episode length."""
+
+    episode_rewards: Dict[int, List[float]]
+    """Map from environment id to episode total reward."""
+
+    episode_logs: Dict[int, list]
+    """Map from environment id to episode logs."""
+
+    def __init__(self, loglevel: int | LogLevel = LogLevel.PERIODIC) -> None:
+        self.loglevel = loglevel
+
+        self.global_step = 0
+        self.global_episode = 0
+
+        # Information, logs of one episode is stored here.
+        # This assumes that episode is not too long to fit into the memory.
+        self.episode_lengths = dict()
+        self.episode_rewards = dict()
+        self.episode_logs = dict()
+
+        self.clear()
+
+    def clear(self):
+        """Clear all the metrics for a fresh start.
+        To make the logger instance reusable.
+        """
+        self.episode_count = self.step_count = 0
+        self.active_env_ids = set()
+
+    def state_dict(self) -> dict:
+        """Save the states of the logger to a dict."""
+        return {
+            "episode_count": self.episode_count,
+            "step_count": self.step_count,
+            "global_step": self.global_step,
+            "global_episode": self.global_episode,
+            "active_env_ids": self.active_env_ids,
+            "episode_lengths": self.episode_lengths,
+            "episode_rewards": self.episode_rewards,
+            "episode_logs": self.episode_logs,
+        }
+
+    def load_state_dict(self, state_dict: dict) -> None:
+        """Load the states of current logger from a dict."""
+        self.episode_count = state_dict["episode_count"]
+        self.step_count = state_dict["step_count"]
+        self.global_step = state_dict["global_step"]
+        self.global_episode = state_dict["global_episode"]
+
+        # These are runtime infos.
+        # Though they are loaded, I don't think it really helps.
+        self.active_env_ids = state_dict["active_env_ids"]
+        self.episode_lengths = state_dict["episode_lengths"]
+        self.episode_rewards = state_dict["episode_rewards"]
+        self.episode_logs = state_dict["episode_logs"]
+
+    @staticmethod
+    def aggregation(array: Sequence[Any], name: str | None = None) -> Any:
+        """Aggregation function from step-wise to episode-wise.
+
+        If it's a sequence of float, take the mean.
+        Otherwise, take the first element.
+
+        If a name is specified and,
+
+        - if it's ``reward``, the reduction will be sum.
+        """
+        assert len(array) > 0, "The aggregated array must be not empty."
+        if all(isinstance(v, float) for v in array):
+            if name == "reward":
+                return np.sum(array)
+            return np.mean(array)
+        else:
+            return array[0]
+
+    def log_episode(self, length: int, rewards: List[float], contents: List[Dict[str, Any]]) -> None:
+        """This is triggered at the end of each trajectory.
+
+        Parameters
+        ----------
+        length
+            Length of this trajectory.
+        rewards
+            A list of rewards at each step of this episode.
+        contents
+            Logged contents for every step.
+        """
+
+    def log_step(self, reward: float, contents: Dict[str, Any]) -> None:
+        """This is triggered at each step.
+
+        Parameters
+        ----------
+        reward
+            Reward for this step.
+        contents
+            Logged contents for this step.
+        """
+
+    def on_env_step(self, env_id: int, obs: ObsType, rew: float, done: bool, info: InfoDict) -> None:
+        """Callback for finite env, on each step."""
+
+        # Update counter
+        self.global_step += 1
+        self.step_count += 1
+
+        self.active_env_ids.add(env_id)
+        self.episode_lengths[env_id] += 1
+        # TODO: reward can be a list of list for MARL
+        self.episode_rewards[env_id].append(rew)
+
+        values: Dict[str, Any] = {}
+
+        for key, (loglevel, value) in info["log"].items():
+            if loglevel >= self.loglevel:  # FIXME: this is actually incorrect (see last FIXME)
+                values[key] = value
+        self.episode_logs[env_id].append(values)
+
+        self.log_step(rew, values)
+
+        if done:
+            # Update counter
+            self.global_episode += 1
+            self.episode_count += 1
+
+            self.log_episode(self.episode_lengths[env_id], self.episode_rewards[env_id], self.episode_logs[env_id])
+
+    def on_env_reset(self, env_id: int, _: ObsType) -> None:
+        """Callback for finite env.
+
+        Reset episode statistics. Nothing task-specific is logged here because of
+        `a limitation of tianshou <https://github.com/thu-ml/tianshou/issues/605>`__.
+        """
+        self.episode_lengths[env_id] = 0
+        self.episode_rewards[env_id] = []
+        self.episode_logs[env_id] = []
+
+    def on_env_all_ready(self) -> None:
+        """When all environments are ready to run.
+        Usually, loggers should be reset here.
+        """
+        self.clear()
+
+    def on_env_all_done(self) -> None:
+        """All done. Time for cleanup."""
+
+
+class LogBuffer(LogWriter):
+    """Keep all numbers in memory.
+
+    Objects that can't be aggregated like strings, tensors, images can't be stored in the buffer.
+    To persist them, please use :class:`PickleWriter`.
+
+    Every time, Log buffer receives a new metric, the callback is triggered,
+    which is useful when tracking metrics inside a trainer.
+
+    Parameters
+    ----------
+    callback
+        A callback receiving three arguments:
+
+        - on_episode: Whether it's called at the end of an episode
+        - on_collect: Whether it's called at the end of a collect
+        - log_buffer: the :class:`LogBbuffer` object
+
+        No return value is expected.
+    """
+
+    # FIXME: needs a metric count
+
+    def __init__(self, callback: Callable[[bool, bool, LogBuffer], None], loglevel: int | LogLevel = LogLevel.PERIODIC):
+        super().__init__(loglevel)
+        self.callback = callback
+
+    def state_dict(self) -> dict:
+        return {
+            **super().state_dict(),
+            "latest_metrics": self._latest_metrics,
+            "aggregated_metrics": self._aggregated_metrics,
+        }
+
+    def load_state_dict(self, state_dict: dict) -> None:
+        self._latest_metrics = state_dict["latest_metrics"]
+        self._aggregated_metrics = state_dict["aggregated_metrics"]
+        return super().load_state_dict(state_dict)
+
+    def clear(self):
+        super().clear()
+        self._latest_metrics: dict[str, float] | None = None
+        self._aggregated_metrics: dict[str, float] = defaultdict(float)
+
+    def log_episode(self, length: int, rewards: list[float], contents: list[dict[str, Any]]) -> None:
+        # FIXME Dup of ConsoleWriter
+        episode_wise_contents: dict[str, list] = defaultdict(list)
+        for step_contents in contents:
+            for name, value in step_contents.items():
+                # FIXME This could be false-negative for some numpy types
+                if isinstance(value, float):
+                    episode_wise_contents[name].append(value)
+
+        logs: dict[str, float] = {}
+        for name, values in episode_wise_contents.items():
+            logs[name] = self.aggregation(values, name)  # type: ignore
+            self._aggregated_metrics[name] += logs[name]
+
+        self._latest_metrics = logs
+
+        self.callback(True, False, self)
+
+    def on_env_all_done(self) -> None:
+        # This happens when collect exits
+        self.callback(False, True, self)
+
+    def episode_metrics(self) -> dict[str, float]:
+        """Retrieve the numeric metrics of the latest episode."""
+        if self._latest_metrics is None:
+            raise ValueError("No episode metrics available yet.")
+        return self._latest_metrics
+
+    def collect_metrics(self) -> dict[str, float]:
+        """Retrieve the aggregated metrics of the latest collect."""
+        return {name: value / self.episode_count for name, value in self._aggregated_metrics.items()}
+
+
+class ConsoleWriter(LogWriter):
+    """Write log messages to console periodically.
+
+    It tracks an average meter for each metric, which is the average value since last ``clear()`` till now.
+    The display format for each metric is ``<name> <latest_value> (<average_value>)``.
+
+    Non-single-number metrics are auto skipped.
+    """
+
+    prefix: str
+    """Prefix can be set via ``writer.prefix``."""
+
+    def __init__(
+        self,
+        log_every_n_episode: int = 20,
+        total_episodes: int | None = None,
+        float_format: str = ":.4f",
+        counter_format: str = ":4d",
+        loglevel: int | LogLevel = LogLevel.PERIODIC,
+    ) -> None:
+        super().__init__(loglevel)
+        # TODO: support log_every_n_step
+        self.log_every_n_episode = log_every_n_episode
+        self.total_episodes = total_episodes
+
+        self.counter_format = counter_format
+        self.float_format = float_format
+
+        self.prefix = ""
+
+        self.console_logger = get_module_logger(__name__, level=logging.INFO)
+
+    # FIXME: save & reload
+
+    def clear(self) -> None:
+        super().clear()
+        # Clear average meters
+        self.metric_counts: Dict[str, int] = defaultdict(int)
+        self.metric_sums: Dict[str, float] = defaultdict(float)
+
+    def log_episode(self, length: int, rewards: List[float], contents: List[Dict[str, Any]]) -> None:
+        # Aggregate step-wise to episode-wise
+        episode_wise_contents: Dict[str, list] = defaultdict(list)
+
+        for step_contents in contents:
+            for name, value in step_contents.items():
+                if isinstance(value, float):
+                    episode_wise_contents[name].append(value)
+
+        # Generate log contents and track them in average-meter.
+        # This should be done at every step, regardless of periodic or not.
+        logs: Dict[str, float] = {}
+        for name, values in episode_wise_contents.items():
+            logs[name] = self.aggregation(values, name)  # type: ignore
+
+        for name, value in logs.items():
+            self.metric_counts[name] += 1
+            self.metric_sums[name] += value
+
+        if self.episode_count % self.log_every_n_episode == 0 or self.episode_count == self.total_episodes:
+            # Only log periodically or at the end
+            self.console_logger.info(self.generate_log_message(logs))
+
+    def generate_log_message(self, logs: Dict[str, float]) -> str:
+        if self.prefix:
+            msg_prefix = self.prefix + " "
+        else:
+            msg_prefix = ""
+        if self.total_episodes is None:
+            msg_prefix += "[Step {" + self.counter_format + "}]"
+        else:
+            msg_prefix += "[{" + self.counter_format + "}/" + str(self.total_episodes) + "]"
+        msg_prefix = msg_prefix.format(self.episode_count)
+
+        msg = ""
+        for name, value in logs.items():
+            # Double-space as delimiter
+            format_template = r"  {} {" + self.float_format + "} ({" + self.float_format + "})"
+            msg += format_template.format(name, value, self.metric_sums[name] / self.metric_counts[name])
+
+        msg = msg_prefix + " " + msg
+
+        return msg
+
+
+class CsvWriter(LogWriter):
+    """Dump all episode metrics to a ``result.csv``.
+
+    This is not the correct implementation. It's only used for first iteration.
+    """
+
+    SUPPORTED_TYPES = (float, str, pd.Timestamp)
+
+    all_records: List[Dict[str, Any]]
+
+    # FIXME: save & reload
+
+    def __init__(self, output_dir: Path, loglevel: int | LogLevel = LogLevel.PERIODIC) -> None:
+        super().__init__(loglevel)
+        self.output_dir = output_dir
+        self.output_dir.mkdir(exist_ok=True)
+
+    def clear(self) -> None:
+        super().clear()
+        self.all_records = []
+
+    def log_episode(self, length: int, rewards: List[float], contents: List[Dict[str, Any]]) -> None:
+        # FIXME Same as ConsoleLogger, needs a refactor to eliminate code-dup
+        episode_wise_contents: Dict[str, list] = defaultdict(list)
+
+        for step_contents in contents:
+            for name, value in step_contents.items():
+                if isinstance(value, self.SUPPORTED_TYPES):
+                    episode_wise_contents[name].append(value)
+
+        logs: Dict[str, float] = {}
+        for name, values in episode_wise_contents.items():
+            logs[name] = self.aggregation(values, name)  # type: ignore
+
+        self.all_records.append(logs)
+
+    def on_env_all_done(self) -> None:
+        # FIXME: this is temporary
+        pd.DataFrame.from_records(self.all_records).to_csv(self.output_dir / "result.csv", index=False)
+
+
+# The following are not implemented yet.
+
+
+class PickleWriter(LogWriter):
+    """Dump logs to pickle files."""
+
+
+class TensorboardWriter(LogWriter):
+    """Write logs to event files that can be visualized with tensorboard."""
+
+
+class MlflowWriter(LogWriter):
+    """Add logs to mlflow."""
```

## qlib/run/get_data.py

 * *Ordering differences only*

```diff
@@ -1,9 +1,9 @@
-#  Copyright (c) Microsoft Corporation.
-#  Licensed under the MIT License.
-
-import fire
-from qlib.tests.data import GetData
-
-
-if __name__ == "__main__":
-    fire.Fire(GetData)
+#  Copyright (c) Microsoft Corporation.
+#  Licensed under the MIT License.
+
+import fire
+from qlib.tests.data import GetData
+
+
+if __name__ == "__main__":
+    fire.Fire(GetData)
```

## qlib/strategy/__init__.py

 * *Ordering differences only*

```diff
@@ -1,2 +1,2 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
```

## qlib/strategy/base.py

 * *Ordering differences only*

```diff
@@ -1,296 +1,296 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-from __future__ import annotations
-
-from abc import ABCMeta, abstractmethod
-from typing import Any, Generator, Optional, TYPE_CHECKING, Union
-
-if TYPE_CHECKING:
-    from qlib.backtest.exchange import Exchange
-    from qlib.backtest.position import BasePosition
-    from qlib.backtest.executor import BaseExecutor
-
-from typing import Tuple
-
-from ..backtest.decision import BaseTradeDecision
-from ..backtest.utils import CommonInfrastructure, LevelInfrastructure, TradeCalendarManager
-from ..rl.interpreter import ActionInterpreter, StateInterpreter
-from ..utils import init_instance_by_config
-
-__all__ = ["BaseStrategy", "RLStrategy", "RLIntStrategy"]
-
-
-class BaseStrategy:
-    """Base strategy for trading"""
-
-    def __init__(
-        self,
-        outer_trade_decision: BaseTradeDecision = None,
-        level_infra: LevelInfrastructure = None,
-        common_infra: CommonInfrastructure = None,
-        trade_exchange: Exchange = None,
-    ) -> None:
-        """
-        Parameters
-        ----------
-        outer_trade_decision : BaseTradeDecision, optional
-            the trade decision of outer strategy which this strategy relies, and it will be traded in
-            [start_time, end_time], by default None
-
-            - If the strategy is used to split trade decision, it will be used
-            - If the strategy is used for portfolio management, it can be ignored
-        level_infra : LevelInfrastructure, optional
-            level shared infrastructure for backtesting, including trade calendar
-        common_infra : CommonInfrastructure, optional
-            common infrastructure for backtesting, including trade_account, trade_exchange, .etc
-
-        trade_exchange : Exchange
-            exchange that provides market info, used to deal order and generate report
-
-            - If `trade_exchange` is None, self.trade_exchange will be set with common_infra
-            - It allows different trade_exchanges is used in different executions.
-            - For example:
-
-                - In daily execution, both daily exchange and minutely are usable, but the daily exchange is
-                  recommended because it run faster.
-                - In minutely execution, the daily exchange is not usable, only the minutely exchange is recommended.
-        """
-
-        self._reset(level_infra=level_infra, common_infra=common_infra, outer_trade_decision=outer_trade_decision)
-        self._trade_exchange = trade_exchange
-
-    @property
-    def executor(self) -> BaseExecutor:
-        return self.level_infra.get("executor")
-
-    @property
-    def trade_calendar(self) -> TradeCalendarManager:
-        return self.level_infra.get("trade_calendar")
-
-    @property
-    def trade_position(self) -> BasePosition:
-        return self.common_infra.get("trade_account").current_position
-
-    @property
-    def trade_exchange(self) -> Exchange:
-        """get trade exchange in a prioritized order"""
-        return getattr(self, "_trade_exchange", None) or self.common_infra.get("trade_exchange")
-
-    def reset_level_infra(self, level_infra: LevelInfrastructure) -> None:
-        if not hasattr(self, "level_infra"):
-            self.level_infra = level_infra
-        else:
-            self.level_infra.update(level_infra)
-
-    def reset_common_infra(self, common_infra: CommonInfrastructure) -> None:
-        if not hasattr(self, "common_infra"):
-            self.common_infra: CommonInfrastructure = common_infra
-        else:
-            self.common_infra.update(common_infra)
-
-    def reset(
-        self,
-        level_infra: LevelInfrastructure = None,
-        common_infra: CommonInfrastructure = None,
-        outer_trade_decision: BaseTradeDecision = None,
-        **kwargs,
-    ) -> None:
-        """
-        - reset `level_infra`, used to reset trade calendar, .etc
-        - reset `common_infra`, used to reset `trade_account`, `trade_exchange`, .etc
-        - reset `outer_trade_decision`, used to make split decision
-
-        **NOTE**:
-        split this function into `reset` and `_reset` will make following cases more convenient
-        1. Users want to initialize his strategy by overriding `reset`, but they don't want to affect the `_reset`
-        called when initialization
-        """
-        self._reset(
-            level_infra=level_infra,
-            common_infra=common_infra,
-            outer_trade_decision=outer_trade_decision,
-        )
-
-    def _reset(
-        self,
-        level_infra: LevelInfrastructure = None,
-        common_infra: CommonInfrastructure = None,
-        outer_trade_decision: BaseTradeDecision = None,
-    ):
-        """
-        Please refer to the docs of `reset`
-        """
-        if level_infra is not None:
-            self.reset_level_infra(level_infra)
-
-        if common_infra is not None:
-            self.reset_common_infra(common_infra)
-
-        if outer_trade_decision is not None:
-            self.outer_trade_decision = outer_trade_decision
-
-    @abstractmethod
-    def generate_trade_decision(
-        self,
-        execute_result: list = None,
-    ) -> Union[BaseTradeDecision, Generator[Any, Any, BaseTradeDecision]]:
-        """Generate trade decision in each trading bar
-
-        Parameters
-        ----------
-        execute_result : List[object], optional
-            the executed result for trade decision, by default None
-
-            - When call the generate_trade_decision firstly, `execute_result` could be None
-        """
-        raise NotImplementedError("generate_trade_decision is not implemented!")
-
-    # helper methods: not necessary but for convenience
-    def get_data_cal_avail_range(self, rtype: str = "full") -> Tuple[int, int]:
-        """
-        return data calendar's available decision range for `self` strategy
-        the range consider following factors
-        - data calendar in the charge of `self` strategy
-        - trading range limitation from the decision of outer strategy
-
-
-        related methods
-        - TradeCalendarManager.get_data_cal_range
-        - BaseTradeDecision.get_data_cal_range_limit
-
-        Parameters
-        ----------
-        rtype: str
-            - "full": return the available data index range of the strategy from `start_time` to `end_time`
-            - "step": return the available data index range of the strategy of current step
-
-        Returns
-        -------
-        Tuple[int, int]:
-            the available range both sides are closed
-        """
-        cal_range = self.trade_calendar.get_data_cal_range(rtype=rtype)
-        if self.outer_trade_decision is None:
-            raise ValueError(f"There is not limitation for strategy {self}")
-        range_limit = self.outer_trade_decision.get_data_cal_range_limit(rtype=rtype)
-        return max(cal_range[0], range_limit[0]), min(cal_range[1], range_limit[1])
-
-    """
-    The following methods are used to do cross-level communications in nested execution.
-    You do not need to care about them if you are implementing a single-level execution.
-    """
-
-    @staticmethod
-    def update_trade_decision(
-        trade_decision: BaseTradeDecision,
-        trade_calendar: TradeCalendarManager,
-    ) -> Optional[BaseTradeDecision]:
-        """
-        update trade decision in each step of inner execution, this method enable all order
-
-        Parameters
-        ----------
-        trade_decision : BaseTradeDecision
-            the trade decision that will be updated
-        trade_calendar : TradeCalendarManager
-            The calendar of the **inner strategy**!!!!!
-
-        Returns
-        -------
-            BaseTradeDecision:
-        """
-        # default to return None, which indicates that the trade decision is not changed
-        return None
-
-    def alter_outer_trade_decision(self, outer_trade_decision: BaseTradeDecision) -> BaseTradeDecision:
-        """
-        A method for updating the outer_trade_decision.
-        The outer strategy may change its decision during updating.
-
-        Parameters
-        ----------
-        outer_trade_decision : BaseTradeDecision
-            the decision updated by the outer strategy
-
-        Returns
-        -------
-            BaseTradeDecision
-        """
-        # default to reset the decision directly
-        # NOTE: normally, user should do something to the strategy due to the change of outer decision
-        return outer_trade_decision
-
-    def post_upper_level_exe_step(self) -> None:
-        """
-        A hook for doing sth after the upper level executor finished its execution (for example, finalize
-        the metrics collection).
-        """
-
-    def post_exe_step(self, execute_result: Optional[list]) -> None:
-        """
-        A hook for doing sth after the corresponding executor finished its execution.
-
-        Parameters
-        ----------
-        execute_result :
-            the execution result
-        """
-
-
-class RLStrategy(BaseStrategy, metaclass=ABCMeta):
-    """RL-based strategy"""
-
-    def __init__(
-        self,
-        policy,
-        outer_trade_decision: BaseTradeDecision = None,
-        level_infra: LevelInfrastructure = None,
-        common_infra: CommonInfrastructure = None,
-        **kwargs,
-    ) -> None:
-        """
-        Parameters
-        ----------
-        policy :
-            RL policy for generate action
-        """
-        super(RLStrategy, self).__init__(outer_trade_decision, level_infra, common_infra, **kwargs)
-        self.policy = policy
-
-
-class RLIntStrategy(RLStrategy, metaclass=ABCMeta):
-    """(RL)-based (Strategy) with (Int)erpreter"""
-
-    def __init__(
-        self,
-        policy,
-        state_interpreter: dict | StateInterpreter,
-        action_interpreter: dict | ActionInterpreter,
-        outer_trade_decision: BaseTradeDecision = None,
-        level_infra: LevelInfrastructure = None,
-        common_infra: CommonInfrastructure = None,
-        **kwargs,
-    ) -> None:
-        """
-        Parameters
-        ----------
-        state_interpreter : Union[dict, StateInterpreter]
-            interpreter that interprets the qlib execute result into rl env state
-        action_interpreter : Union[dict, ActionInterpreter]
-            interpreter that interprets the rl agent action into qlib order list
-        start_time : Union[str, pd.Timestamp], optional
-            start time of trading, by default None
-        end_time : Union[str, pd.Timestamp], optional
-            end time of trading, by default None
-        """
-        super(RLIntStrategy, self).__init__(policy, outer_trade_decision, level_infra, common_infra, **kwargs)
-
-        self.policy = policy
-        self.state_interpreter = init_instance_by_config(state_interpreter, accept_types=StateInterpreter)
-        self.action_interpreter = init_instance_by_config(action_interpreter, accept_types=ActionInterpreter)
-
-    def generate_trade_decision(self, execute_result: list = None) -> BaseTradeDecision:
-        _interpret_state = self.state_interpreter.interpret(execute_result=execute_result)
-        _action = self.policy.step(_interpret_state)
-        _trade_decision = self.action_interpreter.interpret(action=_action)
-        return _trade_decision
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+from __future__ import annotations
+
+from abc import ABCMeta, abstractmethod
+from typing import Any, Generator, Optional, TYPE_CHECKING, Union
+
+if TYPE_CHECKING:
+    from qlib.backtest.exchange import Exchange
+    from qlib.backtest.position import BasePosition
+    from qlib.backtest.executor import BaseExecutor
+
+from typing import Tuple
+
+from ..backtest.decision import BaseTradeDecision
+from ..backtest.utils import CommonInfrastructure, LevelInfrastructure, TradeCalendarManager
+from ..rl.interpreter import ActionInterpreter, StateInterpreter
+from ..utils import init_instance_by_config
+
+__all__ = ["BaseStrategy", "RLStrategy", "RLIntStrategy"]
+
+
+class BaseStrategy:
+    """Base strategy for trading"""
+
+    def __init__(
+        self,
+        outer_trade_decision: BaseTradeDecision = None,
+        level_infra: LevelInfrastructure = None,
+        common_infra: CommonInfrastructure = None,
+        trade_exchange: Exchange = None,
+    ) -> None:
+        """
+        Parameters
+        ----------
+        outer_trade_decision : BaseTradeDecision, optional
+            the trade decision of outer strategy which this strategy relies, and it will be traded in
+            [start_time, end_time], by default None
+
+            - If the strategy is used to split trade decision, it will be used
+            - If the strategy is used for portfolio management, it can be ignored
+        level_infra : LevelInfrastructure, optional
+            level shared infrastructure for backtesting, including trade calendar
+        common_infra : CommonInfrastructure, optional
+            common infrastructure for backtesting, including trade_account, trade_exchange, .etc
+
+        trade_exchange : Exchange
+            exchange that provides market info, used to deal order and generate report
+
+            - If `trade_exchange` is None, self.trade_exchange will be set with common_infra
+            - It allows different trade_exchanges is used in different executions.
+            - For example:
+
+                - In daily execution, both daily exchange and minutely are usable, but the daily exchange is
+                  recommended because it run faster.
+                - In minutely execution, the daily exchange is not usable, only the minutely exchange is recommended.
+        """
+
+        self._reset(level_infra=level_infra, common_infra=common_infra, outer_trade_decision=outer_trade_decision)
+        self._trade_exchange = trade_exchange
+
+    @property
+    def executor(self) -> BaseExecutor:
+        return self.level_infra.get("executor")
+
+    @property
+    def trade_calendar(self) -> TradeCalendarManager:
+        return self.level_infra.get("trade_calendar")
+
+    @property
+    def trade_position(self) -> BasePosition:
+        return self.common_infra.get("trade_account").current_position
+
+    @property
+    def trade_exchange(self) -> Exchange:
+        """get trade exchange in a prioritized order"""
+        return getattr(self, "_trade_exchange", None) or self.common_infra.get("trade_exchange")
+
+    def reset_level_infra(self, level_infra: LevelInfrastructure) -> None:
+        if not hasattr(self, "level_infra"):
+            self.level_infra = level_infra
+        else:
+            self.level_infra.update(level_infra)
+
+    def reset_common_infra(self, common_infra: CommonInfrastructure) -> None:
+        if not hasattr(self, "common_infra"):
+            self.common_infra: CommonInfrastructure = common_infra
+        else:
+            self.common_infra.update(common_infra)
+
+    def reset(
+        self,
+        level_infra: LevelInfrastructure = None,
+        common_infra: CommonInfrastructure = None,
+        outer_trade_decision: BaseTradeDecision = None,
+        **kwargs,
+    ) -> None:
+        """
+        - reset `level_infra`, used to reset trade calendar, .etc
+        - reset `common_infra`, used to reset `trade_account`, `trade_exchange`, .etc
+        - reset `outer_trade_decision`, used to make split decision
+
+        **NOTE**:
+        split this function into `reset` and `_reset` will make following cases more convenient
+        1. Users want to initialize his strategy by overriding `reset`, but they don't want to affect the `_reset`
+        called when initialization
+        """
+        self._reset(
+            level_infra=level_infra,
+            common_infra=common_infra,
+            outer_trade_decision=outer_trade_decision,
+        )
+
+    def _reset(
+        self,
+        level_infra: LevelInfrastructure = None,
+        common_infra: CommonInfrastructure = None,
+        outer_trade_decision: BaseTradeDecision = None,
+    ):
+        """
+        Please refer to the docs of `reset`
+        """
+        if level_infra is not None:
+            self.reset_level_infra(level_infra)
+
+        if common_infra is not None:
+            self.reset_common_infra(common_infra)
+
+        if outer_trade_decision is not None:
+            self.outer_trade_decision = outer_trade_decision
+
+    @abstractmethod
+    def generate_trade_decision(
+        self,
+        execute_result: list = None,
+    ) -> Union[BaseTradeDecision, Generator[Any, Any, BaseTradeDecision]]:
+        """Generate trade decision in each trading bar
+
+        Parameters
+        ----------
+        execute_result : List[object], optional
+            the executed result for trade decision, by default None
+
+            - When call the generate_trade_decision firstly, `execute_result` could be None
+        """
+        raise NotImplementedError("generate_trade_decision is not implemented!")
+
+    # helper methods: not necessary but for convenience
+    def get_data_cal_avail_range(self, rtype: str = "full") -> Tuple[int, int]:
+        """
+        return data calendar's available decision range for `self` strategy
+        the range consider following factors
+        - data calendar in the charge of `self` strategy
+        - trading range limitation from the decision of outer strategy
+
+
+        related methods
+        - TradeCalendarManager.get_data_cal_range
+        - BaseTradeDecision.get_data_cal_range_limit
+
+        Parameters
+        ----------
+        rtype: str
+            - "full": return the available data index range of the strategy from `start_time` to `end_time`
+            - "step": return the available data index range of the strategy of current step
+
+        Returns
+        -------
+        Tuple[int, int]:
+            the available range both sides are closed
+        """
+        cal_range = self.trade_calendar.get_data_cal_range(rtype=rtype)
+        if self.outer_trade_decision is None:
+            raise ValueError(f"There is not limitation for strategy {self}")
+        range_limit = self.outer_trade_decision.get_data_cal_range_limit(rtype=rtype)
+        return max(cal_range[0], range_limit[0]), min(cal_range[1], range_limit[1])
+
+    """
+    The following methods are used to do cross-level communications in nested execution.
+    You do not need to care about them if you are implementing a single-level execution.
+    """
+
+    @staticmethod
+    def update_trade_decision(
+        trade_decision: BaseTradeDecision,
+        trade_calendar: TradeCalendarManager,
+    ) -> Optional[BaseTradeDecision]:
+        """
+        update trade decision in each step of inner execution, this method enable all order
+
+        Parameters
+        ----------
+        trade_decision : BaseTradeDecision
+            the trade decision that will be updated
+        trade_calendar : TradeCalendarManager
+            The calendar of the **inner strategy**!!!!!
+
+        Returns
+        -------
+            BaseTradeDecision:
+        """
+        # default to return None, which indicates that the trade decision is not changed
+        return None
+
+    def alter_outer_trade_decision(self, outer_trade_decision: BaseTradeDecision) -> BaseTradeDecision:
+        """
+        A method for updating the outer_trade_decision.
+        The outer strategy may change its decision during updating.
+
+        Parameters
+        ----------
+        outer_trade_decision : BaseTradeDecision
+            the decision updated by the outer strategy
+
+        Returns
+        -------
+            BaseTradeDecision
+        """
+        # default to reset the decision directly
+        # NOTE: normally, user should do something to the strategy due to the change of outer decision
+        return outer_trade_decision
+
+    def post_upper_level_exe_step(self) -> None:
+        """
+        A hook for doing sth after the upper level executor finished its execution (for example, finalize
+        the metrics collection).
+        """
+
+    def post_exe_step(self, execute_result: Optional[list]) -> None:
+        """
+        A hook for doing sth after the corresponding executor finished its execution.
+
+        Parameters
+        ----------
+        execute_result :
+            the execution result
+        """
+
+
+class RLStrategy(BaseStrategy, metaclass=ABCMeta):
+    """RL-based strategy"""
+
+    def __init__(
+        self,
+        policy,
+        outer_trade_decision: BaseTradeDecision = None,
+        level_infra: LevelInfrastructure = None,
+        common_infra: CommonInfrastructure = None,
+        **kwargs,
+    ) -> None:
+        """
+        Parameters
+        ----------
+        policy :
+            RL policy for generate action
+        """
+        super(RLStrategy, self).__init__(outer_trade_decision, level_infra, common_infra, **kwargs)
+        self.policy = policy
+
+
+class RLIntStrategy(RLStrategy, metaclass=ABCMeta):
+    """(RL)-based (Strategy) with (Int)erpreter"""
+
+    def __init__(
+        self,
+        policy,
+        state_interpreter: dict | StateInterpreter,
+        action_interpreter: dict | ActionInterpreter,
+        outer_trade_decision: BaseTradeDecision = None,
+        level_infra: LevelInfrastructure = None,
+        common_infra: CommonInfrastructure = None,
+        **kwargs,
+    ) -> None:
+        """
+        Parameters
+        ----------
+        state_interpreter : Union[dict, StateInterpreter]
+            interpreter that interprets the qlib execute result into rl env state
+        action_interpreter : Union[dict, ActionInterpreter]
+            interpreter that interprets the rl agent action into qlib order list
+        start_time : Union[str, pd.Timestamp], optional
+            start time of trading, by default None
+        end_time : Union[str, pd.Timestamp], optional
+            end time of trading, by default None
+        """
+        super(RLIntStrategy, self).__init__(policy, outer_trade_decision, level_infra, common_infra, **kwargs)
+
+        self.policy = policy
+        self.state_interpreter = init_instance_by_config(state_interpreter, accept_types=StateInterpreter)
+        self.action_interpreter = init_instance_by_config(action_interpreter, accept_types=ActionInterpreter)
+
+    def generate_trade_decision(self, execute_result: list = None) -> BaseTradeDecision:
+        _interpret_state = self.state_interpreter.interpret(execute_result=execute_result)
+        _action = self.policy.step(_interpret_state)
+        _trade_decision = self.action_interpreter.interpret(action=_action)
+        return _trade_decision
```

## qlib/tests/__init__.py

```diff
@@ -1,291 +1,289 @@
-from typing import Union, List, Dict, Tuple
-import unittest
-import pandas as pd
-import numpy as np
-import io
-
-from .data import GetData
-from .. import init
-from ..constant import REG_CN, REG_TW
-from qlib.data.filter import NameDFilter
-from qlib.data import D
-from qlib.data.data import Cal, DatasetD
-from qlib.data.storage import CalendarStorage, InstrumentStorage, FeatureStorage, CalVT, InstKT, InstVT
-
-
-class TestAutoData(unittest.TestCase):
-
-    _setup_kwargs = {}
-    provider_uri = "~/.qlib/qlib_data/cn_data_simple"  # target_dir
-    provider_uri_1day = "~/.qlib/qlib_data/cn_data"  # target_dir
-    provider_uri_1min = "~/.qlib/qlib_data/cn_data_1min"
-
-    @classmethod
-    def setUpClass(cls, enable_1d_type="simple", enable_1min=False) -> None:
-        # use default data
-
-        if enable_1d_type == "simple":
-            provider_uri_day = cls.provider_uri
-            name_day = "qlib_data_simple"
-        elif enable_1d_type == "full":
-            provider_uri_day = cls.provider_uri_1day
-            name_day = "qlib_data"
-        else:
-            raise NotImplementedError(f"This type of input is not supported")
-
-        GetData().qlib_data(
-            name=name_day,
-            region=REG_CN,
-            interval="1d",
-            target_dir=provider_uri_day,
-            delete_old=False,
-            exists_skip=True,
-        )
-
-        if enable_1min:
-            GetData().qlib_data(
-                name="qlib_data",
-                region=REG_CN,
-                interval="1min",
-                target_dir=cls.provider_uri_1min,
-                delete_old=False,
-                exists_skip=True,
-            )
-
-        provider_uri_map = {"1min": cls.provider_uri_1min, "day": provider_uri_day}
-        init(
-            provider_uri=provider_uri_map,
-            region=REG_CN,
-            expression_cache=None,
-            dataset_cache=None,
-            **cls._setup_kwargs,
-        )
-
-
-class TestOperatorData(TestAutoData):
-    @classmethod
-    def setUpClass(cls, enable_1d_type="simple", enable_1min=False) -> None:
-        # use default data
-        super().setUpClass(enable_1d_type, enable_1min)
-        nameDFilter = NameDFilter(name_rule_re="SH600110")
-        instruments = D.instruments("csi300", filter_pipe=[nameDFilter])
-        start_time = "2005-01-04"
-        end_time = "2005-12-31"
-        freq = "day"
-
-        instruments_d = DatasetD.get_instruments_d(instruments, freq)
-        cls.instruments_d = instruments_d
-        cal = Cal.calendar(start_time, end_time, freq)
-        cls.cal = cal
-        cls.start_time = cal[0]
-        cls.end_time = cal[-1]
-        cls.inst = list(instruments_d.keys())[0]
-        cls.spans = list(instruments_d.values())[0]
-
-
-MOCK_DATA = """
-id,symbol,datetime,interval,volume,open,high,low,close
-20275,0050,2022-01-03 00:00:00,day,6761.0,146.0,147.35,146.0,146.4
-20276,0050,2022-01-04 00:00:00,day,9608.0,147.7,149.6,147.7,149.6
-20277,0050,2022-01-05 00:00:00,day,11387.0,150.1,150.55,149.1,149.3
-20278,0050,2022-01-06 00:00:00,day,8611.0,148.3,148.75,147.0,147.9
-20279,0050,2022-01-07 00:00:00,day,6954.0,148.3,149.0,146.5,146.6
-20280,0050,2022-01-10 00:00:00,day,15684.0,146.0,147.8,145.4,147.55
-20281,0050,2022-01-11 00:00:00,day,17741.0,147.6,148.5,146.7,148.3
-20282,0050,2022-01-12 00:00:00,day,10134.0,149.35,149.6,148.7,149.55
-20283,0050,2022-01-13 00:00:00,day,7431.0,149.55,150.45,149.55,150.3
-20284,0050,2022-01-14 00:00:00,day,10091.0,150.8,151.2,149.05,150.3
-20285,0050,2022-01-17 00:00:00,day,6899.0,151.1,152.4,151.1,152.0
-20286,0050,2022-01-18 00:00:00,day,14360.0,152.2,152.25,150.15,150.3
-20287,0050,2022-01-19 00:00:00,day,14654.0,149.0,149.65,148.25,148.5
-20288,0050,2022-01-20 00:00:00,day,16201.0,148.5,149.2,147.6,149.1
-20289,0050,2022-01-21 00:00:00,day,29848.0,143.9,143.95,142.3,142.65
-20290,0050,2022-01-24 00:00:00,day,13143.0,142.1,144.0,141.7,144.0
-20291,0050,2022-01-25 00:00:00,day,23982.0,142.55,142.55,141.25,141.65
-20292,0050,2022-01-26 00:00:00,day,17729.0,141.15,142.2,141.05,141.55
-8547,1101,2021-12-01 00:00:00,day,16119.0,46.0,46.85,46.0,46.6
-8548,1101,2021-12-02 00:00:00,day,14521.0,46.6,46.7,46.3,46.3
-8549,1101,2021-12-03 00:00:00,day,14357.0,46.55,46.85,46.4,46.4
-8550,1101,2021-12-06 00:00:00,day,15115.0,46.45,47.35,46.4,47.3
-8551,1101,2021-12-07 00:00:00,day,13117.0,47.35,47.55,46.9,47.55
-8552,1101,2021-12-08 00:00:00,day,10329.0,47.75,47.8,47.5,47.7
-8553,1101,2021-12-09 00:00:00,day,9300.0,47.8,47.85,47.1,47.4
-8554,1101,2021-12-10 00:00:00,day,9919.0,47.4,47.6,47.1,47.3
-8555,1101,2021-12-13 00:00:00,day,7784.0,47.3,47.75,47.1,47.1
-8556,1101,2021-12-14 00:00:00,day,9373.0,47.05,47.2,46.95,47.0
-8557,1101,2021-12-15 00:00:00,day,11189.0,47.0,47.3,46.8,46.95
-8558,1101,2021-12-16 00:00:00,day,7516.0,47.0,47.15,46.8,46.9
-8559,1101,2021-12-17 00:00:00,day,18502.0,46.95,47.6,46.9,47.45
-8560,1101,2021-12-20 00:00:00,day,11309.0,47.45,47.5,47.1,47.4
-8561,1101,2021-12-21 00:00:00,day,5666.0,47.4,47.45,47.1,47.25
-8562,1101,2021-12-22 00:00:00,day,5460.0,47.4,47.45,47.2,47.4
-8563,1101,2021-12-23 00:00:00,day,9371.0,47.3,47.7,47.3,47.7
-8564,1101,2021-12-24 00:00:00,day,5980.0,47.75,47.95,47.75,47.9
-8565,1101,2021-12-27 00:00:00,day,5709.0,47.9,48.1,47.9,48.1
-8566,1101,2021-12-28 00:00:00,day,7777.0,48.1,48.15,47.95,48.15
-8567,1101,2021-12-29 00:00:00,day,5309.0,48.15,48.25,48.05,48.15
-8568,1101,2021-12-30 00:00:00,day,4616.0,48.15,48.2,48.0,48.0
-8569,1101,2022-01-03 00:00:00,day,12350.0,48.05,48.15,47.35,47.45
-8570,1101,2022-01-04 00:00:00,day,11439.0,47.5,47.6,47.0,47.3
-8571,1101,2022-01-05 00:00:00,day,9692.0,47.1,47.3,47.0,47.15
-8572,1101,2022-01-06 00:00:00,day,12361.0,47.3,47.6,47.15,47.6
-8573,1101,2022-01-07 00:00:00,day,10921.0,47.6,47.65,47.2,47.45
-8574,1101,2022-01-10 00:00:00,day,11925.0,47.45,47.5,47.0,47.3
-8575,1101,2022-01-11 00:00:00,day,11047.0,47.1,47.5,47.1,47.5
-8576,1101,2022-01-12 00:00:00,day,10817.0,47.5,47.5,47.1,47.5
-8577,1101,2022-01-13 00:00:00,day,13849.0,47.5,47.95,47.4,47.95
-8578,1101,2022-01-14 00:00:00,day,9460.0,47.85,47.85,47.45,47.6
-8579,1101,2022-01-17 00:00:00,day,9057.0,47.55,47.7,47.35,47.6
-8580,1101,2022-01-18 00:00:00,day,8089.0,47.6,47.75,47.45,47.75
-8581,1101,2022-01-19 00:00:00,day,5110.0,47.6,47.7,47.5,47.6
-8582,1101,2022-01-20 00:00:00,day,6327.0,47.55,47.7,47.45,47.5
-8583,1101,2022-01-21 00:00:00,day,9470.0,47.5,47.65,47.15,47.4
-8584,1101,2022-01-24 00:00:00,day,5475.0,47.1,47.3,47.0,47.15
-8585,1101,2022-01-25 00:00:00,day,16153.0,47.0,47.05,46.6,46.8
-8586,1101,2022-01-26 00:00:00,day,7772.0,46.7,47.0,46.55,46.85
-8587,1101,2022-02-07 00:00:00,day,17031.0,46.55,47.1,46.0,47.1
-8588,1101,2022-02-08 00:00:00,day,9741.0,47.1,47.25,46.9,46.95
-8589,1101,2022-02-09 00:00:00,day,7968.0,46.95,47.3,46.9,47.3
-8590,1101,2022-02-10 00:00:00,day,7479.0,47.15,47.55,47.05,47.55
-8591,1101,2022-02-11 00:00:00,day,6841.0,47.3,47.55,47.15,47.55
-8592,1101,2022-02-14 00:00:00,day,9136.0,47.2,47.3,46.95,47.15
-8593,1101,2022-02-15 00:00:00,day,5444.0,47.05,47.1,46.8,47.0
-8594,1101,2022-02-16 00:00:00,day,8751.0,47.0,47.15,47.0,47.0
-8595,1101,2022-02-17 00:00:00,day,10662.0,47.15,47.55,47.1,47.45
-8596,1101,2022-02-18 00:00:00,day,8781.0,47.25,47.55,47.2,47.45
-8597,1101,2022-02-21 00:00:00,day,8201.0,47.35,47.75,47.15,47.6
-8598,1101,2022-02-22 00:00:00,day,10655.0,47.4,47.7,47.1,47.7
-8599,1101,2022-02-23 00:00:00,day,8040.0,47.7,47.85,47.45,47.65
-8600,1101,2022-02-24 00:00:00,day,13124.0,47.5,47.5,47.1,47.3
-8601,1101,2022-02-25 00:00:00,day,14556.0,47.2,47.5,46.9,47.35
-"""
-
-MOCK_DF = pd.read_csv(io.StringIO(MOCK_DATA), header=0, dtype={"symbol": str})
-
-
-class MockStorageBase:
-    def __init__(self, **kwargs):
-        self.df = MOCK_DF
-
-
-class MockCalendarStorage(MockStorageBase, CalendarStorage):
-    def __init__(self, **kwargs):
-        super().__init__()
-        self._data = sorted(self.df["datetime"].unique())
-
-    @property
-    def data(self) -> List[CalVT]:
-        return self._data
-
-    def __getitem__(self, i: Union[int, slice]) -> Union[CalVT, List[CalVT]]:
-        return self.data[i]
-
-    def __len__(self) -> int:
-        return len(self.data)
-
-
-class MockInstrumentStorage(MockStorageBase, InstrumentStorage):
-    def __init__(self, **kwargs):
-        super().__init__()
-        instruments = {}
-        for symbol, group in self.df.groupby(by="symbol"):
-            start = group["datetime"].iloc[0]
-            end = group["datetime"].iloc[-1]
-            instruments[symbol] = [(start, end)]
-        self._data = instruments
-
-    @property
-    def data(self) -> Dict[InstKT, InstVT]:
-        return self._data
-
-    def __getitem__(self, k: InstKT) -> InstVT:
-        return self.data[k]
-
-    def __len__(self) -> int:
-        return len(self.data)
-
-
-class MockFeatureStorage(MockStorageBase, FeatureStorage):
-    def __init__(self, instrument: str, field: str, freq: str, db_region: str = None, **kwargs):  # type: ignore
-        super().__init__(instrument=instrument, field=field, freq=freq, db_region=db_region, **kwargs)
-        self.field = field
-        calendar = sorted(self.df["datetime"].unique())
-        df_calendar = pd.DataFrame(calendar, columns=["datetime"]).set_index("datetime")
-        df = self.df[self.df["symbol"] == instrument]
-        data_dt_field = "datetime"
-        cal_df = df_calendar[
-            (df_calendar.index >= df[data_dt_field].min()) & (df_calendar.index <= df[data_dt_field].max())
-        ]
-        df = df.set_index(data_dt_field)
-        df_data = df.reindex(cal_df.index)
-        date_index = df_calendar.index.get_loc(df_data.index.min())  # type: ignore
-        df_data.reset_index(inplace=True)
-        df_data.index += date_index
-        self._data = df_data
-
-    @property
-    def data(self) -> pd.Series:
-        return self._data[self.field]
-
-    @property
-    def start_index(self) -> Union[int, None]:
-        if self._data.empty:
-            return None
-        return self._data.index[0]
-
-    @property
-    def end_index(self) -> Union[int, None]:
-        if self._data.empty:
-            return None
-        # The next  data appending index point will be  `end_index + 1`
-        return self._data.index[-1]
-
-    def __getitem__(self, i: Union[int, slice]) -> Union[Tuple[int, float], pd.Series]:
-        df = self._data
-        storage_start_index = df.index[0]
-        storage_end_index = df.index[-1]
-        if isinstance(i, int):
-            if storage_start_index > i or i > storage_end_index:
-                raise IndexError(f"{i}: start index is {storage_start_index}")
-            data = self.data[i]
-            return i, data
-        elif isinstance(i, slice):
-            start_index = storage_start_index if i.start is None else i.start
-            end_index = storage_end_index if i.stop is None else i.stop
-            si = max(start_index, storage_start_index)
-            if si > end_index or self.field not in df.columns:
-                return pd.Series(dtype=np.float32)  # type: ignore
-            data = df[self.field].tolist()
-            result = data[si - storage_start_index : end_index - storage_start_index]
-            return pd.Series(result, index=pd.RangeIndex(si, si + len(result)))  # type: ignore
-        else:
-            raise TypeError(f"type(i) = {type(i)}")
-
-    def __len__(self) -> int:
-        return len(self.data)
-
-
-class TestMockData(unittest.TestCase):
-    _setup_kwargs = {
-        "calendar_provider": {
-            "class": "LocalCalendarProvider",
-            "module_path": "qlib.data.data",
-            "kwargs": {"backend": {"class": "MockCalendarStorage", "module_path": "qlib.tests"}},
-        },
-        "instrument_provider": {
-            "class": "LocalInstrumentProvider",
-            "module_path": "qlib.data.data",
-            "kwargs": {"backend": {"class": "MockInstrumentStorage", "module_path": "qlib.tests"}},
-        },
-        "feature_provider": {
-            "class": "LocalFeatureProvider",
-            "module_path": "qlib.data.data",
-            "kwargs": {"backend": {"class": "MockFeatureStorage", "module_path": "qlib.tests"}},
-        },
-    }
-
-    @classmethod
-    def setUpClass(cls) -> None:
-
-        provider_uri = "Not necessary."
-        init(region=REG_TW, provider_uri=provider_uri, expression_cache=None, dataset_cache=None, **cls._setup_kwargs)
+from typing import Union, List, Dict, Tuple
+import unittest
+import pandas as pd
+import numpy as np
+import io
+
+from .data import GetData
+from .. import init
+from ..constant import REG_CN, REG_TW
+from qlib.data.filter import NameDFilter
+from qlib.data import D
+from qlib.data.data import Cal, DatasetD
+from qlib.data.storage import CalendarStorage, InstrumentStorage, FeatureStorage, CalVT, InstKT, InstVT
+
+
+class TestAutoData(unittest.TestCase):
+    _setup_kwargs = {}
+    provider_uri = "~/.qlib/qlib_data/cn_data_simple"  # target_dir
+    provider_uri_1day = "~/.qlib/qlib_data/cn_data"  # target_dir
+    provider_uri_1min = "~/.qlib/qlib_data/cn_data_1min"
+
+    @classmethod
+    def setUpClass(cls, enable_1d_type="simple", enable_1min=False) -> None:
+        # use default data
+
+        if enable_1d_type == "simple":
+            provider_uri_day = cls.provider_uri
+            name_day = "qlib_data_simple"
+        elif enable_1d_type == "full":
+            provider_uri_day = cls.provider_uri_1day
+            name_day = "qlib_data"
+        else:
+            raise NotImplementedError(f"This type of input is not supported")
+
+        GetData().qlib_data(
+            name=name_day,
+            region=REG_CN,
+            interval="1d",
+            target_dir=provider_uri_day,
+            delete_old=False,
+            exists_skip=True,
+        )
+
+        if enable_1min:
+            GetData().qlib_data(
+                name="qlib_data",
+                region=REG_CN,
+                interval="1min",
+                target_dir=cls.provider_uri_1min,
+                delete_old=False,
+                exists_skip=True,
+            )
+
+        provider_uri_map = {"1min": cls.provider_uri_1min, "day": provider_uri_day}
+        init(
+            provider_uri=provider_uri_map,
+            region=REG_CN,
+            expression_cache=None,
+            dataset_cache=None,
+            **cls._setup_kwargs,
+        )
+
+
+class TestOperatorData(TestAutoData):
+    @classmethod
+    def setUpClass(cls, enable_1d_type="simple", enable_1min=False) -> None:
+        # use default data
+        super().setUpClass(enable_1d_type, enable_1min)
+        nameDFilter = NameDFilter(name_rule_re="SH600110")
+        instruments = D.instruments("csi300", filter_pipe=[nameDFilter])
+        start_time = "2005-01-04"
+        end_time = "2005-12-31"
+        freq = "day"
+
+        instruments_d = DatasetD.get_instruments_d(instruments, freq)
+        cls.instruments_d = instruments_d
+        cal = Cal.calendar(start_time, end_time, freq)
+        cls.cal = cal
+        cls.start_time = cal[0]
+        cls.end_time = cal[-1]
+        cls.inst = list(instruments_d.keys())[0]
+        cls.spans = list(instruments_d.values())[0]
+
+
+MOCK_DATA = """
+id,symbol,datetime,interval,volume,open,high,low,close
+20275,0050,2022-01-03 00:00:00,day,6761.0,146.0,147.35,146.0,146.4
+20276,0050,2022-01-04 00:00:00,day,9608.0,147.7,149.6,147.7,149.6
+20277,0050,2022-01-05 00:00:00,day,11387.0,150.1,150.55,149.1,149.3
+20278,0050,2022-01-06 00:00:00,day,8611.0,148.3,148.75,147.0,147.9
+20279,0050,2022-01-07 00:00:00,day,6954.0,148.3,149.0,146.5,146.6
+20280,0050,2022-01-10 00:00:00,day,15684.0,146.0,147.8,145.4,147.55
+20281,0050,2022-01-11 00:00:00,day,17741.0,147.6,148.5,146.7,148.3
+20282,0050,2022-01-12 00:00:00,day,10134.0,149.35,149.6,148.7,149.55
+20283,0050,2022-01-13 00:00:00,day,7431.0,149.55,150.45,149.55,150.3
+20284,0050,2022-01-14 00:00:00,day,10091.0,150.8,151.2,149.05,150.3
+20285,0050,2022-01-17 00:00:00,day,6899.0,151.1,152.4,151.1,152.0
+20286,0050,2022-01-18 00:00:00,day,14360.0,152.2,152.25,150.15,150.3
+20287,0050,2022-01-19 00:00:00,day,14654.0,149.0,149.65,148.25,148.5
+20288,0050,2022-01-20 00:00:00,day,16201.0,148.5,149.2,147.6,149.1
+20289,0050,2022-01-21 00:00:00,day,29848.0,143.9,143.95,142.3,142.65
+20290,0050,2022-01-24 00:00:00,day,13143.0,142.1,144.0,141.7,144.0
+20291,0050,2022-01-25 00:00:00,day,23982.0,142.55,142.55,141.25,141.65
+20292,0050,2022-01-26 00:00:00,day,17729.0,141.15,142.2,141.05,141.55
+8547,1101,2021-12-01 00:00:00,day,16119.0,46.0,46.85,46.0,46.6
+8548,1101,2021-12-02 00:00:00,day,14521.0,46.6,46.7,46.3,46.3
+8549,1101,2021-12-03 00:00:00,day,14357.0,46.55,46.85,46.4,46.4
+8550,1101,2021-12-06 00:00:00,day,15115.0,46.45,47.35,46.4,47.3
+8551,1101,2021-12-07 00:00:00,day,13117.0,47.35,47.55,46.9,47.55
+8552,1101,2021-12-08 00:00:00,day,10329.0,47.75,47.8,47.5,47.7
+8553,1101,2021-12-09 00:00:00,day,9300.0,47.8,47.85,47.1,47.4
+8554,1101,2021-12-10 00:00:00,day,9919.0,47.4,47.6,47.1,47.3
+8555,1101,2021-12-13 00:00:00,day,7784.0,47.3,47.75,47.1,47.1
+8556,1101,2021-12-14 00:00:00,day,9373.0,47.05,47.2,46.95,47.0
+8557,1101,2021-12-15 00:00:00,day,11189.0,47.0,47.3,46.8,46.95
+8558,1101,2021-12-16 00:00:00,day,7516.0,47.0,47.15,46.8,46.9
+8559,1101,2021-12-17 00:00:00,day,18502.0,46.95,47.6,46.9,47.45
+8560,1101,2021-12-20 00:00:00,day,11309.0,47.45,47.5,47.1,47.4
+8561,1101,2021-12-21 00:00:00,day,5666.0,47.4,47.45,47.1,47.25
+8562,1101,2021-12-22 00:00:00,day,5460.0,47.4,47.45,47.2,47.4
+8563,1101,2021-12-23 00:00:00,day,9371.0,47.3,47.7,47.3,47.7
+8564,1101,2021-12-24 00:00:00,day,5980.0,47.75,47.95,47.75,47.9
+8565,1101,2021-12-27 00:00:00,day,5709.0,47.9,48.1,47.9,48.1
+8566,1101,2021-12-28 00:00:00,day,7777.0,48.1,48.15,47.95,48.15
+8567,1101,2021-12-29 00:00:00,day,5309.0,48.15,48.25,48.05,48.15
+8568,1101,2021-12-30 00:00:00,day,4616.0,48.15,48.2,48.0,48.0
+8569,1101,2022-01-03 00:00:00,day,12350.0,48.05,48.15,47.35,47.45
+8570,1101,2022-01-04 00:00:00,day,11439.0,47.5,47.6,47.0,47.3
+8571,1101,2022-01-05 00:00:00,day,9692.0,47.1,47.3,47.0,47.15
+8572,1101,2022-01-06 00:00:00,day,12361.0,47.3,47.6,47.15,47.6
+8573,1101,2022-01-07 00:00:00,day,10921.0,47.6,47.65,47.2,47.45
+8574,1101,2022-01-10 00:00:00,day,11925.0,47.45,47.5,47.0,47.3
+8575,1101,2022-01-11 00:00:00,day,11047.0,47.1,47.5,47.1,47.5
+8576,1101,2022-01-12 00:00:00,day,10817.0,47.5,47.5,47.1,47.5
+8577,1101,2022-01-13 00:00:00,day,13849.0,47.5,47.95,47.4,47.95
+8578,1101,2022-01-14 00:00:00,day,9460.0,47.85,47.85,47.45,47.6
+8579,1101,2022-01-17 00:00:00,day,9057.0,47.55,47.7,47.35,47.6
+8580,1101,2022-01-18 00:00:00,day,8089.0,47.6,47.75,47.45,47.75
+8581,1101,2022-01-19 00:00:00,day,5110.0,47.6,47.7,47.5,47.6
+8582,1101,2022-01-20 00:00:00,day,6327.0,47.55,47.7,47.45,47.5
+8583,1101,2022-01-21 00:00:00,day,9470.0,47.5,47.65,47.15,47.4
+8584,1101,2022-01-24 00:00:00,day,5475.0,47.1,47.3,47.0,47.15
+8585,1101,2022-01-25 00:00:00,day,16153.0,47.0,47.05,46.6,46.8
+8586,1101,2022-01-26 00:00:00,day,7772.0,46.7,47.0,46.55,46.85
+8587,1101,2022-02-07 00:00:00,day,17031.0,46.55,47.1,46.0,47.1
+8588,1101,2022-02-08 00:00:00,day,9741.0,47.1,47.25,46.9,46.95
+8589,1101,2022-02-09 00:00:00,day,7968.0,46.95,47.3,46.9,47.3
+8590,1101,2022-02-10 00:00:00,day,7479.0,47.15,47.55,47.05,47.55
+8591,1101,2022-02-11 00:00:00,day,6841.0,47.3,47.55,47.15,47.55
+8592,1101,2022-02-14 00:00:00,day,9136.0,47.2,47.3,46.95,47.15
+8593,1101,2022-02-15 00:00:00,day,5444.0,47.05,47.1,46.8,47.0
+8594,1101,2022-02-16 00:00:00,day,8751.0,47.0,47.15,47.0,47.0
+8595,1101,2022-02-17 00:00:00,day,10662.0,47.15,47.55,47.1,47.45
+8596,1101,2022-02-18 00:00:00,day,8781.0,47.25,47.55,47.2,47.45
+8597,1101,2022-02-21 00:00:00,day,8201.0,47.35,47.75,47.15,47.6
+8598,1101,2022-02-22 00:00:00,day,10655.0,47.4,47.7,47.1,47.7
+8599,1101,2022-02-23 00:00:00,day,8040.0,47.7,47.85,47.45,47.65
+8600,1101,2022-02-24 00:00:00,day,13124.0,47.5,47.5,47.1,47.3
+8601,1101,2022-02-25 00:00:00,day,14556.0,47.2,47.5,46.9,47.35
+"""
+
+MOCK_DF = pd.read_csv(io.StringIO(MOCK_DATA), header=0, dtype={"symbol": str})
+
+
+class MockStorageBase:
+    def __init__(self, **kwargs):
+        self.df = MOCK_DF
+
+
+class MockCalendarStorage(MockStorageBase, CalendarStorage):
+    def __init__(self, **kwargs):
+        super().__init__()
+        self._data = sorted(self.df["datetime"].unique())
+
+    @property
+    def data(self) -> List[CalVT]:
+        return self._data
+
+    def __getitem__(self, i: Union[int, slice]) -> Union[CalVT, List[CalVT]]:
+        return self.data[i]
+
+    def __len__(self) -> int:
+        return len(self.data)
+
+
+class MockInstrumentStorage(MockStorageBase, InstrumentStorage):
+    def __init__(self, **kwargs):
+        super().__init__()
+        instruments = {}
+        for symbol, group in self.df.groupby(by="symbol"):
+            start = group["datetime"].iloc[0]
+            end = group["datetime"].iloc[-1]
+            instruments[symbol] = [(start, end)]
+        self._data = instruments
+
+    @property
+    def data(self) -> Dict[InstKT, InstVT]:
+        return self._data
+
+    def __getitem__(self, k: InstKT) -> InstVT:
+        return self.data[k]
+
+    def __len__(self) -> int:
+        return len(self.data)
+
+
+class MockFeatureStorage(MockStorageBase, FeatureStorage):
+    def __init__(self, instrument: str, field: str, freq: str, db_region: str = None, **kwargs):  # type: ignore
+        super().__init__(instrument=instrument, field=field, freq=freq, db_region=db_region, **kwargs)
+        self.field = field
+        calendar = sorted(self.df["datetime"].unique())
+        df_calendar = pd.DataFrame(calendar, columns=["datetime"]).set_index("datetime")
+        df = self.df[self.df["symbol"] == instrument]
+        data_dt_field = "datetime"
+        cal_df = df_calendar[
+            (df_calendar.index >= df[data_dt_field].min()) & (df_calendar.index <= df[data_dt_field].max())
+        ]
+        df = df.set_index(data_dt_field)
+        df_data = df.reindex(cal_df.index)
+        date_index = df_calendar.index.get_loc(df_data.index.min())  # type: ignore
+        df_data.reset_index(inplace=True)
+        df_data.index += date_index
+        self._data = df_data
+
+    @property
+    def data(self) -> pd.Series:
+        return self._data[self.field]
+
+    @property
+    def start_index(self) -> Union[int, None]:
+        if self._data.empty:
+            return None
+        return self._data.index[0]
+
+    @property
+    def end_index(self) -> Union[int, None]:
+        if self._data.empty:
+            return None
+        # The next  data appending index point will be  `end_index + 1`
+        return self._data.index[-1]
+
+    def __getitem__(self, i: Union[int, slice]) -> Union[Tuple[int, float], pd.Series]:
+        df = self._data
+        storage_start_index = df.index[0]
+        storage_end_index = df.index[-1]
+        if isinstance(i, int):
+            if storage_start_index > i or i > storage_end_index:
+                raise IndexError(f"{i}: start index is {storage_start_index}")
+            data = self.data[i]
+            return i, data
+        elif isinstance(i, slice):
+            start_index = storage_start_index if i.start is None else i.start
+            end_index = storage_end_index if i.stop is None else i.stop
+            si = max(start_index, storage_start_index)
+            if si > end_index or self.field not in df.columns:
+                return pd.Series(dtype=np.float32)  # type: ignore
+            data = df[self.field].tolist()
+            result = data[si - storage_start_index : end_index - storage_start_index]
+            return pd.Series(result, index=pd.RangeIndex(si, si + len(result)))  # type: ignore
+        else:
+            raise TypeError(f"type(i) = {type(i)}")
+
+    def __len__(self) -> int:
+        return len(self.data)
+
+
+class TestMockData(unittest.TestCase):
+    _setup_kwargs = {
+        "calendar_provider": {
+            "class": "LocalCalendarProvider",
+            "module_path": "qlib.data.data",
+            "kwargs": {"backend": {"class": "MockCalendarStorage", "module_path": "qlib.tests"}},
+        },
+        "instrument_provider": {
+            "class": "LocalInstrumentProvider",
+            "module_path": "qlib.data.data",
+            "kwargs": {"backend": {"class": "MockInstrumentStorage", "module_path": "qlib.tests"}},
+        },
+        "feature_provider": {
+            "class": "LocalFeatureProvider",
+            "module_path": "qlib.data.data",
+            "kwargs": {"backend": {"class": "MockFeatureStorage", "module_path": "qlib.tests"}},
+        },
+    }
+
+    @classmethod
+    def setUpClass(cls) -> None:
+        provider_uri = "Not necessary."
+        init(region=REG_TW, provider_uri=provider_uri, expression_cache=None, dataset_cache=None, **cls._setup_kwargs)
```

## qlib/tests/config.py

 * *Ordering differences only*

```diff
@@ -1,167 +1,167 @@
-#  Copyright (c) Microsoft Corporation.
-#  Licensed under the MIT License.
-
-CSI300_MARKET = "csi300"
-CSI100_MARKET = "csi100"
-
-CSI300_BENCH = "SH000300"
-
-DATASET_ALPHA158_CLASS = "Alpha158"
-DATASET_ALPHA360_CLASS = "Alpha360"
-
-###################################
-# config
-###################################
-
-
-GBDT_MODEL = {
-    "class": "LGBModel",
-    "module_path": "qlib.contrib.model.gbdt",
-    "kwargs": {
-        "loss": "mse",
-        "colsample_bytree": 0.8879,
-        "learning_rate": 0.0421,
-        "subsample": 0.8789,
-        "lambda_l1": 205.6999,
-        "lambda_l2": 580.9768,
-        "max_depth": 8,
-        "num_leaves": 210,
-        "num_threads": 20,
-    },
-}
-
-
-SA_RC = {
-    "class": "SigAnaRecord",
-    "module_path": "qlib.workflow.record_temp",
-}
-
-
-RECORD_CONFIG = [
-    {
-        "class": "SignalRecord",
-        "module_path": "qlib.workflow.record_temp",
-        "kwargs": {
-            "dataset": "<DATASET>",
-            "model": "<MODEL>",
-        },
-    },
-    SA_RC,
-]
-
-
-def get_data_handler_config(
-    start_time="2008-01-01",
-    end_time="2020-08-01",
-    fit_start_time="<dataset.kwargs.segments.train.0>",
-    fit_end_time="<dataset.kwargs.segments.train.1>",
-    instruments=CSI300_MARKET,
-):
-    return {
-        "start_time": start_time,
-        "end_time": end_time,
-        "fit_start_time": fit_start_time,
-        "fit_end_time": fit_end_time,
-        "instruments": instruments,
-    }
-
-
-def get_dataset_config(
-    dataset_class=DATASET_ALPHA158_CLASS,
-    train=("2008-01-01", "2014-12-31"),
-    valid=("2015-01-01", "2016-12-31"),
-    test=("2017-01-01", "2020-08-01"),
-    handler_kwargs={"instruments": CSI300_MARKET},
-):
-    return {
-        "class": "DatasetH",
-        "module_path": "qlib.data.dataset",
-        "kwargs": {
-            "handler": {
-                "class": dataset_class,
-                "module_path": "qlib.contrib.data.handler",
-                "kwargs": get_data_handler_config(**handler_kwargs),
-            },
-            "segments": {
-                "train": train,
-                "valid": valid,
-                "test": test,
-            },
-        },
-    }
-
-
-def get_gbdt_task(dataset_kwargs={}, handler_kwargs={"instruments": CSI300_MARKET}):
-    return {
-        "model": GBDT_MODEL,
-        "dataset": get_dataset_config(**dataset_kwargs, handler_kwargs=handler_kwargs),
-    }
-
-
-def get_record_lgb_config(dataset_kwargs={}, handler_kwargs={"instruments": CSI300_MARKET}):
-    return {
-        "model": {
-            "class": "LGBModel",
-            "module_path": "qlib.contrib.model.gbdt",
-        },
-        "dataset": get_dataset_config(**dataset_kwargs, handler_kwargs=handler_kwargs),
-        "record": RECORD_CONFIG,
-    }
-
-
-def get_record_xgboost_config(dataset_kwargs={}, handler_kwargs={"instruments": CSI300_MARKET}):
-    return {
-        "model": {
-            "class": "XGBModel",
-            "module_path": "qlib.contrib.model.xgboost",
-        },
-        "dataset": get_dataset_config(**dataset_kwargs, handler_kwargs=handler_kwargs),
-        "record": RECORD_CONFIG,
-    }
-
-
-CSI300_DATASET_CONFIG = get_dataset_config(handler_kwargs={"instruments": CSI300_MARKET})
-CSI300_GBDT_TASK = get_gbdt_task(handler_kwargs={"instruments": CSI300_MARKET})
-
-CSI100_RECORD_XGBOOST_TASK_CONFIG = get_record_xgboost_config(handler_kwargs={"instruments": CSI100_MARKET})
-CSI100_RECORD_LGB_TASK_CONFIG = get_record_lgb_config(handler_kwargs={"instruments": CSI100_MARKET})
-
-# use for rolling_online_managment.py
-ROLLING_HANDLER_CONFIG = {
-    "start_time": "2013-01-01",
-    "end_time": "2020-09-25",
-    "fit_start_time": "2013-01-01",
-    "fit_end_time": "2014-12-31",
-    "instruments": CSI100_MARKET,
-}
-ROLLING_DATASET_CONFIG = {
-    "train": ("2013-01-01", "2014-12-31"),
-    "valid": ("2015-01-01", "2015-12-31"),
-    "test": ("2016-01-01", "2020-07-10"),
-}
-CSI100_RECORD_XGBOOST_TASK_CONFIG_ROLLING = get_record_xgboost_config(
-    dataset_kwargs=ROLLING_DATASET_CONFIG, handler_kwargs=ROLLING_HANDLER_CONFIG
-)
-CSI100_RECORD_LGB_TASK_CONFIG_ROLLING = get_record_lgb_config(
-    dataset_kwargs=ROLLING_DATASET_CONFIG, handler_kwargs=ROLLING_HANDLER_CONFIG
-)
-
-# use for online_management_simulate.py
-ONLINE_HANDLER_CONFIG = {
-    "start_time": "2018-01-01",
-    "end_time": "2018-10-31",
-    "fit_start_time": "2018-01-01",
-    "fit_end_time": "2018-03-31",
-    "instruments": CSI100_MARKET,
-}
-ONLINE_DATASET_CONFIG = {
-    "train": ("2018-01-01", "2018-03-31"),
-    "valid": ("2018-04-01", "2018-05-31"),
-    "test": ("2018-06-01", "2018-09-10"),
-}
-CSI100_RECORD_XGBOOST_TASK_CONFIG_ONLINE = get_record_xgboost_config(
-    dataset_kwargs=ONLINE_DATASET_CONFIG, handler_kwargs=ONLINE_HANDLER_CONFIG
-)
-CSI100_RECORD_LGB_TASK_CONFIG_ONLINE = get_record_lgb_config(
-    dataset_kwargs=ONLINE_DATASET_CONFIG, handler_kwargs=ONLINE_HANDLER_CONFIG
-)
+#  Copyright (c) Microsoft Corporation.
+#  Licensed under the MIT License.
+
+CSI300_MARKET = "csi300"
+CSI100_MARKET = "csi100"
+
+CSI300_BENCH = "SH000300"
+
+DATASET_ALPHA158_CLASS = "Alpha158"
+DATASET_ALPHA360_CLASS = "Alpha360"
+
+###################################
+# config
+###################################
+
+
+GBDT_MODEL = {
+    "class": "LGBModel",
+    "module_path": "qlib.contrib.model.gbdt",
+    "kwargs": {
+        "loss": "mse",
+        "colsample_bytree": 0.8879,
+        "learning_rate": 0.0421,
+        "subsample": 0.8789,
+        "lambda_l1": 205.6999,
+        "lambda_l2": 580.9768,
+        "max_depth": 8,
+        "num_leaves": 210,
+        "num_threads": 20,
+    },
+}
+
+
+SA_RC = {
+    "class": "SigAnaRecord",
+    "module_path": "qlib.workflow.record_temp",
+}
+
+
+RECORD_CONFIG = [
+    {
+        "class": "SignalRecord",
+        "module_path": "qlib.workflow.record_temp",
+        "kwargs": {
+            "dataset": "<DATASET>",
+            "model": "<MODEL>",
+        },
+    },
+    SA_RC,
+]
+
+
+def get_data_handler_config(
+    start_time="2008-01-01",
+    end_time="2020-08-01",
+    fit_start_time="<dataset.kwargs.segments.train.0>",
+    fit_end_time="<dataset.kwargs.segments.train.1>",
+    instruments=CSI300_MARKET,
+):
+    return {
+        "start_time": start_time,
+        "end_time": end_time,
+        "fit_start_time": fit_start_time,
+        "fit_end_time": fit_end_time,
+        "instruments": instruments,
+    }
+
+
+def get_dataset_config(
+    dataset_class=DATASET_ALPHA158_CLASS,
+    train=("2008-01-01", "2014-12-31"),
+    valid=("2015-01-01", "2016-12-31"),
+    test=("2017-01-01", "2020-08-01"),
+    handler_kwargs={"instruments": CSI300_MARKET},
+):
+    return {
+        "class": "DatasetH",
+        "module_path": "qlib.data.dataset",
+        "kwargs": {
+            "handler": {
+                "class": dataset_class,
+                "module_path": "qlib.contrib.data.handler",
+                "kwargs": get_data_handler_config(**handler_kwargs),
+            },
+            "segments": {
+                "train": train,
+                "valid": valid,
+                "test": test,
+            },
+        },
+    }
+
+
+def get_gbdt_task(dataset_kwargs={}, handler_kwargs={"instruments": CSI300_MARKET}):
+    return {
+        "model": GBDT_MODEL,
+        "dataset": get_dataset_config(**dataset_kwargs, handler_kwargs=handler_kwargs),
+    }
+
+
+def get_record_lgb_config(dataset_kwargs={}, handler_kwargs={"instruments": CSI300_MARKET}):
+    return {
+        "model": {
+            "class": "LGBModel",
+            "module_path": "qlib.contrib.model.gbdt",
+        },
+        "dataset": get_dataset_config(**dataset_kwargs, handler_kwargs=handler_kwargs),
+        "record": RECORD_CONFIG,
+    }
+
+
+def get_record_xgboost_config(dataset_kwargs={}, handler_kwargs={"instruments": CSI300_MARKET}):
+    return {
+        "model": {
+            "class": "XGBModel",
+            "module_path": "qlib.contrib.model.xgboost",
+        },
+        "dataset": get_dataset_config(**dataset_kwargs, handler_kwargs=handler_kwargs),
+        "record": RECORD_CONFIG,
+    }
+
+
+CSI300_DATASET_CONFIG = get_dataset_config(handler_kwargs={"instruments": CSI300_MARKET})
+CSI300_GBDT_TASK = get_gbdt_task(handler_kwargs={"instruments": CSI300_MARKET})
+
+CSI100_RECORD_XGBOOST_TASK_CONFIG = get_record_xgboost_config(handler_kwargs={"instruments": CSI100_MARKET})
+CSI100_RECORD_LGB_TASK_CONFIG = get_record_lgb_config(handler_kwargs={"instruments": CSI100_MARKET})
+
+# use for rolling_online_managment.py
+ROLLING_HANDLER_CONFIG = {
+    "start_time": "2013-01-01",
+    "end_time": "2020-09-25",
+    "fit_start_time": "2013-01-01",
+    "fit_end_time": "2014-12-31",
+    "instruments": CSI100_MARKET,
+}
+ROLLING_DATASET_CONFIG = {
+    "train": ("2013-01-01", "2014-12-31"),
+    "valid": ("2015-01-01", "2015-12-31"),
+    "test": ("2016-01-01", "2020-07-10"),
+}
+CSI100_RECORD_XGBOOST_TASK_CONFIG_ROLLING = get_record_xgboost_config(
+    dataset_kwargs=ROLLING_DATASET_CONFIG, handler_kwargs=ROLLING_HANDLER_CONFIG
+)
+CSI100_RECORD_LGB_TASK_CONFIG_ROLLING = get_record_lgb_config(
+    dataset_kwargs=ROLLING_DATASET_CONFIG, handler_kwargs=ROLLING_HANDLER_CONFIG
+)
+
+# use for online_management_simulate.py
+ONLINE_HANDLER_CONFIG = {
+    "start_time": "2018-01-01",
+    "end_time": "2018-10-31",
+    "fit_start_time": "2018-01-01",
+    "fit_end_time": "2018-03-31",
+    "instruments": CSI100_MARKET,
+}
+ONLINE_DATASET_CONFIG = {
+    "train": ("2018-01-01", "2018-03-31"),
+    "valid": ("2018-04-01", "2018-05-31"),
+    "test": ("2018-06-01", "2018-09-10"),
+}
+CSI100_RECORD_XGBOOST_TASK_CONFIG_ONLINE = get_record_xgboost_config(
+    dataset_kwargs=ONLINE_DATASET_CONFIG, handler_kwargs=ONLINE_HANDLER_CONFIG
+)
+CSI100_RECORD_LGB_TASK_CONFIG_ONLINE = get_record_lgb_config(
+    dataset_kwargs=ONLINE_DATASET_CONFIG, handler_kwargs=ONLINE_HANDLER_CONFIG
+)
```

## qlib/tests/data.py

```diff
@@ -1,191 +1,191 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-import os
-import re
-import sys
-import qlib
-import shutil
-import zipfile
-import requests
-import datetime
-from tqdm import tqdm
-from pathlib import Path
-from loguru import logger
-from cryptography.fernet import Fernet
-from qlib.utils import exists_qlib_data
-
-
-class GetData:
-    REMOTE_URL = "https://qlibpublic.blob.core.windows.net/data/default/stock_data"
-    # "?" is not included in the token.
-    TOKEN = "gAAAAABkmDhojHc0VSCDdNK1MqmRzNLeDFXe5hy8obHpa6SDQh4de6nW5gtzuD-fa6O_WZb0yyqYOL7ndOfJX_751W3xN5YB4-n-P22jK-t6ucoZqhT70KPD0Lf0_P328QPJVZ1gDnjIdjhi2YLOcP4BFTHLNYO0mvzszR8TKm9iT5AKRvuysWnpi8bbYwGU9zAcJK3x9EPL43hOGtxliFHcPNGMBoJW4g_ercdhi0-Qgv5_JLsV-29_MV-_AhuaYvJuN2dEywBy"
-    KEY = "EYcA8cgorA8X9OhyMwVfuFxn_1W3jGk6jCbs3L2oPoA="
-
-    def __init__(self, delete_zip_file=False):
-        """
-
-        Parameters
-        ----------
-        delete_zip_file : bool, optional
-            Whether to delete the zip file, value from True or False, by default False
-        """
-        self.delete_zip_file = delete_zip_file
-
-    def merge_remote_url(self, file_name: str):
-        fernet = Fernet(self.KEY)
-        token = fernet.decrypt(self.TOKEN).decode()
-        return f"{self.REMOTE_URL}/{file_name}?{token}"
-
-    def download_data(self, file_name: str, target_dir: [Path, str], delete_old: bool = True):
-        """
-        Download the specified file to the target folder.
-
-        Parameters
-        ----------
-        target_dir: str
-            data save directory
-        file_name: str
-            dataset name, needs to endwith .zip, value from [rl_data.zip, csv_data_cn.zip, ...]
-            may contain folder names, for example: v2/qlib_data_simple_cn_1d_latest.zip
-        delete_old: bool
-            delete an existing directory, by default True
-
-        Examples
-        ---------
-        # get rl data
-        python get_data.py download_data --file_name rl_data.zip --target_dir ~/.qlib/qlib_data/rl_data
-        When this command is run, the data will be downloaded from this link: https://qlibpublic.blob.core.windows.net/data/default/stock_data/rl_data.zip?{token}
-
-        # get cn csv data
-        python get_data.py download_data --file_name csv_data_cn.zip --target_dir ~/.qlib/csv_data/cn_data
-        When this command is run, the data will be downloaded from this link: https://qlibpublic.blob.core.windows.net/data/default/stock_data/csv_data_cn.zip?{token}
-        -------
-
-        """
-        target_dir = Path(target_dir).expanduser()
-        target_dir.mkdir(exist_ok=True, parents=True)
-        # saved file name
-        _target_file_name = datetime.datetime.now().strftime("%Y%m%d%H%M%S") + "_" + os.path.basename(file_name)
-        target_path = target_dir.joinpath(_target_file_name)
-
-        url = self.merge_remote_url(file_name)
-        resp = requests.get(url, stream=True, timeout=60)
-        resp.raise_for_status()
-        if resp.status_code != 200:
-            raise requests.exceptions.HTTPError()
-
-        chunk_size = 1024
-        logger.warning(
-            f"The data for the example is collected from Yahoo Finance. Please be aware that the quality of the data might not be perfect. (You can refer to the original data source: https://finance.yahoo.com/lookup.)"
-        )
-        logger.info(f"{os.path.basename(file_name)} downloading......")
-        with tqdm(total=int(resp.headers.get("Content-Length", 0))) as p_bar:
-            with target_path.open("wb") as fp:
-                for chunk in resp.iter_content(chunk_size=chunk_size):
-                    fp.write(chunk)
-                    p_bar.update(chunk_size)
-
-        self._unzip(target_path, target_dir, delete_old)
-        if self.delete_zip_file:
-            target_path.unlink()
-
-    def check_dataset(self, file_name: str):
-        url = self.merge_remote_url(file_name)
-        resp = requests.get(url, stream=True, timeout=60)
-        status = True
-        if resp.status_code == 404:
-            status = False
-        return status
-
-    @staticmethod
-    def _unzip(file_path: Path, target_dir: Path, delete_old: bool = True):
-        if delete_old:
-            logger.warning(
-                f"will delete the old qlib data directory(features, instruments, calendars, features_cache, dataset_cache): {target_dir}"
-            )
-            GetData._delete_qlib_data(target_dir)
-        logger.info(f"{file_path} unzipping......")
-        with zipfile.ZipFile(str(file_path.resolve()), "r") as zp:
-            for _file in tqdm(zp.namelist()):
-                zp.extract(_file, str(target_dir.resolve()))
-
-    @staticmethod
-    def _delete_qlib_data(file_dir: Path):
-        rm_dirs = []
-        for _name in ["features", "calendars", "instruments", "features_cache", "dataset_cache"]:
-            _p = file_dir.joinpath(_name)
-            if _p.exists():
-                rm_dirs.append(str(_p.resolve()))
-        if rm_dirs:
-            flag = input(
-                f"Will be deleted: "
-                f"\n\t{rm_dirs}"
-                f"\nIf you do not need to delete {file_dir}, please change the <--target_dir>"
-                f"\nAre you sure you want to delete, yes(Y/y), no (N/n):"
-            )
-            if str(flag) not in ["Y", "y"]:
-                sys.exit()
-            for _p in rm_dirs:
-                logger.warning(f"delete: {_p}")
-                shutil.rmtree(_p)
-
-    def qlib_data(
-        self,
-        name="qlib_data",
-        target_dir="~/.qlib/qlib_data/cn_data",
-        version=None,
-        interval="1d",
-        region="cn",
-        delete_old=True,
-        exists_skip=False,
-    ):
-        """download cn qlib data from remote
-
-        Parameters
-        ----------
-        target_dir: str
-            data save directory
-        name: str
-            dataset name, value from [qlib_data, qlib_data_simple], by default qlib_data
-        version: str
-            data version, value from [v1, ...], by default None(use script to specify version)
-        interval: str
-            data freq, value from [1d], by default 1d
-        region: str
-            data region, value from [cn, us], by default cn
-        delete_old: bool
-            delete an existing directory, by default True
-        exists_skip: bool
-            exists skip, by default False
-
-        Examples
-        ---------
-        # get 1d data
-        python get_data.py qlib_data --name qlib_data --target_dir ~/.qlib/qlib_data/cn_data --interval 1d --region cn
-        When this command is run, the data will be downloaded from this link: https://qlibpublic.blob.core.windows.net/data/default/stock_data/v2/qlib_data_cn_1d_latest.zip?{token}
-
-        # get 1min data
-        python get_data.py qlib_data --name qlib_data --target_dir ~/.qlib/qlib_data/cn_data_1min --interval 1min --region cn
-        When this command is run, the data will be downloaded from this link: https://qlibpublic.blob.core.windows.net/data/default/stock_data/v2/qlib_data_cn_1min_latest.zip?{token}
-        -------
-
-        """
-        if exists_skip and exists_qlib_data(target_dir):
-            logger.warning(
-                f"Data already exists: {target_dir}, the data download will be skipped\n"
-                f"\tIf downloading is required: `exists_skip=False` or `change target_dir`"
-            )
-            return
-
-        qlib_version = ".".join(re.findall(r"(\d+)\.+", qlib.__version__))
-
-        def _get_file_name_with_version(qlib_version, dataset_version):
-            dataset_version = "v2" if dataset_version is None else dataset_version
-            file_name_with_version = f"{dataset_version}/{name}_{region.lower()}_{interval.lower()}_{qlib_version}.zip"
-            return file_name_with_version
-
-        file_name = _get_file_name_with_version(qlib_version, dataset_version=version)
-        if not self.check_dataset(file_name):
-            file_name = _get_file_name_with_version("latest", dataset_version=version)
-        self.download_data(file_name.lower(), target_dir, delete_old)
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+import os
+import re
+import sys
+import qlib
+import shutil
+import zipfile
+import requests
+import datetime
+from tqdm import tqdm
+from pathlib import Path
+from loguru import logger
+from cryptography.fernet import Fernet
+from qlib.utils import exists_qlib_data
+
+
+class GetData:
+    REMOTE_URL = "https://qlibpublic.blob.core.windows.net/data/default/stock_data"
+    # "?" is not included in the token.
+    TOKEN = b"gAAAAABkmDhojHc0VSCDdNK1MqmRzNLeDFXe5hy8obHpa6SDQh4de6nW5gtzuD-fa6O_WZb0yyqYOL7ndOfJX_751W3xN5YB4-n-P22jK-t6ucoZqhT70KPD0Lf0_P328QPJVZ1gDnjIdjhi2YLOcP4BFTHLNYO0mvzszR8TKm9iT5AKRvuysWnpi8bbYwGU9zAcJK3x9EPL43hOGtxliFHcPNGMBoJW4g_ercdhi0-Qgv5_JLsV-29_MV-_AhuaYvJuN2dEywBy"
+    KEY = "EYcA8cgorA8X9OhyMwVfuFxn_1W3jGk6jCbs3L2oPoA="
+
+    def __init__(self, delete_zip_file=False):
+        """
+
+        Parameters
+        ----------
+        delete_zip_file : bool, optional
+            Whether to delete the zip file, value from True or False, by default False
+        """
+        self.delete_zip_file = delete_zip_file
+
+    def merge_remote_url(self, file_name: str):
+        fernet = Fernet(self.KEY)
+        token = fernet.decrypt(self.TOKEN).decode()
+        return f"{self.REMOTE_URL}/{file_name}?{token}"
+
+    def download_data(self, file_name: str, target_dir: [Path, str], delete_old: bool = True):
+        """
+        Download the specified file to the target folder.
+
+        Parameters
+        ----------
+        target_dir: str
+            data save directory
+        file_name: str
+            dataset name, needs to endwith .zip, value from [rl_data.zip, csv_data_cn.zip, ...]
+            may contain folder names, for example: v2/qlib_data_simple_cn_1d_latest.zip
+        delete_old: bool
+            delete an existing directory, by default True
+
+        Examples
+        ---------
+        # get rl data
+        python get_data.py download_data --file_name rl_data.zip --target_dir ~/.qlib/qlib_data/rl_data
+        When this command is run, the data will be downloaded from this link: https://qlibpublic.blob.core.windows.net/data/default/stock_data/rl_data.zip?{token}
+
+        # get cn csv data
+        python get_data.py download_data --file_name csv_data_cn.zip --target_dir ~/.qlib/csv_data/cn_data
+        When this command is run, the data will be downloaded from this link: https://qlibpublic.blob.core.windows.net/data/default/stock_data/csv_data_cn.zip?{token}
+        -------
+
+        """
+        target_dir = Path(target_dir).expanduser()
+        target_dir.mkdir(exist_ok=True, parents=True)
+        # saved file name
+        _target_file_name = datetime.datetime.now().strftime("%Y%m%d%H%M%S") + "_" + os.path.basename(file_name)
+        target_path = target_dir.joinpath(_target_file_name)
+
+        url = self.merge_remote_url(file_name)
+        resp = requests.get(url, stream=True, timeout=60)
+        resp.raise_for_status()
+        if resp.status_code != 200:
+            raise requests.exceptions.HTTPError()
+
+        chunk_size = 1024
+        logger.warning(
+            f"The data for the example is collected from Yahoo Finance. Please be aware that the quality of the data might not be perfect. (You can refer to the original data source: https://finance.yahoo.com/lookup.)"
+        )
+        logger.info(f"{os.path.basename(file_name)} downloading......")
+        with tqdm(total=int(resp.headers.get("Content-Length", 0))) as p_bar:
+            with target_path.open("wb") as fp:
+                for chunk in resp.iter_content(chunk_size=chunk_size):
+                    fp.write(chunk)
+                    p_bar.update(chunk_size)
+
+        self._unzip(target_path, target_dir, delete_old)
+        if self.delete_zip_file:
+            target_path.unlink()
+
+    def check_dataset(self, file_name: str):
+        url = self.merge_remote_url(file_name)
+        resp = requests.get(url, stream=True, timeout=60)
+        status = True
+        if resp.status_code == 404:
+            status = False
+        return status
+
+    @staticmethod
+    def _unzip(file_path: Path, target_dir: Path, delete_old: bool = True):
+        if delete_old:
+            logger.warning(
+                f"will delete the old qlib data directory(features, instruments, calendars, features_cache, dataset_cache): {target_dir}"
+            )
+            GetData._delete_qlib_data(target_dir)
+        logger.info(f"{file_path} unzipping......")
+        with zipfile.ZipFile(str(file_path.resolve()), "r") as zp:
+            for _file in tqdm(zp.namelist()):
+                zp.extract(_file, str(target_dir.resolve()))
+
+    @staticmethod
+    def _delete_qlib_data(file_dir: Path):
+        rm_dirs = []
+        for _name in ["features", "calendars", "instruments", "features_cache", "dataset_cache"]:
+            _p = file_dir.joinpath(_name)
+            if _p.exists():
+                rm_dirs.append(str(_p.resolve()))
+        if rm_dirs:
+            flag = input(
+                f"Will be deleted: "
+                f"\n\t{rm_dirs}"
+                f"\nIf you do not need to delete {file_dir}, please change the <--target_dir>"
+                f"\nAre you sure you want to delete, yes(Y/y), no (N/n):"
+            )
+            if str(flag) not in ["Y", "y"]:
+                sys.exit()
+            for _p in rm_dirs:
+                logger.warning(f"delete: {_p}")
+                shutil.rmtree(_p)
+
+    def qlib_data(
+        self,
+        name="qlib_data",
+        target_dir="~/.qlib/qlib_data/cn_data",
+        version=None,
+        interval="1d",
+        region="cn",
+        delete_old=True,
+        exists_skip=False,
+    ):
+        """download cn qlib data from remote
+
+        Parameters
+        ----------
+        target_dir: str
+            data save directory
+        name: str
+            dataset name, value from [qlib_data, qlib_data_simple], by default qlib_data
+        version: str
+            data version, value from [v1, ...], by default None(use script to specify version)
+        interval: str
+            data freq, value from [1d], by default 1d
+        region: str
+            data region, value from [cn, us], by default cn
+        delete_old: bool
+            delete an existing directory, by default True
+        exists_skip: bool
+            exists skip, by default False
+
+        Examples
+        ---------
+        # get 1d data
+        python get_data.py qlib_data --name qlib_data --target_dir ~/.qlib/qlib_data/cn_data --interval 1d --region cn
+        When this command is run, the data will be downloaded from this link: https://qlibpublic.blob.core.windows.net/data/default/stock_data/v2/qlib_data_cn_1d_latest.zip?{token}
+
+        # get 1min data
+        python get_data.py qlib_data --name qlib_data --target_dir ~/.qlib/qlib_data/cn_data_1min --interval 1min --region cn
+        When this command is run, the data will be downloaded from this link: https://qlibpublic.blob.core.windows.net/data/default/stock_data/v2/qlib_data_cn_1min_latest.zip?{token}
+        -------
+
+        """
+        if exists_skip and exists_qlib_data(target_dir):
+            logger.warning(
+                f"Data already exists: {target_dir}, the data download will be skipped\n"
+                f"\tIf downloading is required: `exists_skip=False` or `change target_dir`"
+            )
+            return
+
+        qlib_version = ".".join(re.findall(r"(\d+)\.+", qlib.__version__))
+
+        def _get_file_name_with_version(qlib_version, dataset_version):
+            dataset_version = "v2" if dataset_version is None else dataset_version
+            file_name_with_version = f"{dataset_version}/{name}_{region.lower()}_{interval.lower()}_{qlib_version}.zip"
+            return file_name_with_version
+
+        file_name = _get_file_name_with_version(qlib_version, dataset_version=version)
+        if not self.check_dataset(file_name):
+            file_name = _get_file_name_with_version("latest", dataset_version=version)
+        self.download_data(file_name.lower(), target_dir, delete_old)
```

## qlib/utils/__init__.py

```diff
@@ -1,1059 +1,901 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-# TODO: this utils covers too much utilities, please seperat it into sub modules
-
-from __future__ import division
-from __future__ import print_function
-
-import os
-import pickle
-import re
-import sys
-import copy
-import json
-from qlib.typehint import InstConf
-import yaml
-import redis
-import bisect
-import struct
-import difflib
-import inspect
-import hashlib
-import datetime
-import requests
-import importlib
-import contextlib
-import collections
-import numpy as np
-import pandas as pd
-from pathlib import Path
-from typing import List, Dict, Union, Tuple, Any, Optional, Callable
-from types import ModuleType
-from urllib.parse import urlparse
-from packaging import version
-from .file import get_or_create_path, save_multiple_parts_file, unpack_archive_with_buffer, get_tmp_file_with_buffer
-from ..config import C
-from ..log import get_module_logger, set_log_with_config
-
-log = get_module_logger("utils")
-# MultiIndex.is_lexsorted() is a deprecated method in Pandas 1.3.0.
-is_deprecated_lexsorted_pandas = version.parse(pd.__version__) > version.parse("1.3.0")
-
-
-#################### Server ####################
-def get_redis_connection():
-    """get redis connection instance."""
-    return redis.StrictRedis(host=C.redis_host, port=C.redis_port, db=C.redis_task_db, password=C.redis_password)
-
-
-#################### Data ####################
-def read_bin(file_path: Union[str, Path], start_index, end_index):
-    file_path = Path(file_path.expanduser().resolve())
-    with file_path.open("rb") as f:
-        # read start_index
-        ref_start_index = int(np.frombuffer(f.read(4), dtype="<f")[0])
-        si = max(ref_start_index, start_index)
-        if si > end_index:
-            return pd.Series(dtype=np.float32)
-        # calculate offset
-        f.seek(4 * (si - ref_start_index) + 4)
-        # read nbytes
-        count = end_index - si + 1
-        data = np.frombuffer(f.read(4 * count), dtype="<f")
-        series = pd.Series(data, index=pd.RangeIndex(si, si + len(data)))
-    return series
-
-
-def get_period_list(first: int, last: int, quarterly: bool) -> List[int]:
-    """
-    This method will be used in PIT database.
-    It return all the possible values between `first` and `end`  (first and end is included)
-
-    Parameters
-    ----------
-    quarterly : bool
-        will it return quarterly index or yearly index.
-
-    Returns
-    -------
-    List[int]
-        the possible index between [first, last]
-    """
-
-    if not quarterly:
-        assert all(1900 <= x <= 2099 for x in (first, last)), "invalid arguments"
-        return list(range(first, last + 1))
-    else:
-        assert all(190000 <= x <= 209904 for x in (first, last)), "invalid arguments"
-        res = []
-        for year in range(first // 100, last // 100 + 1):
-            for q in range(1, 5):
-                period = year * 100 + q
-                if first <= period <= last:
-                    res.append(year * 100 + q)
-        return res
-
-
-def get_period_offset(first_year, period, quarterly):
-    if quarterly:
-        offset = (period // 100 - first_year) * 4 + period % 100 - 1
-    else:
-        offset = period - first_year
-    return offset
-
-
-def read_period_data(index_path, data_path, period, cur_date_int: int, quarterly, last_period_index: int = None):
-    """
-    At `cur_date`(e.g. 20190102), read the information at `period`(e.g. 201803).
-    Only the updating info before cur_date or at cur_date will be used.
-
-    Parameters
-    ----------
-    period: int
-        date period represented by interger, e.g. 201901 corresponds to the first quarter in 2019
-    cur_date_int: int
-        date which represented by interger, e.g. 20190102
-    last_period_index: int
-        it is a optional parameter; it is designed to avoid repeatedly access the .index data of PIT database when
-        sequentially observing the data (Because the latest index of a specific period of data certainly appear in after the one in last observation).
-
-    Returns
-    -------
-    the query value and byte index the index value
-    """
-    DATA_DTYPE = "".join(
-        [
-            C.pit_record_type["date"],
-            C.pit_record_type["period"],
-            C.pit_record_type["value"],
-            C.pit_record_type["index"],
-        ]
-    )
-
-    PERIOD_DTYPE = C.pit_record_type["period"]
-    INDEX_DTYPE = C.pit_record_type["index"]
-
-    NAN_VALUE = C.pit_record_nan["value"]
-    NAN_INDEX = C.pit_record_nan["index"]
-
-    # find the first index of linked revisions
-    if last_period_index is None:
-        with open(index_path, "rb") as fi:
-            (first_year,) = struct.unpack(PERIOD_DTYPE, fi.read(struct.calcsize(PERIOD_DTYPE)))
-            all_periods = np.fromfile(fi, dtype=INDEX_DTYPE)
-        offset = get_period_offset(first_year, period, quarterly)
-        _next = all_periods[offset]
-    else:
-        _next = last_period_index
-
-    # load data following the `_next` link
-    prev_value = NAN_VALUE
-    prev_next = _next
-
-    with open(data_path, "rb") as fd:
-        while _next != NAN_INDEX:
-            fd.seek(_next)
-            date, period, value, new_next = struct.unpack(DATA_DTYPE, fd.read(struct.calcsize(DATA_DTYPE)))
-            if date > cur_date_int:
-                break
-            prev_next = _next
-            _next = new_next
-            prev_value = value
-    return prev_value, prev_next
-
-
-def np_ffill(arr: np.array):
-    """
-    forward fill a 1D numpy array
-
-    Parameters
-    ----------
-    arr : np.array
-        Input numpy 1D array
-    """
-    mask = np.isnan(arr.astype(float))  # np.isnan only works on np.float
-    # get fill index
-    idx = np.where(~mask, np.arange(mask.shape[0]), 0)
-    np.maximum.accumulate(idx, out=idx)
-    return arr[idx]
-
-
-#################### Search ####################
-def lower_bound(data, val, level=0):
-    """multi fields list lower bound.
-
-    for single field list use `bisect.bisect_left` instead
-    """
-    left = 0
-    right = len(data)
-    while left < right:
-        mid = (left + right) // 2
-        if val <= data[mid][level]:
-            right = mid
-        else:
-            left = mid + 1
-    return left
-
-
-def upper_bound(data, val, level=0):
-    """multi fields list upper bound.
-
-    for single field list use `bisect.bisect_right` instead
-    """
-    left = 0
-    right = len(data)
-    while left < right:
-        mid = (left + right) // 2
-        if val >= data[mid][level]:
-            left = mid + 1
-        else:
-            right = mid
-    return left
-
-
-#################### HTTP ####################
-def requests_with_retry(url, retry=5, **kwargs):
-    while retry > 0:
-        retry -= 1
-        try:
-            res = requests.get(url, timeout=1, **kwargs)
-            assert res.status_code in {200, 206}
-            return res
-        except AssertionError:
-            continue
-        except Exception as e:
-            log.warning("exception encountered {}".format(e))
-            continue
-    raise TimeoutError("ERROR: requests failed!")
-
-
-#################### Parse ####################
-def parse_config(config):
-    # Check whether need parse, all object except str do not need to be parsed
-    if not isinstance(config, str):
-        return config
-    # Check whether config is file
-    if os.path.exists(config):
-        with open(config, "r") as f:
-            return yaml.safe_load(f)
-    # Check whether the str can be parsed
-    try:
-        return yaml.safe_load(config)
-    except BaseException as base_exp:
-        raise ValueError("cannot parse config!") from base_exp
-
-
-#################### Other ####################
-def drop_nan_by_y_index(x, y, weight=None):
-    # x, y, weight: DataFrame
-    # Find index of rows which do not contain Nan in all columns from y.
-    mask = ~y.isna().any(axis=1)
-    # Get related rows from x, y, weight.
-    x = x[mask]
-    y = y[mask]
-    if weight is not None:
-        weight = weight[mask]
-    return x, y, weight
-
-
-def hash_args(*args):
-    # json.dumps will keep the dict keys always sorted.
-    string = json.dumps(args, sort_keys=True, default=str)  # frozenset
-    return hashlib.md5(string.encode()).hexdigest()
-
-
-def parse_field(field):
-    # Following patterns will be matched:
-    # - $close -> Feature("close")
-    # - $close5 -> Feature("close5")
-    # - $open+$close -> Feature("open")+Feature("close")
-    # TODO: this maybe used in the feature if we want to support the computation of different frequency data
-    # - $close@5min -> Feature("close", "5min")
-
-    if not isinstance(field, str):
-        field = str(field)
-    # Chinese punctuation regex:
-    # \u3001 -> 、
-    # \uff1a -> ：
-    # \uff08 -> (
-    # \uff09 -> )
-    chinese_punctuation_regex = r"\u3001\uff1a\uff08\uff09"
-    for pattern, new in [
-        (rf"\$\$([\w{chinese_punctuation_regex}]+)", r'PFeature("\1")'),  # $$ must be before $
-        (rf"\$([\w{chinese_punctuation_regex}]+)", r'Feature("\1")'),
-        (r"(\w+\s*)\(", r"Operators.\1("),
-    ]:  # Features  # Operators
-        field = re.sub(pattern, new, field)
-    return field
-
-
-def get_module_by_module_path(module_path: Union[str, ModuleType]):
-    """Load module path
-
-    :param module_path:
-    :return:
-    :raises: ModuleNotFoundError
-    """
-    if module_path is None:
-        raise ModuleNotFoundError("None is passed in as parameters as module_path")
-
-    if isinstance(module_path, ModuleType):
-        module = module_path
-    else:
-        if module_path.endswith(".py"):
-            module_name = re.sub("^[^a-zA-Z_]+", "", re.sub("[^0-9a-zA-Z_]", "", module_path[:-3].replace("/", "_")))
-            module_spec = importlib.util.spec_from_file_location(module_name, module_path)
-            module = importlib.util.module_from_spec(module_spec)
-            sys.modules[module_name] = module
-            module_spec.loader.exec_module(module)
-        else:
-            module = importlib.import_module(module_path)
-    return module
-
-
-def split_module_path(module_path: str) -> Tuple[str, str]:
-    """
-
-    Parameters
-    ----------
-    module_path : str
-        e.g. "a.b.c.ClassName"
-
-    Returns
-    -------
-    Tuple[str, str]
-        e.g. ("a.b.c", "ClassName")
-    """
-    *m_path, cls = module_path.split(".")
-    m_path = ".".join(m_path)
-    return m_path, cls
-
-
-def get_callable_kwargs(config: InstConf, default_module: Union[str, ModuleType] = None) -> (type, dict):
-    """
-    extract class/func and kwargs from config info
-
-    Parameters
-    ----------
-    config : [dict, str]
-        similar to config
-        please refer to the doc of init_instance_by_config
-
-    default_module : Python module or str
-        It should be a python module to load the class type
-        This function will load class from the config['module_path'] first.
-        If config['module_path'] doesn't exists, it will load the class from default_module.
-
-    Returns
-    -------
-    (type, dict):
-        the class/func object and it's arguments.
-
-    Raises
-    ------
-        ModuleNotFoundError
-    """
-    if isinstance(config, dict):
-        key = "class" if "class" in config else "func"
-        if isinstance(config[key], str):
-            # 1) get module and class
-            # - case 1): "a.b.c.ClassName"
-            # - case 2): {"class": "ClassName", "module_path": "a.b.c"}
-            m_path, cls = split_module_path(config[key])
-            if m_path == "":
-                m_path = config.get("module_path", default_module)
-            module = get_module_by_module_path(m_path)
-
-            # 2) get callable
-            _callable = getattr(module, cls)  # may raise AttributeError
-        else:
-            _callable = config[key]  # the class type itself is passed in
-        kwargs = config.get("kwargs", {})
-    elif isinstance(config, str):
-        # a.b.c.ClassName
-        m_path, cls = split_module_path(config)
-        module = get_module_by_module_path(default_module if m_path == "" else m_path)
-
-        _callable = getattr(module, cls)
-        kwargs = {}
-    else:
-        raise NotImplementedError(f"This type of input is not supported")
-    return _callable, kwargs
-
-
-get_cls_kwargs = get_callable_kwargs  # NOTE: this is for compatibility for the previous version
-
-
-def init_instance_by_config(
-    config: InstConf,
-    default_module=None,
-    accept_types: Union[type, Tuple[type]] = (),
-    try_kwargs: Dict = {},
-    **kwargs,
-) -> Any:
-    """
-    get initialized instance with config
-
-    Parameters
-    ----------
-    config : InstConf
-
-    default_module : Python module
-        Optional. It should be a python module.
-        NOTE: the "module_path" will be override by `module` arguments
-
-        This function will load class from the config['module_path'] first.
-        If config['module_path'] doesn't exists, it will load the class from default_module.
-
-    accept_types: Union[type, Tuple[type]]
-        Optional. If the config is a instance of specific type, return the config directly.
-        This will be passed into the second parameter of isinstance.
-
-    try_kwargs: Dict
-        Try to pass in kwargs in `try_kwargs` when initialized the instance
-        If error occurred, it will fail back to initialization without try_kwargs.
-
-    Returns
-    -------
-    object:
-        An initialized object based on the config info
-    """
-    if isinstance(config, accept_types):
-        return config
-
-    if isinstance(config, (str, Path)):
-        if isinstance(config, str):
-            # path like 'file:///<path to pickle file>/obj.pkl'
-            pr = urlparse(config)
-            if pr.scheme == "file":
-                pr_path = os.path.join(pr.netloc, pr.path) if bool(pr.path) else pr.netloc
-                with open(os.path.normpath(pr_path), "rb") as f:
-                    return pickle.load(f)
-        else:
-            with config.open("rb") as f:
-                return pickle.load(f)
-
-    klass, cls_kwargs = get_callable_kwargs(config, default_module=default_module)
-
-    try:
-        return klass(**cls_kwargs, **try_kwargs, **kwargs)
-    except (TypeError,):
-        # TypeError for handling errors like
-        # 1: `XXX() got multiple values for keyword argument 'YYY'`
-        # 2: `XXX() got an unexpected keyword argument 'YYY'
-        return klass(**cls_kwargs, **kwargs)
-
-
-@contextlib.contextmanager
-def class_casting(obj: object, cls: type):
-    """
-    Python doesn't provide the downcasting mechanism.
-    We use the trick here to downcast the class
-
-    Parameters
-    ----------
-    obj : object
-        the object to be cast
-    cls : type
-        the target class type
-    """
-    orig_cls = obj.__class__
-    obj.__class__ = cls
-    yield
-    obj.__class__ = orig_cls
-
-
-def compare_dict_value(src_data: dict, dst_data: dict):
-    """Compare dict value
-
-    :param src_data:
-    :param dst_data:
-    :return:
-    """
-
-    class DateEncoder(json.JSONEncoder):
-        # FIXME: This class can only be accurate to the day. If it is a minute,
-        # there may be a bug
-        def default(self, o):
-            if isinstance(o, (datetime.datetime, datetime.date)):
-                return o.strftime("%Y-%m-%d %H:%M:%S")
-            return json.JSONEncoder.default(self, o)
-
-    src_data = json.dumps(src_data, indent=4, sort_keys=True, cls=DateEncoder)
-    dst_data = json.dumps(dst_data, indent=4, sort_keys=True, cls=DateEncoder)
-    diff = difflib.ndiff(src_data, dst_data)
-    changes = [line for line in diff if line.startswith("+ ") or line.startswith("- ")]
-    return changes
-
-
-def remove_repeat_field(fields):
-    """remove repeat field
-
-    :param fields: list; features fields
-    :return: list
-    """
-    fields = copy.deepcopy(fields)
-    _fields = set(fields)
-    return sorted(_fields, key=fields.index)
-
-
-def remove_fields_space(fields: [list, str, tuple]):
-    """remove fields space
-
-    :param fields: features fields
-    :return: list or str
-    """
-    if isinstance(fields, str):
-        return fields.replace(" ", "")
-    return [i.replace(" ", "") if isinstance(i, str) else str(i) for i in fields]
-
-
-def normalize_cache_fields(fields: [list, tuple]):
-    """normalize cache fields
-
-    :param fields: features fields
-    :return: list
-    """
-    return sorted(remove_repeat_field(remove_fields_space(fields)))
-
-
-def normalize_cache_instruments(instruments):
-    """normalize cache instruments
-
-    :return: list or dict
-    """
-    if isinstance(instruments, (list, tuple, pd.Index, np.ndarray)):
-        instruments = sorted(list(instruments))
-    else:
-        # dict type stockpool
-        if "market" in instruments:
-            pass
-        else:
-            instruments = {k: sorted(v) for k, v in instruments.items()}
-    return instruments
-
-
-def is_tradable_date(cur_date):
-    """judgy whether date is a tradable date
-    ----------
-    date : pandas.Timestamp
-        current date
-    """
-    from ..data import D  # pylint: disable=C0415
-
-    return str(cur_date.date()) == str(D.calendar(start_time=cur_date, future=True)[0].date())
-
-
-def get_date_range(trading_date, left_shift=0, right_shift=0, future=False):
-    """get trading date range by shift
-
-    Parameters
-    ----------
-    trading_date: pd.Timestamp
-    left_shift: int
-    right_shift: int
-    future: bool
-
-    """
-
-    from ..data import D  # pylint: disable=C0415
-
-    start = get_date_by_shift(trading_date, left_shift, future=future)
-    end = get_date_by_shift(trading_date, right_shift, future=future)
-
-    calendar = D.calendar(start, end, future=future)
-    return calendar
-
-
-def get_date_by_shift(trading_date, shift, future=False, clip_shift=True, freq="day", align: Optional[str] = None):
-    """get trading date with shift bias will cur_date
-        e.g. : shift == 1,  return next trading date
-               shift == -1, return previous trading date
-    ----------
-    trading_date : pandas.Timestamp
-        current date
-    shift : int
-    clip_shift: bool
-    align : Optional[str]
-        When align is None, this function will raise ValueError if `trading_date` is not a trading date
-        when align is "left"/"right", it will try to align to left/right nearest trading date before shifting when `trading_date` is not a trading date
-
-    """
-    from qlib.data import D  # pylint: disable=C0415
-
-    cal = D.calendar(future=future, freq=freq)
-    trading_date = pd.to_datetime(trading_date)
-    if align is None:
-        if trading_date not in list(cal):
-            raise ValueError("{} is not trading day!".format(str(trading_date)))
-        _index = bisect.bisect_left(cal, trading_date)
-    elif align == "left":
-        _index = bisect.bisect_right(cal, trading_date) - 1
-    elif align == "right":
-        _index = bisect.bisect_left(cal, trading_date)
-    else:
-        raise ValueError(f"align with value `{align}` is not supported")
-    shift_index = _index + shift
-    if shift_index < 0 or shift_index >= len(cal):
-        if clip_shift:
-            shift_index = np.clip(shift_index, 0, len(cal) - 1)
-        else:
-            raise IndexError(f"The shift_index({shift_index}) of the trading day ({trading_date}) is out of range")
-    return cal[shift_index]
-
-
-def get_next_trading_date(trading_date, future=False):
-    """get next trading date
-    ----------
-    cur_date : pandas.Timestamp
-        current date
-    """
-    return get_date_by_shift(trading_date, 1, future=future)
-
-
-def get_pre_trading_date(trading_date, future=False):
-    """get previous trading date
-    ----------
-    date : pandas.Timestamp
-        current date
-    """
-    return get_date_by_shift(trading_date, -1, future=future)
-
-
-def transform_end_date(end_date=None, freq="day"):
-    """handle the end date with various format
-
-    If end_date is -1, None, or end_date is greater than the maximum trading day, the last trading date is returned.
-    Otherwise, returns the end_date
-
-    ----------
-    end_date: str
-        end trading date
-    date : pandas.Timestamp
-        current date
-    """
-    from ..data import D  # pylint: disable=C0415
-
-    last_date = D.calendar(freq=freq)[-1]
-    if end_date is None or (str(end_date) == "-1") or (pd.Timestamp(last_date) < pd.Timestamp(end_date)):
-        log.warning(
-            "\nInfo: the end_date in the configuration file is {}, "
-            "so the default last date {} is used.".format(end_date, last_date)
-        )
-        end_date = last_date
-    return end_date
-
-
-def get_date_in_file_name(file_name):
-    """Get the date(YYYY-MM-DD) written in file name
-    Parameter
-            file_name : str
-       :return
-            date : str
-                'YYYY-MM-DD'
-    """
-    pattern = "[0-9]{4}-[0-9]{2}-[0-9]{2}"
-    date = re.search(pattern, str(file_name)).group()
-    return date
-
-
-def split_pred(pred, number=None, split_date=None):
-    """split the score file into two part
-    Parameter
-    ---------
-        pred : pd.DataFrame (index:<instrument, datetime>)
-            A score file of stocks
-        number: the number of dates for pred_left
-        split_date: the last date of the pred_left
-    Return
-    -------
-        pred_left : pd.DataFrame (index:<instrument, datetime>)
-            The first part of original score file
-        pred_right : pd.DataFrame (index:<instrument, datetime>)
-            The second part of original score file
-    """
-    if number is None and split_date is None:
-        raise ValueError("`number` and `split date` cannot both be None")
-    dates = sorted(pred.index.get_level_values("datetime").unique())
-    dates = list(map(pd.Timestamp, dates))
-    if split_date is None:
-        date_left_end = dates[number - 1]
-        date_right_begin = dates[number]
-        date_left_start = None
-    else:
-        split_date = pd.Timestamp(split_date)
-        date_left_end = split_date
-        date_right_begin = split_date + pd.Timedelta(days=1)
-        if number is None:
-            date_left_start = None
-        else:
-            end_idx = bisect.bisect_right(dates, split_date)
-            date_left_start = dates[end_idx - number]
-    pred_temp = pred.sort_index()
-    pred_left = pred_temp.loc(axis=0)[:, date_left_start:date_left_end]
-    pred_right = pred_temp.loc(axis=0)[:, date_right_begin:]
-    return pred_left, pred_right
-
-
-def time_to_slc_point(t: Union[None, str, pd.Timestamp]) -> Union[None, pd.Timestamp]:
-    """
-    Time slicing in Qlib or Pandas is a frequently-used action.
-    However, user often input all kinds of data format to represent time.
-    This function will help user to convert these inputs into a uniform format which is friendly to time slicing.
-
-    Parameters
-    ----------
-    t : Union[None, str, pd.Timestamp]
-        original time
-
-    Returns
-    -------
-    Union[None, pd.Timestamp]:
-    """
-    if t is None:
-        # None represents unbounded in Qlib or Pandas(e.g. df.loc[slice(None, "20210303")]).
-        return t
-    else:
-        return pd.Timestamp(t)
-
-
-def can_use_cache():
-    res = True
-    r = get_redis_connection()
-    try:
-        r.client()
-    except redis.exceptions.ConnectionError:
-        res = False
-    finally:
-        r.close()
-    return res
-
-
-def exists_qlib_data(qlib_dir):
-    qlib_dir = Path(qlib_dir).expanduser()
-    if not qlib_dir.exists():
-        return False
-
-    calendars_dir = qlib_dir.joinpath("calendars")
-    instruments_dir = qlib_dir.joinpath("instruments")
-    features_dir = qlib_dir.joinpath("features")
-    # check dir
-    for _dir in [calendars_dir, instruments_dir, features_dir]:
-        if not (_dir.exists() and list(_dir.iterdir())):
-            return False
-    # check calendar bin
-    for _calendar in calendars_dir.iterdir():
-
-        if ("_future" not in _calendar.name) and (
-            not list(features_dir.rglob(f"*.{_calendar.name.split('.')[0]}.bin"))
-        ):
-            return False
-
-    # check instruments
-    code_names = set(map(lambda x: fname_to_code(x.name.lower()), features_dir.iterdir()))
-    _instrument = instruments_dir.joinpath("all.txt")
-    miss_code = set(pd.read_csv(_instrument, sep="\t", header=None).loc[:, 0].apply(str.lower)) - set(code_names)
-    if miss_code and any(map(lambda x: "sht" not in x, miss_code)):
-        return False
-
-    return True
-
-
-def check_qlib_data(qlib_config):
-    inst_dir = Path(qlib_config["provider_uri"]).joinpath("instruments")
-    for _p in inst_dir.glob("*.txt"):
-        assert len(pd.read_csv(_p, sep="\t", nrows=0, header=None).columns) == 3, (
-            f"\nThe {str(_p.resolve())} of qlib data is not equal to 3 columns:"
-            f"\n\tIf you are using the data provided by qlib: "
-            f"https://qlib.readthedocs.io/en/latest/component/data.html#qlib-format-dataset"
-            f"\n\tIf you are using your own data, please dump the data again: "
-            f"https://qlib.readthedocs.io/en/latest/component/data.html#converting-csv-format-into-qlib-format"
-        )
-
-
-def lazy_sort_index(df: pd.DataFrame, axis=0) -> pd.DataFrame:
-    """
-    make the df index sorted
-
-    df.sort_index() will take a lot of time even when `df.is_lexsorted() == True`
-    This function could avoid such case
-
-    Parameters
-    ----------
-    df : pd.DataFrame
-
-    Returns
-    -------
-    pd.DataFrame:
-        sorted dataframe
-    """
-    idx = df.index if axis == 0 else df.columns
-    if (
-        not idx.is_monotonic_increasing
-        or not is_deprecated_lexsorted_pandas
-        and isinstance(idx, pd.MultiIndex)
-        and not idx.is_lexsorted()
-    ):  # this case is for the old version
-        return df.sort_index(axis=axis)
-    else:
-        return df
-
-
-FLATTEN_TUPLE = "_FLATTEN_TUPLE"
-
-
-def flatten_dict(d, parent_key="", sep=".") -> dict:
-    """
-    Flatten a nested dict.
-
-        >>> flatten_dict({'a': 1, 'c': {'a': 2, 'b': {'x': 5, 'y' : 10}}, 'd': [1, 2, 3]})
-        >>> {'a': 1, 'c.a': 2, 'c.b.x': 5, 'd': [1, 2, 3], 'c.b.y': 10}
-
-        >>> flatten_dict({'a': 1, 'c': {'a': 2, 'b': {'x': 5, 'y' : 10}}, 'd': [1, 2, 3]}, sep=FLATTEN_TUPLE)
-        >>> {'a': 1, ('c','a'): 2, ('c','b','x'): 5, 'd': [1, 2, 3], ('c','b','y'): 10}
-
-    Args:
-        d (dict): the dict waiting for flatting
-        parent_key (str, optional): the parent key, will be a prefix in new key. Defaults to "".
-        sep (str, optional): the separator for string connecting. FLATTEN_TUPLE for tuple connecting.
-
-    Returns:
-        dict: flatten dict
-    """
-    items = []
-    for k, v in d.items():
-        if sep == FLATTEN_TUPLE:
-            new_key = (parent_key, k) if parent_key else k
-        else:
-            new_key = parent_key + sep + k if parent_key else k
-        if isinstance(v, collections.abc.MutableMapping):
-            items.extend(flatten_dict(v, new_key, sep=sep).items())
-        else:
-            items.append((new_key, v))
-    return dict(items)
-
-
-def get_item_from_obj(config: dict, name_path: str) -> object:
-    """
-    Follow the name_path to get values from config
-    For example:
-    If we follow the example in in the Parameters section,
-        Timestamp('2008-01-02 00:00:00') will be returned
-
-    Parameters
-    ----------
-    config : dict
-        e.g.
-        {'dataset': {'class': 'DatasetH',
-          'kwargs': {'handler': {'class': 'Alpha158',
-                                 'kwargs': {'end_time': '2020-08-01',
-                                            'fit_end_time': '<dataset.kwargs.segments.train.1>',
-                                            'fit_start_time': '<dataset.kwargs.segments.train.0>',
-                                            'instruments': 'csi100',
-                                            'start_time': '2008-01-01'},
-                                 'module_path': 'qlib.contrib.data.handler'},
-                     'segments': {'test': (Timestamp('2017-01-03 00:00:00'),
-                                           Timestamp('2019-04-08 00:00:00')),
-                                  'train': (Timestamp('2008-01-02 00:00:00'),
-                                            Timestamp('2014-12-31 00:00:00')),
-                                  'valid': (Timestamp('2015-01-05 00:00:00'),
-                                            Timestamp('2016-12-30 00:00:00'))}}
-        }}
-    name_path : str
-        e.g.
-        "dataset.kwargs.segments.train.1"
-
-    Returns
-    -------
-    object
-        the retrieved object
-    """
-    cur_cfg = config
-    for k in name_path.split("."):
-        if isinstance(cur_cfg, dict):
-            cur_cfg = cur_cfg[k]
-        elif k.isdigit():
-            cur_cfg = cur_cfg[int(k)]
-        else:
-            raise ValueError(f"Error when getting {k} from cur_cfg")
-    return cur_cfg
-
-
-def fill_placeholder(config: dict, config_extend: dict):
-    """
-    Detect placeholder in config and fill them with config_extend.
-    The item of dict must be single item(int, str, etc), dict and list. Tuples are not supported.
-    There are two type of variables:
-    - user-defined variables :
-        e.g. when config_extend is `{"<MODEL>": model, "<DATASET>": dataset}`, "<MODEL>" and "<DATASET>" in `config` will be replaced with `model` `dataset`
-    - variables extracted from `config` :
-        e.g. the variables like "<dataset.kwargs.segments.train.0>" will be replaced with the values from `config`
-
-    Parameters
-    ----------
-    config : dict
-        the parameter dict will be filled
-    config_extend : dict
-        the value of all placeholders
-
-    Returns
-    -------
-    dict
-        the parameter dict
-    """
-    # check the format of config_extend
-    for placeholder in config_extend.keys():
-        assert re.match(r"<[^<>]+>", placeholder)
-
-    # bfs
-    top = 0
-    tail = 1
-    item_queue = [config]
-    while top < tail:
-        now_item = item_queue[top]
-        top += 1
-        if isinstance(now_item, list):
-            item_keys = range(len(now_item))
-        elif isinstance(now_item, dict):
-            item_keys = now_item.keys()
-        for key in item_keys:
-            if isinstance(now_item[key], (list, dict)):
-                item_queue.append(now_item[key])
-                tail += 1
-            elif isinstance(now_item[key], str):
-                if now_item[key] in config_extend.keys():
-                    now_item[key] = config_extend[now_item[key]]
-                else:
-                    m = re.match(r"<(?P<name_path>[^<>]+)>", now_item[key])
-                    if m is not None:
-                        now_item[key] = get_item_from_obj(config, m.groupdict()["name_path"])
-    return config
-
-
-def auto_filter_kwargs(func: Callable, warning=True) -> Callable:
-    """
-    this will work like a decoration function
-
-    The decrated function will ignore and give warning when the parameter is not acceptable
-
-    For example, if you have a function `f` which may optionally consume the keywards `bar`.
-    then you can call it by `auto_filter_kwargs(f)(bar=3)`, which will automatically filter out
-    `bar` when f does not need bar
-
-    Parameters
-    ----------
-    func : Callable
-        The original function
-
-    Returns
-    -------
-    Callable:
-        the new callable function
-    """
-
-    def _func(*args, **kwargs):
-        spec = inspect.getfullargspec(func)
-        new_kwargs = {}
-        for k, v in kwargs.items():
-            # if `func` don't accept variable keyword arguments like `**kwargs` and have not according named arguments
-            if spec.varkw is None and k not in spec.args:
-                if warning:
-                    log.warning(f"The parameter `{k}` with value `{v}` is ignored.")
-            else:
-                new_kwargs[k] = v
-        return func(*args, **new_kwargs)
-
-    return _func
-
-
-#################### Wrapper #####################
-class Wrapper:
-    """Wrapper class for anything that needs to set up during qlib.init"""
-
-    def __init__(self):
-        self._provider = None
-
-    def register(self, provider):
-        self._provider = provider
-
-    def __repr__(self):
-        return "{name}(provider={provider})".format(name=self.__class__.__name__, provider=self._provider)
-
-    def __getattr__(self, key):
-        if self.__dict__.get("_provider", None) is None:
-            raise AttributeError("Please run qlib.init() first using qlib")
-        return getattr(self._provider, key)
-
-
-def register_wrapper(wrapper, cls_or_obj, module_path=None):
-    """register_wrapper
-
-    :param wrapper: A wrapper.
-    :param cls_or_obj:  A class or class name or object instance.
-    """
-    if isinstance(cls_or_obj, str):
-        module = get_module_by_module_path(module_path)
-        cls_or_obj = getattr(module, cls_or_obj)
-    obj = cls_or_obj() if isinstance(cls_or_obj, type) else cls_or_obj
-    wrapper.register(obj)
-
-
-def load_dataset(path_or_obj, index_col=[0, 1]):
-    """load dataset from multiple file formats"""
-    if isinstance(path_or_obj, pd.DataFrame):
-        return path_or_obj
-    if not os.path.exists(path_or_obj):
-        raise ValueError(f"file {path_or_obj} doesn't exist")
-    _, extension = os.path.splitext(path_or_obj)
-    if extension == ".h5":
-        return pd.read_hdf(path_or_obj)
-    elif extension == ".pkl":
-        return pd.read_pickle(path_or_obj)
-    elif extension == ".csv":
-        return pd.read_csv(path_or_obj, parse_dates=True, index_col=index_col)
-    raise ValueError(f"unsupported file type `{extension}`")
-
-
-def code_to_fname(code: str):
-    """stock code to file name
-
-    Parameters
-    ----------
-    code: str
-    """
-    # NOTE: In windows, the following name is I/O device, and the file with the corresponding name cannot be created
-    # reference: https://superuser.com/questions/86999/why-cant-i-name-a-folder-or-file-con-in-windows
-    replace_names = ["CON", "PRN", "AUX", "NUL"]
-    replace_names += [f"COM{i}" for i in range(10)]
-    replace_names += [f"LPT{i}" for i in range(10)]
-
-    prefix = "_qlib_"
-    if str(code).upper() in replace_names:
-        code = prefix + str(code)
-
-    return code
-
-
-def fname_to_code(fname: str):
-    """file name to stock code
-
-    Parameters
-    ----------
-    fname: str
-    """
-
-    prefix = "_qlib_"
-    if fname.startswith(prefix):
-        fname = fname.lstrip(prefix)
-    return fname
-
-
-__all__ = [
-    "get_or_create_path",
-    "save_multiple_parts_file",
-    "unpack_archive_with_buffer",
-    "get_tmp_file_with_buffer",
-    "set_log_with_config",
-    "init_instance_by_config",
-]
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+# TODO: this utils covers too much utilities, please seperat it into sub modules
+
+from __future__ import division
+from __future__ import print_function
+
+import os
+import re
+import copy
+import json
+import yaml
+import redis
+import bisect
+import struct
+import difflib
+import inspect
+import hashlib
+import datetime
+import requests
+import collections
+import numpy as np
+import pandas as pd
+from pathlib import Path
+from typing import List, Union, Optional, Callable
+from packaging import version
+from .file import get_or_create_path, save_multiple_parts_file, unpack_archive_with_buffer, get_tmp_file_with_buffer
+from ..config import C
+from ..log import get_module_logger, set_log_with_config
+
+log = get_module_logger("utils")
+# MultiIndex.is_lexsorted() is a deprecated method in Pandas 1.3.0.
+is_deprecated_lexsorted_pandas = version.parse(pd.__version__) > version.parse("1.3.0")
+
+
+#################### Server ####################
+def get_redis_connection():
+    """get redis connection instance."""
+    return redis.StrictRedis(host=C.redis_host, port=C.redis_port, db=C.redis_task_db, password=C.redis_password)
+
+
+#################### Data ####################
+def read_bin(file_path: Union[str, Path], start_index, end_index):
+    file_path = Path(file_path.expanduser().resolve())
+    with file_path.open("rb") as f:
+        # read start_index
+        ref_start_index = int(np.frombuffer(f.read(4), dtype="<f")[0])
+        si = max(ref_start_index, start_index)
+        if si > end_index:
+            return pd.Series(dtype=np.float32)
+        # calculate offset
+        f.seek(4 * (si - ref_start_index) + 4)
+        # read nbytes
+        count = end_index - si + 1
+        data = np.frombuffer(f.read(4 * count), dtype="<f")
+        series = pd.Series(data, index=pd.RangeIndex(si, si + len(data)))
+    return series
+
+
+def get_period_list(first: int, last: int, quarterly: bool) -> List[int]:
+    """
+    This method will be used in PIT database.
+    It return all the possible values between `first` and `end`  (first and end is included)
+
+    Parameters
+    ----------
+    quarterly : bool
+        will it return quarterly index or yearly index.
+
+    Returns
+    -------
+    List[int]
+        the possible index between [first, last]
+    """
+
+    if not quarterly:
+        assert all(1900 <= x <= 2099 for x in (first, last)), "invalid arguments"
+        return list(range(first, last + 1))
+    else:
+        assert all(190000 <= x <= 209904 for x in (first, last)), "invalid arguments"
+        res = []
+        for year in range(first // 100, last // 100 + 1):
+            for q in range(1, 5):
+                period = year * 100 + q
+                if first <= period <= last:
+                    res.append(year * 100 + q)
+        return res
+
+
+def get_period_offset(first_year, period, quarterly):
+    if quarterly:
+        offset = (period // 100 - first_year) * 4 + period % 100 - 1
+    else:
+        offset = period - first_year
+    return offset
+
+
+def read_period_data(index_path, data_path, period, cur_date_int: int, quarterly, last_period_index: int = None):
+    """
+    At `cur_date`(e.g. 20190102), read the information at `period`(e.g. 201803).
+    Only the updating info before cur_date or at cur_date will be used.
+
+    Parameters
+    ----------
+    period: int
+        date period represented by interger, e.g. 201901 corresponds to the first quarter in 2019
+    cur_date_int: int
+        date which represented by interger, e.g. 20190102
+    last_period_index: int
+        it is a optional parameter; it is designed to avoid repeatedly access the .index data of PIT database when
+        sequentially observing the data (Because the latest index of a specific period of data certainly appear in after the one in last observation).
+
+    Returns
+    -------
+    the query value and byte index the index value
+    """
+    DATA_DTYPE = "".join(
+        [
+            C.pit_record_type["date"],
+            C.pit_record_type["period"],
+            C.pit_record_type["value"],
+            C.pit_record_type["index"],
+        ]
+    )
+
+    PERIOD_DTYPE = C.pit_record_type["period"]
+    INDEX_DTYPE = C.pit_record_type["index"]
+
+    NAN_VALUE = C.pit_record_nan["value"]
+    NAN_INDEX = C.pit_record_nan["index"]
+
+    # find the first index of linked revisions
+    if last_period_index is None:
+        with open(index_path, "rb") as fi:
+            (first_year,) = struct.unpack(PERIOD_DTYPE, fi.read(struct.calcsize(PERIOD_DTYPE)))
+            all_periods = np.fromfile(fi, dtype=INDEX_DTYPE)
+        offset = get_period_offset(first_year, period, quarterly)
+        _next = all_periods[offset]
+    else:
+        _next = last_period_index
+
+    # load data following the `_next` link
+    prev_value = NAN_VALUE
+    prev_next = _next
+
+    with open(data_path, "rb") as fd:
+        while _next != NAN_INDEX:
+            fd.seek(_next)
+            date, period, value, new_next = struct.unpack(DATA_DTYPE, fd.read(struct.calcsize(DATA_DTYPE)))
+            if date > cur_date_int:
+                break
+            prev_next = _next
+            _next = new_next
+            prev_value = value
+    return prev_value, prev_next
+
+
+def np_ffill(arr: np.array):
+    """
+    forward fill a 1D numpy array
+
+    Parameters
+    ----------
+    arr : np.array
+        Input numpy 1D array
+    """
+    mask = np.isnan(arr.astype(float))  # np.isnan only works on np.float
+    # get fill index
+    idx = np.where(~mask, np.arange(mask.shape[0]), 0)
+    np.maximum.accumulate(idx, out=idx)
+    return arr[idx]
+
+
+#################### Search ####################
+def lower_bound(data, val, level=0):
+    """multi fields list lower bound.
+
+    for single field list use `bisect.bisect_left` instead
+    """
+    left = 0
+    right = len(data)
+    while left < right:
+        mid = (left + right) // 2
+        if val <= data[mid][level]:
+            right = mid
+        else:
+            left = mid + 1
+    return left
+
+
+def upper_bound(data, val, level=0):
+    """multi fields list upper bound.
+
+    for single field list use `bisect.bisect_right` instead
+    """
+    left = 0
+    right = len(data)
+    while left < right:
+        mid = (left + right) // 2
+        if val >= data[mid][level]:
+            left = mid + 1
+        else:
+            right = mid
+    return left
+
+
+#################### HTTP ####################
+def requests_with_retry(url, retry=5, **kwargs):
+    while retry > 0:
+        retry -= 1
+        try:
+            res = requests.get(url, timeout=1, **kwargs)
+            assert res.status_code in {200, 206}
+            return res
+        except AssertionError:
+            continue
+        except Exception as e:
+            log.warning("exception encountered {}".format(e))
+            continue
+    raise TimeoutError("ERROR: requests failed!")
+
+
+#################### Parse ####################
+def parse_config(config):
+    # Check whether need parse, all object except str do not need to be parsed
+    if not isinstance(config, str):
+        return config
+    # Check whether config is file
+    if os.path.exists(config):
+        with open(config, "r") as f:
+            return yaml.safe_load(f)
+    # Check whether the str can be parsed
+    try:
+        return yaml.safe_load(config)
+    except BaseException as base_exp:
+        raise ValueError("cannot parse config!") from base_exp
+
+
+#################### Other ####################
+def drop_nan_by_y_index(x, y, weight=None):
+    # x, y, weight: DataFrame
+    # Find index of rows which do not contain Nan in all columns from y.
+    mask = ~y.isna().any(axis=1)
+    # Get related rows from x, y, weight.
+    x = x[mask]
+    y = y[mask]
+    if weight is not None:
+        weight = weight[mask]
+    return x, y, weight
+
+
+def hash_args(*args):
+    # json.dumps will keep the dict keys always sorted.
+    string = json.dumps(args, sort_keys=True, default=str)  # frozenset
+    return hashlib.md5(string.encode()).hexdigest()
+
+
+def parse_field(field):
+    # Following patterns will be matched:
+    # - $close -> Feature("close")
+    # - $close5 -> Feature("close5")
+    # - $open+$close -> Feature("open")+Feature("close")
+    # TODO: this maybe used in the feature if we want to support the computation of different frequency data
+    # - $close@5min -> Feature("close", "5min")
+
+    if not isinstance(field, str):
+        field = str(field)
+    # Chinese punctuation regex:
+    # \u3001 -> 、
+    # \uff1a -> ：
+    # \uff08 -> (
+    # \uff09 -> )
+    chinese_punctuation_regex = r"\u3001\uff1a\uff08\uff09"
+    for pattern, new in [
+        (rf"\$\$([\w{chinese_punctuation_regex}]+)", r'PFeature("\1")'),  # $$ must be before $
+        (rf"\$([\w{chinese_punctuation_regex}]+)", r'Feature("\1")'),
+        (r"(\w+\s*)\(", r"Operators.\1("),
+    ]:  # Features  # Operators
+        field = re.sub(pattern, new, field)
+    return field
+
+
+def compare_dict_value(src_data: dict, dst_data: dict):
+    """Compare dict value
+
+    :param src_data:
+    :param dst_data:
+    :return:
+    """
+
+    class DateEncoder(json.JSONEncoder):
+        # FIXME: This class can only be accurate to the day. If it is a minute,
+        # there may be a bug
+        def default(self, o):
+            if isinstance(o, (datetime.datetime, datetime.date)):
+                return o.strftime("%Y-%m-%d %H:%M:%S")
+            return json.JSONEncoder.default(self, o)
+
+    src_data = json.dumps(src_data, indent=4, sort_keys=True, cls=DateEncoder)
+    dst_data = json.dumps(dst_data, indent=4, sort_keys=True, cls=DateEncoder)
+    diff = difflib.ndiff(src_data, dst_data)
+    changes = [line for line in diff if line.startswith("+ ") or line.startswith("- ")]
+    return changes
+
+
+def remove_repeat_field(fields):
+    """remove repeat field
+
+    :param fields: list; features fields
+    :return: list
+    """
+    fields = copy.deepcopy(fields)
+    _fields = set(fields)
+    return sorted(_fields, key=fields.index)
+
+
+def remove_fields_space(fields: [list, str, tuple]):
+    """remove fields space
+
+    :param fields: features fields
+    :return: list or str
+    """
+    if isinstance(fields, str):
+        return fields.replace(" ", "")
+    return [i.replace(" ", "") if isinstance(i, str) else str(i) for i in fields]
+
+
+def normalize_cache_fields(fields: [list, tuple]):
+    """normalize cache fields
+
+    :param fields: features fields
+    :return: list
+    """
+    return sorted(remove_repeat_field(remove_fields_space(fields)))
+
+
+def normalize_cache_instruments(instruments):
+    """normalize cache instruments
+
+    :return: list or dict
+    """
+    if isinstance(instruments, (list, tuple, pd.Index, np.ndarray)):
+        instruments = sorted(list(instruments))
+    else:
+        # dict type stockpool
+        if "market" in instruments:
+            pass
+        else:
+            instruments = {k: sorted(v) for k, v in instruments.items()}
+    return instruments
+
+
+def is_tradable_date(cur_date):
+    """judgy whether date is a tradable date
+    ----------
+    date : pandas.Timestamp
+        current date
+    """
+    from ..data import D  # pylint: disable=C0415
+
+    return str(cur_date.date()) == str(D.calendar(start_time=cur_date, future=True)[0].date())
+
+
+def get_date_range(trading_date, left_shift=0, right_shift=0, future=False):
+    """get trading date range by shift
+
+    Parameters
+    ----------
+    trading_date: pd.Timestamp
+    left_shift: int
+    right_shift: int
+    future: bool
+
+    """
+
+    from ..data import D  # pylint: disable=C0415
+
+    start = get_date_by_shift(trading_date, left_shift, future=future)
+    end = get_date_by_shift(trading_date, right_shift, future=future)
+
+    calendar = D.calendar(start, end, future=future)
+    return calendar
+
+
+def get_date_by_shift(trading_date, shift, future=False, clip_shift=True, freq="day", align: Optional[str] = None):
+    """get trading date with shift bias will cur_date
+        e.g. : shift == 1,  return next trading date
+               shift == -1, return previous trading date
+    ----------
+    trading_date : pandas.Timestamp
+        current date
+    shift : int
+    clip_shift: bool
+    align : Optional[str]
+        When align is None, this function will raise ValueError if `trading_date` is not a trading date
+        when align is "left"/"right", it will try to align to left/right nearest trading date before shifting when `trading_date` is not a trading date
+
+    """
+    from qlib.data import D  # pylint: disable=C0415
+
+    cal = D.calendar(future=future, freq=freq)
+    trading_date = pd.to_datetime(trading_date)
+    if align is None:
+        if trading_date not in list(cal):
+            raise ValueError("{} is not trading day!".format(str(trading_date)))
+        _index = bisect.bisect_left(cal, trading_date)
+    elif align == "left":
+        _index = bisect.bisect_right(cal, trading_date) - 1
+    elif align == "right":
+        _index = bisect.bisect_left(cal, trading_date)
+    else:
+        raise ValueError(f"align with value `{align}` is not supported")
+    shift_index = _index + shift
+    if shift_index < 0 or shift_index >= len(cal):
+        if clip_shift:
+            shift_index = np.clip(shift_index, 0, len(cal) - 1)
+        else:
+            raise IndexError(f"The shift_index({shift_index}) of the trading day ({trading_date}) is out of range")
+    return cal[shift_index]
+
+
+def get_next_trading_date(trading_date, future=False):
+    """get next trading date
+    ----------
+    cur_date : pandas.Timestamp
+        current date
+    """
+    return get_date_by_shift(trading_date, 1, future=future)
+
+
+def get_pre_trading_date(trading_date, future=False):
+    """get previous trading date
+    ----------
+    date : pandas.Timestamp
+        current date
+    """
+    return get_date_by_shift(trading_date, -1, future=future)
+
+
+def transform_end_date(end_date=None, freq="day"):
+    """handle the end date with various format
+
+    If end_date is -1, None, or end_date is greater than the maximum trading day, the last trading date is returned.
+    Otherwise, returns the end_date
+
+    ----------
+    end_date: str
+        end trading date
+    date : pandas.Timestamp
+        current date
+    """
+    from ..data import D  # pylint: disable=C0415
+
+    last_date = D.calendar(freq=freq)[-1]
+    if end_date is None or (str(end_date) == "-1") or (pd.Timestamp(last_date) < pd.Timestamp(end_date)):
+        log.warning(
+            "\nInfo: the end_date in the configuration file is {}, "
+            "so the default last date {} is used.".format(end_date, last_date)
+        )
+        end_date = last_date
+    return end_date
+
+
+def get_date_in_file_name(file_name):
+    """Get the date(YYYY-MM-DD) written in file name
+    Parameter
+            file_name : str
+       :return
+            date : str
+                'YYYY-MM-DD'
+    """
+    pattern = "[0-9]{4}-[0-9]{2}-[0-9]{2}"
+    date = re.search(pattern, str(file_name)).group()
+    return date
+
+
+def split_pred(pred, number=None, split_date=None):
+    """split the score file into two part
+    Parameter
+    ---------
+        pred : pd.DataFrame (index:<instrument, datetime>)
+            A score file of stocks
+        number: the number of dates for pred_left
+        split_date: the last date of the pred_left
+    Return
+    -------
+        pred_left : pd.DataFrame (index:<instrument, datetime>)
+            The first part of original score file
+        pred_right : pd.DataFrame (index:<instrument, datetime>)
+            The second part of original score file
+    """
+    if number is None and split_date is None:
+        raise ValueError("`number` and `split date` cannot both be None")
+    dates = sorted(pred.index.get_level_values("datetime").unique())
+    dates = list(map(pd.Timestamp, dates))
+    if split_date is None:
+        date_left_end = dates[number - 1]
+        date_right_begin = dates[number]
+        date_left_start = None
+    else:
+        split_date = pd.Timestamp(split_date)
+        date_left_end = split_date
+        date_right_begin = split_date + pd.Timedelta(days=1)
+        if number is None:
+            date_left_start = None
+        else:
+            end_idx = bisect.bisect_right(dates, split_date)
+            date_left_start = dates[end_idx - number]
+    pred_temp = pred.sort_index()
+    pred_left = pred_temp.loc(axis=0)[:, date_left_start:date_left_end]
+    pred_right = pred_temp.loc(axis=0)[:, date_right_begin:]
+    return pred_left, pred_right
+
+
+def time_to_slc_point(t: Union[None, str, pd.Timestamp]) -> Union[None, pd.Timestamp]:
+    """
+    Time slicing in Qlib or Pandas is a frequently-used action.
+    However, user often input all kinds of data format to represent time.
+    This function will help user to convert these inputs into a uniform format which is friendly to time slicing.
+
+    Parameters
+    ----------
+    t : Union[None, str, pd.Timestamp]
+        original time
+
+    Returns
+    -------
+    Union[None, pd.Timestamp]:
+    """
+    if t is None:
+        # None represents unbounded in Qlib or Pandas(e.g. df.loc[slice(None, "20210303")]).
+        return t
+    else:
+        return pd.Timestamp(t)
+
+
+def can_use_cache():
+    res = True
+    r = get_redis_connection()
+    try:
+        r.client()
+    except redis.exceptions.ConnectionError:
+        res = False
+    finally:
+        r.close()
+    return res
+
+
+def exists_qlib_data(qlib_dir):
+    qlib_dir = Path(qlib_dir).expanduser()
+    if not qlib_dir.exists():
+        return False
+
+    calendars_dir = qlib_dir.joinpath("calendars")
+    instruments_dir = qlib_dir.joinpath("instruments")
+    features_dir = qlib_dir.joinpath("features")
+    # check dir
+    for _dir in [calendars_dir, instruments_dir, features_dir]:
+        if not (_dir.exists() and list(_dir.iterdir())):
+            return False
+    # check calendar bin
+    for _calendar in calendars_dir.iterdir():
+        if ("_future" not in _calendar.name) and (
+            not list(features_dir.rglob(f"*.{_calendar.name.split('.')[0]}.bin"))
+        ):
+            return False
+
+    # check instruments
+    code_names = set(map(lambda x: fname_to_code(x.name.lower()), features_dir.iterdir()))
+    _instrument = instruments_dir.joinpath("all.txt")
+    miss_code = set(pd.read_csv(_instrument, sep="\t", header=None).loc[:, 0].apply(str.lower)) - set(code_names)
+    if miss_code and any(map(lambda x: "sht" not in x, miss_code)):
+        return False
+
+    return True
+
+
+def check_qlib_data(qlib_config):
+    inst_dir = Path(qlib_config["provider_uri"]).joinpath("instruments")
+    for _p in inst_dir.glob("*.txt"):
+        assert len(pd.read_csv(_p, sep="\t", nrows=0, header=None).columns) == 3, (
+            f"\nThe {str(_p.resolve())} of qlib data is not equal to 3 columns:"
+            f"\n\tIf you are using the data provided by qlib: "
+            f"https://qlib.readthedocs.io/en/latest/component/data.html#qlib-format-dataset"
+            f"\n\tIf you are using your own data, please dump the data again: "
+            f"https://qlib.readthedocs.io/en/latest/component/data.html#converting-csv-format-into-qlib-format"
+        )
+
+
+def lazy_sort_index(df: pd.DataFrame, axis=0) -> pd.DataFrame:
+    """
+    make the df index sorted
+
+    df.sort_index() will take a lot of time even when `df.is_lexsorted() == True`
+    This function could avoid such case
+
+    Parameters
+    ----------
+    df : pd.DataFrame
+
+    Returns
+    -------
+    pd.DataFrame:
+        sorted dataframe
+    """
+    idx = df.index if axis == 0 else df.columns
+    if (
+        not idx.is_monotonic_increasing
+        or not is_deprecated_lexsorted_pandas
+        and isinstance(idx, pd.MultiIndex)
+        and not idx.is_lexsorted()
+    ):  # this case is for the old version
+        return df.sort_index(axis=axis)
+    else:
+        return df
+
+
+FLATTEN_TUPLE = "_FLATTEN_TUPLE"
+
+
+def flatten_dict(d, parent_key="", sep=".") -> dict:
+    """
+    Flatten a nested dict.
+
+        >>> flatten_dict({'a': 1, 'c': {'a': 2, 'b': {'x': 5, 'y' : 10}}, 'd': [1, 2, 3]})
+        >>> {'a': 1, 'c.a': 2, 'c.b.x': 5, 'd': [1, 2, 3], 'c.b.y': 10}
+
+        >>> flatten_dict({'a': 1, 'c': {'a': 2, 'b': {'x': 5, 'y' : 10}}, 'd': [1, 2, 3]}, sep=FLATTEN_TUPLE)
+        >>> {'a': 1, ('c','a'): 2, ('c','b','x'): 5, 'd': [1, 2, 3], ('c','b','y'): 10}
+
+    Args:
+        d (dict): the dict waiting for flatting
+        parent_key (str, optional): the parent key, will be a prefix in new key. Defaults to "".
+        sep (str, optional): the separator for string connecting. FLATTEN_TUPLE for tuple connecting.
+
+    Returns:
+        dict: flatten dict
+    """
+    items = []
+    for k, v in d.items():
+        if sep == FLATTEN_TUPLE:
+            new_key = (parent_key, k) if parent_key else k
+        else:
+            new_key = parent_key + sep + k if parent_key else k
+        if isinstance(v, collections.abc.MutableMapping):
+            items.extend(flatten_dict(v, new_key, sep=sep).items())
+        else:
+            items.append((new_key, v))
+    return dict(items)
+
+
+def get_item_from_obj(config: dict, name_path: str) -> object:
+    """
+    Follow the name_path to get values from config
+    For example:
+    If we follow the example in in the Parameters section,
+        Timestamp('2008-01-02 00:00:00') will be returned
+
+    Parameters
+    ----------
+    config : dict
+        e.g.
+        {'dataset': {'class': 'DatasetH',
+          'kwargs': {'handler': {'class': 'Alpha158',
+                                 'kwargs': {'end_time': '2020-08-01',
+                                            'fit_end_time': '<dataset.kwargs.segments.train.1>',
+                                            'fit_start_time': '<dataset.kwargs.segments.train.0>',
+                                            'instruments': 'csi100',
+                                            'start_time': '2008-01-01'},
+                                 'module_path': 'qlib.contrib.data.handler'},
+                     'segments': {'test': (Timestamp('2017-01-03 00:00:00'),
+                                           Timestamp('2019-04-08 00:00:00')),
+                                  'train': (Timestamp('2008-01-02 00:00:00'),
+                                            Timestamp('2014-12-31 00:00:00')),
+                                  'valid': (Timestamp('2015-01-05 00:00:00'),
+                                            Timestamp('2016-12-30 00:00:00'))}}
+        }}
+    name_path : str
+        e.g.
+        "dataset.kwargs.segments.train.1"
+
+    Returns
+    -------
+    object
+        the retrieved object
+    """
+    cur_cfg = config
+    for k in name_path.split("."):
+        if isinstance(cur_cfg, dict):
+            cur_cfg = cur_cfg[k]  # may raise KeyError
+        elif k.isdigit():
+            cur_cfg = cur_cfg[int(k)]  # may raise IndexError
+        else:
+            raise ValueError(f"Error when getting {k} from cur_cfg")
+    return cur_cfg
+
+
+def fill_placeholder(config: dict, config_extend: dict):
+    """
+    Detect placeholder in config and fill them with config_extend.
+    The item of dict must be single item(int, str, etc), dict and list. Tuples are not supported.
+    There are two type of variables:
+    - user-defined variables :
+        e.g. when config_extend is `{"<MODEL>": model, "<DATASET>": dataset}`, "<MODEL>" and "<DATASET>" in `config` will be replaced with `model` `dataset`
+    - variables extracted from `config` :
+        e.g. the variables like "<dataset.kwargs.segments.train.0>" will be replaced with the values from `config`
+
+    Parameters
+    ----------
+    config : dict
+        the parameter dict will be filled
+    config_extend : dict
+        the value of all placeholders
+
+    Returns
+    -------
+    dict
+        the parameter dict
+    """
+    # check the format of config_extend
+    for placeholder in config_extend.keys():
+        assert re.match(r"<[^<>]+>", placeholder)
+
+    # bfs
+    top = 0
+    tail = 1
+    item_queue = [config]
+
+    def try_replace_placeholder(value):
+        if value in config_extend.keys():
+            value = config_extend[value]
+        else:
+            m = re.match(r"<(?P<name_path>[^<>]+)>", value)
+            if m is not None:
+                try:
+                    value = get_item_from_obj(config, m.groupdict()["name_path"])
+                except (KeyError, ValueError, IndexError):
+                    get_module_logger("fill_placeholder").info(
+                        f"{value} lookes like a placeholder, but it can't match to any given values"
+                    )
+        return value
+
+    while top < tail:
+        now_item = item_queue[top]
+        top += 1
+        if isinstance(now_item, list):
+            item_keys = range(len(now_item))
+        elif isinstance(now_item, dict):
+            item_keys = now_item.keys()
+        for key in item_keys:  # noqa
+            if isinstance(now_item[key], (list, dict)):
+                item_queue.append(now_item[key])
+                tail += 1
+            elif isinstance(now_item[key], str):
+                # If it is a string, try to replace it with placeholder
+                now_item[key] = try_replace_placeholder(now_item[key])
+    return config
+
+
+def auto_filter_kwargs(func: Callable, warning=True) -> Callable:
+    """
+    this will work like a decoration function
+
+    The decrated function will ignore and give warning when the parameter is not acceptable
+
+    For example, if you have a function `f` which may optionally consume the keywards `bar`.
+    then you can call it by `auto_filter_kwargs(f)(bar=3)`, which will automatically filter out
+    `bar` when f does not need bar
+
+    Parameters
+    ----------
+    func : Callable
+        The original function
+
+    Returns
+    -------
+    Callable:
+        the new callable function
+    """
+
+    def _func(*args, **kwargs):
+        spec = inspect.getfullargspec(func)
+        new_kwargs = {}
+        for k, v in kwargs.items():
+            # if `func` don't accept variable keyword arguments like `**kwargs` and have not according named arguments
+            if spec.varkw is None and k not in spec.args:
+                if warning:
+                    log.warning(f"The parameter `{k}` with value `{v}` is ignored.")
+            else:
+                new_kwargs[k] = v
+        return func(*args, **new_kwargs)
+
+    return _func
+
+
+#################### Wrapper #####################
+class Wrapper:
+    """Wrapper class for anything that needs to set up during qlib.init"""
+
+    def __init__(self):
+        self._provider = None
+
+    def register(self, provider):
+        self._provider = provider
+
+    def __repr__(self):
+        return "{name}(provider={provider})".format(name=self.__class__.__name__, provider=self._provider)
+
+    def __getattr__(self, key):
+        if self.__dict__.get("_provider", None) is None:
+            raise AttributeError("Please run qlib.init() first using qlib")
+        return getattr(self._provider, key)
+
+
+def register_wrapper(wrapper, cls_or_obj, module_path=None):
+    """register_wrapper
+
+    :param wrapper: A wrapper.
+    :param cls_or_obj:  A class or class name or object instance.
+    """
+    if isinstance(cls_or_obj, str):
+        module = get_module_by_module_path(module_path)
+        cls_or_obj = getattr(module, cls_or_obj)
+    obj = cls_or_obj() if isinstance(cls_or_obj, type) else cls_or_obj
+    wrapper.register(obj)
+
+
+def load_dataset(path_or_obj, index_col=[0, 1]):
+    """load dataset from multiple file formats"""
+    if isinstance(path_or_obj, pd.DataFrame):
+        return path_or_obj
+    if not os.path.exists(path_or_obj):
+        raise ValueError(f"file {path_or_obj} doesn't exist")
+    _, extension = os.path.splitext(path_or_obj)
+    if extension == ".h5":
+        return pd.read_hdf(path_or_obj)
+    elif extension == ".pkl":
+        return pd.read_pickle(path_or_obj)
+    elif extension == ".csv":
+        return pd.read_csv(path_or_obj, parse_dates=True, index_col=index_col)
+    raise ValueError(f"unsupported file type `{extension}`")
+
+
+def code_to_fname(code: str):
+    """stock code to file name
+
+    Parameters
+    ----------
+    code: str
+    """
+    # NOTE: In windows, the following name is I/O device, and the file with the corresponding name cannot be created
+    # reference: https://superuser.com/questions/86999/why-cant-i-name-a-folder-or-file-con-in-windows
+    replace_names = ["CON", "PRN", "AUX", "NUL"]
+    replace_names += [f"COM{i}" for i in range(10)]
+    replace_names += [f"LPT{i}" for i in range(10)]
+
+    prefix = "_qlib_"
+    if str(code).upper() in replace_names:
+        code = prefix + str(code)
+
+    return code
+
+
+def fname_to_code(fname: str):
+    """file name to stock code
+
+    Parameters
+    ----------
+    fname: str
+    """
+
+    prefix = "_qlib_"
+    if fname.startswith(prefix):
+        fname = fname.lstrip(prefix)
+    return fname
+
+
+from .mod import (
+    get_module_by_module_path,
+    split_module_path,
+    get_callable_kwargs,
+    get_cls_kwargs,
+    init_instance_by_config,
+    class_casting,
+)
+
+__all__ = [
+    "get_or_create_path",
+    "save_multiple_parts_file",
+    "unpack_archive_with_buffer",
+    "get_tmp_file_with_buffer",
+    "set_log_with_config",
+    "init_instance_by_config",
+    "get_module_by_module_path",
+    "split_module_path",
+    "get_callable_kwargs",
+    "get_cls_kwargs",
+    "init_instance_by_config",
+    "class_casting",
+]
```

## qlib/utils/data.py

 * *Ordering differences only*

```diff
@@ -1,105 +1,105 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-"""
-This module covers some utility functions that operate on data or basic object
-"""
-from copy import deepcopy
-from typing import List, Union
-import pandas as pd
-import numpy as np
-
-
-def robust_zscore(x: pd.Series, zscore=False):
-    """Robust ZScore Normalization
-
-    Use robust statistics for Z-Score normalization:
-        mean(x) = median(x)
-        std(x) = MAD(x) * 1.4826
-
-    Reference:
-        https://en.wikipedia.org/wiki/Median_absolute_deviation.
-    """
-    x = x - x.median()
-    mad = x.abs().median()
-    x = np.clip(x / mad / 1.4826, -3, 3)
-    if zscore:
-        x -= x.mean()
-        x /= x.std()
-    return x
-
-
-def zscore(x: Union[pd.Series, pd.DataFrame]):
-    return (x - x.mean()).div(x.std())
-
-
-def deepcopy_basic_type(obj: object) -> object:
-    """
-    deepcopy an object without copy the complicated objects.
-        This is useful when you want to generate Qlib tasks and share the handler
-
-    NOTE:
-    - This function can't handle recursive objects!!!!!
-
-    Parameters
-    ----------
-    obj : object
-        the object to be copied
-
-    Returns
-    -------
-    object:
-        The copied object
-    """
-    if isinstance(obj, tuple):
-        return tuple(deepcopy_basic_type(i) for i in obj)
-    elif isinstance(obj, list):
-        return list(deepcopy_basic_type(i) for i in obj)
-    elif isinstance(obj, dict):
-        return {k: deepcopy_basic_type(v) for k, v in obj.items()}
-    else:
-        return obj
-
-
-S_DROP = "__DROP__"  # this is a symbol which indicates drop the value
-
-
-def update_config(base_config: dict, ext_config: Union[dict, List[dict]]):
-    """
-    supporting adding base config based on the ext_config
-
-    >>> bc = {"a": "xixi"}
-    >>> ec = {"b": "haha"}
-    >>> new_bc = update_config(bc, ec)
-    >>> print(new_bc)
-    {'a': 'xixi', 'b': 'haha'}
-    >>> print(bc)  # base config should not be changed
-    {'a': 'xixi'}
-    >>> print(update_config(bc, {"b": S_DROP}))
-    {'a': 'xixi'}
-    >>> print(update_config(new_bc, {"b": S_DROP}))
-    {'a': 'xixi'}
-    """
-
-    base_config = deepcopy(base_config)  # in case of modifying base config
-
-    for ec in ext_config if isinstance(ext_config, (list, tuple)) else [ext_config]:
-        for key in ec:
-            if key not in base_config:
-                # if it is not in the default key, then replace it.
-                # ADD if not drop
-                if ec[key] != S_DROP:
-                    base_config[key] = ec[key]
-
-            else:
-                if isinstance(base_config[key], dict) and isinstance(ec[key], dict):
-                    # Recursive
-                    # Both of them are dict, then update it nested
-                    base_config[key] = update_config(base_config[key], ec[key])
-                elif ec[key] == S_DROP:
-                    # DROP
-                    del base_config[key]
-                else:
-                    # REPLACE
-                    # one of then are not dict. Then replace
-                    base_config[key] = ec[key]
-    return base_config
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+"""
+This module covers some utility functions that operate on data or basic object
+"""
+from copy import deepcopy
+from typing import List, Union
+import pandas as pd
+import numpy as np
+
+
+def robust_zscore(x: pd.Series, zscore=False):
+    """Robust ZScore Normalization
+
+    Use robust statistics for Z-Score normalization:
+        mean(x) = median(x)
+        std(x) = MAD(x) * 1.4826
+
+    Reference:
+        https://en.wikipedia.org/wiki/Median_absolute_deviation.
+    """
+    x = x - x.median()
+    mad = x.abs().median()
+    x = np.clip(x / mad / 1.4826, -3, 3)
+    if zscore:
+        x -= x.mean()
+        x /= x.std()
+    return x
+
+
+def zscore(x: Union[pd.Series, pd.DataFrame]):
+    return (x - x.mean()).div(x.std())
+
+
+def deepcopy_basic_type(obj: object) -> object:
+    """
+    deepcopy an object without copy the complicated objects.
+        This is useful when you want to generate Qlib tasks and share the handler
+
+    NOTE:
+    - This function can't handle recursive objects!!!!!
+
+    Parameters
+    ----------
+    obj : object
+        the object to be copied
+
+    Returns
+    -------
+    object:
+        The copied object
+    """
+    if isinstance(obj, tuple):
+        return tuple(deepcopy_basic_type(i) for i in obj)
+    elif isinstance(obj, list):
+        return list(deepcopy_basic_type(i) for i in obj)
+    elif isinstance(obj, dict):
+        return {k: deepcopy_basic_type(v) for k, v in obj.items()}
+    else:
+        return obj
+
+
+S_DROP = "__DROP__"  # this is a symbol which indicates drop the value
+
+
+def update_config(base_config: dict, ext_config: Union[dict, List[dict]]):
+    """
+    supporting adding base config based on the ext_config
+
+    >>> bc = {"a": "xixi"}
+    >>> ec = {"b": "haha"}
+    >>> new_bc = update_config(bc, ec)
+    >>> print(new_bc)
+    {'a': 'xixi', 'b': 'haha'}
+    >>> print(bc)  # base config should not be changed
+    {'a': 'xixi'}
+    >>> print(update_config(bc, {"b": S_DROP}))
+    {'a': 'xixi'}
+    >>> print(update_config(new_bc, {"b": S_DROP}))
+    {'a': 'xixi'}
+    """
+
+    base_config = deepcopy(base_config)  # in case of modifying base config
+
+    for ec in ext_config if isinstance(ext_config, (list, tuple)) else [ext_config]:
+        for key in ec:
+            if key not in base_config:
+                # if it is not in the default key, then replace it.
+                # ADD if not drop
+                if ec[key] != S_DROP:
+                    base_config[key] = ec[key]
+
+            else:
+                if isinstance(base_config[key], dict) and isinstance(ec[key], dict):
+                    # Recursive
+                    # Both of them are dict, then update it nested
+                    base_config[key] = update_config(base_config[key], ec[key])
+                elif ec[key] == S_DROP:
+                    # DROP
+                    del base_config[key]
+                else:
+                    # REPLACE
+                    # one of then are not dict. Then replace
+                    base_config[key] = ec[key]
+    return base_config
```

## qlib/utils/exceptions.py

 * *Ordering differences only*

```diff
@@ -1,19 +1,19 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-
-# Base exception class
-class QlibException(Exception):
-    pass
-
-
-class RecorderInitializationError(QlibException):
-    """Error type for re-initialization when starting an experiment"""
-
-
-class LoadObjectError(QlibException):
-    """Error type for Recorder when can not load object"""
-
-
-class ExpAlreadyExistError(Exception):
-    """Experiment already exists"""
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+
+# Base exception class
+class QlibException(Exception):
+    pass
+
+
+class RecorderInitializationError(QlibException):
+    """Error type for re-initialization when starting an experiment"""
+
+
+class LoadObjectError(QlibException):
+    """Error type for Recorder when can not load object"""
+
+
+class ExpAlreadyExistError(Exception):
+    """Experiment already exists"""
```

## qlib/utils/file.py

 * *Ordering differences only*

```diff
@@ -1,190 +1,190 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-import os
-import shutil
-import tempfile
-import contextlib
-from typing import Optional, Text, IO, Union
-from pathlib import Path
-
-from qlib.log import get_module_logger
-
-log = get_module_logger("utils.file")
-
-
-def get_or_create_path(path: Optional[Text] = None, return_dir: bool = False):
-    """Create or get a file or directory given the path and return_dir.
-
-    Parameters
-    ----------
-    path: a string indicates the path or None indicates creating a temporary path.
-    return_dir: if True, create and return a directory; otherwise c&r a file.
-
-    """
-    if path:
-        if return_dir and not os.path.exists(path):
-            os.makedirs(path)
-        elif not return_dir:  # return a file, thus we need to create its parent directory
-            xpath = os.path.abspath(os.path.join(path, ".."))
-            if not os.path.exists(xpath):
-                os.makedirs(xpath)
-    else:
-        temp_dir = os.path.expanduser("~/tmp")
-        if not os.path.exists(temp_dir):
-            os.makedirs(temp_dir)
-        if return_dir:
-            _, path = tempfile.mkdtemp(dir=temp_dir)
-        else:
-            _, path = tempfile.mkstemp(dir=temp_dir)
-    return path
-
-
-@contextlib.contextmanager
-def save_multiple_parts_file(filename, format="gztar"):
-    """Save multiple parts file
-
-    Implementation process:
-        1. get the absolute path to 'filename'
-        2. create a 'filename' directory
-        3. user does something with file_path('filename/')
-        4. remove 'filename' directory
-        5. make_archive 'filename' directory, and rename 'archive file' to filename
-
-    :param filename: result model path
-    :param format: archive format: one of "zip", "tar", "gztar", "bztar", or "xztar"
-    :return: real model path
-
-    Usage::
-
-        >>> # The following code will create an archive file('~/tmp/test_file') containing 'test_doc_i'(i is 0-10) files.
-        >>> with save_multiple_parts_file('~/tmp/test_file') as filename_dir:
-        ...   for i in range(10):
-        ...       temp_path = os.path.join(filename_dir, 'test_doc_{}'.format(str(i)))
-        ...       with open(temp_path) as fp:
-        ...           fp.write(str(i))
-        ...
-
-    """
-
-    if filename.startswith("~"):
-        filename = os.path.expanduser(filename)
-
-    file_path = os.path.abspath(filename)
-
-    # Create model dir
-    if os.path.exists(file_path):
-        raise FileExistsError("ERROR: file exists: {}, cannot be create the directory.".format(file_path))
-
-    os.makedirs(file_path)
-
-    # return model dir
-    yield file_path
-
-    # filename dir to filename.tar.gz file
-    tar_file = shutil.make_archive(file_path, format=format, root_dir=file_path)
-
-    # Remove filename dir
-    if os.path.exists(file_path):
-        shutil.rmtree(file_path)
-
-    # filename.tar.gz rename to filename
-    os.rename(tar_file, file_path)
-
-
-@contextlib.contextmanager
-def unpack_archive_with_buffer(buffer, format="gztar"):
-    """Unpack archive with archive buffer
-    After the call is finished, the archive file and directory will be deleted.
-
-    Implementation process:
-        1. create 'tempfile' in '~/tmp/' and directory
-        2. 'buffer' write to 'tempfile'
-        3. unpack archive file('tempfile')
-        4. user does something with file_path('tempfile/')
-        5. remove 'tempfile' and 'tempfile directory'
-
-    :param buffer: bytes
-    :param format: archive format: one of "zip", "tar", "gztar", "bztar", or "xztar"
-    :return: unpack archive directory path
-
-    Usage::
-
-        >>> # The following code is to print all the file names in 'test_unpack.tar.gz'
-        >>> with open('test_unpack.tar.gz') as fp:
-        ...     buffer = fp.read()
-        ...
-        >>> with unpack_archive_with_buffer(buffer) as temp_dir:
-        ...     for f_n in os.listdir(temp_dir):
-        ...         print(f_n)
-        ...
-
-    """
-    temp_dir = os.path.expanduser("~/tmp")
-    if not os.path.exists(temp_dir):
-        os.makedirs(temp_dir)
-    with tempfile.NamedTemporaryFile("wb", delete=False, dir=temp_dir) as fp:
-        fp.write(buffer)
-        file_path = fp.name
-
-    try:
-        tar_file = file_path + ".tar.gz"
-        os.rename(file_path, tar_file)
-        # Create dir
-        os.makedirs(file_path)
-        shutil.unpack_archive(tar_file, format=format, extract_dir=file_path)
-
-        # Return temp dir
-        yield file_path
-
-    except Exception as e:
-        log.error(str(e))
-    finally:
-        # Remove temp tar file
-        if os.path.exists(tar_file):
-            os.unlink(tar_file)
-
-        # Remove temp model dir
-        if os.path.exists(file_path):
-            shutil.rmtree(file_path)
-
-
-@contextlib.contextmanager
-def get_tmp_file_with_buffer(buffer):
-    temp_dir = os.path.expanduser("~/tmp")
-    if not os.path.exists(temp_dir):
-        os.makedirs(temp_dir)
-    with tempfile.NamedTemporaryFile("wb", delete=True, dir=temp_dir) as fp:
-        fp.write(buffer)
-        file_path = fp.name
-        yield file_path
-
-
-@contextlib.contextmanager
-def get_io_object(file: Union[IO, str, Path], *args, **kwargs) -> IO:
-    """
-    providing a easy interface to get an IO object
-
-    Parameters
-    ----------
-    file : Union[IO, str, Path]
-        a object representing the file
-
-    Returns
-    -------
-    IO:
-        a IO-like object
-
-    Raises
-    ------
-    NotImplementedError:
-    """
-    if isinstance(file, IO):
-        yield file
-    else:
-        if isinstance(file, str):
-            file = Path(file)
-        if not isinstance(file, Path):
-            raise NotImplementedError(f"This type[{type(file)}] of input is not supported")
-        with file.open(*args, **kwargs) as f:
-            yield f
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+import os
+import shutil
+import tempfile
+import contextlib
+from typing import Optional, Text, IO, Union
+from pathlib import Path
+
+from qlib.log import get_module_logger
+
+log = get_module_logger("utils.file")
+
+
+def get_or_create_path(path: Optional[Text] = None, return_dir: bool = False):
+    """Create or get a file or directory given the path and return_dir.
+
+    Parameters
+    ----------
+    path: a string indicates the path or None indicates creating a temporary path.
+    return_dir: if True, create and return a directory; otherwise c&r a file.
+
+    """
+    if path:
+        if return_dir and not os.path.exists(path):
+            os.makedirs(path)
+        elif not return_dir:  # return a file, thus we need to create its parent directory
+            xpath = os.path.abspath(os.path.join(path, ".."))
+            if not os.path.exists(xpath):
+                os.makedirs(xpath)
+    else:
+        temp_dir = os.path.expanduser("~/tmp")
+        if not os.path.exists(temp_dir):
+            os.makedirs(temp_dir)
+        if return_dir:
+            _, path = tempfile.mkdtemp(dir=temp_dir)
+        else:
+            _, path = tempfile.mkstemp(dir=temp_dir)
+    return path
+
+
+@contextlib.contextmanager
+def save_multiple_parts_file(filename, format="gztar"):
+    """Save multiple parts file
+
+    Implementation process:
+        1. get the absolute path to 'filename'
+        2. create a 'filename' directory
+        3. user does something with file_path('filename/')
+        4. remove 'filename' directory
+        5. make_archive 'filename' directory, and rename 'archive file' to filename
+
+    :param filename: result model path
+    :param format: archive format: one of "zip", "tar", "gztar", "bztar", or "xztar"
+    :return: real model path
+
+    Usage::
+
+        >>> # The following code will create an archive file('~/tmp/test_file') containing 'test_doc_i'(i is 0-10) files.
+        >>> with save_multiple_parts_file('~/tmp/test_file') as filename_dir:
+        ...   for i in range(10):
+        ...       temp_path = os.path.join(filename_dir, 'test_doc_{}'.format(str(i)))
+        ...       with open(temp_path) as fp:
+        ...           fp.write(str(i))
+        ...
+
+    """
+
+    if filename.startswith("~"):
+        filename = os.path.expanduser(filename)
+
+    file_path = os.path.abspath(filename)
+
+    # Create model dir
+    if os.path.exists(file_path):
+        raise FileExistsError("ERROR: file exists: {}, cannot be create the directory.".format(file_path))
+
+    os.makedirs(file_path)
+
+    # return model dir
+    yield file_path
+
+    # filename dir to filename.tar.gz file
+    tar_file = shutil.make_archive(file_path, format=format, root_dir=file_path)
+
+    # Remove filename dir
+    if os.path.exists(file_path):
+        shutil.rmtree(file_path)
+
+    # filename.tar.gz rename to filename
+    os.rename(tar_file, file_path)
+
+
+@contextlib.contextmanager
+def unpack_archive_with_buffer(buffer, format="gztar"):
+    """Unpack archive with archive buffer
+    After the call is finished, the archive file and directory will be deleted.
+
+    Implementation process:
+        1. create 'tempfile' in '~/tmp/' and directory
+        2. 'buffer' write to 'tempfile'
+        3. unpack archive file('tempfile')
+        4. user does something with file_path('tempfile/')
+        5. remove 'tempfile' and 'tempfile directory'
+
+    :param buffer: bytes
+    :param format: archive format: one of "zip", "tar", "gztar", "bztar", or "xztar"
+    :return: unpack archive directory path
+
+    Usage::
+
+        >>> # The following code is to print all the file names in 'test_unpack.tar.gz'
+        >>> with open('test_unpack.tar.gz') as fp:
+        ...     buffer = fp.read()
+        ...
+        >>> with unpack_archive_with_buffer(buffer) as temp_dir:
+        ...     for f_n in os.listdir(temp_dir):
+        ...         print(f_n)
+        ...
+
+    """
+    temp_dir = os.path.expanduser("~/tmp")
+    if not os.path.exists(temp_dir):
+        os.makedirs(temp_dir)
+    with tempfile.NamedTemporaryFile("wb", delete=False, dir=temp_dir) as fp:
+        fp.write(buffer)
+        file_path = fp.name
+
+    try:
+        tar_file = file_path + ".tar.gz"
+        os.rename(file_path, tar_file)
+        # Create dir
+        os.makedirs(file_path)
+        shutil.unpack_archive(tar_file, format=format, extract_dir=file_path)
+
+        # Return temp dir
+        yield file_path
+
+    except Exception as e:
+        log.error(str(e))
+    finally:
+        # Remove temp tar file
+        if os.path.exists(tar_file):
+            os.unlink(tar_file)
+
+        # Remove temp model dir
+        if os.path.exists(file_path):
+            shutil.rmtree(file_path)
+
+
+@contextlib.contextmanager
+def get_tmp_file_with_buffer(buffer):
+    temp_dir = os.path.expanduser("~/tmp")
+    if not os.path.exists(temp_dir):
+        os.makedirs(temp_dir)
+    with tempfile.NamedTemporaryFile("wb", delete=True, dir=temp_dir) as fp:
+        fp.write(buffer)
+        file_path = fp.name
+        yield file_path
+
+
+@contextlib.contextmanager
+def get_io_object(file: Union[IO, str, Path], *args, **kwargs) -> IO:
+    """
+    providing a easy interface to get an IO object
+
+    Parameters
+    ----------
+    file : Union[IO, str, Path]
+        a object representing the file
+
+    Returns
+    -------
+    IO:
+        a IO-like object
+
+    Raises
+    ------
+    NotImplementedError:
+    """
+    if isinstance(file, IO):
+        yield file
+    else:
+        if isinstance(file, str):
+            file = Path(file)
+        if not isinstance(file, Path):
+            raise NotImplementedError(f"This type[{type(file)}] of input is not supported")
+        with file.open(*args, **kwargs) as f:
+            yield f
```

## qlib/utils/index_data.py

```diff
@@ -1,644 +1,643 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-"""
-Motivation of index_data
-- Pandas has a lot of user-friendly interfaces. However, integrating too much features in a single tool bring too much overhead and makes it much slower than numpy.
-    Some users just want a simple numpy dataframe with indices and don't want such a complicated tools.
-    Such users are the target of `index_data`
-
-`index_data` try to behave like pandas (some API will be different because we try to be simpler and more intuitive) but don't compromise the performance. It provides the basic numpy data and simple indexing feature. If users call APIs which may compromise the performance, index_data will raise Errors.
-"""
-
-from __future__ import annotations
-
-from typing import Dict, Tuple, Union, Callable, List
-import bisect
-
-import numpy as np
-import pandas as pd
-
-
-def concat(data_list: Union[SingleData], axis=0) -> MultiData:
-    """concat all SingleData by index.
-    TODO: now just for SingleData.
-
-    Parameters
-    ----------
-    data_list : List[SingleData]
-        the list of all SingleData to concat.
-
-    Returns
-    -------
-    MultiData
-        the MultiData with ndim == 2
-    """
-    if axis == 0:
-        raise NotImplementedError(f"please implement this func when axis == 0")
-    elif axis == 1:
-        # get all index and row
-        all_index = set()
-        for index_data in data_list:
-            all_index = all_index | set(index_data.index)
-        all_index = list(all_index)
-        all_index.sort()
-        all_index_map = dict(zip(all_index, range(len(all_index))))
-
-        # concat all
-        tmp_data = np.full((len(all_index), len(data_list)), np.NaN)
-        for data_id, index_data in enumerate(data_list):
-            assert isinstance(index_data, SingleData)
-            now_data_map = [all_index_map[index] for index in index_data.index]
-            tmp_data[now_data_map, data_id] = index_data.data
-        return MultiData(tmp_data, all_index)
-    else:
-        raise ValueError(f"axis must be 0 or 1")
-
-
-def sum_by_index(data_list: Union[SingleData], new_index: list, fill_value=0) -> SingleData:
-    """concat all SingleData by new index.
-
-    Parameters
-    ----------
-    data_list : List[SingleData]
-        the list of all SingleData to sum.
-    new_index : list
-        the new_index of new SingleData.
-    fill_value : float
-        fill the missing values or replace np.NaN.
-
-    Returns
-    -------
-    SingleData
-        the SingleData with new_index and values after sum.
-    """
-    data_list = [data.to_dict() for data in data_list]
-    data_sum = {}
-    for id in new_index:
-        item_sum = 0
-        for data in data_list:
-            if id in data and not np.isnan(data[id]):
-                item_sum += data[id]
-            else:
-                item_sum += fill_value
-        data_sum[id] = item_sum
-    return SingleData(data_sum)
-
-
-class Index:
-    """
-    This is for indexing(rows or columns)
-
-    Read-only operations has higher priorities than others.
-    So this class is designed in a **read-only** way to shared data for queries.
-    Modifications will results in new Index.
-
-    NOTE: the indexing has following flaws
-    - duplicated index value is not well supported (only the first appearance will be considered)
-    - The order of the index is not considered!!!! So the slicing will not behave like pandas when indexings are ordered
-    """
-
-    def __init__(self, idx_list: Union[List, pd.Index, "Index", int]):
-        self.idx_list: np.ndarray = None  # using array type for index list will make things easier
-        if isinstance(idx_list, Index):
-            # Fast read-only copy
-            self.idx_list = idx_list.idx_list
-            self.index_map = idx_list.index_map
-            self._is_sorted = idx_list._is_sorted
-        elif isinstance(idx_list, int):
-            self.index_map = self.idx_list = np.arange(idx_list)
-            self._is_sorted = True
-        else:
-            self.idx_list = np.array(idx_list)
-            # NOTE: only the first appearance is indexed
-            self.index_map = dict(zip(self.idx_list, range(len(self))))
-            self._is_sorted = False
-
-    def __getitem__(self, i: int):
-        return self.idx_list[i]
-
-    def _convert_type(self, item):
-        """
-
-        After user creates indices with Type A, user may query data with other types with the same info.
-            This method try to make type conversion and make query sane rather than raising KeyError strictly
-
-        Parameters
-        ----------
-        item :
-            The item to query index
-        """
-
-        if self.idx_list.dtype.type is np.datetime64:
-            if isinstance(item, pd.Timestamp):
-                # This happens often when creating index based on pandas.DatetimeIndex and query with pd.Timestamp
-                return item.to_numpy()
-        return item
-
-    def index(self, item) -> int:
-        """
-        Given the index value, get the integer index
-
-        Parameters
-        ----------
-        item :
-            The item to query
-
-        Returns
-        -------
-        int:
-            The index of the item
-
-        Raises
-        ------
-        KeyError:
-            If the query item does not exist
-        """
-        try:
-            return self.index_map[self._convert_type(item)]
-        except IndexError as index_e:
-            raise KeyError(f"{item} can't be found in {self}") from index_e
-
-    def __or__(self, other: "Index"):
-        return Index(idx_list=list(set(self.idx_list) | set(other.idx_list)))
-
-    def __eq__(self, other: "Index"):
-        # NOTE:  np.nan is not supported in the index
-        if self.idx_list.shape != other.idx_list.shape:
-            return False
-        return (self.idx_list == other.idx_list).all()
-
-    def __len__(self):
-        return len(self.idx_list)
-
-    def is_sorted(self):
-        return self._is_sorted
-
-    def sort(self) -> Tuple["Index", np.ndarray]:
-        """
-        sort the index
-
-        Returns
-        -------
-        Tuple["Index", np.ndarray]:
-            the sorted Index and the changed index
-        """
-        sorted_idx = np.argsort(self.idx_list)
-        idx = Index(self.idx_list[sorted_idx])
-        idx._is_sorted = True
-        return idx, sorted_idx
-
-    def tolist(self):
-        """return the index with the format of list."""
-        return self.idx_list.tolist()
-
-
-class LocIndexer:
-    """
-    `Indexer` will behave like the `LocIndexer` in Pandas
-
-    Read-only operations has higher priorities than others.
-    So this class is designed in a read-only way to shared data for queries.
-    Modifications will results in new Index.
-    """
-
-    def __init__(self, index_data: "IndexData", indices: List[Index], int_loc: bool = False):
-        self._indices: List[Index] = indices
-        self._bind_id = index_data  # bind index data
-        self._int_loc = int_loc
-        assert self._bind_id.data.ndim == len(self._indices)
-
-    @staticmethod
-    def proc_idx_l(indices: List[Union[List, pd.Index, Index]], data_shape: Tuple = None) -> List[Index]:
-        """process the indices from user and output a list of `Index`"""
-        res = []
-        for i, idx in enumerate(indices):
-            res.append(Index(data_shape[i] if len(idx) == 0 else idx))
-        return res
-
-    def _slc_convert(self, index: Index, indexing: slice) -> slice:
-        """
-        convert value-based indexing to integer-based indexing.
-
-        Parameters
-        ----------
-        index : Index
-            index data.
-        indexing : slice
-            value based indexing data with slice type for indexing.
-
-        Returns
-        -------
-        slice:
-            the integer based slicing
-        """
-        if index.is_sorted():
-            int_start = None if indexing.start is None else bisect.bisect_left(index, indexing.start)
-            int_stop = None if indexing.stop is None else bisect.bisect_right(index, indexing.stop)
-        else:
-            int_start = None if indexing.start is None else index.index(indexing.start)
-            int_stop = None if indexing.stop is None else index.index(indexing.stop) + 1
-        return slice(int_start, int_stop)
-
-    def __getitem__(self, indexing):
-        """
-
-        Parameters
-        ----------
-        indexing :
-            query for data
-
-        Raises
-        ------
-        KeyError:
-            If the non-slice index is queried but does not exist, `KeyError` is raised.
-        """
-        # 1) convert slices to int loc
-        if not isinstance(indexing, tuple):
-            # NOTE: tuple is not supported for indexing
-            indexing = (indexing,)
-
-        # TODO: create a subclass for single value query
-        assert len(indexing) <= len(self._indices)
-
-        int_indexing = []
-        for dim, index in enumerate(self._indices):
-            if dim < len(indexing):
-                _indexing = indexing[dim]
-                if not self._int_loc:  # type converting is only necessary when it is not `iloc`
-                    if isinstance(_indexing, slice):
-                        _indexing = self._slc_convert(index, _indexing)
-                    elif isinstance(_indexing, (IndexData, np.ndarray)):
-                        if isinstance(_indexing, IndexData):
-                            _indexing = _indexing.data
-                        assert _indexing.ndim == 1
-                        if _indexing.dtype != bool:
-                            _indexing = np.array(list(index.index(i) for i in _indexing))
-                    else:
-                        _indexing = index.index(_indexing)
-            else:
-                # Default to select all when user input is not given
-                _indexing = slice(None)
-            int_indexing.append(_indexing)
-
-        # 2) select data and index
-        new_data = self._bind_id.data[tuple(int_indexing)]
-        # return directly if it is scalar
-        if new_data.ndim == 0:
-            return new_data
-        # otherwise we go on to the index part
-        new_indices = [idx[indexing] for idx, indexing in zip(self._indices, int_indexing)]
-
-        # 3) squash dimensions
-        new_indices = [
-            idx for idx in new_indices if isinstance(idx, np.ndarray) and idx.ndim > 0
-        ]  # squash the zero dim indexing
-
-        if new_data.ndim == 1:
-            cls = SingleData
-        elif new_data.ndim == 2:
-            cls = MultiData
-        else:
-            raise ValueError("Not supported")
-        return cls(new_data, *new_indices)
-
-
-class BinaryOps:
-    def __init__(self, method_name):
-        self.method_name = method_name
-
-    def __get__(self, obj, *args):
-        # bind object
-        self.obj = obj
-        return self
-
-    def __call__(self, other):
-        self_data_method = getattr(self.obj.data, self.method_name)
-
-        if isinstance(other, (int, float, np.number)):
-            return self.obj.__class__(self_data_method(other), *self.obj.indices)
-        elif isinstance(other, self.obj.__class__):
-            other_aligned = self.obj._align_indices(other)
-            return self.obj.__class__(self_data_method(other_aligned.data), *self.obj.indices)
-        else:
-            return NotImplemented
-
-
-def index_data_ops_creator(*args, **kwargs):
-    """
-    meta class for auto generating operations for index data.
-    """
-    for method_name in ["__add__", "__sub__", "__rsub__", "__mul__", "__truediv__", "__eq__", "__gt__", "__lt__"]:
-        args[2][method_name] = BinaryOps(method_name=method_name)
-    return type(*args)
-
-
-class IndexData(metaclass=index_data_ops_creator):
-    """
-    Base data structure of SingleData and MultiData.
-
-    NOTE:
-    - For performance issue, only **np.floating** is supported in the underlayer data !!!
-    - Boolean based on np.floating is also supported. Here are some examples
-
-    .. code-block:: python
-
-        np.array([ np.nan]).any() -> True
-        np.array([ np.nan]).all() -> True
-        np.array([1. , 0.]).any() -> True
-        np.array([1. , 0.]).all() -> False
-    """
-
-    loc_idx_cls = LocIndexer
-
-    def __init__(self, data: np.ndarray, *indices: Union[List, pd.Index, Index]):
-
-        self.data = data
-        self.indices = indices
-
-        # get the expected data shape
-        # - The index has higher priority
-        self.data = np.array(data)
-
-        expected_dim = max(self.data.ndim, len(indices))
-
-        data_shape = []
-        for i in range(expected_dim):
-            idx_l = indices[i] if len(indices) > i else []
-            if len(idx_l) == 0:
-                data_shape.append(self.data.shape[i])
-            else:
-                data_shape.append(len(idx_l))
-        data_shape = tuple(data_shape)
-
-        # broadcast the data to expected shape
-        if self.data.shape != data_shape:
-            self.data = np.broadcast_to(self.data, data_shape)
-
-        self.data = self.data.astype(np.float64)
-        # Please notice following cases when converting the type
-        # - np.array([None, 1]).astype(np.float64) -> array([nan,  1.])
-
-        # create index from user's index data.
-        self.indices: List[Index] = self.loc_idx_cls.proc_idx_l(indices, data_shape)
-
-        for dim in range(expected_dim):
-            assert self.data.shape[dim] == len(self.indices[dim])
-
-        self.ndim = expected_dim
-
-    # indexing related methods
-    @property
-    def loc(self):
-        return self.loc_idx_cls(index_data=self, indices=self.indices)
-
-    @property
-    def iloc(self):
-        return self.loc_idx_cls(index_data=self, indices=self.indices, int_loc=True)
-
-    @property
-    def index(self):
-        return self.indices[0]
-
-    @property
-    def columns(self):
-        return self.indices[1]
-
-    def __getitem__(self, args):
-        # NOTE: this tries to behave like a numpy array to be compatible with numpy aggregating function like nansum and nanmean
-        return self.iloc[args]
-
-    def _align_indices(self, other: "IndexData") -> "IndexData":
-        """
-        Align all indices of `other` to `self` before performing the arithmetic operations.
-        This function will return a new IndexData rather than changing data in `other` inplace
-
-        Parameters
-        ----------
-        other : "IndexData"
-            the index in `other` is to be changed
-
-        Returns
-        -------
-        IndexData:
-            the data in `other` with index aligned to `self`
-        """
-        raise NotImplementedError(f"please implement _align_indices func")
-
-    def sort_index(self, axis=0, inplace=True):
-        assert inplace, "Only support sorting inplace now"
-        self.indices[axis], sorted_idx = self.indices[axis].sort()
-        self.data = np.take(self.data, sorted_idx, axis=axis)
-
-    # The code below could be simpler like methods in __getattribute__
-    def __invert__(self):
-        return self.__class__(~self.data.astype(bool), *self.indices)
-
-    def abs(self):
-        """get the abs of data except np.NaN."""
-        tmp_data = np.absolute(self.data)
-        return self.__class__(tmp_data, *self.indices)
-
-    def replace(self, to_replace: Dict[np.number, np.number]):
-        assert isinstance(to_replace, dict)
-        tmp_data = self.data.copy()
-        for num in to_replace:
-            if num in tmp_data:
-                tmp_data[self.data == num] = to_replace[num]
-        return self.__class__(tmp_data, *self.indices)
-
-    def apply(self, func: Callable):
-        """apply a function to data."""
-        tmp_data = func(self.data)
-        return self.__class__(tmp_data, *self.indices)
-
-    def __len__(self):
-        """the length of the data.
-
-        Returns
-        -------
-        int
-            the length of the data.
-        """
-        return len(self.data)
-
-    def sum(self, axis=None, dtype=None, out=None):
-        assert out is None and dtype is None, "`out` is just for compatible with numpy's aggregating function"
-        # FIXME: weird logic and not general
-        if axis is None:
-            return np.nansum(self.data)
-        elif axis == 0:
-            tmp_data = np.nansum(self.data, axis=0)
-            return SingleData(tmp_data, self.columns)
-        elif axis == 1:
-            tmp_data = np.nansum(self.data, axis=1)
-            return SingleData(tmp_data, self.index)
-        else:
-            raise ValueError(f"axis must be None, 0 or 1")
-
-    def mean(self, axis=None, dtype=None, out=None):
-        assert out is None and dtype is None, "`out` is just for compatible with numpy's aggregating function"
-        # FIXME: weird logic and not general
-        if axis is None:
-            return np.nanmean(self.data)
-        elif axis == 0:
-            tmp_data = np.nanmean(self.data, axis=0)
-            return SingleData(tmp_data, self.columns)
-        elif axis == 1:
-            tmp_data = np.nanmean(self.data, axis=1)
-            return SingleData(tmp_data, self.index)
-        else:
-            raise ValueError(f"axis must be None, 0 or 1")
-
-    def isna(self):
-        return self.__class__(np.isnan(self.data), *self.indices)
-
-    def fillna(self, value=0.0, inplace: bool = False):
-        if inplace:
-            self.data = np.nan_to_num(self.data, nan=value)
-        else:
-            return self.__class__(np.nan_to_num(self.data, nan=value), *self.indices)
-
-    def count(self):
-        return len(self.data[~np.isnan(self.data)])
-
-    def all(self):
-        if None in self.data:
-            return self.data[self.data is not None].all()
-        else:
-            return self.data.all()
-
-    @property
-    def empty(self):
-        return len(self.data) == 0
-
-    @property
-    def values(self):
-        return self.data
-
-
-class SingleData(IndexData):
-    def __init__(
-        self, data: Union[int, float, np.number, list, dict, pd.Series] = [], index: Union[List, pd.Index, Index] = []
-    ):
-        """A data structure of index and numpy data.
-        It's used to replace pd.Series due to high-speed.
-
-        Parameters
-        ----------
-        data : Union[int, float, np.number, list, dict, pd.Series]
-            the input data
-        index : Union[list, pd.Index]
-            the index of data.
-            empty list indicates that auto filling the index to the length of data
-        """
-        # for special data type
-        if isinstance(data, dict):
-            assert len(index) == 0
-            if len(data) > 0:
-                index, data = zip(*data.items())
-            else:
-                index, data = [], []
-        elif isinstance(data, pd.Series):
-            assert len(index) == 0
-            index, data = data.index, data.values
-        elif isinstance(data, (int, float, np.number)):
-            data = [data]
-        super().__init__(data, index)
-        assert self.ndim == 1
-
-    def _align_indices(self, other):
-        if self.index == other.index:
-            return other
-        elif set(self.index) == set(other.index):
-            return other.reindex(self.index)
-        else:
-            raise ValueError(
-                f"The indexes of self and other do not meet the requirements of the four arithmetic operations"
-            )
-
-    def reindex(self, index: Index, fill_value=np.NaN) -> SingleData:
-        """reindex data and fill the missing value with np.NaN.
-
-        Parameters
-        ----------
-        new_index : list
-            new index
-        fill_value:
-            what value to fill if index is missing
-
-        Returns
-        -------
-        SingleData
-            reindex data
-        """
-        # TODO: This method can be more general
-        if self.index == index:
-            return self
-        tmp_data = np.full(len(index), fill_value, dtype=np.float64)
-        for index_id, index_item in enumerate(index):
-            try:
-                tmp_data[index_id] = self.loc[index_item]
-            except KeyError:
-                pass
-        return SingleData(tmp_data, index)
-
-    def add(self, other: SingleData, fill_value=0):
-        # TODO: add and __add__ are a little confusing.
-        # This could be a more general
-        common_index = self.index | other.index
-        common_index, _ = common_index.sort()
-        tmp_data1 = self.reindex(common_index, fill_value)
-        tmp_data2 = other.reindex(common_index, fill_value)
-        return tmp_data1.fillna(fill_value) + tmp_data2.fillna(fill_value)
-
-    def to_dict(self):
-        """convert SingleData to dict.
-
-        Returns
-        -------
-        dict
-            data with the dict format.
-        """
-        return dict(zip(self.index, self.data.tolist()))
-
-    def to_series(self):
-        return pd.Series(self.data, index=self.index)
-
-    def __repr__(self) -> str:
-        return str(pd.Series(self.data, index=self.index))
-
-
-class MultiData(IndexData):
-    def __init__(
-        self,
-        data: Union[int, float, np.number, list] = [],
-        index: Union[List, pd.Index, Index] = [],
-        columns: Union[List, pd.Index, Index] = [],
-    ):
-        """A data structure of index and numpy data.
-        It's used to replace pd.DataFrame due to high-speed.
-
-        Parameters
-        ----------
-        data : Union[list, np.ndarray]
-            the dim of data must be 2.
-        index : Union[List, pd.Index, Index]
-            the index of data.
-        columns: Union[List, pd.Index, Index]
-            the columns of data.
-        """
-        if isinstance(data, pd.DataFrame):
-            index, columns, data = data.index, data.columns, data.values
-        super().__init__(data, index, columns)
-        assert self.ndim == 2
-
-    def _align_indices(self, other):
-        if self.indices == other.indices:
-            return other
-        else:
-            raise ValueError(
-                f"The indexes of self and other do not meet the requirements of the four arithmetic operations"
-            )
-
-    def __repr__(self) -> str:
-        return str(pd.DataFrame(self.data, index=self.index, columns=self.columns))
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+"""
+Motivation of index_data
+- Pandas has a lot of user-friendly interfaces. However, integrating too much features in a single tool bring too much overhead and makes it much slower than numpy.
+    Some users just want a simple numpy dataframe with indices and don't want such a complicated tools.
+    Such users are the target of `index_data`
+
+`index_data` try to behave like pandas (some API will be different because we try to be simpler and more intuitive) but don't compromise the performance. It provides the basic numpy data and simple indexing feature. If users call APIs which may compromise the performance, index_data will raise Errors.
+"""
+
+from __future__ import annotations
+
+from typing import Dict, Tuple, Union, Callable, List
+import bisect
+
+import numpy as np
+import pandas as pd
+
+
+def concat(data_list: Union[SingleData], axis=0) -> MultiData:
+    """concat all SingleData by index.
+    TODO: now just for SingleData.
+
+    Parameters
+    ----------
+    data_list : List[SingleData]
+        the list of all SingleData to concat.
+
+    Returns
+    -------
+    MultiData
+        the MultiData with ndim == 2
+    """
+    if axis == 0:
+        raise NotImplementedError(f"please implement this func when axis == 0")
+    elif axis == 1:
+        # get all index and row
+        all_index = set()
+        for index_data in data_list:
+            all_index = all_index | set(index_data.index)
+        all_index = list(all_index)
+        all_index.sort()
+        all_index_map = dict(zip(all_index, range(len(all_index))))
+
+        # concat all
+        tmp_data = np.full((len(all_index), len(data_list)), np.NaN)
+        for data_id, index_data in enumerate(data_list):
+            assert isinstance(index_data, SingleData)
+            now_data_map = [all_index_map[index] for index in index_data.index]
+            tmp_data[now_data_map, data_id] = index_data.data
+        return MultiData(tmp_data, all_index)
+    else:
+        raise ValueError(f"axis must be 0 or 1")
+
+
+def sum_by_index(data_list: Union[SingleData], new_index: list, fill_value=0) -> SingleData:
+    """concat all SingleData by new index.
+
+    Parameters
+    ----------
+    data_list : List[SingleData]
+        the list of all SingleData to sum.
+    new_index : list
+        the new_index of new SingleData.
+    fill_value : float
+        fill the missing values or replace np.NaN.
+
+    Returns
+    -------
+    SingleData
+        the SingleData with new_index and values after sum.
+    """
+    data_list = [data.to_dict() for data in data_list]
+    data_sum = {}
+    for id in new_index:
+        item_sum = 0
+        for data in data_list:
+            if id in data and not np.isnan(data[id]):
+                item_sum += data[id]
+            else:
+                item_sum += fill_value
+        data_sum[id] = item_sum
+    return SingleData(data_sum)
+
+
+class Index:
+    """
+    This is for indexing(rows or columns)
+
+    Read-only operations has higher priorities than others.
+    So this class is designed in a **read-only** way to shared data for queries.
+    Modifications will results in new Index.
+
+    NOTE: the indexing has following flaws
+    - duplicated index value is not well supported (only the first appearance will be considered)
+    - The order of the index is not considered!!!! So the slicing will not behave like pandas when indexings are ordered
+    """
+
+    def __init__(self, idx_list: Union[List, pd.Index, "Index", int]):
+        self.idx_list: np.ndarray = None  # using array type for index list will make things easier
+        if isinstance(idx_list, Index):
+            # Fast read-only copy
+            self.idx_list = idx_list.idx_list
+            self.index_map = idx_list.index_map
+            self._is_sorted = idx_list._is_sorted
+        elif isinstance(idx_list, int):
+            self.index_map = self.idx_list = np.arange(idx_list)
+            self._is_sorted = True
+        else:
+            self.idx_list = np.array(idx_list)
+            # NOTE: only the first appearance is indexed
+            self.index_map = dict(zip(self.idx_list, range(len(self))))
+            self._is_sorted = False
+
+    def __getitem__(self, i: int):
+        return self.idx_list[i]
+
+    def _convert_type(self, item):
+        """
+
+        After user creates indices with Type A, user may query data with other types with the same info.
+            This method try to make type conversion and make query sane rather than raising KeyError strictly
+
+        Parameters
+        ----------
+        item :
+            The item to query index
+        """
+
+        if self.idx_list.dtype.type is np.datetime64:
+            if isinstance(item, pd.Timestamp):
+                # This happens often when creating index based on pandas.DatetimeIndex and query with pd.Timestamp
+                return item.to_numpy()
+        return item
+
+    def index(self, item) -> int:
+        """
+        Given the index value, get the integer index
+
+        Parameters
+        ----------
+        item :
+            The item to query
+
+        Returns
+        -------
+        int:
+            The index of the item
+
+        Raises
+        ------
+        KeyError:
+            If the query item does not exist
+        """
+        try:
+            return self.index_map[self._convert_type(item)]
+        except IndexError as index_e:
+            raise KeyError(f"{item} can't be found in {self}") from index_e
+
+    def __or__(self, other: "Index"):
+        return Index(idx_list=list(set(self.idx_list) | set(other.idx_list)))
+
+    def __eq__(self, other: "Index"):
+        # NOTE:  np.nan is not supported in the index
+        if self.idx_list.shape != other.idx_list.shape:
+            return False
+        return (self.idx_list == other.idx_list).all()
+
+    def __len__(self):
+        return len(self.idx_list)
+
+    def is_sorted(self):
+        return self._is_sorted
+
+    def sort(self) -> Tuple["Index", np.ndarray]:
+        """
+        sort the index
+
+        Returns
+        -------
+        Tuple["Index", np.ndarray]:
+            the sorted Index and the changed index
+        """
+        sorted_idx = np.argsort(self.idx_list)
+        idx = Index(self.idx_list[sorted_idx])
+        idx._is_sorted = True
+        return idx, sorted_idx
+
+    def tolist(self):
+        """return the index with the format of list."""
+        return self.idx_list.tolist()
+
+
+class LocIndexer:
+    """
+    `Indexer` will behave like the `LocIndexer` in Pandas
+
+    Read-only operations has higher priorities than others.
+    So this class is designed in a read-only way to shared data for queries.
+    Modifications will results in new Index.
+    """
+
+    def __init__(self, index_data: "IndexData", indices: List[Index], int_loc: bool = False):
+        self._indices: List[Index] = indices
+        self._bind_id = index_data  # bind index data
+        self._int_loc = int_loc
+        assert self._bind_id.data.ndim == len(self._indices)
+
+    @staticmethod
+    def proc_idx_l(indices: List[Union[List, pd.Index, Index]], data_shape: Tuple = None) -> List[Index]:
+        """process the indices from user and output a list of `Index`"""
+        res = []
+        for i, idx in enumerate(indices):
+            res.append(Index(data_shape[i] if len(idx) == 0 else idx))
+        return res
+
+    def _slc_convert(self, index: Index, indexing: slice) -> slice:
+        """
+        convert value-based indexing to integer-based indexing.
+
+        Parameters
+        ----------
+        index : Index
+            index data.
+        indexing : slice
+            value based indexing data with slice type for indexing.
+
+        Returns
+        -------
+        slice:
+            the integer based slicing
+        """
+        if index.is_sorted():
+            int_start = None if indexing.start is None else bisect.bisect_left(index, indexing.start)
+            int_stop = None if indexing.stop is None else bisect.bisect_right(index, indexing.stop)
+        else:
+            int_start = None if indexing.start is None else index.index(indexing.start)
+            int_stop = None if indexing.stop is None else index.index(indexing.stop) + 1
+        return slice(int_start, int_stop)
+
+    def __getitem__(self, indexing):
+        """
+
+        Parameters
+        ----------
+        indexing :
+            query for data
+
+        Raises
+        ------
+        KeyError:
+            If the non-slice index is queried but does not exist, `KeyError` is raised.
+        """
+        # 1) convert slices to int loc
+        if not isinstance(indexing, tuple):
+            # NOTE: tuple is not supported for indexing
+            indexing = (indexing,)
+
+        # TODO: create a subclass for single value query
+        assert len(indexing) <= len(self._indices)
+
+        int_indexing = []
+        for dim, index in enumerate(self._indices):
+            if dim < len(indexing):
+                _indexing = indexing[dim]
+                if not self._int_loc:  # type converting is only necessary when it is not `iloc`
+                    if isinstance(_indexing, slice):
+                        _indexing = self._slc_convert(index, _indexing)
+                    elif isinstance(_indexing, (IndexData, np.ndarray)):
+                        if isinstance(_indexing, IndexData):
+                            _indexing = _indexing.data
+                        assert _indexing.ndim == 1
+                        if _indexing.dtype != bool:
+                            _indexing = np.array(list(index.index(i) for i in _indexing))
+                    else:
+                        _indexing = index.index(_indexing)
+            else:
+                # Default to select all when user input is not given
+                _indexing = slice(None)
+            int_indexing.append(_indexing)
+
+        # 2) select data and index
+        new_data = self._bind_id.data[tuple(int_indexing)]
+        # return directly if it is scalar
+        if new_data.ndim == 0:
+            return new_data
+        # otherwise we go on to the index part
+        new_indices = [idx[indexing] for idx, indexing in zip(self._indices, int_indexing)]
+
+        # 3) squash dimensions
+        new_indices = [
+            idx for idx in new_indices if isinstance(idx, np.ndarray) and idx.ndim > 0
+        ]  # squash the zero dim indexing
+
+        if new_data.ndim == 1:
+            cls = SingleData
+        elif new_data.ndim == 2:
+            cls = MultiData
+        else:
+            raise ValueError("Not supported")
+        return cls(new_data, *new_indices)
+
+
+class BinaryOps:
+    def __init__(self, method_name):
+        self.method_name = method_name
+
+    def __get__(self, obj, *args):
+        # bind object
+        self.obj = obj
+        return self
+
+    def __call__(self, other):
+        self_data_method = getattr(self.obj.data, self.method_name)
+
+        if isinstance(other, (int, float, np.number)):
+            return self.obj.__class__(self_data_method(other), *self.obj.indices)
+        elif isinstance(other, self.obj.__class__):
+            other_aligned = self.obj._align_indices(other)
+            return self.obj.__class__(self_data_method(other_aligned.data), *self.obj.indices)
+        else:
+            return NotImplemented
+
+
+def index_data_ops_creator(*args, **kwargs):
+    """
+    meta class for auto generating operations for index data.
+    """
+    for method_name in ["__add__", "__sub__", "__rsub__", "__mul__", "__truediv__", "__eq__", "__gt__", "__lt__"]:
+        args[2][method_name] = BinaryOps(method_name=method_name)
+    return type(*args)
+
+
+class IndexData(metaclass=index_data_ops_creator):
+    """
+    Base data structure of SingleData and MultiData.
+
+    NOTE:
+    - For performance issue, only **np.floating** is supported in the underlayer data !!!
+    - Boolean based on np.floating is also supported. Here are some examples
+
+    .. code-block:: python
+
+        np.array([ np.nan]).any() -> True
+        np.array([ np.nan]).all() -> True
+        np.array([1. , 0.]).any() -> True
+        np.array([1. , 0.]).all() -> False
+    """
+
+    loc_idx_cls = LocIndexer
+
+    def __init__(self, data: np.ndarray, *indices: Union[List, pd.Index, Index]):
+        self.data = data
+        self.indices = indices
+
+        # get the expected data shape
+        # - The index has higher priority
+        self.data = np.array(data)
+
+        expected_dim = max(self.data.ndim, len(indices))
+
+        data_shape = []
+        for i in range(expected_dim):
+            idx_l = indices[i] if len(indices) > i else []
+            if len(idx_l) == 0:
+                data_shape.append(self.data.shape[i])
+            else:
+                data_shape.append(len(idx_l))
+        data_shape = tuple(data_shape)
+
+        # broadcast the data to expected shape
+        if self.data.shape != data_shape:
+            self.data = np.broadcast_to(self.data, data_shape)
+
+        self.data = self.data.astype(np.float64)
+        # Please notice following cases when converting the type
+        # - np.array([None, 1]).astype(np.float64) -> array([nan,  1.])
+
+        # create index from user's index data.
+        self.indices: List[Index] = self.loc_idx_cls.proc_idx_l(indices, data_shape)
+
+        for dim in range(expected_dim):
+            assert self.data.shape[dim] == len(self.indices[dim])
+
+        self.ndim = expected_dim
+
+    # indexing related methods
+    @property
+    def loc(self):
+        return self.loc_idx_cls(index_data=self, indices=self.indices)
+
+    @property
+    def iloc(self):
+        return self.loc_idx_cls(index_data=self, indices=self.indices, int_loc=True)
+
+    @property
+    def index(self):
+        return self.indices[0]
+
+    @property
+    def columns(self):
+        return self.indices[1]
+
+    def __getitem__(self, args):
+        # NOTE: this tries to behave like a numpy array to be compatible with numpy aggregating function like nansum and nanmean
+        return self.iloc[args]
+
+    def _align_indices(self, other: "IndexData") -> "IndexData":
+        """
+        Align all indices of `other` to `self` before performing the arithmetic operations.
+        This function will return a new IndexData rather than changing data in `other` inplace
+
+        Parameters
+        ----------
+        other : "IndexData"
+            the index in `other` is to be changed
+
+        Returns
+        -------
+        IndexData:
+            the data in `other` with index aligned to `self`
+        """
+        raise NotImplementedError(f"please implement _align_indices func")
+
+    def sort_index(self, axis=0, inplace=True):
+        assert inplace, "Only support sorting inplace now"
+        self.indices[axis], sorted_idx = self.indices[axis].sort()
+        self.data = np.take(self.data, sorted_idx, axis=axis)
+
+    # The code below could be simpler like methods in __getattribute__
+    def __invert__(self):
+        return self.__class__(~self.data.astype(bool), *self.indices)
+
+    def abs(self):
+        """get the abs of data except np.NaN."""
+        tmp_data = np.absolute(self.data)
+        return self.__class__(tmp_data, *self.indices)
+
+    def replace(self, to_replace: Dict[np.number, np.number]):
+        assert isinstance(to_replace, dict)
+        tmp_data = self.data.copy()
+        for num in to_replace:
+            if num in tmp_data:
+                tmp_data[self.data == num] = to_replace[num]
+        return self.__class__(tmp_data, *self.indices)
+
+    def apply(self, func: Callable):
+        """apply a function to data."""
+        tmp_data = func(self.data)
+        return self.__class__(tmp_data, *self.indices)
+
+    def __len__(self):
+        """the length of the data.
+
+        Returns
+        -------
+        int
+            the length of the data.
+        """
+        return len(self.data)
+
+    def sum(self, axis=None, dtype=None, out=None):
+        assert out is None and dtype is None, "`out` is just for compatible with numpy's aggregating function"
+        # FIXME: weird logic and not general
+        if axis is None:
+            return np.nansum(self.data)
+        elif axis == 0:
+            tmp_data = np.nansum(self.data, axis=0)
+            return SingleData(tmp_data, self.columns)
+        elif axis == 1:
+            tmp_data = np.nansum(self.data, axis=1)
+            return SingleData(tmp_data, self.index)
+        else:
+            raise ValueError(f"axis must be None, 0 or 1")
+
+    def mean(self, axis=None, dtype=None, out=None):
+        assert out is None and dtype is None, "`out` is just for compatible with numpy's aggregating function"
+        # FIXME: weird logic and not general
+        if axis is None:
+            return np.nanmean(self.data)
+        elif axis == 0:
+            tmp_data = np.nanmean(self.data, axis=0)
+            return SingleData(tmp_data, self.columns)
+        elif axis == 1:
+            tmp_data = np.nanmean(self.data, axis=1)
+            return SingleData(tmp_data, self.index)
+        else:
+            raise ValueError(f"axis must be None, 0 or 1")
+
+    def isna(self):
+        return self.__class__(np.isnan(self.data), *self.indices)
+
+    def fillna(self, value=0.0, inplace: bool = False):
+        if inplace:
+            self.data = np.nan_to_num(self.data, nan=value)
+        else:
+            return self.__class__(np.nan_to_num(self.data, nan=value), *self.indices)
+
+    def count(self):
+        return len(self.data[~np.isnan(self.data)])
+
+    def all(self):
+        if None in self.data:
+            return self.data[self.data is not None].all()
+        else:
+            return self.data.all()
+
+    @property
+    def empty(self):
+        return len(self.data) == 0
+
+    @property
+    def values(self):
+        return self.data
+
+
+class SingleData(IndexData):
+    def __init__(
+        self, data: Union[int, float, np.number, list, dict, pd.Series] = [], index: Union[List, pd.Index, Index] = []
+    ):
+        """A data structure of index and numpy data.
+        It's used to replace pd.Series due to high-speed.
+
+        Parameters
+        ----------
+        data : Union[int, float, np.number, list, dict, pd.Series]
+            the input data
+        index : Union[list, pd.Index]
+            the index of data.
+            empty list indicates that auto filling the index to the length of data
+        """
+        # for special data type
+        if isinstance(data, dict):
+            assert len(index) == 0
+            if len(data) > 0:
+                index, data = zip(*data.items())
+            else:
+                index, data = [], []
+        elif isinstance(data, pd.Series):
+            assert len(index) == 0
+            index, data = data.index, data.values
+        elif isinstance(data, (int, float, np.number)):
+            data = [data]
+        super().__init__(data, index)
+        assert self.ndim == 1
+
+    def _align_indices(self, other):
+        if self.index == other.index:
+            return other
+        elif set(self.index) == set(other.index):
+            return other.reindex(self.index)
+        else:
+            raise ValueError(
+                f"The indexes of self and other do not meet the requirements of the four arithmetic operations"
+            )
+
+    def reindex(self, index: Index, fill_value=np.NaN) -> SingleData:
+        """reindex data and fill the missing value with np.NaN.
+
+        Parameters
+        ----------
+        new_index : list
+            new index
+        fill_value:
+            what value to fill if index is missing
+
+        Returns
+        -------
+        SingleData
+            reindex data
+        """
+        # TODO: This method can be more general
+        if self.index == index:
+            return self
+        tmp_data = np.full(len(index), fill_value, dtype=np.float64)
+        for index_id, index_item in enumerate(index):
+            try:
+                tmp_data[index_id] = self.loc[index_item]
+            except KeyError:
+                pass
+        return SingleData(tmp_data, index)
+
+    def add(self, other: SingleData, fill_value=0):
+        # TODO: add and __add__ are a little confusing.
+        # This could be a more general
+        common_index = self.index | other.index
+        common_index, _ = common_index.sort()
+        tmp_data1 = self.reindex(common_index, fill_value)
+        tmp_data2 = other.reindex(common_index, fill_value)
+        return tmp_data1.fillna(fill_value) + tmp_data2.fillna(fill_value)
+
+    def to_dict(self):
+        """convert SingleData to dict.
+
+        Returns
+        -------
+        dict
+            data with the dict format.
+        """
+        return dict(zip(self.index, self.data.tolist()))
+
+    def to_series(self):
+        return pd.Series(self.data, index=self.index)
+
+    def __repr__(self) -> str:
+        return str(pd.Series(self.data, index=self.index))
+
+
+class MultiData(IndexData):
+    def __init__(
+        self,
+        data: Union[int, float, np.number, list] = [],
+        index: Union[List, pd.Index, Index] = [],
+        columns: Union[List, pd.Index, Index] = [],
+    ):
+        """A data structure of index and numpy data.
+        It's used to replace pd.DataFrame due to high-speed.
+
+        Parameters
+        ----------
+        data : Union[list, np.ndarray]
+            the dim of data must be 2.
+        index : Union[List, pd.Index, Index]
+            the index of data.
+        columns: Union[List, pd.Index, Index]
+            the columns of data.
+        """
+        if isinstance(data, pd.DataFrame):
+            index, columns, data = data.index, data.columns, data.values
+        super().__init__(data, index, columns)
+        assert self.ndim == 2
+
+    def _align_indices(self, other):
+        if self.indices == other.indices:
+            return other
+        else:
+            raise ValueError(
+                f"The indexes of self and other do not meet the requirements of the four arithmetic operations"
+            )
+
+    def __repr__(self) -> str:
+        return str(pd.DataFrame(self.data, index=self.index, columns=self.columns))
```

## qlib/utils/objm.py

 * *Ordering differences only*

```diff
@@ -1,133 +1,133 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-import os
-import pickle
-import tempfile
-from pathlib import Path
-
-from qlib.config import C
-
-
-class ObjManager:
-    def save_obj(self, obj: object, name: str):
-        """
-        save obj as name
-
-        Parameters
-        ----------
-        obj : object
-            object to be saved
-        name : str
-            name of the object
-        """
-        raise NotImplementedError(f"Please implement `save_obj`")
-
-    def save_objs(self, obj_name_l):
-        """
-        save objects
-
-        Parameters
-        ----------
-        obj_name_l : list of <obj, name>
-        """
-        raise NotImplementedError(f"Please implement the `save_objs` method")
-
-    def load_obj(self, name: str) -> object:
-        """
-        load object by name
-
-        Parameters
-        ----------
-        name : str
-            the name of the object
-
-        Returns
-        -------
-        object:
-            loaded object
-        """
-        raise NotImplementedError(f"Please implement the `load_obj` method")
-
-    def exists(self, name: str) -> bool:
-        """
-        if the object named `name` exists
-
-        Parameters
-        ----------
-        name : str
-            name of the objecT
-
-        Returns
-        -------
-        bool:
-            If the object exists
-        """
-        raise NotImplementedError(f"Please implement the `exists` method")
-
-    def list(self) -> list:
-        """
-        list the objects
-
-        Returns
-        -------
-        list:
-            the list of returned objects
-        """
-        raise NotImplementedError(f"Please implement the `list` method")
-
-    def remove(self, fname=None):
-        """remove.
-
-        Parameters
-        ----------
-        fname :
-            if file name is provided. specific file is removed
-            otherwise, The all the objects will be removed.
-        """
-        raise NotImplementedError(f"Please implement the `remove` method")
-
-
-class FileManager(ObjManager):
-    """
-    Use file system to manage objects
-    """
-
-    def __init__(self, path=None):
-        if path is None:
-            self.path = Path(self.create_path())
-        else:
-            self.path = Path(path).resolve()
-
-    def create_path(self) -> str:
-        try:
-            return tempfile.mkdtemp(prefix=str(C["file_manager_path"]) + os.sep)
-        except AttributeError as attribute_e:
-            raise NotImplementedError(
-                f"If path is not given, the `create_path` function should be implemented"
-            ) from attribute_e
-
-    def save_obj(self, obj, name):
-        with (self.path / name).open("wb") as f:
-            pickle.dump(obj, f, protocol=C.dump_protocol_version)
-
-    def save_objs(self, obj_name_l):
-        for obj, name in obj_name_l:
-            self.save_obj(obj, name)
-
-    def load_obj(self, name):
-        with (self.path / name).open("rb") as f:
-            return pickle.load(f)
-
-    def exists(self, name):
-        return (self.path / name).exists()
-
-    def list(self):
-        return list(self.path.iterdir())
-
-    def remove(self, fname=None):
-        if fname is None:
-            for fp in self.path.glob("*"):
-                fp.unlink()
-            self.path.rmdir()
-        else:
-            (self.path / fname).unlink()
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+import os
+import pickle
+import tempfile
+from pathlib import Path
+
+from qlib.config import C
+
+
+class ObjManager:
+    def save_obj(self, obj: object, name: str):
+        """
+        save obj as name
+
+        Parameters
+        ----------
+        obj : object
+            object to be saved
+        name : str
+            name of the object
+        """
+        raise NotImplementedError(f"Please implement `save_obj`")
+
+    def save_objs(self, obj_name_l):
+        """
+        save objects
+
+        Parameters
+        ----------
+        obj_name_l : list of <obj, name>
+        """
+        raise NotImplementedError(f"Please implement the `save_objs` method")
+
+    def load_obj(self, name: str) -> object:
+        """
+        load object by name
+
+        Parameters
+        ----------
+        name : str
+            the name of the object
+
+        Returns
+        -------
+        object:
+            loaded object
+        """
+        raise NotImplementedError(f"Please implement the `load_obj` method")
+
+    def exists(self, name: str) -> bool:
+        """
+        if the object named `name` exists
+
+        Parameters
+        ----------
+        name : str
+            name of the objecT
+
+        Returns
+        -------
+        bool:
+            If the object exists
+        """
+        raise NotImplementedError(f"Please implement the `exists` method")
+
+    def list(self) -> list:
+        """
+        list the objects
+
+        Returns
+        -------
+        list:
+            the list of returned objects
+        """
+        raise NotImplementedError(f"Please implement the `list` method")
+
+    def remove(self, fname=None):
+        """remove.
+
+        Parameters
+        ----------
+        fname :
+            if file name is provided. specific file is removed
+            otherwise, The all the objects will be removed.
+        """
+        raise NotImplementedError(f"Please implement the `remove` method")
+
+
+class FileManager(ObjManager):
+    """
+    Use file system to manage objects
+    """
+
+    def __init__(self, path=None):
+        if path is None:
+            self.path = Path(self.create_path())
+        else:
+            self.path = Path(path).resolve()
+
+    def create_path(self) -> str:
+        try:
+            return tempfile.mkdtemp(prefix=str(C["file_manager_path"]) + os.sep)
+        except AttributeError as attribute_e:
+            raise NotImplementedError(
+                f"If path is not given, the `create_path` function should be implemented"
+            ) from attribute_e
+
+    def save_obj(self, obj, name):
+        with (self.path / name).open("wb") as f:
+            pickle.dump(obj, f, protocol=C.dump_protocol_version)
+
+    def save_objs(self, obj_name_l):
+        for obj, name in obj_name_l:
+            self.save_obj(obj, name)
+
+    def load_obj(self, name):
+        with (self.path / name).open("rb") as f:
+            return pickle.load(f)
+
+    def exists(self, name):
+        return (self.path / name).exists()
+
+    def list(self):
+        return list(self.path.iterdir())
+
+    def remove(self, fname=None):
+        if fname is None:
+            for fp in self.path.glob("*"):
+                fp.unlink()
+            self.path.rmdir()
+        else:
+            (self.path / fname).unlink()
```

## qlib/utils/paral.py

 * *Ordering differences only*

```diff
@@ -1,315 +1,315 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-from functools import partial
-from threading import Thread
-from typing import Callable, Text, Union
-
-from joblib import Parallel, delayed
-from joblib._parallel_backends import MultiprocessingBackend
-import pandas as pd
-
-from queue import Queue
-import concurrent
-
-from qlib.config import C, QlibConfig
-
-
-class ParallelExt(Parallel):
-    def __init__(self, *args, **kwargs):
-        maxtasksperchild = kwargs.pop("maxtasksperchild", None)
-        super(ParallelExt, self).__init__(*args, **kwargs)
-        if isinstance(self._backend, MultiprocessingBackend):
-            self._backend_args["maxtasksperchild"] = maxtasksperchild
-
-
-def datetime_groupby_apply(
-    df, apply_func: Union[Callable, Text], axis=0, level="datetime", resample_rule="M", n_jobs=-1
-):
-    """datetime_groupby_apply
-    This function will apply the `apply_func` on the datetime level index.
-
-    Parameters
-    ----------
-    df :
-        DataFrame for processing
-    apply_func : Union[Callable, Text]
-        apply_func for processing the data
-        if a string is given, then it is treated as naive pandas function
-    axis :
-        which axis is the datetime level located
-    level :
-        which level is the datetime level
-    resample_rule :
-        How to resample the data to calculating parallel
-    n_jobs :
-        n_jobs for joblib
-    Returns:
-        pd.DataFrame
-    """
-
-    def _naive_group_apply(df):
-        if isinstance(apply_func, str):
-            return getattr(df.groupby(axis=axis, level=level), apply_func)()
-        return df.groupby(axis=axis, level=level).apply(apply_func)
-
-    if n_jobs != 1:
-        dfs = ParallelExt(n_jobs=n_jobs)(
-            delayed(_naive_group_apply)(sub_df) for idx, sub_df in df.resample(resample_rule, axis=axis, level=level)
-        )
-        return pd.concat(dfs, axis=axis).sort_index()
-    else:
-        return _naive_group_apply(df)
-
-
-class AsyncCaller:
-    """
-    This AsyncCaller tries to make it easier to async call
-
-    Currently, it is used in MLflowRecorder to make functions like `log_params` async
-
-    NOTE:
-    - This caller didn't consider the return value
-    """
-
-    STOP_MARK = "__STOP"
-
-    def __init__(self) -> None:
-        self._q = Queue()
-        self._stop = False
-        self._t = Thread(target=self.run)
-        self._t.start()
-
-    def close(self):
-        self._q.put(self.STOP_MARK)
-
-    def run(self):
-        while True:
-            data = self._q.get()
-            if data == self.STOP_MARK:
-                break
-            data()
-
-    def __call__(self, func, *args, **kwargs):
-        self._q.put(partial(func, *args, **kwargs))
-
-    def wait(self, close=True):
-        if close:
-            self.close()
-        self._t.join()
-
-    @staticmethod
-    def async_dec(ac_attr):
-        def decorator_func(func):
-            def wrapper(self, *args, **kwargs):
-                if isinstance(getattr(self, ac_attr, None), Callable):
-                    return getattr(self, ac_attr)(func, self, *args, **kwargs)
-                else:
-                    return func(self, *args, **kwargs)
-
-            return wrapper
-
-        return decorator_func
-
-
-# # Outlines: Joblib enhancement
-# The code are for implementing following workflow
-# - Construct complex data structure nested with delayed joblib tasks
-#      - For example,  {"job": [<delayed_joblib_task>,  {"1": <delayed_joblib_task>}]}
-# - executing all the tasks and replace all the <delayed_joblib_task> with its return value
-
-# This will make it easier to convert some existing code to a parallel one
-
-
-class DelayedTask:
-    def get_delayed_tuple(self):
-        """get_delayed_tuple.
-        Return the delayed_tuple created by joblib.delayed
-        """
-        raise NotImplementedError("NotImplemented")
-
-    def set_res(self, res):
-        """set_res.
-
-        Parameters
-        ----------
-        res :
-            the executed result of the delayed tuple
-        """
-        self.res = res
-
-    def get_replacement(self):
-        """return the object to replace the delayed task"""
-        raise NotImplementedError("NotImplemented")
-
-
-class DelayedTuple(DelayedTask):
-    def __init__(self, delayed_tpl):
-        self.delayed_tpl = delayed_tpl
-        self.res = None
-
-    def get_delayed_tuple(self):
-        return self.delayed_tpl
-
-    def get_replacement(self):
-        return self.res
-
-
-class DelayedDict(DelayedTask):
-    """DelayedDict.
-    It is designed for following feature:
-    Converting following existing code to parallel
-    - constructing a dict
-    - key can be gotten instantly
-    - computation of values tasks a lot of time.
-        - AND ALL the values are calculated in a SINGLE function
-    """
-
-    def __init__(self, key_l, delayed_tpl):
-        self.key_l = key_l
-        self.delayed_tpl = delayed_tpl
-
-    def get_delayed_tuple(self):
-        return self.delayed_tpl
-
-    def get_replacement(self):
-        return dict(zip(self.key_l, self.res))
-
-
-def is_delayed_tuple(obj) -> bool:
-    """is_delayed_tuple.
-
-    Parameters
-    ----------
-    obj : object
-
-    Returns
-    -------
-    bool
-        is `obj` joblib.delayed tuple
-    """
-    return isinstance(obj, tuple) and len(obj) == 3 and callable(obj[0])
-
-
-def _replace_and_get_dt(complex_iter):
-    """_replace_and_get_dt.
-
-    FIXME: this function may cause infinite loop when the complex data-structure contains loop-reference
-
-    Parameters
-    ----------
-    complex_iter :
-        complex_iter
-    """
-    if isinstance(complex_iter, DelayedTask):
-        dt = complex_iter
-        return dt, [dt]
-    elif is_delayed_tuple(complex_iter):
-        dt = DelayedTuple(complex_iter)
-        return dt, [dt]
-    elif isinstance(complex_iter, (list, tuple)):
-        new_ci = []
-        dt_all = []
-        for item in complex_iter:
-            new_item, dt_list = _replace_and_get_dt(item)
-            new_ci.append(new_item)
-            dt_all += dt_list
-        return new_ci, dt_all
-    elif isinstance(complex_iter, dict):
-        new_ci = {}
-        dt_all = []
-        for key, item in complex_iter.items():
-            new_item, dt_list = _replace_and_get_dt(item)
-            new_ci[key] = new_item
-            dt_all += dt_list
-        return new_ci, dt_all
-    else:
-        return complex_iter, []
-
-
-def _recover_dt(complex_iter):
-    """_recover_dt.
-
-    replace all the DelayedTask in the `complex_iter` with its `.res` value
-
-    FIXME: this function may cause infinite loop when the complex data-structure contains loop-reference
-
-    Parameters
-    ----------
-    complex_iter :
-        complex_iter
-    """
-    if isinstance(complex_iter, DelayedTask):
-        return complex_iter.get_replacement()
-    elif isinstance(complex_iter, (list, tuple)):
-        return [_recover_dt(item) for item in complex_iter]
-    elif isinstance(complex_iter, dict):
-        return {key: _recover_dt(item) for key, item in complex_iter.items()}
-    else:
-        return complex_iter
-
-
-def complex_parallel(paral: Parallel, complex_iter):
-    """complex_parallel.
-    Find all the delayed function created by delayed in complex_iter, run them parallelly and then replace it with the result
-
-    >>> from qlib.utils.paral import complex_parallel
-    >>> from joblib import Parallel, delayed
-    >>> complex_iter = {"a": delayed(sum)([1,2,3]), "b": [1, 2, delayed(sum)([10, 1])]}
-    >>> complex_parallel(Parallel(), complex_iter)
-    {'a': 6, 'b': [1, 2, 11]}
-
-    Parameters
-    ----------
-    paral : Parallel
-        paral
-    complex_iter :
-        NOTE: only list, tuple and dict will be explored!!!!
-
-    Returns
-    -------
-    complex_iter whose delayed joblib tasks are replaced with its execution results.
-    """
-
-    complex_iter, dt_all = _replace_and_get_dt(complex_iter)
-    for res, dt in zip(paral(dt.get_delayed_tuple() for dt in dt_all), dt_all):
-        dt.set_res(res)
-    complex_iter = _recover_dt(complex_iter)
-    return complex_iter
-
-
-class call_in_subproc:
-    """
-    When we repeatedly run functions, it is hard to avoid memory leakage.
-    So we run it in the subprocess to ensure it is OK.
-
-    NOTE: Because local object can't be pickled. So we can't implement it via closure.
-          We have to implement it via callable Class
-    """
-
-    def __init__(self, func: Callable, qlib_config: QlibConfig = None):
-        """
-        Parameters
-        ----------
-        func : Callable
-            the function to be wrapped
-
-        qlib_config : QlibConfig
-            Qlib config for initialization in subprocess
-
-        Returns
-        -------
-        Callable
-        """
-        self.func = func
-        self.qlib_config = qlib_config
-
-    def _func_mod(self, *args, **kwargs):
-        """Modify the initial function by adding Qlib initialization"""
-        if self.qlib_config is not None:
-            C.register_from_C(self.qlib_config)
-        return self.func(*args, **kwargs)
-
-    def __call__(self, *args, **kwargs):
-        with concurrent.futures.ProcessPoolExecutor(max_workers=1) as executor:
-            return executor.submit(self._func_mod, *args, **kwargs).result()
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+from functools import partial
+from threading import Thread
+from typing import Callable, Text, Union
+
+from joblib import Parallel, delayed
+from joblib._parallel_backends import MultiprocessingBackend
+import pandas as pd
+
+from queue import Queue
+import concurrent
+
+from qlib.config import C, QlibConfig
+
+
+class ParallelExt(Parallel):
+    def __init__(self, *args, **kwargs):
+        maxtasksperchild = kwargs.pop("maxtasksperchild", None)
+        super(ParallelExt, self).__init__(*args, **kwargs)
+        if isinstance(self._backend, MultiprocessingBackend):
+            self._backend_args["maxtasksperchild"] = maxtasksperchild
+
+
+def datetime_groupby_apply(
+    df, apply_func: Union[Callable, Text], axis=0, level="datetime", resample_rule="M", n_jobs=-1
+):
+    """datetime_groupby_apply
+    This function will apply the `apply_func` on the datetime level index.
+
+    Parameters
+    ----------
+    df :
+        DataFrame for processing
+    apply_func : Union[Callable, Text]
+        apply_func for processing the data
+        if a string is given, then it is treated as naive pandas function
+    axis :
+        which axis is the datetime level located
+    level :
+        which level is the datetime level
+    resample_rule :
+        How to resample the data to calculating parallel
+    n_jobs :
+        n_jobs for joblib
+    Returns:
+        pd.DataFrame
+    """
+
+    def _naive_group_apply(df):
+        if isinstance(apply_func, str):
+            return getattr(df.groupby(axis=axis, level=level), apply_func)()
+        return df.groupby(axis=axis, level=level).apply(apply_func)
+
+    if n_jobs != 1:
+        dfs = ParallelExt(n_jobs=n_jobs)(
+            delayed(_naive_group_apply)(sub_df) for idx, sub_df in df.resample(resample_rule, axis=axis, level=level)
+        )
+        return pd.concat(dfs, axis=axis).sort_index()
+    else:
+        return _naive_group_apply(df)
+
+
+class AsyncCaller:
+    """
+    This AsyncCaller tries to make it easier to async call
+
+    Currently, it is used in MLflowRecorder to make functions like `log_params` async
+
+    NOTE:
+    - This caller didn't consider the return value
+    """
+
+    STOP_MARK = "__STOP"
+
+    def __init__(self) -> None:
+        self._q = Queue()
+        self._stop = False
+        self._t = Thread(target=self.run)
+        self._t.start()
+
+    def close(self):
+        self._q.put(self.STOP_MARK)
+
+    def run(self):
+        while True:
+            data = self._q.get()
+            if data == self.STOP_MARK:
+                break
+            data()
+
+    def __call__(self, func, *args, **kwargs):
+        self._q.put(partial(func, *args, **kwargs))
+
+    def wait(self, close=True):
+        if close:
+            self.close()
+        self._t.join()
+
+    @staticmethod
+    def async_dec(ac_attr):
+        def decorator_func(func):
+            def wrapper(self, *args, **kwargs):
+                if isinstance(getattr(self, ac_attr, None), Callable):
+                    return getattr(self, ac_attr)(func, self, *args, **kwargs)
+                else:
+                    return func(self, *args, **kwargs)
+
+            return wrapper
+
+        return decorator_func
+
+
+# # Outlines: Joblib enhancement
+# The code are for implementing following workflow
+# - Construct complex data structure nested with delayed joblib tasks
+#      - For example,  {"job": [<delayed_joblib_task>,  {"1": <delayed_joblib_task>}]}
+# - executing all the tasks and replace all the <delayed_joblib_task> with its return value
+
+# This will make it easier to convert some existing code to a parallel one
+
+
+class DelayedTask:
+    def get_delayed_tuple(self):
+        """get_delayed_tuple.
+        Return the delayed_tuple created by joblib.delayed
+        """
+        raise NotImplementedError("NotImplemented")
+
+    def set_res(self, res):
+        """set_res.
+
+        Parameters
+        ----------
+        res :
+            the executed result of the delayed tuple
+        """
+        self.res = res
+
+    def get_replacement(self):
+        """return the object to replace the delayed task"""
+        raise NotImplementedError("NotImplemented")
+
+
+class DelayedTuple(DelayedTask):
+    def __init__(self, delayed_tpl):
+        self.delayed_tpl = delayed_tpl
+        self.res = None
+
+    def get_delayed_tuple(self):
+        return self.delayed_tpl
+
+    def get_replacement(self):
+        return self.res
+
+
+class DelayedDict(DelayedTask):
+    """DelayedDict.
+    It is designed for following feature:
+    Converting following existing code to parallel
+    - constructing a dict
+    - key can be gotten instantly
+    - computation of values tasks a lot of time.
+        - AND ALL the values are calculated in a SINGLE function
+    """
+
+    def __init__(self, key_l, delayed_tpl):
+        self.key_l = key_l
+        self.delayed_tpl = delayed_tpl
+
+    def get_delayed_tuple(self):
+        return self.delayed_tpl
+
+    def get_replacement(self):
+        return dict(zip(self.key_l, self.res))
+
+
+def is_delayed_tuple(obj) -> bool:
+    """is_delayed_tuple.
+
+    Parameters
+    ----------
+    obj : object
+
+    Returns
+    -------
+    bool
+        is `obj` joblib.delayed tuple
+    """
+    return isinstance(obj, tuple) and len(obj) == 3 and callable(obj[0])
+
+
+def _replace_and_get_dt(complex_iter):
+    """_replace_and_get_dt.
+
+    FIXME: this function may cause infinite loop when the complex data-structure contains loop-reference
+
+    Parameters
+    ----------
+    complex_iter :
+        complex_iter
+    """
+    if isinstance(complex_iter, DelayedTask):
+        dt = complex_iter
+        return dt, [dt]
+    elif is_delayed_tuple(complex_iter):
+        dt = DelayedTuple(complex_iter)
+        return dt, [dt]
+    elif isinstance(complex_iter, (list, tuple)):
+        new_ci = []
+        dt_all = []
+        for item in complex_iter:
+            new_item, dt_list = _replace_and_get_dt(item)
+            new_ci.append(new_item)
+            dt_all += dt_list
+        return new_ci, dt_all
+    elif isinstance(complex_iter, dict):
+        new_ci = {}
+        dt_all = []
+        for key, item in complex_iter.items():
+            new_item, dt_list = _replace_and_get_dt(item)
+            new_ci[key] = new_item
+            dt_all += dt_list
+        return new_ci, dt_all
+    else:
+        return complex_iter, []
+
+
+def _recover_dt(complex_iter):
+    """_recover_dt.
+
+    replace all the DelayedTask in the `complex_iter` with its `.res` value
+
+    FIXME: this function may cause infinite loop when the complex data-structure contains loop-reference
+
+    Parameters
+    ----------
+    complex_iter :
+        complex_iter
+    """
+    if isinstance(complex_iter, DelayedTask):
+        return complex_iter.get_replacement()
+    elif isinstance(complex_iter, (list, tuple)):
+        return [_recover_dt(item) for item in complex_iter]
+    elif isinstance(complex_iter, dict):
+        return {key: _recover_dt(item) for key, item in complex_iter.items()}
+    else:
+        return complex_iter
+
+
+def complex_parallel(paral: Parallel, complex_iter):
+    """complex_parallel.
+    Find all the delayed function created by delayed in complex_iter, run them parallelly and then replace it with the result
+
+    >>> from qlib.utils.paral import complex_parallel
+    >>> from joblib import Parallel, delayed
+    >>> complex_iter = {"a": delayed(sum)([1,2,3]), "b": [1, 2, delayed(sum)([10, 1])]}
+    >>> complex_parallel(Parallel(), complex_iter)
+    {'a': 6, 'b': [1, 2, 11]}
+
+    Parameters
+    ----------
+    paral : Parallel
+        paral
+    complex_iter :
+        NOTE: only list, tuple and dict will be explored!!!!
+
+    Returns
+    -------
+    complex_iter whose delayed joblib tasks are replaced with its execution results.
+    """
+
+    complex_iter, dt_all = _replace_and_get_dt(complex_iter)
+    for res, dt in zip(paral(dt.get_delayed_tuple() for dt in dt_all), dt_all):
+        dt.set_res(res)
+    complex_iter = _recover_dt(complex_iter)
+    return complex_iter
+
+
+class call_in_subproc:
+    """
+    When we repeatedly run functions, it is hard to avoid memory leakage.
+    So we run it in the subprocess to ensure it is OK.
+
+    NOTE: Because local object can't be pickled. So we can't implement it via closure.
+          We have to implement it via callable Class
+    """
+
+    def __init__(self, func: Callable, qlib_config: QlibConfig = None):
+        """
+        Parameters
+        ----------
+        func : Callable
+            the function to be wrapped
+
+        qlib_config : QlibConfig
+            Qlib config for initialization in subprocess
+
+        Returns
+        -------
+        Callable
+        """
+        self.func = func
+        self.qlib_config = qlib_config
+
+    def _func_mod(self, *args, **kwargs):
+        """Modify the initial function by adding Qlib initialization"""
+        if self.qlib_config is not None:
+            C.register_from_C(self.qlib_config)
+        return self.func(*args, **kwargs)
+
+    def __call__(self, *args, **kwargs):
+        with concurrent.futures.ProcessPoolExecutor(max_workers=1) as executor:
+            return executor.submit(self._func_mod, *args, **kwargs).result()
```

## qlib/utils/resam.py

 * *Ordering differences only*

```diff
@@ -1,239 +1,239 @@
-import numpy as np
-import pandas as pd
-
-from functools import partial
-from typing import Union, Callable
-
-from . import lazy_sort_index
-from .time import Freq, cal_sam_minute
-from ..config import C
-
-
-def resam_calendar(
-    calendar_raw: np.ndarray, freq_raw: Union[str, Freq], freq_sam: Union[str, Freq], region: str = None
-) -> np.ndarray:
-    """
-    Resample the calendar with frequency freq_raw into the calendar with frequency freq_sam
-    Assumption:
-        - Fix length (240) of the calendar in each day.
-
-    Parameters
-    ----------
-    calendar_raw : np.ndarray
-        The calendar with frequency  freq_raw
-    freq_raw : str
-        Frequency of the raw calendar
-    freq_sam : str
-        Sample frequency
-    region: str
-        Region, for example, "cn", "us"
-    Returns
-    -------
-    np.ndarray
-        The calendar with frequency freq_sam
-    """
-    if region is None:
-        region = C["region"]
-
-    freq_raw = Freq(freq_raw)
-    freq_sam = Freq(freq_sam)
-    if not len(calendar_raw):
-        return calendar_raw
-
-    # if freq_sam is xminute, divide each trading day into several bars evenly
-    if freq_sam.base == Freq.NORM_FREQ_MINUTE:
-        if freq_raw.base != Freq.NORM_FREQ_MINUTE:
-            raise ValueError("when sampling minute calendar, freq of raw calendar must be minute or min")
-        else:
-            if freq_raw.count > freq_sam.count:
-                raise ValueError("raw freq must be higher than sampling freq")
-        _calendar_minute = np.unique(list(map(lambda x: cal_sam_minute(x, freq_sam.count, region), calendar_raw)))
-        return _calendar_minute
-
-    # else, convert the raw calendar into day calendar, and divide the whole calendar into several bars evenly
-    else:
-        _calendar_day = np.unique(list(map(lambda x: pd.Timestamp(x.year, x.month, x.day, 0, 0, 0), calendar_raw)))
-        if freq_sam.base == Freq.NORM_FREQ_DAY:
-            return _calendar_day[:: freq_sam.count]
-
-        elif freq_sam.base == Freq.NORM_FREQ_WEEK:
-            _day_in_week = np.array(list(map(lambda x: x.dayofweek, _calendar_day)))
-            _calendar_week = _calendar_day[np.ediff1d(_day_in_week, to_begin=-1) < 0]
-            return _calendar_week[:: freq_sam.count]
-
-        elif freq_sam.base == Freq.NORM_FREQ_MONTH:
-            _day_in_month = np.array(list(map(lambda x: x.day, _calendar_day)))
-            _calendar_month = _calendar_day[np.ediff1d(_day_in_month, to_begin=-1) < 0]
-            return _calendar_month[:: freq_sam.count]
-        else:
-            raise ValueError("sampling freq must be xmin, xd, xw, xm")
-
-
-def get_higher_eq_freq_feature(instruments, fields, start_time=None, end_time=None, freq="day", disk_cache=1):
-    """get the feature with higher or equal frequency than `freq`.
-    Returns
-    -------
-    pd.DataFrame
-        the feature with higher or equal frequency
-    """
-
-    from ..data.data import D  # pylint: disable=C0415
-
-    try:
-        _result = D.features(instruments, fields, start_time, end_time, freq=freq, disk_cache=disk_cache)
-        _freq = freq
-    except (ValueError, KeyError) as value_key_e:
-        _, norm_freq = Freq.parse(freq)
-        if norm_freq in [Freq.NORM_FREQ_MONTH, Freq.NORM_FREQ_WEEK, Freq.NORM_FREQ_DAY]:
-            try:
-                _result = D.features(instruments, fields, start_time, end_time, freq="day", disk_cache=disk_cache)
-                _freq = "day"
-            except (ValueError, KeyError):
-                _result = D.features(instruments, fields, start_time, end_time, freq="1min", disk_cache=disk_cache)
-                _freq = "1min"
-        elif norm_freq == Freq.NORM_FREQ_MINUTE:
-            _result = D.features(instruments, fields, start_time, end_time, freq="1min", disk_cache=disk_cache)
-            _freq = "1min"
-        else:
-            raise ValueError(f"freq {freq} is not supported") from value_key_e
-    return _result, _freq
-
-
-def resam_ts_data(
-    ts_feature: Union[pd.DataFrame, pd.Series],
-    start_time: Union[str, pd.Timestamp] = None,
-    end_time: Union[str, pd.Timestamp] = None,
-    method: Union[str, Callable] = "last",
-    method_kwargs: dict = {},
-):
-    """
-    Resample value from time-series data
-
-        - If `feature` has MultiIndex[instrument, datetime], apply the `method` to each instruemnt data with datetime in [start_time, end_time]
-            Example:
-
-            .. code-block::
-
-                print(feature)
-                                        $close      $volume
-                instrument  datetime
-                SH600000    2010-01-04  86.778313   16162960.0
-                            2010-01-05  87.433578   28117442.0
-                            2010-01-06  85.713585   23632884.0
-                            2010-01-07  83.788803   20813402.0
-                            2010-01-08  84.730675   16044853.0
-
-                SH600655    2010-01-04  2699.567383  158193.328125
-                            2010-01-08  2612.359619   77501.406250
-                            2010-01-11  2712.982422  160852.390625
-                            2010-01-12  2788.688232  164587.937500
-                            2010-01-13  2790.604004  145460.453125
-
-                print(resam_ts_data(feature, start_time="2010-01-04", end_time="2010-01-05", fields=["$close", "$volume"], method="last"))
-                            $close      $volume
-                instrument
-                SH600000    87.433578 28117442.0
-                SH600655    2699.567383  158193.328125
-
-        - Else, the `feature` should have Index[datetime], just apply the `method` to `feature` directly
-            Example:
-
-            .. code-block::
-                print(feature)
-                            $close      $volume
-                datetime
-                2010-01-04  86.778313   16162960.0
-                2010-01-05  87.433578   28117442.0
-                2010-01-06  85.713585   23632884.0
-                2010-01-07  83.788803   20813402.0
-                2010-01-08  84.730675   16044853.0
-
-                print(resam_ts_data(feature, start_time="2010-01-04", end_time="2010-01-05", method="last"))
-
-                $close 87.433578
-                $volume 28117442.0
-
-                print(resam_ts_data(feature['$close'], start_time="2010-01-04", end_time="2010-01-05", method="last"))
-
-                87.433578
-
-    Parameters
-    ----------
-    ts_feature : Union[pd.DataFrame, pd.Series]
-        Raw time-series feature to be resampled
-    start_time : Union[str, pd.Timestamp], optional
-        start sampling time, by default None
-    end_time : Union[str, pd.Timestamp], optional
-        end sampling time, by default None
-    method : Union[str, Callable], optional
-        sample method, apply method function to each stock series data, by default "last"
-        - If type(method) is str or callable function, it should be an attribute of SeriesGroupBy or DataFrameGroupby, and applies groupy.method for the sliced time-series data
-        - If method is None, do nothing for the sliced time-series data.
-    method_kwargs : dict, optional
-        arguments of method, by default {}
-
-    Returns
-    -------
-        The resampled DataFrame/Series/value, return None when the resampled data is empty.
-    """
-
-    selector_datetime = slice(start_time, end_time)
-
-    from ..data.dataset.utils import get_level_index  # pylint: disable=C0415
-
-    feature = lazy_sort_index(ts_feature)
-
-    datetime_level = get_level_index(feature, level="datetime") == 0
-    if datetime_level:
-        feature = feature.loc[selector_datetime]
-    else:
-        feature = feature.loc(axis=0)[(slice(None), selector_datetime)]
-
-    if feature.empty:
-        return None
-    if isinstance(feature.index, pd.MultiIndex):
-        if callable(method):
-            method_func = method
-            return feature.groupby(level="instrument").apply(method_func, **method_kwargs)
-        elif isinstance(method, str):
-            return getattr(feature.groupby(level="instrument"), method)(**method_kwargs)
-    else:
-        if callable(method):
-            method_func = method
-            return method_func(feature, **method_kwargs)
-        elif isinstance(method, str):
-            return getattr(feature, method)(**method_kwargs)
-    return feature
-
-
-def get_valid_value(series, last=True):
-    """get the first/last not nan value of pd.Series with single level index
-    Parameters
-    ----------
-    series : pd.Series
-        series should not be empty
-    last : bool, optional
-        whether to get the last valid value, by default True
-        - if last is True, get the last valid value
-        - else, get the first valid value
-
-    Returns
-    -------
-    Nan | float
-        the first/last valid value
-    """
-    return series.fillna(method="ffill").iloc[-1] if last else series.fillna(method="bfill").iloc[0]
-
-
-def _ts_data_valid(ts_feature, last=False):
-    """get the first/last not nan value of pd.Series|DataFrame with single level index"""
-    if isinstance(ts_feature, pd.DataFrame):
-        return ts_feature.apply(lambda column: get_valid_value(column, last=last))
-    elif isinstance(ts_feature, pd.Series):
-        return get_valid_value(ts_feature, last=last)
-    else:
-        raise TypeError(f"ts_feature should be pd.DataFrame/Series, not {type(ts_feature)}")
-
-
-ts_data_last = partial(_ts_data_valid, last=True)
-ts_data_first = partial(_ts_data_valid, last=False)
+import numpy as np
+import pandas as pd
+
+from functools import partial
+from typing import Union, Callable
+
+from . import lazy_sort_index
+from .time import Freq, cal_sam_minute
+from ..config import C
+
+
+def resam_calendar(
+    calendar_raw: np.ndarray, freq_raw: Union[str, Freq], freq_sam: Union[str, Freq], region: str = None
+) -> np.ndarray:
+    """
+    Resample the calendar with frequency freq_raw into the calendar with frequency freq_sam
+    Assumption:
+        - Fix length (240) of the calendar in each day.
+
+    Parameters
+    ----------
+    calendar_raw : np.ndarray
+        The calendar with frequency  freq_raw
+    freq_raw : str
+        Frequency of the raw calendar
+    freq_sam : str
+        Sample frequency
+    region: str
+        Region, for example, "cn", "us"
+    Returns
+    -------
+    np.ndarray
+        The calendar with frequency freq_sam
+    """
+    if region is None:
+        region = C["region"]
+
+    freq_raw = Freq(freq_raw)
+    freq_sam = Freq(freq_sam)
+    if not len(calendar_raw):
+        return calendar_raw
+
+    # if freq_sam is xminute, divide each trading day into several bars evenly
+    if freq_sam.base == Freq.NORM_FREQ_MINUTE:
+        if freq_raw.base != Freq.NORM_FREQ_MINUTE:
+            raise ValueError("when sampling minute calendar, freq of raw calendar must be minute or min")
+        else:
+            if freq_raw.count > freq_sam.count:
+                raise ValueError("raw freq must be higher than sampling freq")
+        _calendar_minute = np.unique(list(map(lambda x: cal_sam_minute(x, freq_sam.count, region), calendar_raw)))
+        return _calendar_minute
+
+    # else, convert the raw calendar into day calendar, and divide the whole calendar into several bars evenly
+    else:
+        _calendar_day = np.unique(list(map(lambda x: pd.Timestamp(x.year, x.month, x.day, 0, 0, 0), calendar_raw)))
+        if freq_sam.base == Freq.NORM_FREQ_DAY:
+            return _calendar_day[:: freq_sam.count]
+
+        elif freq_sam.base == Freq.NORM_FREQ_WEEK:
+            _day_in_week = np.array(list(map(lambda x: x.dayofweek, _calendar_day)))
+            _calendar_week = _calendar_day[np.ediff1d(_day_in_week, to_begin=-1) < 0]
+            return _calendar_week[:: freq_sam.count]
+
+        elif freq_sam.base == Freq.NORM_FREQ_MONTH:
+            _day_in_month = np.array(list(map(lambda x: x.day, _calendar_day)))
+            _calendar_month = _calendar_day[np.ediff1d(_day_in_month, to_begin=-1) < 0]
+            return _calendar_month[:: freq_sam.count]
+        else:
+            raise ValueError("sampling freq must be xmin, xd, xw, xm")
+
+
+def get_higher_eq_freq_feature(instruments, fields, start_time=None, end_time=None, freq="day", disk_cache=1):
+    """get the feature with higher or equal frequency than `freq`.
+    Returns
+    -------
+    pd.DataFrame
+        the feature with higher or equal frequency
+    """
+
+    from ..data.data import D  # pylint: disable=C0415
+
+    try:
+        _result = D.features(instruments, fields, start_time, end_time, freq=freq, disk_cache=disk_cache)
+        _freq = freq
+    except (ValueError, KeyError) as value_key_e:
+        _, norm_freq = Freq.parse(freq)
+        if norm_freq in [Freq.NORM_FREQ_MONTH, Freq.NORM_FREQ_WEEK, Freq.NORM_FREQ_DAY]:
+            try:
+                _result = D.features(instruments, fields, start_time, end_time, freq="day", disk_cache=disk_cache)
+                _freq = "day"
+            except (ValueError, KeyError):
+                _result = D.features(instruments, fields, start_time, end_time, freq="1min", disk_cache=disk_cache)
+                _freq = "1min"
+        elif norm_freq == Freq.NORM_FREQ_MINUTE:
+            _result = D.features(instruments, fields, start_time, end_time, freq="1min", disk_cache=disk_cache)
+            _freq = "1min"
+        else:
+            raise ValueError(f"freq {freq} is not supported") from value_key_e
+    return _result, _freq
+
+
+def resam_ts_data(
+    ts_feature: Union[pd.DataFrame, pd.Series],
+    start_time: Union[str, pd.Timestamp] = None,
+    end_time: Union[str, pd.Timestamp] = None,
+    method: Union[str, Callable] = "last",
+    method_kwargs: dict = {},
+):
+    """
+    Resample value from time-series data
+
+        - If `feature` has MultiIndex[instrument, datetime], apply the `method` to each instruemnt data with datetime in [start_time, end_time]
+            Example:
+
+            .. code-block::
+
+                print(feature)
+                                        $close      $volume
+                instrument  datetime
+                SH600000    2010-01-04  86.778313   16162960.0
+                            2010-01-05  87.433578   28117442.0
+                            2010-01-06  85.713585   23632884.0
+                            2010-01-07  83.788803   20813402.0
+                            2010-01-08  84.730675   16044853.0
+
+                SH600655    2010-01-04  2699.567383  158193.328125
+                            2010-01-08  2612.359619   77501.406250
+                            2010-01-11  2712.982422  160852.390625
+                            2010-01-12  2788.688232  164587.937500
+                            2010-01-13  2790.604004  145460.453125
+
+                print(resam_ts_data(feature, start_time="2010-01-04", end_time="2010-01-05", fields=["$close", "$volume"], method="last"))
+                            $close      $volume
+                instrument
+                SH600000    87.433578 28117442.0
+                SH600655    2699.567383  158193.328125
+
+        - Else, the `feature` should have Index[datetime], just apply the `method` to `feature` directly
+            Example:
+
+            .. code-block::
+                print(feature)
+                            $close      $volume
+                datetime
+                2010-01-04  86.778313   16162960.0
+                2010-01-05  87.433578   28117442.0
+                2010-01-06  85.713585   23632884.0
+                2010-01-07  83.788803   20813402.0
+                2010-01-08  84.730675   16044853.0
+
+                print(resam_ts_data(feature, start_time="2010-01-04", end_time="2010-01-05", method="last"))
+
+                $close 87.433578
+                $volume 28117442.0
+
+                print(resam_ts_data(feature['$close'], start_time="2010-01-04", end_time="2010-01-05", method="last"))
+
+                87.433578
+
+    Parameters
+    ----------
+    ts_feature : Union[pd.DataFrame, pd.Series]
+        Raw time-series feature to be resampled
+    start_time : Union[str, pd.Timestamp], optional
+        start sampling time, by default None
+    end_time : Union[str, pd.Timestamp], optional
+        end sampling time, by default None
+    method : Union[str, Callable], optional
+        sample method, apply method function to each stock series data, by default "last"
+        - If type(method) is str or callable function, it should be an attribute of SeriesGroupBy or DataFrameGroupby, and applies groupy.method for the sliced time-series data
+        - If method is None, do nothing for the sliced time-series data.
+    method_kwargs : dict, optional
+        arguments of method, by default {}
+
+    Returns
+    -------
+        The resampled DataFrame/Series/value, return None when the resampled data is empty.
+    """
+
+    selector_datetime = slice(start_time, end_time)
+
+    from ..data.dataset.utils import get_level_index  # pylint: disable=C0415
+
+    feature = lazy_sort_index(ts_feature)
+
+    datetime_level = get_level_index(feature, level="datetime") == 0
+    if datetime_level:
+        feature = feature.loc[selector_datetime]
+    else:
+        feature = feature.loc(axis=0)[(slice(None), selector_datetime)]
+
+    if feature.empty:
+        return None
+    if isinstance(feature.index, pd.MultiIndex):
+        if callable(method):
+            method_func = method
+            return feature.groupby(level="instrument").apply(method_func, **method_kwargs)
+        elif isinstance(method, str):
+            return getattr(feature.groupby(level="instrument"), method)(**method_kwargs)
+    else:
+        if callable(method):
+            method_func = method
+            return method_func(feature, **method_kwargs)
+        elif isinstance(method, str):
+            return getattr(feature, method)(**method_kwargs)
+    return feature
+
+
+def get_valid_value(series, last=True):
+    """get the first/last not nan value of pd.Series with single level index
+    Parameters
+    ----------
+    series : pd.Series
+        series should not be empty
+    last : bool, optional
+        whether to get the last valid value, by default True
+        - if last is True, get the last valid value
+        - else, get the first valid value
+
+    Returns
+    -------
+    Nan | float
+        the first/last valid value
+    """
+    return series.fillna(method="ffill").iloc[-1] if last else series.fillna(method="bfill").iloc[0]
+
+
+def _ts_data_valid(ts_feature, last=False):
+    """get the first/last not nan value of pd.Series|DataFrame with single level index"""
+    if isinstance(ts_feature, pd.DataFrame):
+        return ts_feature.apply(lambda column: get_valid_value(column, last=last))
+    elif isinstance(ts_feature, pd.Series):
+        return get_valid_value(ts_feature, last=last)
+    else:
+        raise TypeError(f"ts_feature should be pd.DataFrame/Series, not {type(ts_feature)}")
+
+
+ts_data_last = partial(_ts_data_valid, last=True)
+ts_data_first = partial(_ts_data_valid, last=False)
```

## qlib/utils/serial.py

 * *Ordering differences only*

```diff
@@ -1,189 +1,189 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-import pickle
-import dill
-from pathlib import Path
-from typing import Union
-from ..config import C
-
-
-class Serializable:
-    """
-    Serializable will change the behaviors of pickle.
-
-        The rule to tell if a attribute will be kept or dropped when dumping.
-        The rule with higher priorities is on the top
-        - in the config attribute list -> always dropped
-        - in the include attribute list -> always kept
-        - in the exclude attribute list -> always dropped
-        - name not starts with `_` -> kept
-        - name starts with `_` -> kept if `dump_all` is true else dropped
-
-    It provides a syntactic sugar for distinguish the attributes which user doesn't want.
-    - For examples, a learnable Datahandler just wants to save the parameters without data when dumping to disk
-    """
-
-    pickle_backend = "pickle"  # another optional value is "dill" which can pickle more things of python.
-    default_dump_all = False  # if dump all things
-    config_attr = ["_include", "_exclude"]
-    exclude_attr = []  # exclude_attr have lower priorities than `self._exclude`
-    include_attr = []  # include_attr have lower priorities then `self._include`
-    FLAG_KEY = "_qlib_serial_flag"
-
-    def __init__(self):
-        self._dump_all = self.default_dump_all
-        self._exclude = None  # this attribute have higher priorities than `exclude_attr`
-
-    def _is_kept(self, key):
-        if key in self.config_attr:
-            return False
-        if key in self._get_attr_list("include"):
-            return True
-        if key in self._get_attr_list("exclude"):
-            return False
-        return self.dump_all or not key.startswith("_")
-
-    def __getstate__(self) -> dict:
-        return {k: v for k, v in self.__dict__.items() if self._is_kept(k)}
-
-    def __setstate__(self, state: dict):
-        self.__dict__.update(state)
-
-    @property
-    def dump_all(self):
-        """
-        will the object dump all object
-        """
-        return getattr(self, "_dump_all", False)
-
-    def _get_attr_list(self, attr_type: str) -> list:
-        """
-        What attribute will not be in specific list
-
-        Parameters
-        ----------
-        attr_type : str
-            "include" or "exclude"
-
-        Returns
-        -------
-        list:
-        """
-        if hasattr(self, f"_{attr_type}"):
-            res = getattr(self, f"_{attr_type}", [])
-        else:
-            res = getattr(self.__class__, f"{attr_type}_attr", [])
-        if res is None:
-            return []
-        return res
-
-    def config(self, recursive=False, **kwargs):
-        """
-        configure the serializable object
-
-        Parameters
-        ----------
-        kwargs may include following keys
-
-            dump_all : bool
-                will the object dump all object
-            exclude : list
-                What attribute will not be dumped
-            include : list
-                What attribute will be dumped
-
-        recursive : bool
-            will the configuration be recursive
-        """
-        keys = {"dump_all", "exclude", "include"}
-        for k, v in kwargs.items():
-            if k in keys:
-                attr_name = f"_{k}"
-                setattr(self, attr_name, v)
-            else:
-                raise KeyError(f"Unknown parameter: {k}")
-
-        if recursive:
-            for obj in self.__dict__.values():
-                # set flag to prevent endless loop
-                self.__dict__[self.FLAG_KEY] = True
-                if isinstance(obj, Serializable) and self.FLAG_KEY not in obj.__dict__:
-                    obj.config(recursive=True, **kwargs)
-                del self.__dict__[self.FLAG_KEY]
-
-    def to_pickle(self, path: Union[Path, str], **kwargs):
-        """
-        Dump self to a pickle file.
-
-        path (Union[Path, str]): the path to dump
-
-        kwargs may include following keys
-
-            dump_all : bool
-                will the object dump all object
-            exclude : list
-                What attribute will not be dumped
-            include : list
-                What attribute will be dumped
-        """
-        self.config(**kwargs)
-        with Path(path).open("wb") as f:
-            # pickle interface like backend; such as dill
-            self.get_backend().dump(self, f, protocol=C.dump_protocol_version)
-
-    @classmethod
-    def load(cls, filepath):
-        """
-        Load the serializable class from a filepath.
-
-        Args:
-            filepath (str): the path of file
-
-        Raises:
-            TypeError: the pickled file must be `type(cls)`
-
-        Returns:
-            `type(cls)`: the instance of `type(cls)`
-        """
-        with open(filepath, "rb") as f:
-            object = cls.get_backend().load(f)
-        if isinstance(object, cls):
-            return object
-        else:
-            raise TypeError(f"The instance of {type(object)} is not a valid `{type(cls)}`!")
-
-    @classmethod
-    def get_backend(cls):
-        """
-        Return the real backend of a Serializable class. The pickle_backend value can be "pickle" or "dill".
-
-        Returns:
-            module: pickle or dill module based on pickle_backend
-        """
-        # NOTE: pickle interface like backend; such as dill
-        if cls.pickle_backend == "pickle":
-            return pickle
-        elif cls.pickle_backend == "dill":
-            return dill
-        else:
-            raise ValueError("Unknown pickle backend, please use 'pickle' or 'dill'.")
-
-    @staticmethod
-    def general_dump(obj, path: Union[Path, str]):
-        """
-        A general dumping method for object
-
-        Parameters
-        ----------
-        obj : object
-            the object to be dumped
-        path : Union[Path, str]
-            the target path the data will be dumped
-        """
-        path = Path(path)
-        if isinstance(obj, Serializable):
-            obj.to_pickle(path)
-        else:
-            with path.open("wb") as f:
-                pickle.dump(obj, f, protocol=C.dump_protocol_version)
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+import pickle
+import dill
+from pathlib import Path
+from typing import Union
+from ..config import C
+
+
+class Serializable:
+    """
+    Serializable will change the behaviors of pickle.
+
+        The rule to tell if a attribute will be kept or dropped when dumping.
+        The rule with higher priorities is on the top
+        - in the config attribute list -> always dropped
+        - in the include attribute list -> always kept
+        - in the exclude attribute list -> always dropped
+        - name not starts with `_` -> kept
+        - name starts with `_` -> kept if `dump_all` is true else dropped
+
+    It provides a syntactic sugar for distinguish the attributes which user doesn't want.
+    - For examples, a learnable Datahandler just wants to save the parameters without data when dumping to disk
+    """
+
+    pickle_backend = "pickle"  # another optional value is "dill" which can pickle more things of python.
+    default_dump_all = False  # if dump all things
+    config_attr = ["_include", "_exclude"]
+    exclude_attr = []  # exclude_attr have lower priorities than `self._exclude`
+    include_attr = []  # include_attr have lower priorities then `self._include`
+    FLAG_KEY = "_qlib_serial_flag"
+
+    def __init__(self):
+        self._dump_all = self.default_dump_all
+        self._exclude = None  # this attribute have higher priorities than `exclude_attr`
+
+    def _is_kept(self, key):
+        if key in self.config_attr:
+            return False
+        if key in self._get_attr_list("include"):
+            return True
+        if key in self._get_attr_list("exclude"):
+            return False
+        return self.dump_all or not key.startswith("_")
+
+    def __getstate__(self) -> dict:
+        return {k: v for k, v in self.__dict__.items() if self._is_kept(k)}
+
+    def __setstate__(self, state: dict):
+        self.__dict__.update(state)
+
+    @property
+    def dump_all(self):
+        """
+        will the object dump all object
+        """
+        return getattr(self, "_dump_all", False)
+
+    def _get_attr_list(self, attr_type: str) -> list:
+        """
+        What attribute will not be in specific list
+
+        Parameters
+        ----------
+        attr_type : str
+            "include" or "exclude"
+
+        Returns
+        -------
+        list:
+        """
+        if hasattr(self, f"_{attr_type}"):
+            res = getattr(self, f"_{attr_type}", [])
+        else:
+            res = getattr(self.__class__, f"{attr_type}_attr", [])
+        if res is None:
+            return []
+        return res
+
+    def config(self, recursive=False, **kwargs):
+        """
+        configure the serializable object
+
+        Parameters
+        ----------
+        kwargs may include following keys
+
+            dump_all : bool
+                will the object dump all object
+            exclude : list
+                What attribute will not be dumped
+            include : list
+                What attribute will be dumped
+
+        recursive : bool
+            will the configuration be recursive
+        """
+        keys = {"dump_all", "exclude", "include"}
+        for k, v in kwargs.items():
+            if k in keys:
+                attr_name = f"_{k}"
+                setattr(self, attr_name, v)
+            else:
+                raise KeyError(f"Unknown parameter: {k}")
+
+        if recursive:
+            for obj in self.__dict__.values():
+                # set flag to prevent endless loop
+                self.__dict__[self.FLAG_KEY] = True
+                if isinstance(obj, Serializable) and self.FLAG_KEY not in obj.__dict__:
+                    obj.config(recursive=True, **kwargs)
+                del self.__dict__[self.FLAG_KEY]
+
+    def to_pickle(self, path: Union[Path, str], **kwargs):
+        """
+        Dump self to a pickle file.
+
+        path (Union[Path, str]): the path to dump
+
+        kwargs may include following keys
+
+            dump_all : bool
+                will the object dump all object
+            exclude : list
+                What attribute will not be dumped
+            include : list
+                What attribute will be dumped
+        """
+        self.config(**kwargs)
+        with Path(path).open("wb") as f:
+            # pickle interface like backend; such as dill
+            self.get_backend().dump(self, f, protocol=C.dump_protocol_version)
+
+    @classmethod
+    def load(cls, filepath):
+        """
+        Load the serializable class from a filepath.
+
+        Args:
+            filepath (str): the path of file
+
+        Raises:
+            TypeError: the pickled file must be `type(cls)`
+
+        Returns:
+            `type(cls)`: the instance of `type(cls)`
+        """
+        with open(filepath, "rb") as f:
+            object = cls.get_backend().load(f)
+        if isinstance(object, cls):
+            return object
+        else:
+            raise TypeError(f"The instance of {type(object)} is not a valid `{type(cls)}`!")
+
+    @classmethod
+    def get_backend(cls):
+        """
+        Return the real backend of a Serializable class. The pickle_backend value can be "pickle" or "dill".
+
+        Returns:
+            module: pickle or dill module based on pickle_backend
+        """
+        # NOTE: pickle interface like backend; such as dill
+        if cls.pickle_backend == "pickle":
+            return pickle
+        elif cls.pickle_backend == "dill":
+            return dill
+        else:
+            raise ValueError("Unknown pickle backend, please use 'pickle' or 'dill'.")
+
+    @staticmethod
+    def general_dump(obj, path: Union[Path, str]):
+        """
+        A general dumping method for object
+
+        Parameters
+        ----------
+        obj : object
+            the object to be dumped
+        path : Union[Path, str]
+            the target path the data will be dumped
+        """
+        path = Path(path)
+        if isinstance(obj, Serializable):
+            obj.to_pickle(path)
+        else:
+            with path.open("wb") as f:
+                pickle.dump(obj, f, protocol=C.dump_protocol_version)
```

## qlib/utils/time.py

 * *Ordering differences only*

```diff
@@ -1,377 +1,377 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-"""
-Time related utils are compiled in this script
-"""
-import bisect
-from datetime import datetime, time, date, timedelta
-from typing import List, Optional, Tuple, Union
-import functools
-import re
-
-import pandas as pd
-
-from qlib.config import C
-from qlib.constant import REG_CN, REG_TW, REG_US
-
-
-CN_TIME = [
-    datetime.strptime("9:30", "%H:%M"),
-    datetime.strptime("11:30", "%H:%M"),
-    datetime.strptime("13:00", "%H:%M"),
-    datetime.strptime("15:00", "%H:%M"),
-]
-US_TIME = [datetime.strptime("9:30", "%H:%M"), datetime.strptime("16:00", "%H:%M")]
-TW_TIME = [
-    datetime.strptime("9:00", "%H:%M"),
-    datetime.strptime("13:30", "%H:%M"),
-]
-
-
-@functools.lru_cache(maxsize=240)
-def get_min_cal(shift: int = 0, region: str = REG_CN) -> List[time]:
-    """
-    get the minute level calendar in day period
-
-    Parameters
-    ----------
-    shift : int
-        the shift direction would be like pandas shift.
-        series.shift(1) will replace the value at `i`-th with the one at `i-1`-th
-    region: str
-        Region, for example, "cn", "us"
-
-    Returns
-    -------
-    List[time]:
-
-    """
-    cal = []
-
-    if region == REG_CN:
-        for ts in list(
-            pd.date_range(CN_TIME[0], CN_TIME[1] - timedelta(minutes=1), freq="1min") - pd.Timedelta(minutes=shift)
-        ) + list(
-            pd.date_range(CN_TIME[2], CN_TIME[3] - timedelta(minutes=1), freq="1min") - pd.Timedelta(minutes=shift)
-        ):
-            cal.append(ts.time())
-    elif region == REG_TW:
-        for ts in list(
-            pd.date_range(TW_TIME[0], TW_TIME[1] - timedelta(minutes=1), freq="1min") - pd.Timedelta(minutes=shift)
-        ):
-            cal.append(ts.time())
-    elif region == REG_US:
-        for ts in list(
-            pd.date_range(US_TIME[0], US_TIME[1] - timedelta(minutes=1), freq="1min") - pd.Timedelta(minutes=shift)
-        ):
-            cal.append(ts.time())
-    else:
-        raise ValueError(f"{region} is not supported")
-    return cal
-
-
-def is_single_value(start_time, end_time, freq, region: str = REG_CN):
-    """Is there only one piece of data for stock market.
-
-    Parameters
-    ----------
-    start_time : Union[pd.Timestamp, str]
-        closed start time for data.
-    end_time : Union[pd.Timestamp, str]
-        closed end time for data.
-    freq :
-    region: str
-        Region, for example, "cn", "us"
-    Returns
-    -------
-    bool
-        True means one piece of data to obtain.
-    """
-    if region == REG_CN:
-        if end_time - start_time < freq:
-            return True
-        if start_time.hour == 11 and start_time.minute == 29 and start_time.second == 0:
-            return True
-        if start_time.hour == 14 and start_time.minute == 59 and start_time.second == 0:
-            return True
-        return False
-    elif region == REG_TW:
-        if end_time - start_time < freq:
-            return True
-        if start_time.hour == 13 and start_time.minute >= 25 and start_time.second == 0:
-            return True
-        return False
-    elif region == REG_US:
-        if end_time - start_time < freq:
-            return True
-        if start_time.hour == 15 and start_time.minute == 59 and start_time.second == 0:
-            return True
-        return False
-    else:
-        raise NotImplementedError(f"please implement the is_single_value func for {region}")
-
-
-class Freq:
-    NORM_FREQ_MONTH = "month"
-    NORM_FREQ_WEEK = "week"
-    NORM_FREQ_DAY = "day"
-    NORM_FREQ_MINUTE = "min"  # using min instead of minute for align with Qlib's data filename
-    SUPPORT_CAL_LIST = [NORM_FREQ_MINUTE, NORM_FREQ_DAY]  # FIXME: this list should from data
-
-    def __init__(self, freq: Union[str, "Freq"]) -> None:
-        if isinstance(freq, str):
-            self.count, self.base = self.parse(freq)
-        elif isinstance(freq, Freq):
-            self.count, self.base = freq.count, freq.base
-        else:
-            raise NotImplementedError(f"This type of input is not supported")
-
-    def __eq__(self, freq):
-        freq = Freq(freq)
-        return freq.count == self.count and freq.base == self.base
-
-    def __str__(self):
-        # trying to align to the filename of Qlib: day, 30min, 5min, 1min...
-        return f"{self.count if self.count != 1 or self.base != 'day' else ''}{self.base}"
-
-    def __repr__(self) -> str:
-        return f"{self.__class__.__name__}({str(self)})"
-
-    @staticmethod
-    def parse(freq: str) -> Tuple[int, str]:
-        """
-        Parse freq into a unified format
-
-        Parameters
-        ----------
-        freq : str
-            Raw freq, supported freq should match the re '^([0-9]*)(month|mon|week|w|day|d|minute|min)$'
-
-        Returns
-        -------
-        freq: Tuple[int, str]
-            Unified freq, including freq count and unified freq unit. The freq unit should be '[month|week|day|minute]'.
-                Example:
-
-                .. code-block::
-
-                    print(Freq.parse("day"))
-                    (1, "day" )
-                    print(Freq.parse("2mon"))
-                    (2, "month")
-                    print(Freq.parse("10w"))
-                    (10, "week")
-
-        """
-        freq = freq.lower()
-        match_obj = re.match("^([0-9]*)(month|mon|week|w|day|d|minute|min)$", freq)
-        if match_obj is None:
-            raise ValueError(
-                "freq format is not supported, the freq should be like (n)month/mon, (n)week/w, (n)day/d, (n)minute/min"
-            )
-        _count = int(match_obj.group(1)) if match_obj.group(1) else 1
-        _freq = match_obj.group(2)
-        _freq_format_dict = {
-            "month": Freq.NORM_FREQ_MONTH,
-            "mon": Freq.NORM_FREQ_MONTH,
-            "week": Freq.NORM_FREQ_WEEK,
-            "w": Freq.NORM_FREQ_WEEK,
-            "day": Freq.NORM_FREQ_DAY,
-            "d": Freq.NORM_FREQ_DAY,
-            "minute": Freq.NORM_FREQ_MINUTE,
-            "min": Freq.NORM_FREQ_MINUTE,
-        }
-        return _count, _freq_format_dict[_freq]
-
-    @staticmethod
-    def get_timedelta(n: int, freq: str) -> pd.Timedelta:
-        """
-        get pd.Timedeta object
-
-        Parameters
-        ----------
-        n : int
-        freq : str
-            Typically, they are the return value of Freq.parse
-
-        Returns
-        -------
-        pd.Timedelta:
-        """
-        return pd.Timedelta(f"{n}{freq}")
-
-    @staticmethod
-    def get_min_delta(left_frq: str, right_freq: str):
-        """Calculate freq delta
-
-        Parameters
-        ----------
-        left_frq: str
-        right_freq: str
-
-        Returns
-        -------
-
-        """
-        minutes_map = {
-            Freq.NORM_FREQ_MINUTE: 1,
-            Freq.NORM_FREQ_DAY: 60 * 24,
-            Freq.NORM_FREQ_WEEK: 7 * 60 * 24,
-            Freq.NORM_FREQ_MONTH: 30 * 7 * 60 * 24,
-        }
-        left_freq = Freq(left_frq)
-        left_minutes = left_freq.count * minutes_map[left_freq.base]
-        right_freq = Freq(right_freq)
-        right_minutes = right_freq.count * minutes_map[right_freq.base]
-        return left_minutes - right_minutes
-
-    @staticmethod
-    def get_recent_freq(base_freq: Union[str, "Freq"], freq_list: List[Union[str, "Freq"]]) -> Optional["Freq"]:
-        """Get the closest freq to base_freq from freq_list
-
-        Parameters
-        ----------
-        base_freq
-        freq_list
-
-        Returns
-        -------
-        if the recent frequency is found
-            Freq
-        else:
-            None
-        """
-        base_freq = Freq(base_freq)
-        # use the nearest freq greater than 0
-        min_freq = None
-        for _freq in freq_list:
-            _min_delta = Freq.get_min_delta(base_freq, _freq)
-            if _min_delta < 0:
-                continue
-            if min_freq is None:
-                min_freq = (_min_delta, str(_freq))
-                continue
-            min_freq = min_freq if min_freq[0] <= _min_delta else (_min_delta, _freq)
-        return min_freq[1] if min_freq else None
-
-
-def time_to_day_index(time_obj: Union[str, datetime], region: str = REG_CN):
-    if isinstance(time_obj, str):
-        time_obj = datetime.strptime(time_obj, "%H:%M")
-
-    if region == REG_CN:
-        if CN_TIME[0] <= time_obj < CN_TIME[1]:
-            return int((time_obj - CN_TIME[0]).total_seconds() / 60)
-        elif CN_TIME[2] <= time_obj < CN_TIME[3]:
-            return int((time_obj - CN_TIME[2]).total_seconds() / 60) + 120
-        else:
-            raise ValueError(f"{time_obj} is not the opening time of the {region} stock market")
-    elif region == REG_US:
-        if US_TIME[0] <= time_obj < US_TIME[1]:
-            return int((time_obj - US_TIME[0]).total_seconds() / 60)
-        else:
-            raise ValueError(f"{time_obj} is not the opening time of the {region} stock market")
-    elif region == REG_TW:
-        if TW_TIME[0] <= time_obj < TW_TIME[1]:
-            return int((time_obj - TW_TIME[0]).total_seconds() / 60)
-        else:
-            raise ValueError(f"{time_obj} is not the opening time of the {region} stock market")
-    else:
-        raise ValueError(f"{region} is not supported")
-
-
-def get_day_min_idx_range(start: str, end: str, freq: str, region: str) -> Tuple[int, int]:
-    """
-    get the min-bar index in a day for a time range (both left and right is closed) given a fixed frequency
-    Parameters
-    ----------
-    start : str
-        e.g. "9:30"
-    end : str
-        e.g. "14:30"
-    freq : str
-        "1min"
-
-    Returns
-    -------
-    Tuple[int, int]:
-        The index of start and end in the calendar. Both left and right are **closed**
-    """
-    start = pd.Timestamp(start).time()
-    end = pd.Timestamp(end).time()
-    freq = Freq(freq)
-    in_day_cal = get_min_cal(region=region)[:: freq.count]
-    left_idx = bisect.bisect_left(in_day_cal, start)
-    right_idx = bisect.bisect_right(in_day_cal, end) - 1
-    return left_idx, right_idx
-
-
-def concat_date_time(date_obj: date, time_obj: time) -> pd.Timestamp:
-    return pd.Timestamp(
-        datetime(
-            date_obj.year,
-            month=date_obj.month,
-            day=date_obj.day,
-            hour=time_obj.hour,
-            minute=time_obj.minute,
-            second=time_obj.second,
-            microsecond=time_obj.microsecond,
-        )
-    )
-
-
-def cal_sam_minute(x: pd.Timestamp, sam_minutes: int, region: str = REG_CN) -> pd.Timestamp:
-    """
-    align the minute-level data to a down sampled calendar
-
-    e.g. align 10:38 to 10:35 in 5 minute-level(10:30 in 10 minute-level)
-
-    Parameters
-    ----------
-    x : pd.Timestamp
-        datetime to be aligned
-    sam_minutes : int
-        align to `sam_minutes` minute-level calendar
-    region: str
-        Region, for example, "cn", "us"
-
-    Returns
-    -------
-    pd.Timestamp:
-        the datetime after aligned
-    """
-    cal = get_min_cal(C.min_data_shift, region)[::sam_minutes]
-    idx = bisect.bisect_right(cal, x.time()) - 1
-    _date, new_time = x.date(), cal[idx]
-    return concat_date_time(_date, new_time)
-
-
-def epsilon_change(date_time: pd.Timestamp, direction: str = "backward") -> pd.Timestamp:
-    """
-    change the time by infinitely small quantity.
-
-
-    Parameters
-    ----------
-    date_time : pd.Timestamp
-        the original time
-    direction : str
-        the direction the time are going to
-        - "backward" for going to history
-        - "forward" for going to the future
-
-    Returns
-    -------
-    pd.Timestamp:
-        the shifted time
-    """
-    if direction == "backward":
-        return date_time - pd.Timedelta(seconds=1)
-    elif direction == "forward":
-        return date_time + pd.Timedelta(seconds=1)
-    else:
-        raise ValueError("Wrong input")
-
-
-if __name__ == "__main__":
-    print(get_day_min_idx_range("8:30", "14:59", "10min", REG_CN))
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+"""
+Time related utils are compiled in this script
+"""
+import bisect
+from datetime import datetime, time, date, timedelta
+from typing import List, Optional, Tuple, Union
+import functools
+import re
+
+import pandas as pd
+
+from qlib.config import C
+from qlib.constant import REG_CN, REG_TW, REG_US
+
+
+CN_TIME = [
+    datetime.strptime("9:30", "%H:%M"),
+    datetime.strptime("11:30", "%H:%M"),
+    datetime.strptime("13:00", "%H:%M"),
+    datetime.strptime("15:00", "%H:%M"),
+]
+US_TIME = [datetime.strptime("9:30", "%H:%M"), datetime.strptime("16:00", "%H:%M")]
+TW_TIME = [
+    datetime.strptime("9:00", "%H:%M"),
+    datetime.strptime("13:30", "%H:%M"),
+]
+
+
+@functools.lru_cache(maxsize=240)
+def get_min_cal(shift: int = 0, region: str = REG_CN) -> List[time]:
+    """
+    get the minute level calendar in day period
+
+    Parameters
+    ----------
+    shift : int
+        the shift direction would be like pandas shift.
+        series.shift(1) will replace the value at `i`-th with the one at `i-1`-th
+    region: str
+        Region, for example, "cn", "us"
+
+    Returns
+    -------
+    List[time]:
+
+    """
+    cal = []
+
+    if region == REG_CN:
+        for ts in list(
+            pd.date_range(CN_TIME[0], CN_TIME[1] - timedelta(minutes=1), freq="1min") - pd.Timedelta(minutes=shift)
+        ) + list(
+            pd.date_range(CN_TIME[2], CN_TIME[3] - timedelta(minutes=1), freq="1min") - pd.Timedelta(minutes=shift)
+        ):
+            cal.append(ts.time())
+    elif region == REG_TW:
+        for ts in list(
+            pd.date_range(TW_TIME[0], TW_TIME[1] - timedelta(minutes=1), freq="1min") - pd.Timedelta(minutes=shift)
+        ):
+            cal.append(ts.time())
+    elif region == REG_US:
+        for ts in list(
+            pd.date_range(US_TIME[0], US_TIME[1] - timedelta(minutes=1), freq="1min") - pd.Timedelta(minutes=shift)
+        ):
+            cal.append(ts.time())
+    else:
+        raise ValueError(f"{region} is not supported")
+    return cal
+
+
+def is_single_value(start_time, end_time, freq, region: str = REG_CN):
+    """Is there only one piece of data for stock market.
+
+    Parameters
+    ----------
+    start_time : Union[pd.Timestamp, str]
+        closed start time for data.
+    end_time : Union[pd.Timestamp, str]
+        closed end time for data.
+    freq :
+    region: str
+        Region, for example, "cn", "us"
+    Returns
+    -------
+    bool
+        True means one piece of data to obtain.
+    """
+    if region == REG_CN:
+        if end_time - start_time < freq:
+            return True
+        if start_time.hour == 11 and start_time.minute == 29 and start_time.second == 0:
+            return True
+        if start_time.hour == 14 and start_time.minute == 59 and start_time.second == 0:
+            return True
+        return False
+    elif region == REG_TW:
+        if end_time - start_time < freq:
+            return True
+        if start_time.hour == 13 and start_time.minute >= 25 and start_time.second == 0:
+            return True
+        return False
+    elif region == REG_US:
+        if end_time - start_time < freq:
+            return True
+        if start_time.hour == 15 and start_time.minute == 59 and start_time.second == 0:
+            return True
+        return False
+    else:
+        raise NotImplementedError(f"please implement the is_single_value func for {region}")
+
+
+class Freq:
+    NORM_FREQ_MONTH = "month"
+    NORM_FREQ_WEEK = "week"
+    NORM_FREQ_DAY = "day"
+    NORM_FREQ_MINUTE = "min"  # using min instead of minute for align with Qlib's data filename
+    SUPPORT_CAL_LIST = [NORM_FREQ_MINUTE, NORM_FREQ_DAY]  # FIXME: this list should from data
+
+    def __init__(self, freq: Union[str, "Freq"]) -> None:
+        if isinstance(freq, str):
+            self.count, self.base = self.parse(freq)
+        elif isinstance(freq, Freq):
+            self.count, self.base = freq.count, freq.base
+        else:
+            raise NotImplementedError(f"This type of input is not supported")
+
+    def __eq__(self, freq):
+        freq = Freq(freq)
+        return freq.count == self.count and freq.base == self.base
+
+    def __str__(self):
+        # trying to align to the filename of Qlib: day, 30min, 5min, 1min...
+        return f"{self.count if self.count != 1 or self.base != 'day' else ''}{self.base}"
+
+    def __repr__(self) -> str:
+        return f"{self.__class__.__name__}({str(self)})"
+
+    @staticmethod
+    def parse(freq: str) -> Tuple[int, str]:
+        """
+        Parse freq into a unified format
+
+        Parameters
+        ----------
+        freq : str
+            Raw freq, supported freq should match the re '^([0-9]*)(month|mon|week|w|day|d|minute|min)$'
+
+        Returns
+        -------
+        freq: Tuple[int, str]
+            Unified freq, including freq count and unified freq unit. The freq unit should be '[month|week|day|minute]'.
+                Example:
+
+                .. code-block::
+
+                    print(Freq.parse("day"))
+                    (1, "day" )
+                    print(Freq.parse("2mon"))
+                    (2, "month")
+                    print(Freq.parse("10w"))
+                    (10, "week")
+
+        """
+        freq = freq.lower()
+        match_obj = re.match("^([0-9]*)(month|mon|week|w|day|d|minute|min)$", freq)
+        if match_obj is None:
+            raise ValueError(
+                "freq format is not supported, the freq should be like (n)month/mon, (n)week/w, (n)day/d, (n)minute/min"
+            )
+        _count = int(match_obj.group(1)) if match_obj.group(1) else 1
+        _freq = match_obj.group(2)
+        _freq_format_dict = {
+            "month": Freq.NORM_FREQ_MONTH,
+            "mon": Freq.NORM_FREQ_MONTH,
+            "week": Freq.NORM_FREQ_WEEK,
+            "w": Freq.NORM_FREQ_WEEK,
+            "day": Freq.NORM_FREQ_DAY,
+            "d": Freq.NORM_FREQ_DAY,
+            "minute": Freq.NORM_FREQ_MINUTE,
+            "min": Freq.NORM_FREQ_MINUTE,
+        }
+        return _count, _freq_format_dict[_freq]
+
+    @staticmethod
+    def get_timedelta(n: int, freq: str) -> pd.Timedelta:
+        """
+        get pd.Timedeta object
+
+        Parameters
+        ----------
+        n : int
+        freq : str
+            Typically, they are the return value of Freq.parse
+
+        Returns
+        -------
+        pd.Timedelta:
+        """
+        return pd.Timedelta(f"{n}{freq}")
+
+    @staticmethod
+    def get_min_delta(left_frq: str, right_freq: str):
+        """Calculate freq delta
+
+        Parameters
+        ----------
+        left_frq: str
+        right_freq: str
+
+        Returns
+        -------
+
+        """
+        minutes_map = {
+            Freq.NORM_FREQ_MINUTE: 1,
+            Freq.NORM_FREQ_DAY: 60 * 24,
+            Freq.NORM_FREQ_WEEK: 7 * 60 * 24,
+            Freq.NORM_FREQ_MONTH: 30 * 7 * 60 * 24,
+        }
+        left_freq = Freq(left_frq)
+        left_minutes = left_freq.count * minutes_map[left_freq.base]
+        right_freq = Freq(right_freq)
+        right_minutes = right_freq.count * minutes_map[right_freq.base]
+        return left_minutes - right_minutes
+
+    @staticmethod
+    def get_recent_freq(base_freq: Union[str, "Freq"], freq_list: List[Union[str, "Freq"]]) -> Optional["Freq"]:
+        """Get the closest freq to base_freq from freq_list
+
+        Parameters
+        ----------
+        base_freq
+        freq_list
+
+        Returns
+        -------
+        if the recent frequency is found
+            Freq
+        else:
+            None
+        """
+        base_freq = Freq(base_freq)
+        # use the nearest freq greater than 0
+        min_freq = None
+        for _freq in freq_list:
+            _min_delta = Freq.get_min_delta(base_freq, _freq)
+            if _min_delta < 0:
+                continue
+            if min_freq is None:
+                min_freq = (_min_delta, str(_freq))
+                continue
+            min_freq = min_freq if min_freq[0] <= _min_delta else (_min_delta, _freq)
+        return min_freq[1] if min_freq else None
+
+
+def time_to_day_index(time_obj: Union[str, datetime], region: str = REG_CN):
+    if isinstance(time_obj, str):
+        time_obj = datetime.strptime(time_obj, "%H:%M")
+
+    if region == REG_CN:
+        if CN_TIME[0] <= time_obj < CN_TIME[1]:
+            return int((time_obj - CN_TIME[0]).total_seconds() / 60)
+        elif CN_TIME[2] <= time_obj < CN_TIME[3]:
+            return int((time_obj - CN_TIME[2]).total_seconds() / 60) + 120
+        else:
+            raise ValueError(f"{time_obj} is not the opening time of the {region} stock market")
+    elif region == REG_US:
+        if US_TIME[0] <= time_obj < US_TIME[1]:
+            return int((time_obj - US_TIME[0]).total_seconds() / 60)
+        else:
+            raise ValueError(f"{time_obj} is not the opening time of the {region} stock market")
+    elif region == REG_TW:
+        if TW_TIME[0] <= time_obj < TW_TIME[1]:
+            return int((time_obj - TW_TIME[0]).total_seconds() / 60)
+        else:
+            raise ValueError(f"{time_obj} is not the opening time of the {region} stock market")
+    else:
+        raise ValueError(f"{region} is not supported")
+
+
+def get_day_min_idx_range(start: str, end: str, freq: str, region: str) -> Tuple[int, int]:
+    """
+    get the min-bar index in a day for a time range (both left and right is closed) given a fixed frequency
+    Parameters
+    ----------
+    start : str
+        e.g. "9:30"
+    end : str
+        e.g. "14:30"
+    freq : str
+        "1min"
+
+    Returns
+    -------
+    Tuple[int, int]:
+        The index of start and end in the calendar. Both left and right are **closed**
+    """
+    start = pd.Timestamp(start).time()
+    end = pd.Timestamp(end).time()
+    freq = Freq(freq)
+    in_day_cal = get_min_cal(region=region)[:: freq.count]
+    left_idx = bisect.bisect_left(in_day_cal, start)
+    right_idx = bisect.bisect_right(in_day_cal, end) - 1
+    return left_idx, right_idx
+
+
+def concat_date_time(date_obj: date, time_obj: time) -> pd.Timestamp:
+    return pd.Timestamp(
+        datetime(
+            date_obj.year,
+            month=date_obj.month,
+            day=date_obj.day,
+            hour=time_obj.hour,
+            minute=time_obj.minute,
+            second=time_obj.second,
+            microsecond=time_obj.microsecond,
+        )
+    )
+
+
+def cal_sam_minute(x: pd.Timestamp, sam_minutes: int, region: str = REG_CN) -> pd.Timestamp:
+    """
+    align the minute-level data to a down sampled calendar
+
+    e.g. align 10:38 to 10:35 in 5 minute-level(10:30 in 10 minute-level)
+
+    Parameters
+    ----------
+    x : pd.Timestamp
+        datetime to be aligned
+    sam_minutes : int
+        align to `sam_minutes` minute-level calendar
+    region: str
+        Region, for example, "cn", "us"
+
+    Returns
+    -------
+    pd.Timestamp:
+        the datetime after aligned
+    """
+    cal = get_min_cal(C.min_data_shift, region)[::sam_minutes]
+    idx = bisect.bisect_right(cal, x.time()) - 1
+    _date, new_time = x.date(), cal[idx]
+    return concat_date_time(_date, new_time)
+
+
+def epsilon_change(date_time: pd.Timestamp, direction: str = "backward") -> pd.Timestamp:
+    """
+    change the time by infinitely small quantity.
+
+
+    Parameters
+    ----------
+    date_time : pd.Timestamp
+        the original time
+    direction : str
+        the direction the time are going to
+        - "backward" for going to history
+        - "forward" for going to the future
+
+    Returns
+    -------
+    pd.Timestamp:
+        the shifted time
+    """
+    if direction == "backward":
+        return date_time - pd.Timedelta(seconds=1)
+    elif direction == "forward":
+        return date_time + pd.Timedelta(seconds=1)
+    else:
+        raise ValueError("Wrong input")
+
+
+if __name__ == "__main__":
+    print(get_day_min_idx_range("8:30", "14:59", "10min", REG_CN))
```

## qlib/workflow/__init__.py

 * *Ordering differences only*

```diff
@@ -1,668 +1,668 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-from contextlib import contextmanager
-from typing import Text, Optional, Any, Dict
-from .expm import ExpManager
-from .exp import Experiment
-from .recorder import Recorder
-from ..utils import Wrapper
-from ..utils.exceptions import RecorderInitializationError
-
-
-class QlibRecorder:
-    """
-    A global system that helps to manage the experiments.
-    """
-
-    def __init__(self, exp_manager: ExpManager):
-        self.exp_manager: ExpManager = exp_manager
-
-    def __repr__(self):
-        return "{name}(manager={manager})".format(name=self.__class__.__name__, manager=self.exp_manager)
-
-    @contextmanager
-    def start(
-        self,
-        *,
-        experiment_id: Optional[Text] = None,
-        experiment_name: Optional[Text] = None,
-        recorder_id: Optional[Text] = None,
-        recorder_name: Optional[Text] = None,
-        uri: Optional[Text] = None,
-        resume: bool = False,
-    ):
-        """
-        Method to start an experiment. This method can only be called within a Python's `with` statement. Here is the example code:
-
-        .. code-block:: Python
-
-            # start new experiment and recorder
-            with R.start(experiment_name='test', recorder_name='recorder_1'):
-                model.fit(dataset)
-                R.log...
-                ... # further operations
-
-            # resume previous experiment and recorder
-            with R.start(experiment_name='test', recorder_name='recorder_1', resume=True): # if users want to resume recorder, they have to specify the exact same name for experiment and recorder.
-                ... # further operations
-
-
-        Parameters
-        ----------
-        experiment_id : str
-            id of the experiment one wants to start.
-        experiment_name : str
-            name of the experiment one wants to start.
-        recorder_id : str
-            id of the recorder under the experiment one wants to start.
-        recorder_name : str
-            name of the recorder under the experiment one wants to start.
-        uri : str
-            The tracking uri of the experiment, where all the artifacts/metrics etc. will be stored.
-            The default uri is set in the qlib.config. Note that this uri argument will not change the one defined in the config file.
-            Therefore, the next time when users call this function in the same experiment,
-            they have to also specify this argument with the same value. Otherwise, inconsistent uri may occur.
-        resume : bool
-            whether to resume the specific recorder with given name under the given experiment.
-        """
-        run = self.start_exp(
-            experiment_id=experiment_id,
-            experiment_name=experiment_name,
-            recorder_id=recorder_id,
-            recorder_name=recorder_name,
-            uri=uri,
-            resume=resume,
-        )
-        try:
-            yield run
-        except Exception as e:
-            self.end_exp(Recorder.STATUS_FA)  # end the experiment if something went wrong
-            raise e
-        self.end_exp(Recorder.STATUS_FI)
-
-    def start_exp(
-        self,
-        *,
-        experiment_id=None,
-        experiment_name=None,
-        recorder_id=None,
-        recorder_name=None,
-        uri=None,
-        resume=False,
-    ):
-        """
-        Lower level method for starting an experiment. When use this method, one should end the experiment manually
-        and the status of the recorder may not be handled properly. Here is the example code:
-
-        .. code-block:: Python
-
-            R.start_exp(experiment_name='test', recorder_name='recorder_1')
-            ... # further operations
-            R.end_exp('FINISHED') or R.end_exp(Recorder.STATUS_S)
-
-
-        Parameters
-        ----------
-        experiment_id : str
-            id of the experiment one wants to start.
-        experiment_name : str
-            the name of the experiment to be started
-        recorder_id : str
-            id of the recorder under the experiment one wants to start.
-        recorder_name : str
-            name of the recorder under the experiment one wants to start.
-        uri : str
-            the tracking uri of the experiment, where all the artifacts/metrics etc. will be stored.
-            The default uri are set in the qlib.config.
-        resume : bool
-            whether to resume the specific recorder with given name under the given experiment.
-
-        Returns
-        -------
-        An experiment instance being started.
-        """
-        return self.exp_manager.start_exp(
-            experiment_id=experiment_id,
-            experiment_name=experiment_name,
-            recorder_id=recorder_id,
-            recorder_name=recorder_name,
-            uri=uri,
-            resume=resume,
-        )
-
-    def end_exp(self, recorder_status=Recorder.STATUS_FI):
-        """
-        Method for ending an experiment manually. It will end the current active experiment, as well as its
-        active recorder with the specified `status` type. Here is the example code of the method:
-
-        .. code-block:: Python
-
-            R.start_exp(experiment_name='test')
-            ... # further operations
-            R.end_exp('FINISHED') or R.end_exp(Recorder.STATUS_S)
-
-        Parameters
-        ----------
-        status : str
-            The status of a recorder, which can be SCHEDULED, RUNNING, FINISHED, FAILED.
-        """
-        self.exp_manager.end_exp(recorder_status)
-
-    def search_records(self, experiment_ids, **kwargs):
-        """
-        Get a pandas DataFrame of records that fit the search criteria.
-
-        The arguments of this function are not set to be rigid, and they will be different with different implementation of
-        ``ExpManager`` in ``Qlib``. ``Qlib`` now provides an implementation of ``ExpManager`` with mlflow, and here is the
-        example code of the method with the ``MLflowExpManager``:
-
-        .. code-block:: Python
-
-            R.log_metrics(m=2.50, step=0)
-            records = R.search_records([experiment_id], order_by=["metrics.m DESC"])
-
-        Parameters
-        ----------
-        experiment_ids : list
-            list of experiment IDs.
-        filter_string : str
-            filter query string, defaults to searching all runs.
-        run_view_type : int
-            one of enum values ACTIVE_ONLY, DELETED_ONLY, or ALL (e.g. in mlflow.entities.ViewType).
-        max_results  : int
-            the maximum number of runs to put in the dataframe.
-        order_by : list
-            list of columns to order by (e.g., “metrics.rmse”).
-
-        Returns
-        -------
-        A pandas.DataFrame of records, where each metric, parameter, and tag
-        are expanded into their own columns named metrics.*, params.*, and tags.*
-        respectively. For records that don't have a particular metric, parameter, or tag, their
-        value will be (NumPy) Nan, None, or None respectively.
-        """
-        return self.exp_manager.search_records(experiment_ids, **kwargs)
-
-    def list_experiments(self):
-        """
-        Method for listing all the existing experiments (except for those being deleted.)
-
-        .. code-block:: Python
-
-            exps = R.list_experiments()
-
-        Returns
-        -------
-        A dictionary (name -> experiment) of experiments information that being stored.
-        """
-        return self.exp_manager.list_experiments()
-
-    def list_recorders(self, experiment_id=None, experiment_name=None):
-        """
-        Method for listing all the recorders of experiment with given id or name.
-
-        If user doesn't provide the id or name of the experiment, this method will try to retrieve the default experiment and
-        list all the recorders of the default experiment. If the default experiment doesn't exist, the method will first
-        create the default experiment, and then create a new recorder under it. (More information about the default experiment
-        can be found `here <../component/recorder.html#qlib.workflow.exp.Experiment>`__).
-
-        Here is the example code:
-
-        .. code-block:: Python
-
-            recorders = R.list_recorders(experiment_name='test')
-
-        Parameters
-        ----------
-        experiment_id : str
-            id of the experiment.
-        experiment_name : str
-            name of the experiment.
-
-        Returns
-        -------
-        A dictionary (id -> recorder) of recorder information that being stored.
-        """
-        return self.get_exp(experiment_id=experiment_id, experiment_name=experiment_name).list_recorders()
-
-    def get_exp(
-        self, *, experiment_id=None, experiment_name=None, create: bool = True, start: bool = False
-    ) -> Experiment:
-        """
-        Method for retrieving an experiment with given id or name. Once the `create` argument is set to
-        True, if no valid experiment is found, this method will create one for you. Otherwise, it will
-        only retrieve a specific experiment or raise an Error.
-
-        - If '`create`' is True:
-
-            - If `active experiment` exists:
-
-                - no id or name specified, return the active experiment.
-
-                - if id or name is specified, return the specified experiment. If no such exp found, create a new experiment with given id or name.
-
-            - If `active experiment` not exists:
-
-                - no id or name specified, create a default experiment, and the experiment is set to be active.
-
-                - if id or name is specified, return the specified experiment. If no such exp found, create a new experiment with given name or the default experiment.
-
-        - Else If '`create`' is False:
-
-            - If `active experiment` exists:
-
-                - no id or name specified, return the active experiment.
-
-                - if id or name is specified, return the specified experiment. If no such exp found, raise Error.
-
-            - If `active experiment` not exists:
-
-                - no id or name specified. If the default experiment exists, return it, otherwise, raise Error.
-
-                - if id or name is specified, return the specified experiment. If no such exp found, raise Error.
-
-        Here are some use cases:
-
-        .. code-block:: Python
-
-            # Case 1
-            with R.start('test'):
-                exp = R.get_exp()
-                recorders = exp.list_recorders()
-
-            # Case 2
-            with R.start('test'):
-                exp = R.get_exp(experiment_name='test1')
-
-            # Case 3
-            exp = R.get_exp() -> a default experiment.
-
-            # Case 4
-            exp = R.get_exp(experiment_name='test')
-
-            # Case 5
-            exp = R.get_exp(create=False) -> the default experiment if exists.
-
-        Parameters
-        ----------
-        experiment_id : str
-            id of the experiment.
-        experiment_name : str
-            name of the experiment.
-        create : boolean
-            an argument determines whether the method will automatically create a new experiment
-            according to user's specification if the experiment hasn't been created before.
-        start : bool
-            when start is True,
-            if the experiment has not started(not activated), it will start
-            It is designed for R.log_params to auto start experiments
-
-        Returns
-        -------
-        An experiment instance with given id or name.
-        """
-        return self.exp_manager.get_exp(
-            experiment_id=experiment_id,
-            experiment_name=experiment_name,
-            create=create,
-            start=start,
-        )
-
-    def delete_exp(self, experiment_id=None, experiment_name=None):
-        """
-        Method for deleting the experiment with given id or name. At least one of id or name must be given,
-        otherwise, error will occur.
-
-        Here is the example code:
-
-        .. code-block:: Python
-
-            R.delete_exp(experiment_name='test')
-
-        Parameters
-        ----------
-        experiment_id : str
-            id of the experiment.
-        experiment_name : str
-            name of the experiment.
-        """
-        self.exp_manager.delete_exp(experiment_id, experiment_name)
-
-    def get_uri(self):
-        """
-        Method for retrieving the uri of current experiment manager.
-
-        Here is the example code:
-
-        .. code-block:: Python
-
-            uri = R.get_uri()
-
-        Returns
-        -------
-        The uri of current experiment manager.
-        """
-        return self.exp_manager.uri
-
-    def set_uri(self, uri: Optional[Text]):
-        """
-        Method to reset the **default** uri of current experiment manager.
-
-        NOTE:
-
-        - When the uri is refer to a file path, please using the absolute path instead of strings like "~/mlruns/"
-          The backend don't support strings like this.
-        """
-        self.exp_manager.default_uri = uri
-
-    @contextmanager
-    def uri_context(self, uri: Text):
-        """
-        Temporarily set the exp_manager's **default_uri** to uri
-
-        NOTE:
-        - Please refer to the NOTE in the `set_uri`
-
-        Parameters
-        ----------
-        uri : Text
-            the temporal uri
-        """
-        prev_uri = self.exp_manager.default_uri
-        self.exp_manager.default_uri = uri
-        try:
-            yield
-        finally:
-            self.exp_manager.default_uri = prev_uri
-
-    def get_recorder(
-        self,
-        *,
-        recorder_id=None,
-        recorder_name=None,
-        experiment_id=None,
-        experiment_name=None,
-    ) -> Recorder:
-        """
-        Method for retrieving a recorder.
-
-        - If `active recorder` exists:
-
-            - no id or name specified, return the active recorder.
-
-            - if id or name is specified, return the specified recorder.
-
-        - If `active recorder` not exists:
-
-            - no id or name specified, raise Error.
-
-            - if id or name is specified, and the corresponding experiment_name must be given, return the specified recorder. Otherwise, raise Error.
-
-        The recorder can be used for further process such as `save_object`, `load_object`, `log_params`,
-        `log_metrics`, etc.
-
-        Here are some use cases:
-
-        .. code-block:: Python
-
-            # Case 1
-            with R.start(experiment_name='test'):
-                recorder = R.get_recorder()
-
-            # Case 2
-            with R.start(experiment_name='test'):
-                recorder = R.get_recorder(recorder_id='2e7a4efd66574fa49039e00ffaefa99d')
-
-            # Case 3
-            recorder = R.get_recorder() -> Error
-
-            # Case 4
-            recorder = R.get_recorder(recorder_id='2e7a4efd66574fa49039e00ffaefa99d') -> Error
-
-            # Case 5
-            recorder = R.get_recorder(recorder_id='2e7a4efd66574fa49039e00ffaefa99d', experiment_name='test')
-
-
-        Here are some things users may concern
-        - Q: What recorder will it return if multiple recorder meets the query (e.g. query with experiment_name)
-        - A: If mlflow backend is used, then the recorder with the latest `start_time` will be returned. Because MLflow's `search_runs` function guarantee it
-
-        Parameters
-        ----------
-        recorder_id : str
-            id of the recorder.
-        recorder_name : str
-            name of the recorder.
-        experiment_name : str
-            name of the experiment.
-
-        Returns
-        -------
-        A recorder instance.
-        """
-        return self.get_exp(experiment_name=experiment_name, experiment_id=experiment_id, create=False).get_recorder(
-            recorder_id, recorder_name, create=False, start=False
-        )
-
-    def delete_recorder(self, recorder_id=None, recorder_name=None):
-        """
-        Method for deleting the recorders with given id or name. At least one of id or name must be given,
-        otherwise, error will occur.
-
-        Here is the example code:
-
-        .. code-block:: Python
-
-            R.delete_recorder(recorder_id='2e7a4efd66574fa49039e00ffaefa99d')
-
-        Parameters
-        ----------
-        recorder_id : str
-            id of the experiment.
-        recorder_name : str
-            name of the experiment.
-        """
-        self.get_exp().delete_recorder(recorder_id, recorder_name)
-
-    def save_objects(self, local_path=None, artifact_path=None, **kwargs: Dict[Text, Any]):
-        """
-        Method for saving objects as artifacts in the experiment to the uri. It supports either saving
-        from a local file/directory, or directly saving objects. User can use valid python's keywords arguments
-        to specify the object to be saved as well as its name (name: value).
-
-        In summary, this API is designs for saving **objects** to **the experiments management backend path**,
-        1. Qlib provide two methods to specify **objects**
-        - Passing in the object directly by passing with `**kwargs` (e.g. R.save_objects(trained_model=model))
-        - Passing in the local path to the object, i.e. `local_path` parameter.
-        2. `artifact_path` represents the  **the experiments management backend path**
-
-        - If `active recorder` exists: it will save the objects through the active recorder.
-        - If `active recorder` not exists: the system will create a default experiment, and a new recorder and save objects under it.
-
-        .. note::
-
-            If one wants to save objects with a specific recorder. It is recommended to first get the specific recorder through `get_recorder` API and use the recorder the save objects. The supported arguments are the same as this method.
-
-        Here are some use cases:
-
-        .. code-block:: Python
-
-            # Case 1
-            with R.start(experiment_name='test'):
-                pred = model.predict(dataset)
-                R.save_objects(**{"pred.pkl": pred}, artifact_path='prediction')
-                rid = R.get_recorder().id
-            ...
-            R.get_recorder(recorder_id=rid).load_object("prediction/pred.pkl")  #  after saving objects, you can load the previous object with this api
-
-            # Case 2
-            with R.start(experiment_name='test'):
-                R.save_objects(local_path='results/pred.pkl', artifact_path="prediction")
-                rid = R.get_recorder().id
-            ...
-            R.get_recorder(recorder_id=rid).load_object("prediction/pred.pkl")  #  after saving objects, you can load the previous object with this api
-
-
-        Parameters
-        ----------
-        local_path : str
-            if provided, them save the file or directory to the artifact URI.
-        artifact_path : str
-            the relative path for the artifact to be stored in the URI.
-        **kwargs: Dict[Text, Any]
-            the object to be saved.
-            For example, `{"pred.pkl": pred}`
-        """
-        if local_path is not None and len(kwargs) > 0:
-            raise ValueError(
-                "You can choose only one of `local_path`(save the files in a path) or `kwargs`(pass in the objects directly)"
-            )
-        self.get_exp().get_recorder(start=True).save_objects(local_path, artifact_path, **kwargs)
-
-    def load_object(self, name: Text):
-        """
-        Method for loading an object from artifacts in the experiment in the uri.
-        """
-        return self.get_exp().get_recorder(start=True).load_object(name)
-
-    def log_params(self, **kwargs):
-        """
-        Method for logging parameters during an experiment. In addition to using ``R``, one can also log to a specific recorder after getting it with `get_recorder` API.
-
-        - If `active recorder` exists: it will log parameters through the active recorder.
-        - If `active recorder` not exists: the system will create a default experiment as well as a new recorder, and log parameters under it.
-
-        Here are some use cases:
-
-        .. code-block:: Python
-
-            # Case 1
-            with R.start('test'):
-                R.log_params(learning_rate=0.01)
-
-            # Case 2
-            R.log_params(learning_rate=0.01)
-
-        Parameters
-        ----------
-        keyword argument:
-            name1=value1, name2=value2, ...
-        """
-        self.get_exp(start=True).get_recorder(start=True).log_params(**kwargs)
-
-    def log_metrics(self, step=None, **kwargs):
-        """
-        Method for logging metrics during an experiment. In addition to using ``R``, one can also log to a specific recorder after getting it with `get_recorder` API.
-
-        - If `active recorder` exists: it will log metrics through the active recorder.
-        - If `active recorder` not exists: the system will create a default experiment as well as a new recorder, and log metrics under it.
-
-        Here are some use cases:
-
-        .. code-block:: Python
-
-            # Case 1
-            with R.start('test'):
-                R.log_metrics(train_loss=0.33, step=1)
-
-            # Case 2
-            R.log_metrics(train_loss=0.33, step=1)
-
-        Parameters
-        ----------
-        keyword argument:
-            name1=value1, name2=value2, ...
-        """
-        self.get_exp(start=True).get_recorder(start=True).log_metrics(step, **kwargs)
-
-    def log_artifact(self, local_path: str, artifact_path: Optional[str] = None):
-        """
-        Log a local file or directory as an artifact of the currently active run
-
-        - If `active recorder` exists: it will set tags through the active recorder.
-        - If `active recorder` not exists: the system will create a default experiment as well as a new recorder, and set the tags under it.
-
-        Parameters
-        ----------
-        local_path : str
-            Path to the file to write.
-        artifact_path : Optional[str]
-            If provided, the directory in ``artifact_uri`` to write to.
-        """
-        self.get_exp(start=True).get_recorder(start=True).log_artifact(local_path, artifact_path)
-
-    def download_artifact(self, path: str, dst_path: Optional[str] = None) -> str:
-        """
-        Download an artifact file or directory from a run to a local directory if applicable,
-        and return a local path for it.
-
-        Parameters
-        ----------
-        path : str
-            Relative source path to the desired artifact.
-        dst_path : Optional[str]
-            Absolute path of the local filesystem destination directory to which to
-            download the specified artifacts. This directory must already exist.
-            If unspecified, the artifacts will either be downloaded to a new
-            uniquely-named directory on the local filesystem.
-
-        Returns
-        -------
-        str
-            Local path of desired artifact.
-        """
-        self.get_exp(start=True).get_recorder(start=True).download_artifact(path, dst_path)
-
-    def set_tags(self, **kwargs):
-        """
-        Method for setting tags for a recorder. In addition to using ``R``, one can also set the tag to a specific recorder after getting it with `get_recorder` API.
-
-        - If `active recorder` exists: it will set tags through the active recorder.
-        - If `active recorder` not exists: the system will create a default experiment as well as a new recorder, and set the tags under it.
-
-        Here are some use cases:
-
-        .. code-block:: Python
-
-            # Case 1
-            with R.start('test'):
-                R.set_tags(release_version="2.2.0")
-
-            # Case 2
-            R.set_tags(release_version="2.2.0")
-
-        Parameters
-        ----------
-        keyword argument:
-            name1=value1, name2=value2, ...
-        """
-        self.get_exp(start=True).get_recorder(start=True).set_tags(**kwargs)
-
-
-class RecorderWrapper(Wrapper):
-    """
-    Wrapper class for QlibRecorder, which detects whether users reinitialize qlib when already starting an experiment.
-    """
-
-    def register(self, provider):
-        if self._provider is not None:
-            expm = getattr(self._provider, "exp_manager")
-            if expm.active_experiment is not None:
-                raise RecorderInitializationError(
-                    "Please don't reinitialize Qlib if QlibRecorder is already activated. Otherwise, the experiment stored location will be modified."
-                )
-        self._provider = provider
-
-
-import sys
-
-if sys.version_info >= (3, 9):
-    from typing import Annotated
-
-    QlibRecorderWrapper = Annotated[QlibRecorder, RecorderWrapper]
-else:
-    QlibRecorderWrapper = QlibRecorder
-
-# global record
-R: QlibRecorderWrapper = RecorderWrapper()
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+from contextlib import contextmanager
+from typing import Text, Optional, Any, Dict
+from .expm import ExpManager
+from .exp import Experiment
+from .recorder import Recorder
+from ..utils import Wrapper
+from ..utils.exceptions import RecorderInitializationError
+
+
+class QlibRecorder:
+    """
+    A global system that helps to manage the experiments.
+    """
+
+    def __init__(self, exp_manager: ExpManager):
+        self.exp_manager: ExpManager = exp_manager
+
+    def __repr__(self):
+        return "{name}(manager={manager})".format(name=self.__class__.__name__, manager=self.exp_manager)
+
+    @contextmanager
+    def start(
+        self,
+        *,
+        experiment_id: Optional[Text] = None,
+        experiment_name: Optional[Text] = None,
+        recorder_id: Optional[Text] = None,
+        recorder_name: Optional[Text] = None,
+        uri: Optional[Text] = None,
+        resume: bool = False,
+    ):
+        """
+        Method to start an experiment. This method can only be called within a Python's `with` statement. Here is the example code:
+
+        .. code-block:: Python
+
+            # start new experiment and recorder
+            with R.start(experiment_name='test', recorder_name='recorder_1'):
+                model.fit(dataset)
+                R.log...
+                ... # further operations
+
+            # resume previous experiment and recorder
+            with R.start(experiment_name='test', recorder_name='recorder_1', resume=True): # if users want to resume recorder, they have to specify the exact same name for experiment and recorder.
+                ... # further operations
+
+
+        Parameters
+        ----------
+        experiment_id : str
+            id of the experiment one wants to start.
+        experiment_name : str
+            name of the experiment one wants to start.
+        recorder_id : str
+            id of the recorder under the experiment one wants to start.
+        recorder_name : str
+            name of the recorder under the experiment one wants to start.
+        uri : str
+            The tracking uri of the experiment, where all the artifacts/metrics etc. will be stored.
+            The default uri is set in the qlib.config. Note that this uri argument will not change the one defined in the config file.
+            Therefore, the next time when users call this function in the same experiment,
+            they have to also specify this argument with the same value. Otherwise, inconsistent uri may occur.
+        resume : bool
+            whether to resume the specific recorder with given name under the given experiment.
+        """
+        run = self.start_exp(
+            experiment_id=experiment_id,
+            experiment_name=experiment_name,
+            recorder_id=recorder_id,
+            recorder_name=recorder_name,
+            uri=uri,
+            resume=resume,
+        )
+        try:
+            yield run
+        except Exception as e:
+            self.end_exp(Recorder.STATUS_FA)  # end the experiment if something went wrong
+            raise e
+        self.end_exp(Recorder.STATUS_FI)
+
+    def start_exp(
+        self,
+        *,
+        experiment_id=None,
+        experiment_name=None,
+        recorder_id=None,
+        recorder_name=None,
+        uri=None,
+        resume=False,
+    ):
+        """
+        Lower level method for starting an experiment. When use this method, one should end the experiment manually
+        and the status of the recorder may not be handled properly. Here is the example code:
+
+        .. code-block:: Python
+
+            R.start_exp(experiment_name='test', recorder_name='recorder_1')
+            ... # further operations
+            R.end_exp('FINISHED') or R.end_exp(Recorder.STATUS_S)
+
+
+        Parameters
+        ----------
+        experiment_id : str
+            id of the experiment one wants to start.
+        experiment_name : str
+            the name of the experiment to be started
+        recorder_id : str
+            id of the recorder under the experiment one wants to start.
+        recorder_name : str
+            name of the recorder under the experiment one wants to start.
+        uri : str
+            the tracking uri of the experiment, where all the artifacts/metrics etc. will be stored.
+            The default uri are set in the qlib.config.
+        resume : bool
+            whether to resume the specific recorder with given name under the given experiment.
+
+        Returns
+        -------
+        An experiment instance being started.
+        """
+        return self.exp_manager.start_exp(
+            experiment_id=experiment_id,
+            experiment_name=experiment_name,
+            recorder_id=recorder_id,
+            recorder_name=recorder_name,
+            uri=uri,
+            resume=resume,
+        )
+
+    def end_exp(self, recorder_status=Recorder.STATUS_FI):
+        """
+        Method for ending an experiment manually. It will end the current active experiment, as well as its
+        active recorder with the specified `status` type. Here is the example code of the method:
+
+        .. code-block:: Python
+
+            R.start_exp(experiment_name='test')
+            ... # further operations
+            R.end_exp('FINISHED') or R.end_exp(Recorder.STATUS_S)
+
+        Parameters
+        ----------
+        status : str
+            The status of a recorder, which can be SCHEDULED, RUNNING, FINISHED, FAILED.
+        """
+        self.exp_manager.end_exp(recorder_status)
+
+    def search_records(self, experiment_ids, **kwargs):
+        """
+        Get a pandas DataFrame of records that fit the search criteria.
+
+        The arguments of this function are not set to be rigid, and they will be different with different implementation of
+        ``ExpManager`` in ``Qlib``. ``Qlib`` now provides an implementation of ``ExpManager`` with mlflow, and here is the
+        example code of the method with the ``MLflowExpManager``:
+
+        .. code-block:: Python
+
+            R.log_metrics(m=2.50, step=0)
+            records = R.search_records([experiment_id], order_by=["metrics.m DESC"])
+
+        Parameters
+        ----------
+        experiment_ids : list
+            list of experiment IDs.
+        filter_string : str
+            filter query string, defaults to searching all runs.
+        run_view_type : int
+            one of enum values ACTIVE_ONLY, DELETED_ONLY, or ALL (e.g. in mlflow.entities.ViewType).
+        max_results  : int
+            the maximum number of runs to put in the dataframe.
+        order_by : list
+            list of columns to order by (e.g., “metrics.rmse”).
+
+        Returns
+        -------
+        A pandas.DataFrame of records, where each metric, parameter, and tag
+        are expanded into their own columns named metrics.*, params.*, and tags.*
+        respectively. For records that don't have a particular metric, parameter, or tag, their
+        value will be (NumPy) Nan, None, or None respectively.
+        """
+        return self.exp_manager.search_records(experiment_ids, **kwargs)
+
+    def list_experiments(self):
+        """
+        Method for listing all the existing experiments (except for those being deleted.)
+
+        .. code-block:: Python
+
+            exps = R.list_experiments()
+
+        Returns
+        -------
+        A dictionary (name -> experiment) of experiments information that being stored.
+        """
+        return self.exp_manager.list_experiments()
+
+    def list_recorders(self, experiment_id=None, experiment_name=None):
+        """
+        Method for listing all the recorders of experiment with given id or name.
+
+        If user doesn't provide the id or name of the experiment, this method will try to retrieve the default experiment and
+        list all the recorders of the default experiment. If the default experiment doesn't exist, the method will first
+        create the default experiment, and then create a new recorder under it. (More information about the default experiment
+        can be found `here <../component/recorder.html#qlib.workflow.exp.Experiment>`__).
+
+        Here is the example code:
+
+        .. code-block:: Python
+
+            recorders = R.list_recorders(experiment_name='test')
+
+        Parameters
+        ----------
+        experiment_id : str
+            id of the experiment.
+        experiment_name : str
+            name of the experiment.
+
+        Returns
+        -------
+        A dictionary (id -> recorder) of recorder information that being stored.
+        """
+        return self.get_exp(experiment_id=experiment_id, experiment_name=experiment_name).list_recorders()
+
+    def get_exp(
+        self, *, experiment_id=None, experiment_name=None, create: bool = True, start: bool = False
+    ) -> Experiment:
+        """
+        Method for retrieving an experiment with given id or name. Once the `create` argument is set to
+        True, if no valid experiment is found, this method will create one for you. Otherwise, it will
+        only retrieve a specific experiment or raise an Error.
+
+        - If '`create`' is True:
+
+            - If `active experiment` exists:
+
+                - no id or name specified, return the active experiment.
+
+                - if id or name is specified, return the specified experiment. If no such exp found, create a new experiment with given id or name.
+
+            - If `active experiment` not exists:
+
+                - no id or name specified, create a default experiment, and the experiment is set to be active.
+
+                - if id or name is specified, return the specified experiment. If no such exp found, create a new experiment with given name or the default experiment.
+
+        - Else If '`create`' is False:
+
+            - If `active experiment` exists:
+
+                - no id or name specified, return the active experiment.
+
+                - if id or name is specified, return the specified experiment. If no such exp found, raise Error.
+
+            - If `active experiment` not exists:
+
+                - no id or name specified. If the default experiment exists, return it, otherwise, raise Error.
+
+                - if id or name is specified, return the specified experiment. If no such exp found, raise Error.
+
+        Here are some use cases:
+
+        .. code-block:: Python
+
+            # Case 1
+            with R.start('test'):
+                exp = R.get_exp()
+                recorders = exp.list_recorders()
+
+            # Case 2
+            with R.start('test'):
+                exp = R.get_exp(experiment_name='test1')
+
+            # Case 3
+            exp = R.get_exp() -> a default experiment.
+
+            # Case 4
+            exp = R.get_exp(experiment_name='test')
+
+            # Case 5
+            exp = R.get_exp(create=False) -> the default experiment if exists.
+
+        Parameters
+        ----------
+        experiment_id : str
+            id of the experiment.
+        experiment_name : str
+            name of the experiment.
+        create : boolean
+            an argument determines whether the method will automatically create a new experiment
+            according to user's specification if the experiment hasn't been created before.
+        start : bool
+            when start is True,
+            if the experiment has not started(not activated), it will start
+            It is designed for R.log_params to auto start experiments
+
+        Returns
+        -------
+        An experiment instance with given id or name.
+        """
+        return self.exp_manager.get_exp(
+            experiment_id=experiment_id,
+            experiment_name=experiment_name,
+            create=create,
+            start=start,
+        )
+
+    def delete_exp(self, experiment_id=None, experiment_name=None):
+        """
+        Method for deleting the experiment with given id or name. At least one of id or name must be given,
+        otherwise, error will occur.
+
+        Here is the example code:
+
+        .. code-block:: Python
+
+            R.delete_exp(experiment_name='test')
+
+        Parameters
+        ----------
+        experiment_id : str
+            id of the experiment.
+        experiment_name : str
+            name of the experiment.
+        """
+        self.exp_manager.delete_exp(experiment_id, experiment_name)
+
+    def get_uri(self):
+        """
+        Method for retrieving the uri of current experiment manager.
+
+        Here is the example code:
+
+        .. code-block:: Python
+
+            uri = R.get_uri()
+
+        Returns
+        -------
+        The uri of current experiment manager.
+        """
+        return self.exp_manager.uri
+
+    def set_uri(self, uri: Optional[Text]):
+        """
+        Method to reset the **default** uri of current experiment manager.
+
+        NOTE:
+
+        - When the uri is refer to a file path, please using the absolute path instead of strings like "~/mlruns/"
+          The backend don't support strings like this.
+        """
+        self.exp_manager.default_uri = uri
+
+    @contextmanager
+    def uri_context(self, uri: Text):
+        """
+        Temporarily set the exp_manager's **default_uri** to uri
+
+        NOTE:
+        - Please refer to the NOTE in the `set_uri`
+
+        Parameters
+        ----------
+        uri : Text
+            the temporal uri
+        """
+        prev_uri = self.exp_manager.default_uri
+        self.exp_manager.default_uri = uri
+        try:
+            yield
+        finally:
+            self.exp_manager.default_uri = prev_uri
+
+    def get_recorder(
+        self,
+        *,
+        recorder_id=None,
+        recorder_name=None,
+        experiment_id=None,
+        experiment_name=None,
+    ) -> Recorder:
+        """
+        Method for retrieving a recorder.
+
+        - If `active recorder` exists:
+
+            - no id or name specified, return the active recorder.
+
+            - if id or name is specified, return the specified recorder.
+
+        - If `active recorder` not exists:
+
+            - no id or name specified, raise Error.
+
+            - if id or name is specified, and the corresponding experiment_name must be given, return the specified recorder. Otherwise, raise Error.
+
+        The recorder can be used for further process such as `save_object`, `load_object`, `log_params`,
+        `log_metrics`, etc.
+
+        Here are some use cases:
+
+        .. code-block:: Python
+
+            # Case 1
+            with R.start(experiment_name='test'):
+                recorder = R.get_recorder()
+
+            # Case 2
+            with R.start(experiment_name='test'):
+                recorder = R.get_recorder(recorder_id='2e7a4efd66574fa49039e00ffaefa99d')
+
+            # Case 3
+            recorder = R.get_recorder() -> Error
+
+            # Case 4
+            recorder = R.get_recorder(recorder_id='2e7a4efd66574fa49039e00ffaefa99d') -> Error
+
+            # Case 5
+            recorder = R.get_recorder(recorder_id='2e7a4efd66574fa49039e00ffaefa99d', experiment_name='test')
+
+
+        Here are some things users may concern
+        - Q: What recorder will it return if multiple recorder meets the query (e.g. query with experiment_name)
+        - A: If mlflow backend is used, then the recorder with the latest `start_time` will be returned. Because MLflow's `search_runs` function guarantee it
+
+        Parameters
+        ----------
+        recorder_id : str
+            id of the recorder.
+        recorder_name : str
+            name of the recorder.
+        experiment_name : str
+            name of the experiment.
+
+        Returns
+        -------
+        A recorder instance.
+        """
+        return self.get_exp(experiment_name=experiment_name, experiment_id=experiment_id, create=False).get_recorder(
+            recorder_id, recorder_name, create=False, start=False
+        )
+
+    def delete_recorder(self, recorder_id=None, recorder_name=None):
+        """
+        Method for deleting the recorders with given id or name. At least one of id or name must be given,
+        otherwise, error will occur.
+
+        Here is the example code:
+
+        .. code-block:: Python
+
+            R.delete_recorder(recorder_id='2e7a4efd66574fa49039e00ffaefa99d')
+
+        Parameters
+        ----------
+        recorder_id : str
+            id of the experiment.
+        recorder_name : str
+            name of the experiment.
+        """
+        self.get_exp().delete_recorder(recorder_id, recorder_name)
+
+    def save_objects(self, local_path=None, artifact_path=None, **kwargs: Dict[Text, Any]):
+        """
+        Method for saving objects as artifacts in the experiment to the uri. It supports either saving
+        from a local file/directory, or directly saving objects. User can use valid python's keywords arguments
+        to specify the object to be saved as well as its name (name: value).
+
+        In summary, this API is designs for saving **objects** to **the experiments management backend path**,
+        1. Qlib provide two methods to specify **objects**
+        - Passing in the object directly by passing with `**kwargs` (e.g. R.save_objects(trained_model=model))
+        - Passing in the local path to the object, i.e. `local_path` parameter.
+        2. `artifact_path` represents the  **the experiments management backend path**
+
+        - If `active recorder` exists: it will save the objects through the active recorder.
+        - If `active recorder` not exists: the system will create a default experiment, and a new recorder and save objects under it.
+
+        .. note::
+
+            If one wants to save objects with a specific recorder. It is recommended to first get the specific recorder through `get_recorder` API and use the recorder the save objects. The supported arguments are the same as this method.
+
+        Here are some use cases:
+
+        .. code-block:: Python
+
+            # Case 1
+            with R.start(experiment_name='test'):
+                pred = model.predict(dataset)
+                R.save_objects(**{"pred.pkl": pred}, artifact_path='prediction')
+                rid = R.get_recorder().id
+            ...
+            R.get_recorder(recorder_id=rid).load_object("prediction/pred.pkl")  #  after saving objects, you can load the previous object with this api
+
+            # Case 2
+            with R.start(experiment_name='test'):
+                R.save_objects(local_path='results/pred.pkl', artifact_path="prediction")
+                rid = R.get_recorder().id
+            ...
+            R.get_recorder(recorder_id=rid).load_object("prediction/pred.pkl")  #  after saving objects, you can load the previous object with this api
+
+
+        Parameters
+        ----------
+        local_path : str
+            if provided, them save the file or directory to the artifact URI.
+        artifact_path : str
+            the relative path for the artifact to be stored in the URI.
+        **kwargs: Dict[Text, Any]
+            the object to be saved.
+            For example, `{"pred.pkl": pred}`
+        """
+        if local_path is not None and len(kwargs) > 0:
+            raise ValueError(
+                "You can choose only one of `local_path`(save the files in a path) or `kwargs`(pass in the objects directly)"
+            )
+        self.get_exp().get_recorder(start=True).save_objects(local_path, artifact_path, **kwargs)
+
+    def load_object(self, name: Text):
+        """
+        Method for loading an object from artifacts in the experiment in the uri.
+        """
+        return self.get_exp().get_recorder(start=True).load_object(name)
+
+    def log_params(self, **kwargs):
+        """
+        Method for logging parameters during an experiment. In addition to using ``R``, one can also log to a specific recorder after getting it with `get_recorder` API.
+
+        - If `active recorder` exists: it will log parameters through the active recorder.
+        - If `active recorder` not exists: the system will create a default experiment as well as a new recorder, and log parameters under it.
+
+        Here are some use cases:
+
+        .. code-block:: Python
+
+            # Case 1
+            with R.start('test'):
+                R.log_params(learning_rate=0.01)
+
+            # Case 2
+            R.log_params(learning_rate=0.01)
+
+        Parameters
+        ----------
+        keyword argument:
+            name1=value1, name2=value2, ...
+        """
+        self.get_exp(start=True).get_recorder(start=True).log_params(**kwargs)
+
+    def log_metrics(self, step=None, **kwargs):
+        """
+        Method for logging metrics during an experiment. In addition to using ``R``, one can also log to a specific recorder after getting it with `get_recorder` API.
+
+        - If `active recorder` exists: it will log metrics through the active recorder.
+        - If `active recorder` not exists: the system will create a default experiment as well as a new recorder, and log metrics under it.
+
+        Here are some use cases:
+
+        .. code-block:: Python
+
+            # Case 1
+            with R.start('test'):
+                R.log_metrics(train_loss=0.33, step=1)
+
+            # Case 2
+            R.log_metrics(train_loss=0.33, step=1)
+
+        Parameters
+        ----------
+        keyword argument:
+            name1=value1, name2=value2, ...
+        """
+        self.get_exp(start=True).get_recorder(start=True).log_metrics(step, **kwargs)
+
+    def log_artifact(self, local_path: str, artifact_path: Optional[str] = None):
+        """
+        Log a local file or directory as an artifact of the currently active run
+
+        - If `active recorder` exists: it will set tags through the active recorder.
+        - If `active recorder` not exists: the system will create a default experiment as well as a new recorder, and set the tags under it.
+
+        Parameters
+        ----------
+        local_path : str
+            Path to the file to write.
+        artifact_path : Optional[str]
+            If provided, the directory in ``artifact_uri`` to write to.
+        """
+        self.get_exp(start=True).get_recorder(start=True).log_artifact(local_path, artifact_path)
+
+    def download_artifact(self, path: str, dst_path: Optional[str] = None) -> str:
+        """
+        Download an artifact file or directory from a run to a local directory if applicable,
+        and return a local path for it.
+
+        Parameters
+        ----------
+        path : str
+            Relative source path to the desired artifact.
+        dst_path : Optional[str]
+            Absolute path of the local filesystem destination directory to which to
+            download the specified artifacts. This directory must already exist.
+            If unspecified, the artifacts will either be downloaded to a new
+            uniquely-named directory on the local filesystem.
+
+        Returns
+        -------
+        str
+            Local path of desired artifact.
+        """
+        self.get_exp(start=True).get_recorder(start=True).download_artifact(path, dst_path)
+
+    def set_tags(self, **kwargs):
+        """
+        Method for setting tags for a recorder. In addition to using ``R``, one can also set the tag to a specific recorder after getting it with `get_recorder` API.
+
+        - If `active recorder` exists: it will set tags through the active recorder.
+        - If `active recorder` not exists: the system will create a default experiment as well as a new recorder, and set the tags under it.
+
+        Here are some use cases:
+
+        .. code-block:: Python
+
+            # Case 1
+            with R.start('test'):
+                R.set_tags(release_version="2.2.0")
+
+            # Case 2
+            R.set_tags(release_version="2.2.0")
+
+        Parameters
+        ----------
+        keyword argument:
+            name1=value1, name2=value2, ...
+        """
+        self.get_exp(start=True).get_recorder(start=True).set_tags(**kwargs)
+
+
+class RecorderWrapper(Wrapper):
+    """
+    Wrapper class for QlibRecorder, which detects whether users reinitialize qlib when already starting an experiment.
+    """
+
+    def register(self, provider):
+        if self._provider is not None:
+            expm = getattr(self._provider, "exp_manager")
+            if expm.active_experiment is not None:
+                raise RecorderInitializationError(
+                    "Please don't reinitialize Qlib if QlibRecorder is already activated. Otherwise, the experiment stored location will be modified."
+                )
+        self._provider = provider
+
+
+import sys
+
+if sys.version_info >= (3, 9):
+    from typing import Annotated
+
+    QlibRecorderWrapper = Annotated[QlibRecorder, RecorderWrapper]
+else:
+    QlibRecorderWrapper = QlibRecorder
+
+# global record
+R: QlibRecorderWrapper = RecorderWrapper()
```

## qlib/workflow/cli.py

 * *Ordering differences only*

```diff
@@ -1,119 +1,119 @@
-#  Copyright (c) Microsoft Corporation.
-#  Licensed under the MIT License.
-import logging
-import sys
-import os
-from pathlib import Path
-
-import qlib
-import fire
-import ruamel.yaml as yaml
-from qlib.config import C
-from qlib.model.trainer import task_train
-from qlib.utils.data import update_config
-from qlib.log import get_module_logger
-from qlib.utils import set_log_with_config
-
-set_log_with_config(C.logging_config)
-logger = get_module_logger("qrun", logging.INFO)
-
-
-def get_path_list(path):
-    if isinstance(path, str):
-        return [path]
-    else:
-        return list(path)
-
-
-def sys_config(config, config_path):
-    """
-    Configure the `sys` section
-
-    Parameters
-    ----------
-    config : dict
-        configuration of the workflow.
-    config_path : str
-        path of the configuration
-    """
-    sys_config = config.get("sys", {})
-
-    # abspath
-    for p in get_path_list(sys_config.get("path", [])):
-        sys.path.append(p)
-
-    # relative path to config path
-    for p in get_path_list(sys_config.get("rel_path", [])):
-        sys.path.append(str(Path(config_path).parent.resolve().absolute() / p))
-
-
-# workflow handler function
-def workflow(config_path, experiment_name="workflow", uri_folder="mlruns"):
-    """
-    This is a Qlib CLI entrance.
-    User can run the whole Quant research workflow defined by a configure file
-    - the code is located here ``qlib/workflow/cli.py`
-
-    User can specify a base_config file in your workflow.yml file by adding "BASE_CONFIG_PATH".
-    Qlib will load the configuration in BASE_CONFIG_PATH first, and the user only needs to update the custom fields
-    in their own workflow.yml file.
-
-    For examples:
-
-        qlib_init:
-            provider_uri: "~/.qlib/qlib_data/cn_data"
-            region: cn
-        BASE_CONFIG_PATH: "workflow_config_lightgbm_Alpha158_csi500.yaml"
-        market: csi300
-
-    """
-    with open(config_path) as fp:
-        config = yaml.safe_load(fp)
-
-    base_config_path = config.get("BASE_CONFIG_PATH", None)
-    if base_config_path:
-        logger.info(f"Use BASE_CONFIG_PATH: {base_config_path}")
-        base_config_path = Path(base_config_path)
-
-        # it will find config file in absolute path and relative path
-        if base_config_path.exists():
-            path = base_config_path
-        else:
-            logger.info(
-                f"Can't find BASE_CONFIG_PATH base on: {Path.cwd()}, "
-                f"try using relative path to config path: {Path(config_path).absolute()}"
-            )
-            relative_path = Path(config_path).absolute().parent.joinpath(base_config_path)
-            if relative_path.exists():
-                path = relative_path
-            else:
-                raise FileNotFoundError(f"Can't find the BASE_CONFIG file: {base_config_path}")
-
-        with open(path) as fp:
-            base_config = yaml.safe_load(fp)
-        logger.info(f"Load BASE_CONFIG_PATH succeed: {path.resolve()}")
-        config = update_config(base_config, config)
-
-    # config the `sys` section
-    sys_config(config, config_path)
-
-    if "exp_manager" in config.get("qlib_init"):
-        qlib.init(**config.get("qlib_init"))
-    else:
-        exp_manager = C["exp_manager"]
-        exp_manager["kwargs"]["uri"] = "file:" + str(Path(os.getcwd()).resolve() / uri_folder)
-        qlib.init(**config.get("qlib_init"), exp_manager=exp_manager)
-
-    if "experiment_name" in config:
-        experiment_name = config["experiment_name"]
-    recorder = task_train(config.get("task"), experiment_name=experiment_name)
-    recorder.save_objects(config=config)
-
-
-# function to run workflow by config
-def run():
-    fire.Fire(workflow)
-
-
-if __name__ == "__main__":
-    run()
+#  Copyright (c) Microsoft Corporation.
+#  Licensed under the MIT License.
+import logging
+import sys
+import os
+from pathlib import Path
+
+import qlib
+import fire
+import ruamel.yaml as yaml
+from qlib.config import C
+from qlib.model.trainer import task_train
+from qlib.utils.data import update_config
+from qlib.log import get_module_logger
+from qlib.utils import set_log_with_config
+
+set_log_with_config(C.logging_config)
+logger = get_module_logger("qrun", logging.INFO)
+
+
+def get_path_list(path):
+    if isinstance(path, str):
+        return [path]
+    else:
+        return list(path)
+
+
+def sys_config(config, config_path):
+    """
+    Configure the `sys` section
+
+    Parameters
+    ----------
+    config : dict
+        configuration of the workflow.
+    config_path : str
+        path of the configuration
+    """
+    sys_config = config.get("sys", {})
+
+    # abspath
+    for p in get_path_list(sys_config.get("path", [])):
+        sys.path.append(p)
+
+    # relative path to config path
+    for p in get_path_list(sys_config.get("rel_path", [])):
+        sys.path.append(str(Path(config_path).parent.resolve().absolute() / p))
+
+
+# workflow handler function
+def workflow(config_path, experiment_name="workflow", uri_folder="mlruns"):
+    """
+    This is a Qlib CLI entrance.
+    User can run the whole Quant research workflow defined by a configure file
+    - the code is located here ``qlib/workflow/cli.py`
+
+    User can specify a base_config file in your workflow.yml file by adding "BASE_CONFIG_PATH".
+    Qlib will load the configuration in BASE_CONFIG_PATH first, and the user only needs to update the custom fields
+    in their own workflow.yml file.
+
+    For examples:
+
+        qlib_init:
+            provider_uri: "~/.qlib/qlib_data/cn_data"
+            region: cn
+        BASE_CONFIG_PATH: "workflow_config_lightgbm_Alpha158_csi500.yaml"
+        market: csi300
+
+    """
+    with open(config_path) as fp:
+        config = yaml.safe_load(fp)
+
+    base_config_path = config.get("BASE_CONFIG_PATH", None)
+    if base_config_path:
+        logger.info(f"Use BASE_CONFIG_PATH: {base_config_path}")
+        base_config_path = Path(base_config_path)
+
+        # it will find config file in absolute path and relative path
+        if base_config_path.exists():
+            path = base_config_path
+        else:
+            logger.info(
+                f"Can't find BASE_CONFIG_PATH base on: {Path.cwd()}, "
+                f"try using relative path to config path: {Path(config_path).absolute()}"
+            )
+            relative_path = Path(config_path).absolute().parent.joinpath(base_config_path)
+            if relative_path.exists():
+                path = relative_path
+            else:
+                raise FileNotFoundError(f"Can't find the BASE_CONFIG file: {base_config_path}")
+
+        with open(path) as fp:
+            base_config = yaml.safe_load(fp)
+        logger.info(f"Load BASE_CONFIG_PATH succeed: {path.resolve()}")
+        config = update_config(base_config, config)
+
+    # config the `sys` section
+    sys_config(config, config_path)
+
+    if "exp_manager" in config.get("qlib_init"):
+        qlib.init(**config.get("qlib_init"))
+    else:
+        exp_manager = C["exp_manager"]
+        exp_manager["kwargs"]["uri"] = "file:" + str(Path(os.getcwd()).resolve() / uri_folder)
+        qlib.init(**config.get("qlib_init"), exp_manager=exp_manager)
+
+    if "experiment_name" in config:
+        experiment_name = config["experiment_name"]
+    recorder = task_train(config.get("task"), experiment_name=experiment_name)
+    recorder.save_objects(config=config)
+
+
+# function to run workflow by config
+def run():
+    fire.Fire(workflow)
+
+
+if __name__ == "__main__":
+    run()
```

## qlib/workflow/expm.py

 * *Ordering differences only*

```diff
@@ -1,429 +1,429 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-from urllib.parse import urlparse
-import mlflow
-from filelock import FileLock
-from mlflow.exceptions import MlflowException, RESOURCE_ALREADY_EXISTS, ErrorCode
-from mlflow.entities import ViewType
-import os
-from typing import Optional, Text
-
-from .exp import MLflowExperiment, Experiment
-from ..config import C
-from .recorder import Recorder
-from ..log import get_module_logger
-from ..utils.exceptions import ExpAlreadyExistError
-
-
-logger = get_module_logger("workflow")
-
-
-class ExpManager:
-    """
-    This is the `ExpManager` class for managing experiments. The API is designed similar to mlflow.
-    (The link: https://mlflow.org/docs/latest/python_api/mlflow.html)
-
-    The `ExpManager` is expected to be a singleton (btw, we can have multiple `Experiment`s with different uri. user can get different experiments from different uri, and then compare records of them). Global Config (i.e. `C`)  is also a singleton.
-
-    So we try to align them together.  They share the same variable, which is called **default uri**. Please refer to `ExpManager.default_uri` for details of variable sharing.
-
-    When the user starts an experiment, the user may want to set the uri to a specific uri (it will override **default uri** during this period), and then unset the **specific uri** and fallback to the **default uri**.    `ExpManager._active_exp_uri` is that **specific uri**.
-    """
-
-    active_experiment: Optional[Experiment]
-
-    def __init__(self, uri: Text, default_exp_name: Optional[Text]):
-        self.default_uri = uri
-        self._active_exp_uri = None  # No active experiments. So it is set to None
-        self._default_exp_name = default_exp_name
-        self.active_experiment = None  # only one experiment can be active each time
-        logger.debug(f"experiment manager uri is at {self.uri}")
-
-    def __repr__(self):
-        return "{name}(uri={uri})".format(name=self.__class__.__name__, uri=self.uri)
-
-    def start_exp(
-        self,
-        *,
-        experiment_id: Optional[Text] = None,
-        experiment_name: Optional[Text] = None,
-        recorder_id: Optional[Text] = None,
-        recorder_name: Optional[Text] = None,
-        uri: Optional[Text] = None,
-        resume: bool = False,
-        **kwargs,
-    ) -> Experiment:
-        """
-        Start an experiment. This method includes first get_or_create an experiment, and then
-        set it to be active.
-
-        Maintaining `_active_exp_uri` is included in start_exp, remaining implementation should be included in _end_exp in subclass
-
-        Parameters
-        ----------
-        experiment_id : str
-            id of the active experiment.
-        experiment_name : str
-            name of the active experiment.
-        recorder_id : str
-            id of the recorder to be started.
-        recorder_name : str
-            name of the recorder to be started.
-        uri : str
-            the current tracking URI.
-        resume : boolean
-            whether to resume the experiment and recorder.
-
-        Returns
-        -------
-        An active experiment.
-        """
-        self._active_exp_uri = uri
-        # The subclass may set the underlying uri back.
-        # So setting `_active_exp_uri` come before `_start_exp`
-        return self._start_exp(
-            experiment_id=experiment_id,
-            experiment_name=experiment_name,
-            recorder_id=recorder_id,
-            recorder_name=recorder_name,
-            resume=resume,
-            **kwargs,
-        )
-
-    def _start_exp(self, *args, **kwargs) -> Experiment:
-        """Please refer to the doc of `start_exp`"""
-        raise NotImplementedError(f"Please implement the `start_exp` method.")
-
-    def end_exp(self, recorder_status: Text = Recorder.STATUS_S, **kwargs):
-        """
-        End an active experiment.
-
-        Maintaining `_active_exp_uri` is included in end_exp, remaining implementation should be included in _end_exp in subclass
-
-        Parameters
-        ----------
-        experiment_name : str
-            name of the active experiment.
-        recorder_status : str
-            the status of the active recorder of the experiment.
-        """
-        self._active_exp_uri = None
-        # The subclass may set the underlying uri back.
-        # So setting `_active_exp_uri` come before `_end_exp`
-        self._end_exp(recorder_status=recorder_status, **kwargs)
-
-    def _end_exp(self, recorder_status: Text = Recorder.STATUS_S, **kwargs):
-        raise NotImplementedError(f"Please implement the `end_exp` method.")
-
-    def create_exp(self, experiment_name: Optional[Text] = None):
-        """
-        Create an experiment.
-
-        Parameters
-        ----------
-        experiment_name : str
-            the experiment name, which must be unique.
-
-        Returns
-        -------
-        An experiment object.
-
-        Raise
-        -----
-        ExpAlreadyExistError
-        """
-        raise NotImplementedError(f"Please implement the `create_exp` method.")
-
-    def search_records(self, experiment_ids=None, **kwargs):
-        """
-        Get a pandas DataFrame of records that fit the search criteria of the experiment.
-        Inputs are the search criteria user want to apply.
-
-        Returns
-        -------
-        A pandas.DataFrame of records, where each metric, parameter, and tag
-        are expanded into their own columns named metrics.*, params.*, and tags.*
-        respectively. For records that don't have a particular metric, parameter, or tag, their
-        value will be (NumPy) Nan, None, or None respectively.
-        """
-        raise NotImplementedError(f"Please implement the `search_records` method.")
-
-    def get_exp(self, *, experiment_id=None, experiment_name=None, create: bool = True, start: bool = False):
-        """
-        Retrieve an experiment. This method includes getting an active experiment, and get_or_create a specific experiment.
-
-        When user specify experiment id and name, the method will try to return the specific experiment.
-        When user does not provide recorder id or name, the method will try to return the current active experiment.
-        The `create` argument determines whether the method will automatically create a new experiment according
-        to user's specification if the experiment hasn't been created before.
-
-        * If `create` is True:
-
-            * If `active experiment` exists:
-
-                * no id or name specified, return the active experiment.
-                * if id or name is specified, return the specified experiment. If no such exp found, create a new experiment with given id or name. If `start` is set to be True, the experiment is set to be active.
-
-            * If `active experiment` not exists:
-
-                * no id or name specified, create a default experiment.
-                * if id or name is specified, return the specified experiment. If no such exp found, create a new experiment with given id or name. If `start` is set to be True, the experiment is set to be active.
-
-        * Else If `create` is False:
-
-            * If `active experiment` exists:
-
-                * no id or name specified, return the active experiment.
-                * if id or name is specified, return the specified experiment. If no such exp found, raise Error.
-
-            * If `active experiment` not exists:
-
-                *  no id or name specified. If the default experiment exists, return it, otherwise, raise Error.
-                * if id or name is specified, return the specified experiment. If no such exp found, raise Error.
-
-        Parameters
-        ----------
-        experiment_id : str
-            id of the experiment to return.
-        experiment_name : str
-            name of the experiment to return.
-        create : boolean
-            create the experiment it if hasn't been created before.
-        start : boolean
-            start the new experiment if one is created.
-
-        Returns
-        -------
-        An experiment object.
-        """
-        # special case of getting experiment
-        if experiment_id is None and experiment_name is None:
-            if self.active_experiment is not None:
-                return self.active_experiment
-            # User don't want get active code now.
-            experiment_name = self._default_exp_name
-
-        if create:
-            exp, _ = self._get_or_create_exp(experiment_id=experiment_id, experiment_name=experiment_name)
-        else:
-            exp = self._get_exp(experiment_id=experiment_id, experiment_name=experiment_name)
-        if self.active_experiment is None and start:
-            self.active_experiment = exp
-            # start the recorder
-            self.active_experiment.start()
-        return exp
-
-    def _get_or_create_exp(self, experiment_id=None, experiment_name=None) -> (object, bool):
-        """
-        Method for getting or creating an experiment. It will try to first get a valid experiment, if exception occurs, it will
-        automatically create a new experiment based on the given id and name.
-        """
-        try:
-            return (
-                self._get_exp(experiment_id=experiment_id, experiment_name=experiment_name),
-                False,
-            )
-        except ValueError:
-            if experiment_name is None:
-                experiment_name = self._default_exp_name
-            logger.warning(f"No valid experiment found. Create a new experiment with name {experiment_name}.")
-
-            # NOTE: mlflow doesn't consider the lock for recording multiple runs
-            # So we supported it in the interface wrapper
-            pr = urlparse(self.uri)
-            if pr.scheme == "file":
-                with FileLock(os.path.join(pr.netloc, pr.path, "filelock")):  # pylint: disable=E0110
-                    return self.create_exp(experiment_name), True
-            # NOTE: for other schemes like http, we double check to avoid create exp conflicts
-            try:
-                return self.create_exp(experiment_name), True
-            except ExpAlreadyExistError:
-                return (
-                    self._get_exp(experiment_id=experiment_id, experiment_name=experiment_name),
-                    False,
-                )
-
-    def _get_exp(self, experiment_id=None, experiment_name=None) -> Experiment:
-        """
-        Get specific experiment by name or id. If it does not exist, raise ValueError.
-
-        Parameters
-        ----------
-        experiment_id :
-            The id of experiment
-        experiment_name :
-            The name of experiment
-
-        Returns
-        -------
-        Experiment:
-            The searched experiment
-
-        Raises
-        ------
-        ValueError
-        """
-        raise NotImplementedError(f"Please implement the `_get_exp` method")
-
-    def delete_exp(self, experiment_id=None, experiment_name=None):
-        """
-        Delete an experiment.
-
-        Parameters
-        ----------
-        experiment_id  : str
-            the experiment id.
-        experiment_name  : str
-            the experiment name.
-        """
-        raise NotImplementedError(f"Please implement the `delete_exp` method.")
-
-    @property
-    def default_uri(self):
-        """
-        Get the default tracking URI from qlib.config.C
-        """
-        if "kwargs" not in C.exp_manager or "uri" not in C.exp_manager["kwargs"]:
-            raise ValueError("The default URI is not set in qlib.config.C")
-        return C.exp_manager["kwargs"]["uri"]
-
-    @default_uri.setter
-    def default_uri(self, value):
-        C.exp_manager.setdefault("kwargs", {})["uri"] = value
-
-    @property
-    def uri(self):
-        """
-        Get the default tracking URI or current URI.
-
-        Returns
-        -------
-        The tracking URI string.
-        """
-        return self._active_exp_uri or self.default_uri
-
-    def list_experiments(self):
-        """
-        List all the existing experiments.
-
-        Returns
-        -------
-        A dictionary (name -> experiment) of experiments information that being stored.
-        """
-        raise NotImplementedError(f"Please implement the `list_experiments` method.")
-
-
-class MLflowExpManager(ExpManager):
-    """
-    Use mlflow to implement ExpManager.
-    """
-
-    @property
-    def client(self):
-        # Please refer to `tests/dependency_tests/test_mlflow.py::MLflowTest::test_creating_client`
-        # The test ensure the speed of create a new client
-        return mlflow.tracking.MlflowClient(tracking_uri=self.uri)
-
-    def _start_exp(
-        self,
-        *,
-        experiment_id: Optional[Text] = None,
-        experiment_name: Optional[Text] = None,
-        recorder_id: Optional[Text] = None,
-        recorder_name: Optional[Text] = None,
-        resume: bool = False,
-    ):
-        # Create experiment
-        if experiment_name is None:
-            experiment_name = self._default_exp_name
-        experiment, _ = self._get_or_create_exp(experiment_id=experiment_id, experiment_name=experiment_name)
-        # Set up active experiment
-        self.active_experiment = experiment
-        # Start the experiment
-        self.active_experiment.start(recorder_id=recorder_id, recorder_name=recorder_name, resume=resume)
-
-        return self.active_experiment
-
-    def _end_exp(self, recorder_status: Text = Recorder.STATUS_S):
-        if self.active_experiment is not None:
-            self.active_experiment.end(recorder_status)
-            self.active_experiment = None
-
-    def create_exp(self, experiment_name: Optional[Text] = None):
-        assert experiment_name is not None
-        # init experiment
-        try:
-            experiment_id = self.client.create_experiment(experiment_name)
-        except MlflowException as e:
-            if e.error_code == ErrorCode.Name(RESOURCE_ALREADY_EXISTS):
-                raise ExpAlreadyExistError() from e
-            raise e
-
-        return MLflowExperiment(experiment_id, experiment_name, self.uri)
-
-    def _get_exp(self, experiment_id=None, experiment_name=None):
-        """
-        Method for getting or creating an experiment. It will try to first get a valid experiment, if exception occurs, it will
-        raise errors.
-        """
-        assert (
-            experiment_id is not None or experiment_name is not None
-        ), "Please input at least one of experiment/recorder id or name before retrieving experiment/recorder."
-        if experiment_id is not None:
-            try:
-                # NOTE: the mlflow's experiment_id must be str type...
-                # https://www.mlflow.org/docs/latest/python_api/mlflow.tracking.html#mlflow.tracking.MlflowClient.get_experiment
-                exp = self.client.get_experiment(experiment_id)
-                if exp.lifecycle_stage.upper() == "DELETED":
-                    raise MlflowException("No valid experiment has been found.")
-                experiment = MLflowExperiment(exp.experiment_id, exp.name, self.uri)
-                return experiment
-            except MlflowException as e:
-                raise ValueError(
-                    "No valid experiment has been found, please make sure the input experiment id is correct."
-                ) from e
-        elif experiment_name is not None:
-            try:
-                exp = self.client.get_experiment_by_name(experiment_name)
-                if exp is None or exp.lifecycle_stage.upper() == "DELETED":
-                    raise MlflowException("No valid experiment has been found.")
-                experiment = MLflowExperiment(exp.experiment_id, experiment_name, self.uri)
-                return experiment
-            except MlflowException as e:
-                raise ValueError(
-                    "No valid experiment has been found, please make sure the input experiment name is correct."
-                ) from e
-
-    def search_records(self, experiment_ids=None, **kwargs):
-        filter_string = "" if kwargs.get("filter_string") is None else kwargs.get("filter_string")
-        run_view_type = 1 if kwargs.get("run_view_type") is None else kwargs.get("run_view_type")
-        max_results = 100000 if kwargs.get("max_results") is None else kwargs.get("max_results")
-        order_by = kwargs.get("order_by")
-        return self.client.search_runs(experiment_ids, filter_string, run_view_type, max_results, order_by)
-
-    def delete_exp(self, experiment_id=None, experiment_name=None):
-        assert (
-            experiment_id is not None or experiment_name is not None
-        ), "Please input a valid experiment id or name before deleting."
-        try:
-            if experiment_id is not None:
-                self.client.delete_experiment(experiment_id)
-            else:
-                experiment = self.client.get_experiment_by_name(experiment_name)
-                if experiment is None:
-                    raise MlflowException("No valid experiment has been found.")
-                self.client.delete_experiment(experiment.experiment_id)
-        except MlflowException as e:
-            raise ValueError(
-                f"Error: {e}. Something went wrong when deleting experiment. Please check if the name/id of the experiment is correct."
-            ) from e
-
-    def list_experiments(self):
-        # retrieve all the existing experiments
-        exps = self.client.list_experiments(view_type=ViewType.ACTIVE_ONLY)
-        experiments = dict()
-        for exp in exps:
-            experiment = MLflowExperiment(exp.experiment_id, exp.name, self.uri)
-            experiments[exp.name] = experiment
-        return experiments
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+from urllib.parse import urlparse
+import mlflow
+from filelock import FileLock
+from mlflow.exceptions import MlflowException, RESOURCE_ALREADY_EXISTS, ErrorCode
+from mlflow.entities import ViewType
+import os
+from typing import Optional, Text
+
+from .exp import MLflowExperiment, Experiment
+from ..config import C
+from .recorder import Recorder
+from ..log import get_module_logger
+from ..utils.exceptions import ExpAlreadyExistError
+
+
+logger = get_module_logger("workflow")
+
+
+class ExpManager:
+    """
+    This is the `ExpManager` class for managing experiments. The API is designed similar to mlflow.
+    (The link: https://mlflow.org/docs/latest/python_api/mlflow.html)
+
+    The `ExpManager` is expected to be a singleton (btw, we can have multiple `Experiment`s with different uri. user can get different experiments from different uri, and then compare records of them). Global Config (i.e. `C`)  is also a singleton.
+
+    So we try to align them together.  They share the same variable, which is called **default uri**. Please refer to `ExpManager.default_uri` for details of variable sharing.
+
+    When the user starts an experiment, the user may want to set the uri to a specific uri (it will override **default uri** during this period), and then unset the **specific uri** and fallback to the **default uri**.    `ExpManager._active_exp_uri` is that **specific uri**.
+    """
+
+    active_experiment: Optional[Experiment]
+
+    def __init__(self, uri: Text, default_exp_name: Optional[Text]):
+        self.default_uri = uri
+        self._active_exp_uri = None  # No active experiments. So it is set to None
+        self._default_exp_name = default_exp_name
+        self.active_experiment = None  # only one experiment can be active each time
+        logger.debug(f"experiment manager uri is at {self.uri}")
+
+    def __repr__(self):
+        return "{name}(uri={uri})".format(name=self.__class__.__name__, uri=self.uri)
+
+    def start_exp(
+        self,
+        *,
+        experiment_id: Optional[Text] = None,
+        experiment_name: Optional[Text] = None,
+        recorder_id: Optional[Text] = None,
+        recorder_name: Optional[Text] = None,
+        uri: Optional[Text] = None,
+        resume: bool = False,
+        **kwargs,
+    ) -> Experiment:
+        """
+        Start an experiment. This method includes first get_or_create an experiment, and then
+        set it to be active.
+
+        Maintaining `_active_exp_uri` is included in start_exp, remaining implementation should be included in _end_exp in subclass
+
+        Parameters
+        ----------
+        experiment_id : str
+            id of the active experiment.
+        experiment_name : str
+            name of the active experiment.
+        recorder_id : str
+            id of the recorder to be started.
+        recorder_name : str
+            name of the recorder to be started.
+        uri : str
+            the current tracking URI.
+        resume : boolean
+            whether to resume the experiment and recorder.
+
+        Returns
+        -------
+        An active experiment.
+        """
+        self._active_exp_uri = uri
+        # The subclass may set the underlying uri back.
+        # So setting `_active_exp_uri` come before `_start_exp`
+        return self._start_exp(
+            experiment_id=experiment_id,
+            experiment_name=experiment_name,
+            recorder_id=recorder_id,
+            recorder_name=recorder_name,
+            resume=resume,
+            **kwargs,
+        )
+
+    def _start_exp(self, *args, **kwargs) -> Experiment:
+        """Please refer to the doc of `start_exp`"""
+        raise NotImplementedError(f"Please implement the `start_exp` method.")
+
+    def end_exp(self, recorder_status: Text = Recorder.STATUS_S, **kwargs):
+        """
+        End an active experiment.
+
+        Maintaining `_active_exp_uri` is included in end_exp, remaining implementation should be included in _end_exp in subclass
+
+        Parameters
+        ----------
+        experiment_name : str
+            name of the active experiment.
+        recorder_status : str
+            the status of the active recorder of the experiment.
+        """
+        self._active_exp_uri = None
+        # The subclass may set the underlying uri back.
+        # So setting `_active_exp_uri` come before `_end_exp`
+        self._end_exp(recorder_status=recorder_status, **kwargs)
+
+    def _end_exp(self, recorder_status: Text = Recorder.STATUS_S, **kwargs):
+        raise NotImplementedError(f"Please implement the `end_exp` method.")
+
+    def create_exp(self, experiment_name: Optional[Text] = None):
+        """
+        Create an experiment.
+
+        Parameters
+        ----------
+        experiment_name : str
+            the experiment name, which must be unique.
+
+        Returns
+        -------
+        An experiment object.
+
+        Raise
+        -----
+        ExpAlreadyExistError
+        """
+        raise NotImplementedError(f"Please implement the `create_exp` method.")
+
+    def search_records(self, experiment_ids=None, **kwargs):
+        """
+        Get a pandas DataFrame of records that fit the search criteria of the experiment.
+        Inputs are the search criteria user want to apply.
+
+        Returns
+        -------
+        A pandas.DataFrame of records, where each metric, parameter, and tag
+        are expanded into their own columns named metrics.*, params.*, and tags.*
+        respectively. For records that don't have a particular metric, parameter, or tag, their
+        value will be (NumPy) Nan, None, or None respectively.
+        """
+        raise NotImplementedError(f"Please implement the `search_records` method.")
+
+    def get_exp(self, *, experiment_id=None, experiment_name=None, create: bool = True, start: bool = False):
+        """
+        Retrieve an experiment. This method includes getting an active experiment, and get_or_create a specific experiment.
+
+        When user specify experiment id and name, the method will try to return the specific experiment.
+        When user does not provide recorder id or name, the method will try to return the current active experiment.
+        The `create` argument determines whether the method will automatically create a new experiment according
+        to user's specification if the experiment hasn't been created before.
+
+        * If `create` is True:
+
+            * If `active experiment` exists:
+
+                * no id or name specified, return the active experiment.
+                * if id or name is specified, return the specified experiment. If no such exp found, create a new experiment with given id or name. If `start` is set to be True, the experiment is set to be active.
+
+            * If `active experiment` not exists:
+
+                * no id or name specified, create a default experiment.
+                * if id or name is specified, return the specified experiment. If no such exp found, create a new experiment with given id or name. If `start` is set to be True, the experiment is set to be active.
+
+        * Else If `create` is False:
+
+            * If `active experiment` exists:
+
+                * no id or name specified, return the active experiment.
+                * if id or name is specified, return the specified experiment. If no such exp found, raise Error.
+
+            * If `active experiment` not exists:
+
+                *  no id or name specified. If the default experiment exists, return it, otherwise, raise Error.
+                * if id or name is specified, return the specified experiment. If no such exp found, raise Error.
+
+        Parameters
+        ----------
+        experiment_id : str
+            id of the experiment to return.
+        experiment_name : str
+            name of the experiment to return.
+        create : boolean
+            create the experiment it if hasn't been created before.
+        start : boolean
+            start the new experiment if one is created.
+
+        Returns
+        -------
+        An experiment object.
+        """
+        # special case of getting experiment
+        if experiment_id is None and experiment_name is None:
+            if self.active_experiment is not None:
+                return self.active_experiment
+            # User don't want get active code now.
+            experiment_name = self._default_exp_name
+
+        if create:
+            exp, _ = self._get_or_create_exp(experiment_id=experiment_id, experiment_name=experiment_name)
+        else:
+            exp = self._get_exp(experiment_id=experiment_id, experiment_name=experiment_name)
+        if self.active_experiment is None and start:
+            self.active_experiment = exp
+            # start the recorder
+            self.active_experiment.start()
+        return exp
+
+    def _get_or_create_exp(self, experiment_id=None, experiment_name=None) -> (object, bool):
+        """
+        Method for getting or creating an experiment. It will try to first get a valid experiment, if exception occurs, it will
+        automatically create a new experiment based on the given id and name.
+        """
+        try:
+            return (
+                self._get_exp(experiment_id=experiment_id, experiment_name=experiment_name),
+                False,
+            )
+        except ValueError:
+            if experiment_name is None:
+                experiment_name = self._default_exp_name
+            logger.warning(f"No valid experiment found. Create a new experiment with name {experiment_name}.")
+
+            # NOTE: mlflow doesn't consider the lock for recording multiple runs
+            # So we supported it in the interface wrapper
+            pr = urlparse(self.uri)
+            if pr.scheme == "file":
+                with FileLock(os.path.join(pr.netloc, pr.path, "filelock")):  # pylint: disable=E0110
+                    return self.create_exp(experiment_name), True
+            # NOTE: for other schemes like http, we double check to avoid create exp conflicts
+            try:
+                return self.create_exp(experiment_name), True
+            except ExpAlreadyExistError:
+                return (
+                    self._get_exp(experiment_id=experiment_id, experiment_name=experiment_name),
+                    False,
+                )
+
+    def _get_exp(self, experiment_id=None, experiment_name=None) -> Experiment:
+        """
+        Get specific experiment by name or id. If it does not exist, raise ValueError.
+
+        Parameters
+        ----------
+        experiment_id :
+            The id of experiment
+        experiment_name :
+            The name of experiment
+
+        Returns
+        -------
+        Experiment:
+            The searched experiment
+
+        Raises
+        ------
+        ValueError
+        """
+        raise NotImplementedError(f"Please implement the `_get_exp` method")
+
+    def delete_exp(self, experiment_id=None, experiment_name=None):
+        """
+        Delete an experiment.
+
+        Parameters
+        ----------
+        experiment_id  : str
+            the experiment id.
+        experiment_name  : str
+            the experiment name.
+        """
+        raise NotImplementedError(f"Please implement the `delete_exp` method.")
+
+    @property
+    def default_uri(self):
+        """
+        Get the default tracking URI from qlib.config.C
+        """
+        if "kwargs" not in C.exp_manager or "uri" not in C.exp_manager["kwargs"]:
+            raise ValueError("The default URI is not set in qlib.config.C")
+        return C.exp_manager["kwargs"]["uri"]
+
+    @default_uri.setter
+    def default_uri(self, value):
+        C.exp_manager.setdefault("kwargs", {})["uri"] = value
+
+    @property
+    def uri(self):
+        """
+        Get the default tracking URI or current URI.
+
+        Returns
+        -------
+        The tracking URI string.
+        """
+        return self._active_exp_uri or self.default_uri
+
+    def list_experiments(self):
+        """
+        List all the existing experiments.
+
+        Returns
+        -------
+        A dictionary (name -> experiment) of experiments information that being stored.
+        """
+        raise NotImplementedError(f"Please implement the `list_experiments` method.")
+
+
+class MLflowExpManager(ExpManager):
+    """
+    Use mlflow to implement ExpManager.
+    """
+
+    @property
+    def client(self):
+        # Please refer to `tests/dependency_tests/test_mlflow.py::MLflowTest::test_creating_client`
+        # The test ensure the speed of create a new client
+        return mlflow.tracking.MlflowClient(tracking_uri=self.uri)
+
+    def _start_exp(
+        self,
+        *,
+        experiment_id: Optional[Text] = None,
+        experiment_name: Optional[Text] = None,
+        recorder_id: Optional[Text] = None,
+        recorder_name: Optional[Text] = None,
+        resume: bool = False,
+    ):
+        # Create experiment
+        if experiment_name is None:
+            experiment_name = self._default_exp_name
+        experiment, _ = self._get_or_create_exp(experiment_id=experiment_id, experiment_name=experiment_name)
+        # Set up active experiment
+        self.active_experiment = experiment
+        # Start the experiment
+        self.active_experiment.start(recorder_id=recorder_id, recorder_name=recorder_name, resume=resume)
+
+        return self.active_experiment
+
+    def _end_exp(self, recorder_status: Text = Recorder.STATUS_S):
+        if self.active_experiment is not None:
+            self.active_experiment.end(recorder_status)
+            self.active_experiment = None
+
+    def create_exp(self, experiment_name: Optional[Text] = None):
+        assert experiment_name is not None
+        # init experiment
+        try:
+            experiment_id = self.client.create_experiment(experiment_name)
+        except MlflowException as e:
+            if e.error_code == ErrorCode.Name(RESOURCE_ALREADY_EXISTS):
+                raise ExpAlreadyExistError() from e
+            raise e
+
+        return MLflowExperiment(experiment_id, experiment_name, self.uri)
+
+    def _get_exp(self, experiment_id=None, experiment_name=None):
+        """
+        Method for getting or creating an experiment. It will try to first get a valid experiment, if exception occurs, it will
+        raise errors.
+        """
+        assert (
+            experiment_id is not None or experiment_name is not None
+        ), "Please input at least one of experiment/recorder id or name before retrieving experiment/recorder."
+        if experiment_id is not None:
+            try:
+                # NOTE: the mlflow's experiment_id must be str type...
+                # https://www.mlflow.org/docs/latest/python_api/mlflow.tracking.html#mlflow.tracking.MlflowClient.get_experiment
+                exp = self.client.get_experiment(experiment_id)
+                if exp.lifecycle_stage.upper() == "DELETED":
+                    raise MlflowException("No valid experiment has been found.")
+                experiment = MLflowExperiment(exp.experiment_id, exp.name, self.uri)
+                return experiment
+            except MlflowException as e:
+                raise ValueError(
+                    "No valid experiment has been found, please make sure the input experiment id is correct."
+                ) from e
+        elif experiment_name is not None:
+            try:
+                exp = self.client.get_experiment_by_name(experiment_name)
+                if exp is None or exp.lifecycle_stage.upper() == "DELETED":
+                    raise MlflowException("No valid experiment has been found.")
+                experiment = MLflowExperiment(exp.experiment_id, experiment_name, self.uri)
+                return experiment
+            except MlflowException as e:
+                raise ValueError(
+                    "No valid experiment has been found, please make sure the input experiment name is correct."
+                ) from e
+
+    def search_records(self, experiment_ids=None, **kwargs):
+        filter_string = "" if kwargs.get("filter_string") is None else kwargs.get("filter_string")
+        run_view_type = 1 if kwargs.get("run_view_type") is None else kwargs.get("run_view_type")
+        max_results = 100000 if kwargs.get("max_results") is None else kwargs.get("max_results")
+        order_by = kwargs.get("order_by")
+        return self.client.search_runs(experiment_ids, filter_string, run_view_type, max_results, order_by)
+
+    def delete_exp(self, experiment_id=None, experiment_name=None):
+        assert (
+            experiment_id is not None or experiment_name is not None
+        ), "Please input a valid experiment id or name before deleting."
+        try:
+            if experiment_id is not None:
+                self.client.delete_experiment(experiment_id)
+            else:
+                experiment = self.client.get_experiment_by_name(experiment_name)
+                if experiment is None:
+                    raise MlflowException("No valid experiment has been found.")
+                self.client.delete_experiment(experiment.experiment_id)
+        except MlflowException as e:
+            raise ValueError(
+                f"Error: {e}. Something went wrong when deleting experiment. Please check if the name/id of the experiment is correct."
+            ) from e
+
+    def list_experiments(self):
+        # retrieve all the existing experiments
+        exps = self.client.list_experiments(view_type=ViewType.ACTIVE_ONLY)
+        experiments = dict()
+        for exp in exps:
+            experiment = MLflowExperiment(exp.experiment_id, exp.name, self.uri)
+            experiments[exp.name] = experiment
+        return experiments
```

## qlib/workflow/record_temp.py

```diff
@@ -1,556 +1,555 @@
-#  Copyright (c) Microsoft Corporation.
-#  Licensed under the MIT License.
-
-import logging
-import warnings
-import pandas as pd
-from pprint import pprint
-from typing import Union, List, Optional
-
-from qlib.utils.exceptions import LoadObjectError
-from ..contrib.evaluate import risk_analysis, indicator_analysis
-
-from ..data.dataset import DatasetH
-from ..data.dataset.handler import DataHandlerLP
-from ..backtest import backtest as normal_backtest
-from ..log import get_module_logger
-from ..utils import fill_placeholder, flatten_dict, class_casting, get_date_by_shift
-from ..utils.time import Freq
-from ..utils.data import deepcopy_basic_type
-from ..contrib.eva.alpha import calc_ic, calc_long_short_return, calc_long_short_prec
-
-
-logger = get_module_logger("workflow", logging.INFO)
-
-
-class RecordTemp:
-    """
-    This is the Records Template class that enables user to generate experiment results such as IC and
-    backtest in a certain format.
-    """
-
-    artifact_path = None
-    depend_cls = None  # the dependant class of the record; the record will depend on the results generated by
-    # `depend_cls`
-
-    @classmethod
-    def get_path(cls, path=None):
-        names = []
-        if cls.artifact_path is not None:
-            names.append(cls.artifact_path)
-
-        if path is not None:
-            names.append(path)
-
-        return "/".join(names)
-
-    def save(self, **kwargs):
-        """
-        It behaves the same as self.recorder.save_objects.
-        But it is an easier interface because users don't have to care about `get_path` and `artifact_path`
-        """
-        art_path = self.get_path()
-        if art_path == "":
-            art_path = None
-        self.recorder.save_objects(artifact_path=art_path, **kwargs)
-
-    def __init__(self, recorder):
-        self._recorder = recorder
-
-    @property
-    def recorder(self):
-        if self._recorder is None:
-            raise ValueError("This RecordTemp did not set recorder yet.")
-        return self._recorder
-
-    def generate(self, **kwargs):
-        """
-        Generate certain records such as IC, backtest etc., and save them.
-
-        Parameters
-        ----------
-        kwargs
-
-        Return
-        ------
-        """
-        raise NotImplementedError(f"Please implement the `generate` method.")
-
-    def load(self, name: str, parents: bool = True):
-        """
-        It behaves the same as self.recorder.load_object.
-        But it is an easier interface because users don't have to care about `get_path` and `artifact_path`
-
-        Parameters
-        ----------
-        name : str
-            the name for the file to be load.
-
-        parents : bool
-            Each recorder has different `artifact_path`.
-            So parents recursively find the path in parents
-            Sub classes has higher priority
-
-        Return
-        ------
-        The stored records.
-        """
-        try:
-            return self.recorder.load_object(self.get_path(name))
-        except LoadObjectError as e:
-            if parents:
-                if self.depend_cls is not None:
-                    with class_casting(self, self.depend_cls):
-                        return self.load(name, parents=True)
-            raise e
-
-    def list(self):
-        """
-        List the supported artifacts.
-        Users don't have to consider self.get_path
-
-        Return
-        ------
-        A list of all the supported artifacts.
-        """
-        return []
-
-    def check(self, include_self: bool = False, parents: bool = True):
-        """
-        Check if the records is properly generated and saved.
-        It is useful in following examples
-
-        - checking if the dependant files complete before generating new things.
-        - checking if the final files is completed
-
-        Parameters
-        ----------
-        include_self : bool
-            is the file generated by self included
-        parents : bool
-            will we check parents
-
-        Raise
-        ------
-        FileNotFoundError
-            whether the records are stored properly.
-        """
-        if include_self:
-
-            # Some mlflow backend will not list the directly recursively.
-            # So we force to the directly
-            artifacts = {}
-
-            def _get_arts(dirn):
-                if dirn not in artifacts:
-                    artifacts[dirn] = self.recorder.list_artifacts(dirn)
-                return artifacts[dirn]
-
-            for item in self.list():
-                ps = self.get_path(item).split("/")
-                dirn = "/".join(ps[:-1])
-                if self.get_path(item) not in _get_arts(dirn):
-                    raise FileNotFoundError
-        if parents:
-            if self.depend_cls is not None:
-                with class_casting(self, self.depend_cls):
-                    self.check(include_self=True)
-
-
-class SignalRecord(RecordTemp):
-    """
-    This is the Signal Record class that generates the signal prediction. This class inherits the ``RecordTemp`` class.
-    """
-
-    def __init__(self, model=None, dataset=None, recorder=None):
-        super().__init__(recorder=recorder)
-        self.model = model
-        self.dataset = dataset
-
-    @staticmethod
-    def generate_label(dataset):
-        with class_casting(dataset, DatasetH):
-            params = dict(segments="test", col_set="label", data_key=DataHandlerLP.DK_R)
-            try:
-                # Assume the backend handler is DataHandlerLP
-                raw_label = dataset.prepare(**params)
-            except TypeError:
-                # The argument number is not right
-                del params["data_key"]
-                # The backend handler should be DataHandler
-                raw_label = dataset.prepare(**params)
-            except AttributeError as e:
-                # The data handler is initialized with `drop_raw=True`...
-                # So raw_label is not available
-                logger.warning(f"Exception: {e}")
-                raw_label = None
-        return raw_label
-
-    def generate(self, **kwargs):
-        # generate prediction
-        pred = self.model.predict(self.dataset)
-        if isinstance(pred, pd.Series):
-            pred = pred.to_frame("score")
-        self.save(**{"pred.pkl": pred})
-
-        logger.info(
-            f"Signal record 'pred.pkl' has been saved as the artifact of the Experiment {self.recorder.experiment_id}"
-        )
-        # print out results
-        pprint(f"The following are prediction results of the {type(self.model).__name__} model.")
-        pprint(pred.head(5))
-
-        if isinstance(self.dataset, DatasetH):
-            raw_label = self.generate_label(self.dataset)
-            self.save(**{"label.pkl": raw_label})
-
-    def list(self):
-        return ["pred.pkl", "label.pkl"]
-
-
-class ACRecordTemp(RecordTemp):
-    """Automatically checking record template"""
-
-    def __init__(self, recorder, skip_existing=False):
-        self.skip_existing = skip_existing
-        super().__init__(recorder=recorder)
-
-    def generate(self, *args, **kwargs):
-        """automatically checking the files and then run the concrete generating task"""
-        if self.skip_existing:
-            try:
-                self.check(include_self=True, parents=False)
-            except FileNotFoundError:
-                pass  # continue to generating metrics
-            else:
-                logger.info("The results has previously generated, Generation skipped.")
-                return
-
-        try:
-            self.check()
-        except FileNotFoundError:
-            logger.warning("The dependent data does not exists. Generation skipped.")
-            return
-        return self._generate(*args, **kwargs)
-
-    def _generate(self, *args, **kwargs):
-        raise NotImplementedError(f"Please implement the `_generate` method")
-
-
-class HFSignalRecord(SignalRecord):
-    """
-    This is the Signal Analysis Record class that generates the analysis results such as IC and IR. This class inherits the ``RecordTemp`` class.
-    """
-
-    artifact_path = "hg_sig_analysis"
-    depend_cls = SignalRecord
-
-    def __init__(self, recorder, **kwargs):
-        super().__init__(recorder=recorder)
-
-    def generate(self):
-        pred = self.load("pred.pkl")
-        raw_label = self.load("label.pkl")
-        long_pre, short_pre = calc_long_short_prec(pred.iloc[:, 0], raw_label.iloc[:, 0], is_alpha=True)
-        ic, ric = calc_ic(pred.iloc[:, 0], raw_label.iloc[:, 0])
-        metrics = {
-            "IC": ic.mean(),
-            "ICIR": ic.mean() / ic.std(),
-            "Rank IC": ric.mean(),
-            "Rank ICIR": ric.mean() / ric.std(),
-            "Long precision": long_pre.mean(),
-            "Short precision": short_pre.mean(),
-        }
-        objects = {"ic.pkl": ic, "ric.pkl": ric}
-        objects.update({"long_pre.pkl": long_pre, "short_pre.pkl": short_pre})
-        long_short_r, long_avg_r = calc_long_short_return(pred.iloc[:, 0], raw_label.iloc[:, 0])
-        metrics.update(
-            {
-                "Long-Short Average Return": long_short_r.mean(),
-                "Long-Short Average Sharpe": long_short_r.mean() / long_short_r.std(),
-            }
-        )
-        objects.update(
-            {
-                "long_short_r.pkl": long_short_r,
-                "long_avg_r.pkl": long_avg_r,
-            }
-        )
-        self.recorder.log_metrics(**metrics)
-        self.save(**objects)
-        pprint(metrics)
-
-    def list(self):
-        return ["ic.pkl", "ric.pkl", "long_pre.pkl", "short_pre.pkl", "long_short_r.pkl", "long_avg_r.pkl"]
-
-
-class SigAnaRecord(ACRecordTemp):
-    """
-    This is the Signal Analysis Record class that generates the analysis results such as IC and IR.
-    This class inherits the ``RecordTemp`` class.
-    """
-
-    artifact_path = "sig_analysis"
-    depend_cls = SignalRecord
-
-    def __init__(self, recorder, ana_long_short=False, ann_scaler=252, label_col=0, skip_existing=False):
-        super().__init__(recorder=recorder, skip_existing=skip_existing)
-        self.ana_long_short = ana_long_short
-        self.ann_scaler = ann_scaler
-        self.label_col = label_col
-
-    def _generate(self, label: Optional[pd.DataFrame] = None, **kwargs):
-        """
-        Parameters
-        ----------
-        label : Optional[pd.DataFrame]
-            Label should be a dataframe.
-        """
-        pred = self.load("pred.pkl")
-        if label is None:
-            label = self.load("label.pkl")
-        if label is None or not isinstance(label, pd.DataFrame) or label.empty:
-            logger.warning(f"Empty label.")
-            return
-        ic, ric = calc_ic(pred.iloc[:, 0], label.iloc[:, self.label_col])
-        metrics = {
-            "IC": ic.mean(),
-            "ICIR": ic.mean() / ic.std(),
-            "Rank IC": ric.mean(),
-            "Rank ICIR": ric.mean() / ric.std(),
-        }
-        objects = {"ic.pkl": ic, "ric.pkl": ric}
-        if self.ana_long_short:
-            long_short_r, long_avg_r = calc_long_short_return(pred.iloc[:, 0], label.iloc[:, self.label_col])
-            metrics.update(
-                {
-                    "Long-Short Ann Return": long_short_r.mean() * self.ann_scaler,
-                    "Long-Short Ann Sharpe": long_short_r.mean() / long_short_r.std() * self.ann_scaler**0.5,
-                    "Long-Avg Ann Return": long_avg_r.mean() * self.ann_scaler,
-                    "Long-Avg Ann Sharpe": long_avg_r.mean() / long_avg_r.std() * self.ann_scaler**0.5,
-                }
-            )
-            objects.update(
-                {
-                    "long_short_r.pkl": long_short_r,
-                    "long_avg_r.pkl": long_avg_r,
-                }
-            )
-        self.recorder.log_metrics(**metrics)
-        self.save(**objects)
-        pprint(metrics)
-
-    def list(self):
-        paths = ["ic.pkl", "ric.pkl"]
-        if self.ana_long_short:
-            paths.extend(["long_short_r.pkl", "long_avg_r.pkl"])
-        return paths
-
-
-class PortAnaRecord(ACRecordTemp):
-    """
-    This is the Portfolio Analysis Record class that generates the analysis results such as those of backtest. This class inherits the ``RecordTemp`` class.
-
-    The following files will be stored in recorder
-
-    - report_normal.pkl & positions_normal.pkl:
-
-        - The return report and detailed positions of the backtest, returned by `qlib/contrib/evaluate.py:backtest`
-    - port_analysis.pkl : The risk analysis of your portfolio, returned by `qlib/contrib/evaluate.py:risk_analysis`
-    """
-
-    artifact_path = "portfolio_analysis"
-    depend_cls = SignalRecord
-
-    def __init__(
-        self,
-        recorder,
-        config=None,
-        risk_analysis_freq: Union[List, str] = None,
-        indicator_analysis_freq: Union[List, str] = None,
-        indicator_analysis_method=None,
-        skip_existing=False,
-        **kwargs,
-    ):
-        """
-        config["strategy"] : dict
-            define the strategy class as well as the kwargs.
-        config["executor"] : dict
-            define the executor class as well as the kwargs.
-        config["backtest"] : dict
-            define the backtest kwargs.
-        risk_analysis_freq : str|List[str]
-            risk analysis freq of report
-        indicator_analysis_freq : str|List[str]
-            indicator analysis freq of report
-        indicator_analysis_method : str, optional, default by None
-            the candidate values include 'mean', 'amount_weighted', 'value_weighted'
-        """
-        super().__init__(recorder=recorder, skip_existing=skip_existing, **kwargs)
-
-        if config is None:
-            config = {  # Default config for daily trading
-                "strategy": {
-                    "class": "TopkDropoutStrategy",
-                    "module_path": "qlib.contrib.strategy",
-                    "kwargs": {"signal": "<PRED>", "topk": 50, "n_drop": 5},
-                },
-                "backtest": {
-                    "start_time": None,
-                    "end_time": None,
-                    "account": 100000000,
-                    "benchmark": "SH000300",
-                    "exchange_kwargs": {
-                        "limit_threshold": 0.095,
-                        "deal_price": "close",
-                        "open_cost": 0.0005,
-                        "close_cost": 0.0015,
-                        "min_cost": 5,
-                    },
-                },
-            }
-        # We only deepcopy_basic_type because
-        # - We don't want to affect the config outside.
-        # - We don't want to deepcopy complex object to avoid overhead
-        config = deepcopy_basic_type(config)
-
-        self.strategy_config = config["strategy"]
-        _default_executor_config = {
-            "class": "SimulatorExecutor",
-            "module_path": "qlib.backtest.executor",
-            "kwargs": {
-                "time_per_step": "day",
-                "generate_portfolio_metrics": True,
-            },
-        }
-        self.executor_config = config.get("executor", _default_executor_config)
-        self.backtest_config = config["backtest"]
-
-        self.all_freq = self._get_report_freq(self.executor_config)
-        if risk_analysis_freq is None:
-            risk_analysis_freq = [self.all_freq[0]]
-        if indicator_analysis_freq is None:
-            indicator_analysis_freq = [self.all_freq[0]]
-
-        if isinstance(risk_analysis_freq, str):
-            risk_analysis_freq = [risk_analysis_freq]
-        if isinstance(indicator_analysis_freq, str):
-            indicator_analysis_freq = [indicator_analysis_freq]
-
-        self.risk_analysis_freq = [
-            "{0}{1}".format(*Freq.parse(_analysis_freq)) for _analysis_freq in risk_analysis_freq
-        ]
-        self.indicator_analysis_freq = [
-            "{0}{1}".format(*Freq.parse(_analysis_freq)) for _analysis_freq in indicator_analysis_freq
-        ]
-        self.indicator_analysis_method = indicator_analysis_method
-
-    def _get_report_freq(self, executor_config):
-        ret_freq = []
-        if executor_config["kwargs"].get("generate_portfolio_metrics", False):
-            _count, _freq = Freq.parse(executor_config["kwargs"]["time_per_step"])
-            ret_freq.append(f"{_count}{_freq}")
-        if "inner_executor" in executor_config["kwargs"]:
-            ret_freq.extend(self._get_report_freq(executor_config["kwargs"]["inner_executor"]))
-        return ret_freq
-
-    def _generate(self, **kwargs):
-        pred = self.load("pred.pkl")
-
-        # replace the "<PRED>" with prediction saved before
-        placeholder_value = {"<PRED>": pred}
-        for k in "executor_config", "strategy_config":
-            setattr(self, k, fill_placeholder(getattr(self, k), placeholder_value))
-
-        # if the backtesting time range is not set, it will automatically extract time range from the prediction file
-        dt_values = pred.index.get_level_values("datetime")
-        if self.backtest_config["start_time"] is None:
-            self.backtest_config["start_time"] = dt_values.min()
-        if self.backtest_config["end_time"] is None:
-            self.backtest_config["end_time"] = get_date_by_shift(dt_values.max(), 1)
-
-        # custom strategy and get backtest
-        portfolio_metric_dict, indicator_dict = normal_backtest(
-            executor=self.executor_config, strategy=self.strategy_config, **self.backtest_config
-        )
-        for _freq, (report_normal, positions_normal) in portfolio_metric_dict.items():
-            self.save(**{f"report_normal_{_freq}.pkl": report_normal})
-            self.save(**{f"positions_normal_{_freq}.pkl": positions_normal})
-
-        for _freq, indicators_normal in indicator_dict.items():
-            self.save(**{f"indicators_normal_{_freq}.pkl": indicators_normal[0]})
-            self.save(**{f"indicators_normal_{_freq}_obj.pkl": indicators_normal[1]})
-
-        for _analysis_freq in self.risk_analysis_freq:
-            if _analysis_freq not in portfolio_metric_dict:
-                warnings.warn(
-                    f"the freq {_analysis_freq} report is not found, please set the corresponding env with `generate_portfolio_metrics=True`"
-                )
-            else:
-                report_normal, _ = portfolio_metric_dict.get(_analysis_freq)
-                analysis = dict()
-                analysis["excess_return_without_cost"] = risk_analysis(
-                    report_normal["return"] - report_normal["bench"], freq=_analysis_freq
-                )
-                analysis["excess_return_with_cost"] = risk_analysis(
-                    report_normal["return"] - report_normal["bench"] - report_normal["cost"], freq=_analysis_freq
-                )
-
-                analysis_df = pd.concat(analysis)  # type: pd.DataFrame
-                # log metrics
-                analysis_dict = flatten_dict(analysis_df["risk"].unstack().T.to_dict())
-                self.recorder.log_metrics(**{f"{_analysis_freq}.{k}": v for k, v in analysis_dict.items()})
-                # save results
-                self.save(**{f"port_analysis_{_analysis_freq}.pkl": analysis_df})
-                logger.info(
-                    f"Portfolio analysis record 'port_analysis_{_analysis_freq}.pkl' has been saved as the artifact of the Experiment {self.recorder.experiment_id}"
-                )
-                # print out results
-                pprint(f"The following are analysis results of benchmark return({_analysis_freq}).")
-                pprint(risk_analysis(report_normal["bench"], freq=_analysis_freq))
-                pprint(f"The following are analysis results of the excess return without cost({_analysis_freq}).")
-                pprint(analysis["excess_return_without_cost"])
-                pprint(f"The following are analysis results of the excess return with cost({_analysis_freq}).")
-                pprint(analysis["excess_return_with_cost"])
-
-        for _analysis_freq in self.indicator_analysis_freq:
-            if _analysis_freq not in indicator_dict:
-                warnings.warn(f"the freq {_analysis_freq} indicator is not found")
-            else:
-                indicators_normal = indicator_dict.get(_analysis_freq)[0]
-                if self.indicator_analysis_method is None:
-                    analysis_df = indicator_analysis(indicators_normal)
-                else:
-                    analysis_df = indicator_analysis(indicators_normal, method=self.indicator_analysis_method)
-                # log metrics
-                analysis_dict = analysis_df["value"].to_dict()
-                self.recorder.log_metrics(**{f"{_analysis_freq}.{k}": v for k, v in analysis_dict.items()})
-                # save results
-                self.save(**{f"indicator_analysis_{_analysis_freq}.pkl": analysis_df})
-                logger.info(
-                    f"Indicator analysis record 'indicator_analysis_{_analysis_freq}.pkl' has been saved as the artifact of the Experiment {self.recorder.experiment_id}"
-                )
-                pprint(f"The following are analysis results of indicators({_analysis_freq}).")
-                pprint(analysis_df)
-
-    def list(self):
-        list_path = []
-        for _freq in self.all_freq:
-            list_path.extend(
-                [
-                    f"report_normal_{_freq}.pkl",
-                    f"positions_normal_{_freq}.pkl",
-                ]
-            )
-        for _analysis_freq in self.risk_analysis_freq:
-            if _analysis_freq in self.all_freq:
-                list_path.append(f"port_analysis_{_analysis_freq}.pkl")
-            else:
-                warnings.warn(f"risk_analysis freq {_analysis_freq} is not found")
-
-        for _analysis_freq in self.indicator_analysis_freq:
-            if _analysis_freq in self.all_freq:
-                list_path.append(f"indicator_analysis_{_analysis_freq}.pkl")
-            else:
-                warnings.warn(f"indicator_analysis freq {_analysis_freq} is not found")
-        return list_path
+#  Copyright (c) Microsoft Corporation.
+#  Licensed under the MIT License.
+
+import logging
+import warnings
+import pandas as pd
+from pprint import pprint
+from typing import Union, List, Optional
+
+from qlib.utils.exceptions import LoadObjectError
+from ..contrib.evaluate import risk_analysis, indicator_analysis
+
+from ..data.dataset import DatasetH
+from ..data.dataset.handler import DataHandlerLP
+from ..backtest import backtest as normal_backtest
+from ..log import get_module_logger
+from ..utils import fill_placeholder, flatten_dict, class_casting, get_date_by_shift
+from ..utils.time import Freq
+from ..utils.data import deepcopy_basic_type
+from ..contrib.eva.alpha import calc_ic, calc_long_short_return, calc_long_short_prec
+
+
+logger = get_module_logger("workflow", logging.INFO)
+
+
+class RecordTemp:
+    """
+    This is the Records Template class that enables user to generate experiment results such as IC and
+    backtest in a certain format.
+    """
+
+    artifact_path = None
+    depend_cls = None  # the dependant class of the record; the record will depend on the results generated by
+    # `depend_cls`
+
+    @classmethod
+    def get_path(cls, path=None):
+        names = []
+        if cls.artifact_path is not None:
+            names.append(cls.artifact_path)
+
+        if path is not None:
+            names.append(path)
+
+        return "/".join(names)
+
+    def save(self, **kwargs):
+        """
+        It behaves the same as self.recorder.save_objects.
+        But it is an easier interface because users don't have to care about `get_path` and `artifact_path`
+        """
+        art_path = self.get_path()
+        if art_path == "":
+            art_path = None
+        self.recorder.save_objects(artifact_path=art_path, **kwargs)
+
+    def __init__(self, recorder):
+        self._recorder = recorder
+
+    @property
+    def recorder(self):
+        if self._recorder is None:
+            raise ValueError("This RecordTemp did not set recorder yet.")
+        return self._recorder
+
+    def generate(self, **kwargs):
+        """
+        Generate certain records such as IC, backtest etc., and save them.
+
+        Parameters
+        ----------
+        kwargs
+
+        Return
+        ------
+        """
+        raise NotImplementedError(f"Please implement the `generate` method.")
+
+    def load(self, name: str, parents: bool = True):
+        """
+        It behaves the same as self.recorder.load_object.
+        But it is an easier interface because users don't have to care about `get_path` and `artifact_path`
+
+        Parameters
+        ----------
+        name : str
+            the name for the file to be load.
+
+        parents : bool
+            Each recorder has different `artifact_path`.
+            So parents recursively find the path in parents
+            Sub classes has higher priority
+
+        Return
+        ------
+        The stored records.
+        """
+        try:
+            return self.recorder.load_object(self.get_path(name))
+        except LoadObjectError as e:
+            if parents:
+                if self.depend_cls is not None:
+                    with class_casting(self, self.depend_cls):
+                        return self.load(name, parents=True)
+            raise e
+
+    def list(self):
+        """
+        List the supported artifacts.
+        Users don't have to consider self.get_path
+
+        Return
+        ------
+        A list of all the supported artifacts.
+        """
+        return []
+
+    def check(self, include_self: bool = False, parents: bool = True):
+        """
+        Check if the records is properly generated and saved.
+        It is useful in following examples
+
+        - checking if the dependant files complete before generating new things.
+        - checking if the final files is completed
+
+        Parameters
+        ----------
+        include_self : bool
+            is the file generated by self included
+        parents : bool
+            will we check parents
+
+        Raise
+        ------
+        FileNotFoundError
+            whether the records are stored properly.
+        """
+        if include_self:
+            # Some mlflow backend will not list the directly recursively.
+            # So we force to the directly
+            artifacts = {}
+
+            def _get_arts(dirn):
+                if dirn not in artifacts:
+                    artifacts[dirn] = self.recorder.list_artifacts(dirn)
+                return artifacts[dirn]
+
+            for item in self.list():
+                ps = self.get_path(item).split("/")
+                dirn = "/".join(ps[:-1])
+                if self.get_path(item) not in _get_arts(dirn):
+                    raise FileNotFoundError
+        if parents:
+            if self.depend_cls is not None:
+                with class_casting(self, self.depend_cls):
+                    self.check(include_self=True)
+
+
+class SignalRecord(RecordTemp):
+    """
+    This is the Signal Record class that generates the signal prediction. This class inherits the ``RecordTemp`` class.
+    """
+
+    def __init__(self, model=None, dataset=None, recorder=None):
+        super().__init__(recorder=recorder)
+        self.model = model
+        self.dataset = dataset
+
+    @staticmethod
+    def generate_label(dataset):
+        with class_casting(dataset, DatasetH):
+            params = dict(segments="test", col_set="label", data_key=DataHandlerLP.DK_R)
+            try:
+                # Assume the backend handler is DataHandlerLP
+                raw_label = dataset.prepare(**params)
+            except TypeError:
+                # The argument number is not right
+                del params["data_key"]
+                # The backend handler should be DataHandler
+                raw_label = dataset.prepare(**params)
+            except AttributeError as e:
+                # The data handler is initialized with `drop_raw=True`...
+                # So raw_label is not available
+                logger.warning(f"Exception: {e}")
+                raw_label = None
+        return raw_label
+
+    def generate(self, **kwargs):
+        # generate prediction
+        pred = self.model.predict(self.dataset)
+        if isinstance(pred, pd.Series):
+            pred = pred.to_frame("score")
+        self.save(**{"pred.pkl": pred})
+
+        logger.info(
+            f"Signal record 'pred.pkl' has been saved as the artifact of the Experiment {self.recorder.experiment_id}"
+        )
+        # print out results
+        pprint(f"The following are prediction results of the {type(self.model).__name__} model.")
+        pprint(pred.head(5))
+
+        if isinstance(self.dataset, DatasetH):
+            raw_label = self.generate_label(self.dataset)
+            self.save(**{"label.pkl": raw_label})
+
+    def list(self):
+        return ["pred.pkl", "label.pkl"]
+
+
+class ACRecordTemp(RecordTemp):
+    """Automatically checking record template"""
+
+    def __init__(self, recorder, skip_existing=False):
+        self.skip_existing = skip_existing
+        super().__init__(recorder=recorder)
+
+    def generate(self, *args, **kwargs):
+        """automatically checking the files and then run the concrete generating task"""
+        if self.skip_existing:
+            try:
+                self.check(include_self=True, parents=False)
+            except FileNotFoundError:
+                pass  # continue to generating metrics
+            else:
+                logger.info("The results has previously generated, Generation skipped.")
+                return
+
+        try:
+            self.check()
+        except FileNotFoundError:
+            logger.warning("The dependent data does not exists. Generation skipped.")
+            return
+        return self._generate(*args, **kwargs)
+
+    def _generate(self, *args, **kwargs):
+        raise NotImplementedError(f"Please implement the `_generate` method")
+
+
+class HFSignalRecord(SignalRecord):
+    """
+    This is the Signal Analysis Record class that generates the analysis results such as IC and IR. This class inherits the ``RecordTemp`` class.
+    """
+
+    artifact_path = "hg_sig_analysis"
+    depend_cls = SignalRecord
+
+    def __init__(self, recorder, **kwargs):
+        super().__init__(recorder=recorder)
+
+    def generate(self):
+        pred = self.load("pred.pkl")
+        raw_label = self.load("label.pkl")
+        long_pre, short_pre = calc_long_short_prec(pred.iloc[:, 0], raw_label.iloc[:, 0], is_alpha=True)
+        ic, ric = calc_ic(pred.iloc[:, 0], raw_label.iloc[:, 0])
+        metrics = {
+            "IC": ic.mean(),
+            "ICIR": ic.mean() / ic.std(),
+            "Rank IC": ric.mean(),
+            "Rank ICIR": ric.mean() / ric.std(),
+            "Long precision": long_pre.mean(),
+            "Short precision": short_pre.mean(),
+        }
+        objects = {"ic.pkl": ic, "ric.pkl": ric}
+        objects.update({"long_pre.pkl": long_pre, "short_pre.pkl": short_pre})
+        long_short_r, long_avg_r = calc_long_short_return(pred.iloc[:, 0], raw_label.iloc[:, 0])
+        metrics.update(
+            {
+                "Long-Short Average Return": long_short_r.mean(),
+                "Long-Short Average Sharpe": long_short_r.mean() / long_short_r.std(),
+            }
+        )
+        objects.update(
+            {
+                "long_short_r.pkl": long_short_r,
+                "long_avg_r.pkl": long_avg_r,
+            }
+        )
+        self.recorder.log_metrics(**metrics)
+        self.save(**objects)
+        pprint(metrics)
+
+    def list(self):
+        return ["ic.pkl", "ric.pkl", "long_pre.pkl", "short_pre.pkl", "long_short_r.pkl", "long_avg_r.pkl"]
+
+
+class SigAnaRecord(ACRecordTemp):
+    """
+    This is the Signal Analysis Record class that generates the analysis results such as IC and IR.
+    This class inherits the ``RecordTemp`` class.
+    """
+
+    artifact_path = "sig_analysis"
+    depend_cls = SignalRecord
+
+    def __init__(self, recorder, ana_long_short=False, ann_scaler=252, label_col=0, skip_existing=False):
+        super().__init__(recorder=recorder, skip_existing=skip_existing)
+        self.ana_long_short = ana_long_short
+        self.ann_scaler = ann_scaler
+        self.label_col = label_col
+
+    def _generate(self, label: Optional[pd.DataFrame] = None, **kwargs):
+        """
+        Parameters
+        ----------
+        label : Optional[pd.DataFrame]
+            Label should be a dataframe.
+        """
+        pred = self.load("pred.pkl")
+        if label is None:
+            label = self.load("label.pkl")
+        if label is None or not isinstance(label, pd.DataFrame) or label.empty:
+            logger.warning(f"Empty label.")
+            return
+        ic, ric = calc_ic(pred.iloc[:, 0], label.iloc[:, self.label_col])
+        metrics = {
+            "IC": ic.mean(),
+            "ICIR": ic.mean() / ic.std(),
+            "Rank IC": ric.mean(),
+            "Rank ICIR": ric.mean() / ric.std(),
+        }
+        objects = {"ic.pkl": ic, "ric.pkl": ric}
+        if self.ana_long_short:
+            long_short_r, long_avg_r = calc_long_short_return(pred.iloc[:, 0], label.iloc[:, self.label_col])
+            metrics.update(
+                {
+                    "Long-Short Ann Return": long_short_r.mean() * self.ann_scaler,
+                    "Long-Short Ann Sharpe": long_short_r.mean() / long_short_r.std() * self.ann_scaler**0.5,
+                    "Long-Avg Ann Return": long_avg_r.mean() * self.ann_scaler,
+                    "Long-Avg Ann Sharpe": long_avg_r.mean() / long_avg_r.std() * self.ann_scaler**0.5,
+                }
+            )
+            objects.update(
+                {
+                    "long_short_r.pkl": long_short_r,
+                    "long_avg_r.pkl": long_avg_r,
+                }
+            )
+        self.recorder.log_metrics(**metrics)
+        self.save(**objects)
+        pprint(metrics)
+
+    def list(self):
+        paths = ["ic.pkl", "ric.pkl"]
+        if self.ana_long_short:
+            paths.extend(["long_short_r.pkl", "long_avg_r.pkl"])
+        return paths
+
+
+class PortAnaRecord(ACRecordTemp):
+    """
+    This is the Portfolio Analysis Record class that generates the analysis results such as those of backtest. This class inherits the ``RecordTemp`` class.
+
+    The following files will be stored in recorder
+
+    - report_normal.pkl & positions_normal.pkl:
+
+        - The return report and detailed positions of the backtest, returned by `qlib/contrib/evaluate.py:backtest`
+    - port_analysis.pkl : The risk analysis of your portfolio, returned by `qlib/contrib/evaluate.py:risk_analysis`
+    """
+
+    artifact_path = "portfolio_analysis"
+    depend_cls = SignalRecord
+
+    def __init__(
+        self,
+        recorder,
+        config=None,
+        risk_analysis_freq: Union[List, str] = None,
+        indicator_analysis_freq: Union[List, str] = None,
+        indicator_analysis_method=None,
+        skip_existing=False,
+        **kwargs,
+    ):
+        """
+        config["strategy"] : dict
+            define the strategy class as well as the kwargs.
+        config["executor"] : dict
+            define the executor class as well as the kwargs.
+        config["backtest"] : dict
+            define the backtest kwargs.
+        risk_analysis_freq : str|List[str]
+            risk analysis freq of report
+        indicator_analysis_freq : str|List[str]
+            indicator analysis freq of report
+        indicator_analysis_method : str, optional, default by None
+            the candidate values include 'mean', 'amount_weighted', 'value_weighted'
+        """
+        super().__init__(recorder=recorder, skip_existing=skip_existing, **kwargs)
+
+        if config is None:
+            config = {  # Default config for daily trading
+                "strategy": {
+                    "class": "TopkDropoutStrategy",
+                    "module_path": "qlib.contrib.strategy",
+                    "kwargs": {"signal": "<PRED>", "topk": 50, "n_drop": 5},
+                },
+                "backtest": {
+                    "start_time": None,
+                    "end_time": None,
+                    "account": 100000000,
+                    "benchmark": "SH000300",
+                    "exchange_kwargs": {
+                        "limit_threshold": 0.095,
+                        "deal_price": "close",
+                        "open_cost": 0.0005,
+                        "close_cost": 0.0015,
+                        "min_cost": 5,
+                    },
+                },
+            }
+        # We only deepcopy_basic_type because
+        # - We don't want to affect the config outside.
+        # - We don't want to deepcopy complex object to avoid overhead
+        config = deepcopy_basic_type(config)
+
+        self.strategy_config = config["strategy"]
+        _default_executor_config = {
+            "class": "SimulatorExecutor",
+            "module_path": "qlib.backtest.executor",
+            "kwargs": {
+                "time_per_step": "day",
+                "generate_portfolio_metrics": True,
+            },
+        }
+        self.executor_config = config.get("executor", _default_executor_config)
+        self.backtest_config = config["backtest"]
+
+        self.all_freq = self._get_report_freq(self.executor_config)
+        if risk_analysis_freq is None:
+            risk_analysis_freq = [self.all_freq[0]]
+        if indicator_analysis_freq is None:
+            indicator_analysis_freq = [self.all_freq[0]]
+
+        if isinstance(risk_analysis_freq, str):
+            risk_analysis_freq = [risk_analysis_freq]
+        if isinstance(indicator_analysis_freq, str):
+            indicator_analysis_freq = [indicator_analysis_freq]
+
+        self.risk_analysis_freq = [
+            "{0}{1}".format(*Freq.parse(_analysis_freq)) for _analysis_freq in risk_analysis_freq
+        ]
+        self.indicator_analysis_freq = [
+            "{0}{1}".format(*Freq.parse(_analysis_freq)) for _analysis_freq in indicator_analysis_freq
+        ]
+        self.indicator_analysis_method = indicator_analysis_method
+
+    def _get_report_freq(self, executor_config):
+        ret_freq = []
+        if executor_config["kwargs"].get("generate_portfolio_metrics", False):
+            _count, _freq = Freq.parse(executor_config["kwargs"]["time_per_step"])
+            ret_freq.append(f"{_count}{_freq}")
+        if "inner_executor" in executor_config["kwargs"]:
+            ret_freq.extend(self._get_report_freq(executor_config["kwargs"]["inner_executor"]))
+        return ret_freq
+
+    def _generate(self, **kwargs):
+        pred = self.load("pred.pkl")
+
+        # replace the "<PRED>" with prediction saved before
+        placeholder_value = {"<PRED>": pred}
+        for k in "executor_config", "strategy_config":
+            setattr(self, k, fill_placeholder(getattr(self, k), placeholder_value))
+
+        # if the backtesting time range is not set, it will automatically extract time range from the prediction file
+        dt_values = pred.index.get_level_values("datetime")
+        if self.backtest_config["start_time"] is None:
+            self.backtest_config["start_time"] = dt_values.min()
+        if self.backtest_config["end_time"] is None:
+            self.backtest_config["end_time"] = get_date_by_shift(dt_values.max(), 1)
+
+        # custom strategy and get backtest
+        portfolio_metric_dict, indicator_dict = normal_backtest(
+            executor=self.executor_config, strategy=self.strategy_config, **self.backtest_config
+        )
+        for _freq, (report_normal, positions_normal) in portfolio_metric_dict.items():
+            self.save(**{f"report_normal_{_freq}.pkl": report_normal})
+            self.save(**{f"positions_normal_{_freq}.pkl": positions_normal})
+
+        for _freq, indicators_normal in indicator_dict.items():
+            self.save(**{f"indicators_normal_{_freq}.pkl": indicators_normal[0]})
+            self.save(**{f"indicators_normal_{_freq}_obj.pkl": indicators_normal[1]})
+
+        for _analysis_freq in self.risk_analysis_freq:
+            if _analysis_freq not in portfolio_metric_dict:
+                warnings.warn(
+                    f"the freq {_analysis_freq} report is not found, please set the corresponding env with `generate_portfolio_metrics=True`"
+                )
+            else:
+                report_normal, _ = portfolio_metric_dict.get(_analysis_freq)
+                analysis = dict()
+                analysis["excess_return_without_cost"] = risk_analysis(
+                    report_normal["return"] - report_normal["bench"], freq=_analysis_freq
+                )
+                analysis["excess_return_with_cost"] = risk_analysis(
+                    report_normal["return"] - report_normal["bench"] - report_normal["cost"], freq=_analysis_freq
+                )
+
+                analysis_df = pd.concat(analysis)  # type: pd.DataFrame
+                # log metrics
+                analysis_dict = flatten_dict(analysis_df["risk"].unstack().T.to_dict())
+                self.recorder.log_metrics(**{f"{_analysis_freq}.{k}": v for k, v in analysis_dict.items()})
+                # save results
+                self.save(**{f"port_analysis_{_analysis_freq}.pkl": analysis_df})
+                logger.info(
+                    f"Portfolio analysis record 'port_analysis_{_analysis_freq}.pkl' has been saved as the artifact of the Experiment {self.recorder.experiment_id}"
+                )
+                # print out results
+                pprint(f"The following are analysis results of benchmark return({_analysis_freq}).")
+                pprint(risk_analysis(report_normal["bench"], freq=_analysis_freq))
+                pprint(f"The following are analysis results of the excess return without cost({_analysis_freq}).")
+                pprint(analysis["excess_return_without_cost"])
+                pprint(f"The following are analysis results of the excess return with cost({_analysis_freq}).")
+                pprint(analysis["excess_return_with_cost"])
+
+        for _analysis_freq in self.indicator_analysis_freq:
+            if _analysis_freq not in indicator_dict:
+                warnings.warn(f"the freq {_analysis_freq} indicator is not found")
+            else:
+                indicators_normal = indicator_dict.get(_analysis_freq)[0]
+                if self.indicator_analysis_method is None:
+                    analysis_df = indicator_analysis(indicators_normal)
+                else:
+                    analysis_df = indicator_analysis(indicators_normal, method=self.indicator_analysis_method)
+                # log metrics
+                analysis_dict = analysis_df["value"].to_dict()
+                self.recorder.log_metrics(**{f"{_analysis_freq}.{k}": v for k, v in analysis_dict.items()})
+                # save results
+                self.save(**{f"indicator_analysis_{_analysis_freq}.pkl": analysis_df})
+                logger.info(
+                    f"Indicator analysis record 'indicator_analysis_{_analysis_freq}.pkl' has been saved as the artifact of the Experiment {self.recorder.experiment_id}"
+                )
+                pprint(f"The following are analysis results of indicators({_analysis_freq}).")
+                pprint(analysis_df)
+
+    def list(self):
+        list_path = []
+        for _freq in self.all_freq:
+            list_path.extend(
+                [
+                    f"report_normal_{_freq}.pkl",
+                    f"positions_normal_{_freq}.pkl",
+                ]
+            )
+        for _analysis_freq in self.risk_analysis_freq:
+            if _analysis_freq in self.all_freq:
+                list_path.append(f"port_analysis_{_analysis_freq}.pkl")
+            else:
+                warnings.warn(f"risk_analysis freq {_analysis_freq} is not found")
+
+        for _analysis_freq in self.indicator_analysis_freq:
+            if _analysis_freq in self.all_freq:
+                list_path.append(f"indicator_analysis_{_analysis_freq}.pkl")
+            else:
+                warnings.warn(f"indicator_analysis freq {_analysis_freq} is not found")
+        return list_path
```

## qlib/workflow/utils.py

 * *Ordering differences only*

```diff
@@ -1,47 +1,47 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-import atexit
-import logging
-import sys
-import traceback
-
-from ..log import get_module_logger
-from . import R
-from .recorder import Recorder
-
-logger = get_module_logger("workflow", logging.INFO)
-
-
-# function to handle the experiment when unusual program ending occurs
-def experiment_exit_handler():
-    """
-    Method for handling the experiment when any unusual program ending occurs.
-    The `atexit` handler should be put in the last, since, as long as the program ends, it will be called.
-    Thus, if any exception or user interruption occurs beforehand, we should handle them first. Once `R` is
-    ended, another call of `R.end_exp` will not take effect.
-
-    Limitations:
-    - If pdb is used in your program, excepthook will not be triggered when it ends.  The status will be finished
-    """
-    sys.excepthook = experiment_exception_hook  # handle uncaught exception
-    atexit.register(R.end_exp, recorder_status=Recorder.STATUS_FI)  # will not take effect if experiment ends
-
-
-def experiment_exception_hook(exc_type, value, tb):
-    """
-    End an experiment with status to be "FAILED". This exception tries to catch those uncaught exception
-    and end the experiment automatically.
-
-    Parameters
-    exc_type: Exception type
-    value: Exception's value
-    tb: Exception's traceback
-    """
-    logger.error(f"An exception has been raised[{exc_type.__name__}: {value}].")
-
-    # Same as original format
-    traceback.print_tb(tb)
-    print(f"{exc_type.__name__}: {value}")
-
-    R.end_exp(recorder_status=Recorder.STATUS_FA)
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+import atexit
+import logging
+import sys
+import traceback
+
+from ..log import get_module_logger
+from . import R
+from .recorder import Recorder
+
+logger = get_module_logger("workflow", logging.INFO)
+
+
+# function to handle the experiment when unusual program ending occurs
+def experiment_exit_handler():
+    """
+    Method for handling the experiment when any unusual program ending occurs.
+    The `atexit` handler should be put in the last, since, as long as the program ends, it will be called.
+    Thus, if any exception or user interruption occurs beforehand, we should handle them first. Once `R` is
+    ended, another call of `R.end_exp` will not take effect.
+
+    Limitations:
+    - If pdb is used in your program, excepthook will not be triggered when it ends.  The status will be finished
+    """
+    sys.excepthook = experiment_exception_hook  # handle uncaught exception
+    atexit.register(R.end_exp, recorder_status=Recorder.STATUS_FI)  # will not take effect if experiment ends
+
+
+def experiment_exception_hook(exc_type, value, tb):
+    """
+    End an experiment with status to be "FAILED". This exception tries to catch those uncaught exception
+    and end the experiment automatically.
+
+    Parameters
+    exc_type: Exception type
+    value: Exception's value
+    tb: Exception's traceback
+    """
+    logger.error(f"An exception has been raised[{exc_type.__name__}: {value}].")
+
+    # Same as original format
+    traceback.print_tb(tb)
+    print(f"{exc_type.__name__}: {value}")
+
+    R.end_exp(recorder_status=Recorder.STATUS_FA)
```

## qlib/workflow/online/manager.py

 * *Ordering differences only*

```diff
@@ -1,382 +1,382 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-"""
-OnlineManager can manage a set of `Online Strategy <#Online Strategy>`_ and run them dynamically.
-
-With the change of time, the decisive models will be also changed. In this module, we call those contributing models `online` models.
-In every routine(such as every day or every minute), the `online` models may be changed and the prediction of them needs to be updated.
-So this module provides a series of methods to control this process.
-
-This module also provides a method to simulate `Online Strategy <#Online Strategy>`_ in history.
-Which means you can verify your strategy or find a better one.
-
-There are 4 total situations for using different trainers in different situations:
-
-
-
-=========================  ===================================================================================
-Situations                 Description
-=========================  ===================================================================================
-Online + Trainer           When you want to do a REAL routine, the Trainer will help you train the models. It
-                           will train models task by task and strategy by strategy.
-
-Online + DelayTrainer      DelayTrainer will skip concrete training until all tasks have been prepared by
-                           different strategies. It makes users can parallelly train all tasks at the end of
-                           `routine` or `first_train`. Otherwise, these functions will get stuck when each
-                           strategy prepare tasks.
-
-Simulation + Trainer       It will behave in the same way as `Online + Trainer`. The only difference is that it
-                           is for simulation/backtesting instead of online trading
-
-Simulation + DelayTrainer  When your models don't have any temporal dependence, you can use DelayTrainer
-                           for the ability to multitasking. It means all tasks in all routines
-                           can be REAL trained at the end of simulating. The signals will be prepared well at
-                           different time segments (based on whether or not any new model is online).
-=========================  ===================================================================================
-
-Here is some pseudo code that demonstrate the workflow of each situation
-
-For simplicity
-    - Only one strategy is used in the strategy
-    - `update_online_pred` is only called in the online mode and is ignored
-
-1) `Online + Trainer`
-
-.. code-block:: python
-
-    tasks = first_train()
-    models = trainer.train(tasks)
-    trainer.end_train(models)
-    for day in online_trading_days:
-        # OnlineManager.routine
-        models = trainer.train(strategy.prepare_tasks())  # for each strategy
-        strategy.prepare_online_models(models)  # for each strategy
-
-        trainer.end_train(models)
-        prepare_signals()  # prepare trading signals daily
-
-
-`Online + DelayTrainer`: the workflow is the same as `Online + Trainer`.
-
-
-2) `Simulation + DelayTrainer`
-
-.. code-block:: python
-
-    # simulate
-    tasks = first_train()
-    models = trainer.train(tasks)
-    for day in historical_calendars:
-        # OnlineManager.routine
-        models = trainer.train(strategy.prepare_tasks())  # for each strategy
-        strategy.prepare_online_models(models)  # for each strategy
-    # delay_prepare()
-    # FIXME: Currently the delay_prepare is not implemented in a proper way.
-    trainer.end_train(<for all previous models>)
-    prepare_signals()
-
-
-# Can we simplify current workflow?
-
-- Can reduce the number of state of tasks?
-
-    - For each task, we have three phases (i.e. task, partly trained task, final trained task)
-"""
-
-import logging
-from typing import Callable, List, Union
-
-import pandas as pd
-from qlib import get_module_logger
-from qlib.data.data import D
-from qlib.log import set_global_logger_level
-from qlib.model.ens.ensemble import AverageEnsemble
-from qlib.model.trainer import Trainer, TrainerR
-from qlib.utils.serial import Serializable
-from qlib.workflow.online.strategy import OnlineStrategy
-from qlib.workflow.task.collect import MergeCollector
-
-
-class OnlineManager(Serializable):
-    """
-    OnlineManager can manage online models with `Online Strategy <#Online Strategy>`_.
-    It also provides a history recording of which models are online at what time.
-    """
-
-    STATUS_SIMULATING = "simulating"  # when calling `simulate`
-    STATUS_ONLINE = "online"  # the normal status. It is used when online trading
-
-    def __init__(
-        self,
-        strategies: Union[OnlineStrategy, List[OnlineStrategy]],
-        trainer: Trainer = None,
-        begin_time: Union[str, pd.Timestamp] = None,
-        freq="day",
-    ):
-        """
-        Init OnlineManager.
-        One OnlineManager must have at least one OnlineStrategy.
-
-        Args:
-            strategies (Union[OnlineStrategy, List[OnlineStrategy]]): an instance of OnlineStrategy or a list of OnlineStrategy
-            begin_time (Union[str,pd.Timestamp], optional): the OnlineManager will begin at this time. Defaults to None for using the latest date.
-            trainer (qlib.model.trainer.Trainer): the trainer to train task. None for using TrainerR.
-            freq (str, optional): data frequency. Defaults to "day".
-        """
-        self.logger = get_module_logger(self.__class__.__name__)
-        if not isinstance(strategies, list):
-            strategies = [strategies]
-        self.strategies = strategies
-        self.freq = freq
-        if begin_time is None:
-            begin_time = D.calendar(freq=self.freq).max()
-        self.begin_time = pd.Timestamp(begin_time)
-        self.cur_time = self.begin_time
-        # OnlineManager will recorder the history of online models, which is a dict like {pd.Timestamp, {strategy, [online_models]}}.
-        # It records the online servnig models of each strategy for each day.
-        self.history = {}
-        if trainer is None:
-            trainer = TrainerR()
-        self.trainer = trainer
-        self.signals = None
-        self.status = self.STATUS_ONLINE
-
-    def _postpone_action(self):
-        """
-        Should the workflow to postpone the following actions to the end (in delay_prepare)
-        - trainer.end_train
-        - prepare_signals
-
-        Postpone these actions is to support simulating/backtest online strategies without time dependencies.
-        All the actions can be done parallelly at the end.
-        """
-        return self.status == self.STATUS_SIMULATING and self.trainer.is_delay()
-
-    def first_train(self, strategies: List[OnlineStrategy] = None, model_kwargs: dict = {}):
-        """
-        Get tasks from every strategy's first_tasks method and train them.
-        If using DelayTrainer, it can finish training all together after every strategy's first_tasks.
-
-        Args:
-            strategies (List[OnlineStrategy]): the strategies list (need this param when adding strategies). None for use default strategies.
-            model_kwargs (dict): the params for `prepare_online_models`
-        """
-        if strategies is None:
-            strategies = self.strategies
-
-        models_list = []
-        for strategy in strategies:
-            self.logger.info(f"Strategy `{strategy.name_id}` begins first training...")
-            tasks = strategy.first_tasks()
-            models = self.trainer.train(tasks, experiment_name=strategy.name_id)
-            models_list.append(models)
-            self.logger.info(f"Finished training {len(models)} models.")
-            # FIXME: Train multiple online models at `first_train` will result in getting too much online models at the
-            # start.
-            online_models = strategy.prepare_online_models(models, **model_kwargs)
-            self.history.setdefault(self.cur_time, {})[strategy] = online_models
-
-        if not self._postpone_action():
-            for strategy, models in zip(strategies, models_list):
-                models = self.trainer.end_train(models, experiment_name=strategy.name_id)
-
-    def routine(
-        self,
-        cur_time: Union[str, pd.Timestamp] = None,
-        task_kwargs: dict = {},
-        model_kwargs: dict = {},
-        signal_kwargs: dict = {},
-    ):
-        """
-        Typical update process for every strategy and record the online history.
-
-        The typical update process after a routine, such as day by day or month by month.
-        The process is: Update predictions -> Prepare tasks -> Prepare online models -> Prepare signals.
-
-        If using DelayTrainer, it can finish training all together after every strategy's prepare_tasks.
-
-        Args:
-            cur_time (Union[str,pd.Timestamp], optional): run routine method in this time. Defaults to None.
-            task_kwargs (dict): the params for `prepare_tasks`
-            model_kwargs (dict): the params for `prepare_online_models`
-            signal_kwargs (dict): the params for `prepare_signals`
-        """
-        if cur_time is None:
-            cur_time = D.calendar(freq=self.freq).max()
-        self.cur_time = pd.Timestamp(cur_time)  # None for latest date
-
-        models_list = []
-        for strategy in self.strategies:
-            self.logger.info(f"Strategy `{strategy.name_id}` begins routine...")
-
-            tasks = strategy.prepare_tasks(self.cur_time, **task_kwargs)
-            models = self.trainer.train(tasks, experiment_name=strategy.name_id)
-            models_list.append(models)
-            self.logger.info(f"Finished training {len(models)} models.")
-            online_models = strategy.prepare_online_models(models, **model_kwargs)
-            self.history.setdefault(self.cur_time, {})[strategy] = online_models
-
-            # The online model may changes in the above processes
-            # So updating the predictions of online models should be the last step
-            if self.status == self.STATUS_ONLINE:
-                strategy.tool.update_online_pred()
-
-        if not self._postpone_action():
-            for strategy, models in zip(self.strategies, models_list):
-                models = self.trainer.end_train(models, experiment_name=strategy.name_id)
-            self.prepare_signals(**signal_kwargs)
-
-    def get_collector(self, **kwargs) -> MergeCollector:
-        """
-        Get the instance of `Collector <../advanced/task_management.html#Task Collecting>`_ to collect results from every strategy.
-        This collector can be a basis as the signals preparation.
-
-        Args:
-            **kwargs: the params for get_collector.
-
-        Returns:
-            MergeCollector: the collector to merge other collectors.
-        """
-        collector_dict = {}
-        for strategy in self.strategies:
-            collector_dict[strategy.name_id] = strategy.get_collector(**kwargs)
-        return MergeCollector(collector_dict, process_list=[])
-
-    def add_strategy(self, strategies: Union[OnlineStrategy, List[OnlineStrategy]]):
-        """
-        Add some new strategies to OnlineManager.
-
-        Args:
-            strategy (Union[OnlineStrategy, List[OnlineStrategy]]): a list of OnlineStrategy
-        """
-        if not isinstance(strategies, list):
-            strategies = [strategies]
-        self.first_train(strategies)
-        self.strategies.extend(strategies)
-
-    def prepare_signals(self, prepare_func: Callable = AverageEnsemble(), over_write=False):
-        """
-        After preparing the data of the last routine (a box in box-plot) which means the end of the routine, we can prepare trading signals for the next routine.
-
-        NOTE: Given a set prediction, all signals before these prediction end times will be prepared well.
-
-        Even if the latest signal already exists, the latest calculation result will be overwritten.
-
-        .. note::
-
-            Given a prediction of a certain time, all signals before this time will be prepared well.
-
-        Args:
-            prepare_func (Callable, optional): Get signals from a dict after collecting. Defaults to AverageEnsemble(), the results collected by MergeCollector must be {xxx:pred}.
-            over_write (bool, optional): If True, the new signals will overwrite. If False, the new signals will append to the end of signals. Defaults to False.
-
-        Returns:
-            pd.DataFrame: the signals.
-        """
-        signals = prepare_func(self.get_collector()())
-        old_signals = self.signals
-        if old_signals is not None and not over_write:
-            old_max = old_signals.index.get_level_values("datetime").max()
-            new_signals = signals.loc[old_max:]
-            signals = pd.concat([old_signals, new_signals], axis=0)
-        else:
-            new_signals = signals
-        self.logger.info(f"Finished preparing new {len(new_signals)} signals.")
-        self.signals = signals
-        return new_signals
-
-    def get_signals(self) -> Union[pd.Series, pd.DataFrame]:
-        """
-        Get prepared online signals.
-
-        Returns:
-            Union[pd.Series, pd.DataFrame]: pd.Series for only one signals every datetime.
-            pd.DataFrame for multiple signals, for example, buy and sell operations use different trading signals.
-        """
-        return self.signals
-
-    SIM_LOG_LEVEL = logging.INFO + 1  # when simulating, reduce information
-    SIM_LOG_NAME = "SIMULATE_INFO"
-
-    def simulate(
-        self, end_time=None, frequency="day", task_kwargs={}, model_kwargs={}, signal_kwargs={}
-    ) -> Union[pd.Series, pd.DataFrame]:
-        """
-        Starting from the current time, this method will simulate every routine in OnlineManager until the end time.
-
-        Considering the parallel training, the models and signals can be prepared after all routine simulating.
-
-        The delay training way can be ``DelayTrainer`` and the delay preparing signals way can be ``delay_prepare``.
-
-        Args:
-            end_time: the time the simulation will end
-            frequency: the calendar frequency
-            task_kwargs (dict): the params for `prepare_tasks`
-            model_kwargs (dict): the params for `prepare_online_models`
-            signal_kwargs (dict): the params for `prepare_signals`
-
-        Returns:
-            Union[pd.Series, pd.DataFrame]: pd.Series for only one signals every datetime.
-            pd.DataFrame for multiple signals, for example, buy and sell operations use different trading signals.
-        """
-        self.status = self.STATUS_SIMULATING
-        cal = D.calendar(start_time=self.cur_time, end_time=end_time, freq=frequency)
-        self.first_train()
-
-        simulate_level = self.SIM_LOG_LEVEL
-        set_global_logger_level(simulate_level)
-        logging.addLevelName(simulate_level, self.SIM_LOG_NAME)
-
-        for cur_time in cal:
-            self.logger.log(level=simulate_level, msg=f"Simulating at {str(cur_time)}......")
-            self.routine(
-                cur_time,
-                task_kwargs=task_kwargs,
-                model_kwargs=model_kwargs,
-                signal_kwargs=signal_kwargs,
-            )
-        # delay prepare the models and signals
-        if self._postpone_action():
-            self.delay_prepare(model_kwargs=model_kwargs, signal_kwargs=signal_kwargs)
-
-        # FIXME: get logging level firstly and restore it here
-        set_global_logger_level(logging.DEBUG)
-        self.logger.info(f"Finished preparing signals")
-        self.status = self.STATUS_ONLINE
-        return self.get_signals()
-
-    def delay_prepare(self, model_kwargs={}, signal_kwargs={}):
-        """
-        Prepare all models and signals if something is waiting for preparation.
-
-        Args:
-            model_kwargs: the params for `end_train`
-            signal_kwargs: the params for `prepare_signals`
-        """
-        # FIXME:
-        # This method is not implemented in the proper way!!!
-        last_models = {}
-        signals_time = D.calendar()[0]
-        need_prepare = False
-        for cur_time, strategy_models in self.history.items():
-            self.cur_time = cur_time
-
-            for strategy, models in strategy_models.items():
-                # only new online models need to prepare
-                if last_models.setdefault(strategy, set()) != set(models):
-                    models = self.trainer.end_train(models, experiment_name=strategy.name_id, **model_kwargs)
-                    strategy.tool.reset_online_tag(models)
-                    need_prepare = True
-                last_models[strategy] = set(models)
-
-            if need_prepare:
-                # NOTE: Assumption: the predictions of online models need less than next cur_time, or this method will work in a wrong way.
-                self.prepare_signals(**signal_kwargs)
-                if signals_time > cur_time:
-                    # FIXME: if use DelayTrainer and worker (and worker is faster than main progress), there are some possibilities of showing this warning.
-                    self.logger.warn(
-                        f"The signals have already parpred to {signals_time} by last preparation, but current time is only {cur_time}. This may be because the online models predict more than they should, which can cause signals to be contaminated by the offline models."
-                    )
-                need_prepare = False
-                signals_time = self.signals.index.get_level_values("datetime").max()
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+"""
+OnlineManager can manage a set of `Online Strategy <#Online Strategy>`_ and run them dynamically.
+
+With the change of time, the decisive models will be also changed. In this module, we call those contributing models `online` models.
+In every routine(such as every day or every minute), the `online` models may be changed and the prediction of them needs to be updated.
+So this module provides a series of methods to control this process.
+
+This module also provides a method to simulate `Online Strategy <#Online Strategy>`_ in history.
+Which means you can verify your strategy or find a better one.
+
+There are 4 total situations for using different trainers in different situations:
+
+
+
+=========================  ===================================================================================
+Situations                 Description
+=========================  ===================================================================================
+Online + Trainer           When you want to do a REAL routine, the Trainer will help you train the models. It
+                           will train models task by task and strategy by strategy.
+
+Online + DelayTrainer      DelayTrainer will skip concrete training until all tasks have been prepared by
+                           different strategies. It makes users can parallelly train all tasks at the end of
+                           `routine` or `first_train`. Otherwise, these functions will get stuck when each
+                           strategy prepare tasks.
+
+Simulation + Trainer       It will behave in the same way as `Online + Trainer`. The only difference is that it
+                           is for simulation/backtesting instead of online trading
+
+Simulation + DelayTrainer  When your models don't have any temporal dependence, you can use DelayTrainer
+                           for the ability to multitasking. It means all tasks in all routines
+                           can be REAL trained at the end of simulating. The signals will be prepared well at
+                           different time segments (based on whether or not any new model is online).
+=========================  ===================================================================================
+
+Here is some pseudo code that demonstrate the workflow of each situation
+
+For simplicity
+    - Only one strategy is used in the strategy
+    - `update_online_pred` is only called in the online mode and is ignored
+
+1) `Online + Trainer`
+
+.. code-block:: python
+
+    tasks = first_train()
+    models = trainer.train(tasks)
+    trainer.end_train(models)
+    for day in online_trading_days:
+        # OnlineManager.routine
+        models = trainer.train(strategy.prepare_tasks())  # for each strategy
+        strategy.prepare_online_models(models)  # for each strategy
+
+        trainer.end_train(models)
+        prepare_signals()  # prepare trading signals daily
+
+
+`Online + DelayTrainer`: the workflow is the same as `Online + Trainer`.
+
+
+2) `Simulation + DelayTrainer`
+
+.. code-block:: python
+
+    # simulate
+    tasks = first_train()
+    models = trainer.train(tasks)
+    for day in historical_calendars:
+        # OnlineManager.routine
+        models = trainer.train(strategy.prepare_tasks())  # for each strategy
+        strategy.prepare_online_models(models)  # for each strategy
+    # delay_prepare()
+    # FIXME: Currently the delay_prepare is not implemented in a proper way.
+    trainer.end_train(<for all previous models>)
+    prepare_signals()
+
+
+# Can we simplify current workflow?
+
+- Can reduce the number of state of tasks?
+
+    - For each task, we have three phases (i.e. task, partly trained task, final trained task)
+"""
+
+import logging
+from typing import Callable, List, Union
+
+import pandas as pd
+from qlib import get_module_logger
+from qlib.data.data import D
+from qlib.log import set_global_logger_level
+from qlib.model.ens.ensemble import AverageEnsemble
+from qlib.model.trainer import Trainer, TrainerR
+from qlib.utils.serial import Serializable
+from qlib.workflow.online.strategy import OnlineStrategy
+from qlib.workflow.task.collect import MergeCollector
+
+
+class OnlineManager(Serializable):
+    """
+    OnlineManager can manage online models with `Online Strategy <#Online Strategy>`_.
+    It also provides a history recording of which models are online at what time.
+    """
+
+    STATUS_SIMULATING = "simulating"  # when calling `simulate`
+    STATUS_ONLINE = "online"  # the normal status. It is used when online trading
+
+    def __init__(
+        self,
+        strategies: Union[OnlineStrategy, List[OnlineStrategy]],
+        trainer: Trainer = None,
+        begin_time: Union[str, pd.Timestamp] = None,
+        freq="day",
+    ):
+        """
+        Init OnlineManager.
+        One OnlineManager must have at least one OnlineStrategy.
+
+        Args:
+            strategies (Union[OnlineStrategy, List[OnlineStrategy]]): an instance of OnlineStrategy or a list of OnlineStrategy
+            begin_time (Union[str,pd.Timestamp], optional): the OnlineManager will begin at this time. Defaults to None for using the latest date.
+            trainer (qlib.model.trainer.Trainer): the trainer to train task. None for using TrainerR.
+            freq (str, optional): data frequency. Defaults to "day".
+        """
+        self.logger = get_module_logger(self.__class__.__name__)
+        if not isinstance(strategies, list):
+            strategies = [strategies]
+        self.strategies = strategies
+        self.freq = freq
+        if begin_time is None:
+            begin_time = D.calendar(freq=self.freq).max()
+        self.begin_time = pd.Timestamp(begin_time)
+        self.cur_time = self.begin_time
+        # OnlineManager will recorder the history of online models, which is a dict like {pd.Timestamp, {strategy, [online_models]}}.
+        # It records the online servnig models of each strategy for each day.
+        self.history = {}
+        if trainer is None:
+            trainer = TrainerR()
+        self.trainer = trainer
+        self.signals = None
+        self.status = self.STATUS_ONLINE
+
+    def _postpone_action(self):
+        """
+        Should the workflow to postpone the following actions to the end (in delay_prepare)
+        - trainer.end_train
+        - prepare_signals
+
+        Postpone these actions is to support simulating/backtest online strategies without time dependencies.
+        All the actions can be done parallelly at the end.
+        """
+        return self.status == self.STATUS_SIMULATING and self.trainer.is_delay()
+
+    def first_train(self, strategies: List[OnlineStrategy] = None, model_kwargs: dict = {}):
+        """
+        Get tasks from every strategy's first_tasks method and train them.
+        If using DelayTrainer, it can finish training all together after every strategy's first_tasks.
+
+        Args:
+            strategies (List[OnlineStrategy]): the strategies list (need this param when adding strategies). None for use default strategies.
+            model_kwargs (dict): the params for `prepare_online_models`
+        """
+        if strategies is None:
+            strategies = self.strategies
+
+        models_list = []
+        for strategy in strategies:
+            self.logger.info(f"Strategy `{strategy.name_id}` begins first training...")
+            tasks = strategy.first_tasks()
+            models = self.trainer.train(tasks, experiment_name=strategy.name_id)
+            models_list.append(models)
+            self.logger.info(f"Finished training {len(models)} models.")
+            # FIXME: Train multiple online models at `first_train` will result in getting too much online models at the
+            # start.
+            online_models = strategy.prepare_online_models(models, **model_kwargs)
+            self.history.setdefault(self.cur_time, {})[strategy] = online_models
+
+        if not self._postpone_action():
+            for strategy, models in zip(strategies, models_list):
+                models = self.trainer.end_train(models, experiment_name=strategy.name_id)
+
+    def routine(
+        self,
+        cur_time: Union[str, pd.Timestamp] = None,
+        task_kwargs: dict = {},
+        model_kwargs: dict = {},
+        signal_kwargs: dict = {},
+    ):
+        """
+        Typical update process for every strategy and record the online history.
+
+        The typical update process after a routine, such as day by day or month by month.
+        The process is: Update predictions -> Prepare tasks -> Prepare online models -> Prepare signals.
+
+        If using DelayTrainer, it can finish training all together after every strategy's prepare_tasks.
+
+        Args:
+            cur_time (Union[str,pd.Timestamp], optional): run routine method in this time. Defaults to None.
+            task_kwargs (dict): the params for `prepare_tasks`
+            model_kwargs (dict): the params for `prepare_online_models`
+            signal_kwargs (dict): the params for `prepare_signals`
+        """
+        if cur_time is None:
+            cur_time = D.calendar(freq=self.freq).max()
+        self.cur_time = pd.Timestamp(cur_time)  # None for latest date
+
+        models_list = []
+        for strategy in self.strategies:
+            self.logger.info(f"Strategy `{strategy.name_id}` begins routine...")
+
+            tasks = strategy.prepare_tasks(self.cur_time, **task_kwargs)
+            models = self.trainer.train(tasks, experiment_name=strategy.name_id)
+            models_list.append(models)
+            self.logger.info(f"Finished training {len(models)} models.")
+            online_models = strategy.prepare_online_models(models, **model_kwargs)
+            self.history.setdefault(self.cur_time, {})[strategy] = online_models
+
+            # The online model may changes in the above processes
+            # So updating the predictions of online models should be the last step
+            if self.status == self.STATUS_ONLINE:
+                strategy.tool.update_online_pred()
+
+        if not self._postpone_action():
+            for strategy, models in zip(self.strategies, models_list):
+                models = self.trainer.end_train(models, experiment_name=strategy.name_id)
+            self.prepare_signals(**signal_kwargs)
+
+    def get_collector(self, **kwargs) -> MergeCollector:
+        """
+        Get the instance of `Collector <../advanced/task_management.html#Task Collecting>`_ to collect results from every strategy.
+        This collector can be a basis as the signals preparation.
+
+        Args:
+            **kwargs: the params for get_collector.
+
+        Returns:
+            MergeCollector: the collector to merge other collectors.
+        """
+        collector_dict = {}
+        for strategy in self.strategies:
+            collector_dict[strategy.name_id] = strategy.get_collector(**kwargs)
+        return MergeCollector(collector_dict, process_list=[])
+
+    def add_strategy(self, strategies: Union[OnlineStrategy, List[OnlineStrategy]]):
+        """
+        Add some new strategies to OnlineManager.
+
+        Args:
+            strategy (Union[OnlineStrategy, List[OnlineStrategy]]): a list of OnlineStrategy
+        """
+        if not isinstance(strategies, list):
+            strategies = [strategies]
+        self.first_train(strategies)
+        self.strategies.extend(strategies)
+
+    def prepare_signals(self, prepare_func: Callable = AverageEnsemble(), over_write=False):
+        """
+        After preparing the data of the last routine (a box in box-plot) which means the end of the routine, we can prepare trading signals for the next routine.
+
+        NOTE: Given a set prediction, all signals before these prediction end times will be prepared well.
+
+        Even if the latest signal already exists, the latest calculation result will be overwritten.
+
+        .. note::
+
+            Given a prediction of a certain time, all signals before this time will be prepared well.
+
+        Args:
+            prepare_func (Callable, optional): Get signals from a dict after collecting. Defaults to AverageEnsemble(), the results collected by MergeCollector must be {xxx:pred}.
+            over_write (bool, optional): If True, the new signals will overwrite. If False, the new signals will append to the end of signals. Defaults to False.
+
+        Returns:
+            pd.DataFrame: the signals.
+        """
+        signals = prepare_func(self.get_collector()())
+        old_signals = self.signals
+        if old_signals is not None and not over_write:
+            old_max = old_signals.index.get_level_values("datetime").max()
+            new_signals = signals.loc[old_max:]
+            signals = pd.concat([old_signals, new_signals], axis=0)
+        else:
+            new_signals = signals
+        self.logger.info(f"Finished preparing new {len(new_signals)} signals.")
+        self.signals = signals
+        return new_signals
+
+    def get_signals(self) -> Union[pd.Series, pd.DataFrame]:
+        """
+        Get prepared online signals.
+
+        Returns:
+            Union[pd.Series, pd.DataFrame]: pd.Series for only one signals every datetime.
+            pd.DataFrame for multiple signals, for example, buy and sell operations use different trading signals.
+        """
+        return self.signals
+
+    SIM_LOG_LEVEL = logging.INFO + 1  # when simulating, reduce information
+    SIM_LOG_NAME = "SIMULATE_INFO"
+
+    def simulate(
+        self, end_time=None, frequency="day", task_kwargs={}, model_kwargs={}, signal_kwargs={}
+    ) -> Union[pd.Series, pd.DataFrame]:
+        """
+        Starting from the current time, this method will simulate every routine in OnlineManager until the end time.
+
+        Considering the parallel training, the models and signals can be prepared after all routine simulating.
+
+        The delay training way can be ``DelayTrainer`` and the delay preparing signals way can be ``delay_prepare``.
+
+        Args:
+            end_time: the time the simulation will end
+            frequency: the calendar frequency
+            task_kwargs (dict): the params for `prepare_tasks`
+            model_kwargs (dict): the params for `prepare_online_models`
+            signal_kwargs (dict): the params for `prepare_signals`
+
+        Returns:
+            Union[pd.Series, pd.DataFrame]: pd.Series for only one signals every datetime.
+            pd.DataFrame for multiple signals, for example, buy and sell operations use different trading signals.
+        """
+        self.status = self.STATUS_SIMULATING
+        cal = D.calendar(start_time=self.cur_time, end_time=end_time, freq=frequency)
+        self.first_train()
+
+        simulate_level = self.SIM_LOG_LEVEL
+        set_global_logger_level(simulate_level)
+        logging.addLevelName(simulate_level, self.SIM_LOG_NAME)
+
+        for cur_time in cal:
+            self.logger.log(level=simulate_level, msg=f"Simulating at {str(cur_time)}......")
+            self.routine(
+                cur_time,
+                task_kwargs=task_kwargs,
+                model_kwargs=model_kwargs,
+                signal_kwargs=signal_kwargs,
+            )
+        # delay prepare the models and signals
+        if self._postpone_action():
+            self.delay_prepare(model_kwargs=model_kwargs, signal_kwargs=signal_kwargs)
+
+        # FIXME: get logging level firstly and restore it here
+        set_global_logger_level(logging.DEBUG)
+        self.logger.info(f"Finished preparing signals")
+        self.status = self.STATUS_ONLINE
+        return self.get_signals()
+
+    def delay_prepare(self, model_kwargs={}, signal_kwargs={}):
+        """
+        Prepare all models and signals if something is waiting for preparation.
+
+        Args:
+            model_kwargs: the params for `end_train`
+            signal_kwargs: the params for `prepare_signals`
+        """
+        # FIXME:
+        # This method is not implemented in the proper way!!!
+        last_models = {}
+        signals_time = D.calendar()[0]
+        need_prepare = False
+        for cur_time, strategy_models in self.history.items():
+            self.cur_time = cur_time
+
+            for strategy, models in strategy_models.items():
+                # only new online models need to prepare
+                if last_models.setdefault(strategy, set()) != set(models):
+                    models = self.trainer.end_train(models, experiment_name=strategy.name_id, **model_kwargs)
+                    strategy.tool.reset_online_tag(models)
+                    need_prepare = True
+                last_models[strategy] = set(models)
+
+            if need_prepare:
+                # NOTE: Assumption: the predictions of online models need less than next cur_time, or this method will work in a wrong way.
+                self.prepare_signals(**signal_kwargs)
+                if signals_time > cur_time:
+                    # FIXME: if use DelayTrainer and worker (and worker is faster than main progress), there are some possibilities of showing this warning.
+                    self.logger.warn(
+                        f"The signals have already parpred to {signals_time} by last preparation, but current time is only {cur_time}. This may be because the online models predict more than they should, which can cause signals to be contaminated by the offline models."
+                    )
+                need_prepare = False
+                signals_time = self.signals.index.get_level_values("datetime").max()
```

## qlib/workflow/online/strategy.py

 * *Ordering differences only*

```diff
@@ -1,209 +1,209 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-"""
-OnlineStrategy module is an element of online serving.
-"""
-
-from typing import List, Union
-from qlib.log import get_module_logger
-from qlib.model.ens.group import RollingGroup
-from qlib.utils import transform_end_date
-from qlib.workflow.online.utils import OnlineTool, OnlineToolR
-from qlib.workflow.recorder import Recorder
-from qlib.workflow.task.collect import Collector, RecorderCollector
-from qlib.workflow.task.gen import RollingGen, task_generator
-from qlib.workflow.task.utils import TimeAdjuster
-
-
-class OnlineStrategy:
-    """
-    OnlineStrategy is working with `Online Manager <#Online Manager>`_, responding to how the tasks are generated, the models are updated and signals are prepared.
-    """
-
-    def __init__(self, name_id: str):
-        """
-        Init OnlineStrategy.
-        This module **MUST** use `Trainer <../reference/api.html#qlib.model.trainer.Trainer>`_ to finishing model training.
-
-        Args:
-            name_id (str): a unique name or id.
-            trainer (qlib.model.trainer.Trainer, optional): a instance of Trainer. Defaults to None.
-        """
-        self.name_id = name_id
-        self.logger = get_module_logger(self.__class__.__name__)
-        self.tool = OnlineTool()
-
-    def prepare_tasks(self, cur_time, **kwargs) -> List[dict]:
-        """
-        After the end of a routine, check whether we need to prepare and train some new tasks based on cur_time (None for latest)..
-        Return the new tasks waiting for training.
-
-        You can find the last online models by OnlineTool.online_models.
-        """
-        raise NotImplementedError(f"Please implement the `prepare_tasks` method.")
-
-    def prepare_online_models(self, trained_models, cur_time=None) -> List[object]:
-        """
-        Select some models from trained models and set them to online models.
-        This is a typical implementation to online all trained models, you can override it to implement the complex method.
-        You can find the last online models by OnlineTool.online_models if you still need them.
-
-        NOTE: Reset all online models to trained models. If there are no trained models, then do nothing.
-
-        **NOTE**:
-            Current implementation is very naive. Here is a more complex situation which is more closer to the
-            practical scenarios.
-            1. Train new models at the day before `test_start` (at time stamp `T`)
-            2. Switch models at the `test_start` (at time timestamp `T + 1` typically)
-
-        Args:
-            models (list): a list of models.
-            cur_time (pd.Dataframe): current time from OnlineManger. None for the latest.
-
-        Returns:
-            List[object]: a list of online models.
-        """
-        if not trained_models:
-            return self.tool.online_models()
-        self.tool.reset_online_tag(trained_models)
-        return trained_models
-
-    def first_tasks(self) -> List[dict]:
-        """
-        Generate a series of tasks firstly and return them.
-        """
-        raise NotImplementedError(f"Please implement the `first_tasks` method.")
-
-    def get_collector(self) -> Collector:
-        """
-        Get the instance of `Collector <../advanced/task_management.html#Task Collecting>`_ to collect different results of this strategy.
-
-        For example:
-            1) collect predictions in Recorder
-            2) collect signals in a txt file
-
-        Returns:
-            Collector
-        """
-        raise NotImplementedError(f"Please implement the `get_collector` method.")
-
-
-class RollingStrategy(OnlineStrategy):
-
-    """
-    This example strategy always uses the latest rolling model sas online models.
-    """
-
-    def __init__(
-        self,
-        name_id: str,
-        task_template: Union[dict, List[dict]],
-        rolling_gen: RollingGen,
-    ):
-        """
-        Init RollingStrategy.
-
-        Assumption: the str of name_id, the experiment name, and the trainer's experiment name are the same.
-
-        Args:
-            name_id (str): a unique name or id. Will be also the name of the Experiment.
-            task_template (Union[dict, List[dict]]): a list of task_template or a single template, which will be used to generate many tasks using rolling_gen.
-            rolling_gen (RollingGen): an instance of RollingGen
-        """
-        super().__init__(name_id=name_id)
-        self.exp_name = self.name_id
-        if not isinstance(task_template, list):
-            task_template = [task_template]
-        self.task_template = task_template
-        self.rg = rolling_gen
-        assert issubclass(self.rg.__class__, RollingGen), "The rolling strategy relies on the feature if RollingGen"
-        self.tool = OnlineToolR(self.exp_name)
-        self.ta = TimeAdjuster()
-
-    def get_collector(self, process_list=[RollingGroup()], rec_key_func=None, rec_filter_func=None, artifacts_key=None):
-        """
-        Get the instance of `Collector <../advanced/task_management.html#Task Collecting>`_ to collect results. The returned collector must distinguish results in different models.
-
-        Assumption: the models can be distinguished based on the model name and rolling test segments.
-        If you do not want this assumption, please implement your method or use another rec_key_func.
-
-        Args:
-            rec_key_func (Callable): a function to get the key of a recorder. If None, use recorder id.
-            rec_filter_func (Callable, optional): filter the recorder by return True or False. Defaults to None.
-            artifacts_key (List[str], optional): the artifacts key you want to get. If None, get all artifacts.
-        """
-
-        def rec_key(recorder):
-            task_config = recorder.load_object("task")
-            model_key = task_config["model"]["class"]
-            rolling_key = task_config["dataset"]["kwargs"]["segments"]["test"]
-            return model_key, rolling_key
-
-        if rec_key_func is None:
-            rec_key_func = rec_key
-
-        artifacts_collector = RecorderCollector(
-            experiment=self.exp_name,
-            process_list=process_list,
-            rec_key_func=rec_key_func,
-            rec_filter_func=rec_filter_func,
-            artifacts_key=artifacts_key,
-        )
-
-        return artifacts_collector
-
-    def first_tasks(self) -> List[dict]:
-        """
-        Use rolling_gen to generate different tasks based on task_template.
-
-        Returns:
-            List[dict]: a list of tasks
-        """
-        return task_generator(
-            tasks=self.task_template,
-            generators=self.rg,  # generate different date segment
-        )
-
-    def prepare_tasks(self, cur_time) -> List[dict]:
-        """
-        Prepare new tasks based on cur_time (None for the latest).
-
-        You can find the last online models by OnlineToolR.online_models.
-
-        Returns:
-            List[dict]: a list of new tasks.
-        """
-        # TODO: filter recorders by latest test segments is not a necessary
-        latest_records, max_test = self._list_latest(self.tool.online_models())
-        if max_test is None:
-            self.logger.warn(f"No latest online recorders, no new tasks.")
-            return []
-        calendar_latest = transform_end_date(cur_time)
-        self.logger.info(
-            f"The interval between current time {calendar_latest} and last rolling test begin time {max_test[0]} is {self.ta.cal_interval(calendar_latest, max_test[0])}, the rolling step is {self.rg.step}"
-        )
-        res = []
-        for rec in latest_records:
-            task = rec.load_object("task")
-            res.extend(self.rg.gen_following_tasks(task, calendar_latest))
-        return res
-
-    def _list_latest(self, rec_list: List[Recorder]):
-        """
-        List latest recorder form rec_list
-
-        Args:
-            rec_list (List[Recorder]): a list of Recorder
-
-        Returns:
-            List[Recorder], pd.Timestamp: the latest recorders and their test end time
-        """
-        if len(rec_list) == 0:
-            return rec_list, None
-        max_test = max(rec.load_object("task")["dataset"]["kwargs"]["segments"]["test"] for rec in rec_list)
-        latest_rec = []
-        for rec in rec_list:
-            if rec.load_object("task")["dataset"]["kwargs"]["segments"]["test"] == max_test:
-                latest_rec.append(rec)
-        return latest_rec, max_test
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+"""
+OnlineStrategy module is an element of online serving.
+"""
+
+from typing import List, Union
+from qlib.log import get_module_logger
+from qlib.model.ens.group import RollingGroup
+from qlib.utils import transform_end_date
+from qlib.workflow.online.utils import OnlineTool, OnlineToolR
+from qlib.workflow.recorder import Recorder
+from qlib.workflow.task.collect import Collector, RecorderCollector
+from qlib.workflow.task.gen import RollingGen, task_generator
+from qlib.workflow.task.utils import TimeAdjuster
+
+
+class OnlineStrategy:
+    """
+    OnlineStrategy is working with `Online Manager <#Online Manager>`_, responding to how the tasks are generated, the models are updated and signals are prepared.
+    """
+
+    def __init__(self, name_id: str):
+        """
+        Init OnlineStrategy.
+        This module **MUST** use `Trainer <../reference/api.html#qlib.model.trainer.Trainer>`_ to finishing model training.
+
+        Args:
+            name_id (str): a unique name or id.
+            trainer (qlib.model.trainer.Trainer, optional): a instance of Trainer. Defaults to None.
+        """
+        self.name_id = name_id
+        self.logger = get_module_logger(self.__class__.__name__)
+        self.tool = OnlineTool()
+
+    def prepare_tasks(self, cur_time, **kwargs) -> List[dict]:
+        """
+        After the end of a routine, check whether we need to prepare and train some new tasks based on cur_time (None for latest)..
+        Return the new tasks waiting for training.
+
+        You can find the last online models by OnlineTool.online_models.
+        """
+        raise NotImplementedError(f"Please implement the `prepare_tasks` method.")
+
+    def prepare_online_models(self, trained_models, cur_time=None) -> List[object]:
+        """
+        Select some models from trained models and set them to online models.
+        This is a typical implementation to online all trained models, you can override it to implement the complex method.
+        You can find the last online models by OnlineTool.online_models if you still need them.
+
+        NOTE: Reset all online models to trained models. If there are no trained models, then do nothing.
+
+        **NOTE**:
+            Current implementation is very naive. Here is a more complex situation which is more closer to the
+            practical scenarios.
+            1. Train new models at the day before `test_start` (at time stamp `T`)
+            2. Switch models at the `test_start` (at time timestamp `T + 1` typically)
+
+        Args:
+            models (list): a list of models.
+            cur_time (pd.Dataframe): current time from OnlineManger. None for the latest.
+
+        Returns:
+            List[object]: a list of online models.
+        """
+        if not trained_models:
+            return self.tool.online_models()
+        self.tool.reset_online_tag(trained_models)
+        return trained_models
+
+    def first_tasks(self) -> List[dict]:
+        """
+        Generate a series of tasks firstly and return them.
+        """
+        raise NotImplementedError(f"Please implement the `first_tasks` method.")
+
+    def get_collector(self) -> Collector:
+        """
+        Get the instance of `Collector <../advanced/task_management.html#Task Collecting>`_ to collect different results of this strategy.
+
+        For example:
+            1) collect predictions in Recorder
+            2) collect signals in a txt file
+
+        Returns:
+            Collector
+        """
+        raise NotImplementedError(f"Please implement the `get_collector` method.")
+
+
+class RollingStrategy(OnlineStrategy):
+
+    """
+    This example strategy always uses the latest rolling model sas online models.
+    """
+
+    def __init__(
+        self,
+        name_id: str,
+        task_template: Union[dict, List[dict]],
+        rolling_gen: RollingGen,
+    ):
+        """
+        Init RollingStrategy.
+
+        Assumption: the str of name_id, the experiment name, and the trainer's experiment name are the same.
+
+        Args:
+            name_id (str): a unique name or id. Will be also the name of the Experiment.
+            task_template (Union[dict, List[dict]]): a list of task_template or a single template, which will be used to generate many tasks using rolling_gen.
+            rolling_gen (RollingGen): an instance of RollingGen
+        """
+        super().__init__(name_id=name_id)
+        self.exp_name = self.name_id
+        if not isinstance(task_template, list):
+            task_template = [task_template]
+        self.task_template = task_template
+        self.rg = rolling_gen
+        assert issubclass(self.rg.__class__, RollingGen), "The rolling strategy relies on the feature if RollingGen"
+        self.tool = OnlineToolR(self.exp_name)
+        self.ta = TimeAdjuster()
+
+    def get_collector(self, process_list=[RollingGroup()], rec_key_func=None, rec_filter_func=None, artifacts_key=None):
+        """
+        Get the instance of `Collector <../advanced/task_management.html#Task Collecting>`_ to collect results. The returned collector must distinguish results in different models.
+
+        Assumption: the models can be distinguished based on the model name and rolling test segments.
+        If you do not want this assumption, please implement your method or use another rec_key_func.
+
+        Args:
+            rec_key_func (Callable): a function to get the key of a recorder. If None, use recorder id.
+            rec_filter_func (Callable, optional): filter the recorder by return True or False. Defaults to None.
+            artifacts_key (List[str], optional): the artifacts key you want to get. If None, get all artifacts.
+        """
+
+        def rec_key(recorder):
+            task_config = recorder.load_object("task")
+            model_key = task_config["model"]["class"]
+            rolling_key = task_config["dataset"]["kwargs"]["segments"]["test"]
+            return model_key, rolling_key
+
+        if rec_key_func is None:
+            rec_key_func = rec_key
+
+        artifacts_collector = RecorderCollector(
+            experiment=self.exp_name,
+            process_list=process_list,
+            rec_key_func=rec_key_func,
+            rec_filter_func=rec_filter_func,
+            artifacts_key=artifacts_key,
+        )
+
+        return artifacts_collector
+
+    def first_tasks(self) -> List[dict]:
+        """
+        Use rolling_gen to generate different tasks based on task_template.
+
+        Returns:
+            List[dict]: a list of tasks
+        """
+        return task_generator(
+            tasks=self.task_template,
+            generators=self.rg,  # generate different date segment
+        )
+
+    def prepare_tasks(self, cur_time) -> List[dict]:
+        """
+        Prepare new tasks based on cur_time (None for the latest).
+
+        You can find the last online models by OnlineToolR.online_models.
+
+        Returns:
+            List[dict]: a list of new tasks.
+        """
+        # TODO: filter recorders by latest test segments is not a necessary
+        latest_records, max_test = self._list_latest(self.tool.online_models())
+        if max_test is None:
+            self.logger.warn(f"No latest online recorders, no new tasks.")
+            return []
+        calendar_latest = transform_end_date(cur_time)
+        self.logger.info(
+            f"The interval between current time {calendar_latest} and last rolling test begin time {max_test[0]} is {self.ta.cal_interval(calendar_latest, max_test[0])}, the rolling step is {self.rg.step}"
+        )
+        res = []
+        for rec in latest_records:
+            task = rec.load_object("task")
+            res.extend(self.rg.gen_following_tasks(task, calendar_latest))
+        return res
+
+    def _list_latest(self, rec_list: List[Recorder]):
+        """
+        List latest recorder form rec_list
+
+        Args:
+            rec_list (List[Recorder]): a list of Recorder
+
+        Returns:
+            List[Recorder], pd.Timestamp: the latest recorders and their test end time
+        """
+        if len(rec_list) == 0:
+            return rec_list, None
+        max_test = max(rec.load_object("task")["dataset"]["kwargs"]["segments"]["test"] for rec in rec_list)
+        latest_rec = []
+        for rec in rec_list:
+            if rec.load_object("task")["dataset"]["kwargs"]["segments"]["test"] == max_test:
+                latest_rec.append(rec)
+        return latest_rec, max_test
```

## qlib/workflow/online/update.py

 * *Ordering differences only*

```diff
@@ -1,298 +1,298 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-"""
-Updater is a module to update artifacts such as predictions when the stock data is updating.
-"""
-
-from abc import ABCMeta, abstractmethod
-from typing import Optional
-
-import pandas as pd
-from qlib import get_module_logger
-from qlib.data import D
-from qlib.data.dataset import Dataset, DatasetH, TSDatasetH
-from qlib.data.dataset.handler import DataHandlerLP
-from qlib.model import Model
-from qlib.utils import get_date_by_shift
-from qlib.workflow.recorder import Recorder
-from qlib.workflow.record_temp import SignalRecord
-
-
-class RMDLoader:
-    """
-    Recorder Model Dataset Loader
-    """
-
-    def __init__(self, rec: Recorder):
-        self.rec = rec
-
-    def get_dataset(
-        self, start_time, end_time, segments=None, unprepared_dataset: Optional[DatasetH] = None
-    ) -> DatasetH:
-        """
-        Load, config and setup dataset.
-
-        This dataset is for inference.
-
-        Args:
-            start_time :
-                the start_time of underlying data
-            end_time :
-                the end_time of underlying data
-            segments : dict
-                the segments config for dataset
-                Due to the time series dataset (TSDatasetH), the test segments maybe different from start_time and end_time
-            unprepared_dataset: Optional[DatasetH]
-                if user don't want to load dataset from recorder, please specify user's dataset
-
-        Returns:
-            DatasetH: the instance of DatasetH
-
-        """
-        if segments is None:
-            segments = {"test": (start_time, end_time)}
-        if unprepared_dataset is None:
-            dataset: DatasetH = self.rec.load_object("dataset")
-        else:
-            dataset = unprepared_dataset
-        dataset.config(handler_kwargs={"start_time": start_time, "end_time": end_time}, segments=segments)
-        dataset.setup_data(handler_kwargs={"init_type": DataHandlerLP.IT_LS})
-        return dataset
-
-    def get_model(self) -> Model:
-        return self.rec.load_object("params.pkl")
-
-
-class RecordUpdater(metaclass=ABCMeta):
-    """
-    Update a specific recorders
-    """
-
-    def __init__(self, record: Recorder, *args, **kwargs):
-        self.record = record
-        self.logger = get_module_logger(self.__class__.__name__)
-
-    @abstractmethod
-    def update(self, *args, **kwargs):
-        """
-        Update info for specific recorder
-        """
-
-
-class DSBasedUpdater(RecordUpdater, metaclass=ABCMeta):
-    """
-    Dataset-Based Updater
-
-    - Providing updating feature for Updating data based on Qlib Dataset
-
-    Assumption
-
-    - Based on Qlib dataset
-    - The data to be updated is a multi-level index pd.DataFrame. For example label, prediction.
-
-        .. code-block::
-
-                                     LABEL0
-            datetime   instrument
-            2021-05-10 SH600000    0.006965
-                       SH600004    0.003407
-            ...                         ...
-            2021-05-28 SZ300498    0.015748
-                       SZ300676   -0.001321
-    """
-
-    def __init__(
-        self,
-        record: Recorder,
-        to_date=None,
-        from_date=None,
-        hist_ref: Optional[int] = None,
-        freq="day",
-        fname="pred.pkl",
-        loader_cls: type = RMDLoader,
-    ):
-        """
-        Init PredUpdater.
-
-        Expected behavior in following cases:
-
-        - if `to_date` is greater than the max date in the calendar, the data will be updated to the latest date
-        - if there are data before `from_date` or after `to_date`, only the data between `from_date` and `to_date` are affected.
-
-        Args:
-            record : Recorder
-            to_date :
-                update to prediction to the `to_date`
-
-                if to_date is None:
-
-                    data will updated to the latest date.
-            from_date :
-                the update will start from `from_date`
-
-                if from_date is None:
-
-                    the updating will occur on the next tick after the latest data in historical data
-            hist_ref : int
-                Sometimes, the dataset will have historical depends.
-                Leave the problem to users to set the length of historical dependency
-                If user doesn't specify this parameter, Updater will try to load dataset to automatically determine the hist_ref
-
-                .. note::
-
-                    the start_time is not included in the `hist_ref`; So the `hist_ref` will be `step_len - 1` in most cases
-
-            loader_cls : type
-                the class to load the model and dataset
-
-        """
-        # TODO: automate this hist_ref in the future.
-        super().__init__(record=record)
-
-        self.to_date = to_date
-        self.hist_ref = hist_ref
-        self.freq = freq
-        self.fname = fname
-        self.rmdl = loader_cls(rec=record)
-
-        latest_date = D.calendar(freq=freq)[-1]
-        if to_date is None:
-            to_date = latest_date
-        to_date = pd.Timestamp(to_date)
-
-        if to_date >= latest_date:
-            self.logger.warning(
-                f"The given `to_date`({to_date}) is later than `latest_date`({latest_date}). So `to_date` is clipped to `latest_date`."
-            )
-            to_date = latest_date
-        self.to_date = to_date
-
-        # FIXME: it will raise error when running routine with delay trainer
-        # should we use another prediction updater for delay trainer?
-        self.old_data: pd.DataFrame = record.load_object(fname)
-        if from_date is None:
-            # dropna is for being compatible to some data with future information(e.g. label)
-            # The recent label data should be updated together
-            self.last_end = self.old_data.dropna().index.get_level_values("datetime").max()
-        else:
-            self.last_end = get_date_by_shift(from_date, -1, align="right")
-
-    def prepare_data(self, unprepared_dataset: Optional[DatasetH] = None) -> DatasetH:
-        """
-        Load dataset
-        - if unprepared_dataset is specified, then prepare the dataset directly
-        - Otherwise,
-
-        Separating this function will make it easier to reuse the dataset
-
-        Returns:
-            DatasetH: the instance of DatasetH
-        """
-        # automatically getting the historical dependency if not specified
-        if self.hist_ref is None:
-            dataset: DatasetH = self.record.load_object("dataset") if unprepared_dataset is None else unprepared_dataset
-            # Special treatment of historical dependencies
-            if isinstance(dataset, TSDatasetH):
-                hist_ref = dataset.step_len - 1
-            else:
-                hist_ref = 0  # if only the lastest data is used, then only current data will be used and no historical data will be used
-        else:
-            hist_ref = self.hist_ref
-
-        start_time_buffer = get_date_by_shift(
-            self.last_end, -hist_ref + 1, clip_shift=False, freq=self.freq  # pylint: disable=E1130
-        )
-        start_time = get_date_by_shift(self.last_end, 1, freq=self.freq)
-        seg = {"test": (start_time, self.to_date)}
-        return self.rmdl.get_dataset(
-            start_time=start_time_buffer, end_time=self.to_date, segments=seg, unprepared_dataset=unprepared_dataset
-        )
-
-    def update(self, dataset: DatasetH = None, write: bool = True, ret_new: bool = False) -> Optional[object]:
-        """
-        Parameters
-        ----------
-        dataset : DatasetH
-            DatasetH: the instance of DatasetH. None for prepare it again.
-        write : bool
-            will the the write action be executed
-        ret_new : bool
-            will the updated data be returned
-
-        Returns
-        -------
-        Optional[object]
-            the updated dataset
-        """
-        # FIXME: the problem below is not solved
-        # The model dumped on GPU instances can not be loaded on CPU instance. Follow exception will raised
-        # RuntimeError: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.
-        # https://github.com/pytorch/pytorch/issues/16797
-
-        if self.last_end >= self.to_date:
-            self.logger.info(
-                f"The data in {self.record.info['id']} are latest ({self.last_end}). No need to update to {self.to_date}."
-            )
-            return
-
-        # load dataset
-        if dataset is None:
-            # For reusing the dataset
-            dataset = self.prepare_data()
-
-        updated_data = self.get_update_data(dataset)
-
-        if write:
-            self.record.save_objects(**{self.fname: updated_data})
-        if ret_new:
-            return updated_data
-
-    @abstractmethod
-    def get_update_data(self, dataset: Dataset) -> pd.DataFrame:
-        """
-        return the updated data based on the given dataset
-
-        The difference between `get_update_data` and `update`
-        - `update_date` only include some data specific feature
-        - `update` include some general routine steps(e.g. prepare dataset, checking)
-        """
-
-
-def _replace_range(data, new_data):
-    dates = new_data.index.get_level_values("datetime")
-    data = data.sort_index()
-    data = data.drop(data.loc[dates.min() : dates.max()].index)
-    cb_data = pd.concat([data, new_data], axis=0)
-    cb_data = cb_data[~cb_data.index.duplicated(keep="last")].sort_index()
-    return cb_data
-
-
-class PredUpdater(DSBasedUpdater):
-    """
-    Update the prediction in the Recorder
-    """
-
-    def get_update_data(self, dataset: Dataset) -> pd.DataFrame:
-        # Load model
-        model = self.rmdl.get_model()
-        new_pred: pd.Series = model.predict(dataset)
-        data = _replace_range(self.old_data, new_pred.to_frame("score"))
-        self.logger.info(f"Finish updating new {new_pred.shape[0]} predictions in {self.record.info['id']}.")
-        return data
-
-
-class LabelUpdater(DSBasedUpdater):
-    """
-    Update the label in the recorder
-
-    Assumption
-    - The label is generated from record_temp.SignalRecord.
-    """
-
-    def __init__(self, record: Recorder, to_date=None, **kwargs):
-        super().__init__(record, to_date=to_date, fname="label.pkl", **kwargs)
-
-    def get_update_data(self, dataset: Dataset) -> pd.DataFrame:
-        new_label = SignalRecord.generate_label(dataset)
-        cb_data = _replace_range(self.old_data.sort_index(), new_label)
-        return cb_data
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+"""
+Updater is a module to update artifacts such as predictions when the stock data is updating.
+"""
+
+from abc import ABCMeta, abstractmethod
+from typing import Optional
+
+import pandas as pd
+from qlib import get_module_logger
+from qlib.data import D
+from qlib.data.dataset import Dataset, DatasetH, TSDatasetH
+from qlib.data.dataset.handler import DataHandlerLP
+from qlib.model import Model
+from qlib.utils import get_date_by_shift
+from qlib.workflow.recorder import Recorder
+from qlib.workflow.record_temp import SignalRecord
+
+
+class RMDLoader:
+    """
+    Recorder Model Dataset Loader
+    """
+
+    def __init__(self, rec: Recorder):
+        self.rec = rec
+
+    def get_dataset(
+        self, start_time, end_time, segments=None, unprepared_dataset: Optional[DatasetH] = None
+    ) -> DatasetH:
+        """
+        Load, config and setup dataset.
+
+        This dataset is for inference.
+
+        Args:
+            start_time :
+                the start_time of underlying data
+            end_time :
+                the end_time of underlying data
+            segments : dict
+                the segments config for dataset
+                Due to the time series dataset (TSDatasetH), the test segments maybe different from start_time and end_time
+            unprepared_dataset: Optional[DatasetH]
+                if user don't want to load dataset from recorder, please specify user's dataset
+
+        Returns:
+            DatasetH: the instance of DatasetH
+
+        """
+        if segments is None:
+            segments = {"test": (start_time, end_time)}
+        if unprepared_dataset is None:
+            dataset: DatasetH = self.rec.load_object("dataset")
+        else:
+            dataset = unprepared_dataset
+        dataset.config(handler_kwargs={"start_time": start_time, "end_time": end_time}, segments=segments)
+        dataset.setup_data(handler_kwargs={"init_type": DataHandlerLP.IT_LS})
+        return dataset
+
+    def get_model(self) -> Model:
+        return self.rec.load_object("params.pkl")
+
+
+class RecordUpdater(metaclass=ABCMeta):
+    """
+    Update a specific recorders
+    """
+
+    def __init__(self, record: Recorder, *args, **kwargs):
+        self.record = record
+        self.logger = get_module_logger(self.__class__.__name__)
+
+    @abstractmethod
+    def update(self, *args, **kwargs):
+        """
+        Update info for specific recorder
+        """
+
+
+class DSBasedUpdater(RecordUpdater, metaclass=ABCMeta):
+    """
+    Dataset-Based Updater
+
+    - Providing updating feature for Updating data based on Qlib Dataset
+
+    Assumption
+
+    - Based on Qlib dataset
+    - The data to be updated is a multi-level index pd.DataFrame. For example label, prediction.
+
+        .. code-block::
+
+                                     LABEL0
+            datetime   instrument
+            2021-05-10 SH600000    0.006965
+                       SH600004    0.003407
+            ...                         ...
+            2021-05-28 SZ300498    0.015748
+                       SZ300676   -0.001321
+    """
+
+    def __init__(
+        self,
+        record: Recorder,
+        to_date=None,
+        from_date=None,
+        hist_ref: Optional[int] = None,
+        freq="day",
+        fname="pred.pkl",
+        loader_cls: type = RMDLoader,
+    ):
+        """
+        Init PredUpdater.
+
+        Expected behavior in following cases:
+
+        - if `to_date` is greater than the max date in the calendar, the data will be updated to the latest date
+        - if there are data before `from_date` or after `to_date`, only the data between `from_date` and `to_date` are affected.
+
+        Args:
+            record : Recorder
+            to_date :
+                update to prediction to the `to_date`
+
+                if to_date is None:
+
+                    data will updated to the latest date.
+            from_date :
+                the update will start from `from_date`
+
+                if from_date is None:
+
+                    the updating will occur on the next tick after the latest data in historical data
+            hist_ref : int
+                Sometimes, the dataset will have historical depends.
+                Leave the problem to users to set the length of historical dependency
+                If user doesn't specify this parameter, Updater will try to load dataset to automatically determine the hist_ref
+
+                .. note::
+
+                    the start_time is not included in the `hist_ref`; So the `hist_ref` will be `step_len - 1` in most cases
+
+            loader_cls : type
+                the class to load the model and dataset
+
+        """
+        # TODO: automate this hist_ref in the future.
+        super().__init__(record=record)
+
+        self.to_date = to_date
+        self.hist_ref = hist_ref
+        self.freq = freq
+        self.fname = fname
+        self.rmdl = loader_cls(rec=record)
+
+        latest_date = D.calendar(freq=freq)[-1]
+        if to_date is None:
+            to_date = latest_date
+        to_date = pd.Timestamp(to_date)
+
+        if to_date >= latest_date:
+            self.logger.warning(
+                f"The given `to_date`({to_date}) is later than `latest_date`({latest_date}). So `to_date` is clipped to `latest_date`."
+            )
+            to_date = latest_date
+        self.to_date = to_date
+
+        # FIXME: it will raise error when running routine with delay trainer
+        # should we use another prediction updater for delay trainer?
+        self.old_data: pd.DataFrame = record.load_object(fname)
+        if from_date is None:
+            # dropna is for being compatible to some data with future information(e.g. label)
+            # The recent label data should be updated together
+            self.last_end = self.old_data.dropna().index.get_level_values("datetime").max()
+        else:
+            self.last_end = get_date_by_shift(from_date, -1, align="right")
+
+    def prepare_data(self, unprepared_dataset: Optional[DatasetH] = None) -> DatasetH:
+        """
+        Load dataset
+        - if unprepared_dataset is specified, then prepare the dataset directly
+        - Otherwise,
+
+        Separating this function will make it easier to reuse the dataset
+
+        Returns:
+            DatasetH: the instance of DatasetH
+        """
+        # automatically getting the historical dependency if not specified
+        if self.hist_ref is None:
+            dataset: DatasetH = self.record.load_object("dataset") if unprepared_dataset is None else unprepared_dataset
+            # Special treatment of historical dependencies
+            if isinstance(dataset, TSDatasetH):
+                hist_ref = dataset.step_len - 1
+            else:
+                hist_ref = 0  # if only the lastest data is used, then only current data will be used and no historical data will be used
+        else:
+            hist_ref = self.hist_ref
+
+        start_time_buffer = get_date_by_shift(
+            self.last_end, -hist_ref + 1, clip_shift=False, freq=self.freq  # pylint: disable=E1130
+        )
+        start_time = get_date_by_shift(self.last_end, 1, freq=self.freq)
+        seg = {"test": (start_time, self.to_date)}
+        return self.rmdl.get_dataset(
+            start_time=start_time_buffer, end_time=self.to_date, segments=seg, unprepared_dataset=unprepared_dataset
+        )
+
+    def update(self, dataset: DatasetH = None, write: bool = True, ret_new: bool = False) -> Optional[object]:
+        """
+        Parameters
+        ----------
+        dataset : DatasetH
+            DatasetH: the instance of DatasetH. None for prepare it again.
+        write : bool
+            will the the write action be executed
+        ret_new : bool
+            will the updated data be returned
+
+        Returns
+        -------
+        Optional[object]
+            the updated dataset
+        """
+        # FIXME: the problem below is not solved
+        # The model dumped on GPU instances can not be loaded on CPU instance. Follow exception will raised
+        # RuntimeError: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.
+        # https://github.com/pytorch/pytorch/issues/16797
+
+        if self.last_end >= self.to_date:
+            self.logger.info(
+                f"The data in {self.record.info['id']} are latest ({self.last_end}). No need to update to {self.to_date}."
+            )
+            return
+
+        # load dataset
+        if dataset is None:
+            # For reusing the dataset
+            dataset = self.prepare_data()
+
+        updated_data = self.get_update_data(dataset)
+
+        if write:
+            self.record.save_objects(**{self.fname: updated_data})
+        if ret_new:
+            return updated_data
+
+    @abstractmethod
+    def get_update_data(self, dataset: Dataset) -> pd.DataFrame:
+        """
+        return the updated data based on the given dataset
+
+        The difference between `get_update_data` and `update`
+        - `update_date` only include some data specific feature
+        - `update` include some general routine steps(e.g. prepare dataset, checking)
+        """
+
+
+def _replace_range(data, new_data):
+    dates = new_data.index.get_level_values("datetime")
+    data = data.sort_index()
+    data = data.drop(data.loc[dates.min() : dates.max()].index)
+    cb_data = pd.concat([data, new_data], axis=0)
+    cb_data = cb_data[~cb_data.index.duplicated(keep="last")].sort_index()
+    return cb_data
+
+
+class PredUpdater(DSBasedUpdater):
+    """
+    Update the prediction in the Recorder
+    """
+
+    def get_update_data(self, dataset: Dataset) -> pd.DataFrame:
+        # Load model
+        model = self.rmdl.get_model()
+        new_pred: pd.Series = model.predict(dataset)
+        data = _replace_range(self.old_data, new_pred.to_frame("score"))
+        self.logger.info(f"Finish updating new {new_pred.shape[0]} predictions in {self.record.info['id']}.")
+        return data
+
+
+class LabelUpdater(DSBasedUpdater):
+    """
+    Update the label in the recorder
+
+    Assumption
+    - The label is generated from record_temp.SignalRecord.
+    """
+
+    def __init__(self, record: Recorder, to_date=None, **kwargs):
+        super().__init__(record, to_date=to_date, fname="label.pkl", **kwargs)
+
+    def get_update_data(self, dataset: Dataset) -> pd.DataFrame:
+        new_label = SignalRecord.generate_label(dataset)
+        cb_data = _replace_range(self.old_data.sort_index(), new_label)
+        return cb_data
```

## qlib/workflow/online/utils.py

 * *Ordering differences only*

```diff
@@ -1,187 +1,187 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-"""
-OnlineTool is a module to set and unset a series of `online` models.
-The `online` models are some decisive models in some time points, which can be changed with the change of time.
-This allows us to use efficient submodels as the market-style changing.
-"""
-
-from typing import List, Union
-
-from qlib.log import get_module_logger
-from qlib.utils.exceptions import LoadObjectError
-from qlib.workflow.online.update import PredUpdater
-from qlib.workflow.recorder import Recorder
-from qlib.workflow.task.utils import list_recorders
-
-
-class OnlineTool:
-    """
-    OnlineTool will manage `online` models in an experiment that includes the model recorders.
-    """
-
-    ONLINE_KEY = "online_status"  # the online status key in recorder
-    ONLINE_TAG = "online"  # the 'online' model
-    OFFLINE_TAG = "offline"  # the 'offline' model, not for online serving
-
-    def __init__(self):
-        """
-        Init OnlineTool.
-        """
-        self.logger = get_module_logger(self.__class__.__name__)
-
-    def set_online_tag(self, tag, recorder: Union[list, object]):
-        """
-        Set `tag` to the model to sign whether online.
-
-        Args:
-            tag (str): the tags in `ONLINE_TAG`, `OFFLINE_TAG`
-            recorder (Union[list,object]): the model's recorder
-        """
-        raise NotImplementedError(f"Please implement the `set_online_tag` method.")
-
-    def get_online_tag(self, recorder: object) -> str:
-        """
-        Given a model recorder and return its online tag.
-
-        Args:
-            recorder (Object): the model's recorder
-
-        Returns:
-            str: the online tag
-        """
-        raise NotImplementedError(f"Please implement the `get_online_tag` method.")
-
-    def reset_online_tag(self, recorder: Union[list, object]):
-        """
-        Offline all models and set the recorders to 'online'.
-
-        Args:
-            recorder (Union[list,object]):
-                the recorder you want to reset to 'online'.
-
-        """
-        raise NotImplementedError(f"Please implement the `reset_online_tag` method.")
-
-    def online_models(self) -> list:
-        """
-        Get current `online` models
-
-        Returns:
-            list: a list of `online` models.
-        """
-        raise NotImplementedError(f"Please implement the `online_models` method.")
-
-    def update_online_pred(self, to_date=None):
-        """
-        Update the predictions of `online` models to to_date.
-
-        Args:
-            to_date (pd.Timestamp): the pred before this date will be updated. None for updating to the latest.
-
-        """
-        raise NotImplementedError(f"Please implement the `update_online_pred` method.")
-
-
-class OnlineToolR(OnlineTool):
-    """
-    The implementation of OnlineTool based on (R)ecorder.
-    """
-
-    def __init__(self, default_exp_name: str = None):
-        """
-        Init OnlineToolR.
-
-        Args:
-            default_exp_name (str): the default experiment name.
-        """
-        super().__init__()
-        self.default_exp_name = default_exp_name
-
-    def set_online_tag(self, tag, recorder: Union[Recorder, List]):
-        """
-        Set `tag` to the model's recorder to sign whether online.
-
-        Args:
-            tag (str): the tags in `ONLINE_TAG`, `NEXT_ONLINE_TAG`, `OFFLINE_TAG`
-            recorder (Union[Recorder, List]): a list of Recorder or an instance of Recorder
-        """
-        if isinstance(recorder, Recorder):
-            recorder = [recorder]
-        for rec in recorder:
-            rec.set_tags(**{self.ONLINE_KEY: tag})
-        self.logger.info(f"Set {len(recorder)} models to '{tag}'.")
-
-    def get_online_tag(self, recorder: Recorder) -> str:
-        """
-        Given a model recorder and return its online tag.
-
-        Args:
-            recorder (Recorder): an instance of recorder
-
-        Returns:
-            str: the online tag
-        """
-        tags = recorder.list_tags()
-        return tags.get(self.ONLINE_KEY, self.OFFLINE_TAG)
-
-    def reset_online_tag(self, recorder: Union[Recorder, List], exp_name: str = None):
-        """
-        Offline all models and set the recorders to 'online'.
-
-        Args:
-            recorder (Union[Recorder, List]):
-                the recorder you want to reset to 'online'.
-            exp_name (str): the experiment name. If None, then use default_exp_name.
-
-        """
-        exp_name = self._get_exp_name(exp_name)
-        if isinstance(recorder, Recorder):
-            recorder = [recorder]
-        recs = list_recorders(exp_name)
-        self.set_online_tag(self.OFFLINE_TAG, list(recs.values()))
-        self.set_online_tag(self.ONLINE_TAG, recorder)
-
-    def online_models(self, exp_name: str = None) -> list:
-        """
-        Get current `online` models
-
-        Args:
-            exp_name (str): the experiment name. If None, then use default_exp_name.
-
-        Returns:
-            list: a list of `online` models.
-        """
-        exp_name = self._get_exp_name(exp_name)
-        return list(list_recorders(exp_name, lambda rec: self.get_online_tag(rec) == self.ONLINE_TAG).values())
-
-    def update_online_pred(self, to_date=None, from_date=None, exp_name: str = None):
-        """
-        Update the predictions of online models to to_date.
-
-        Args:
-            to_date (pd.Timestamp): the pred before this date will be updated. None for updating to latest time in Calendar.
-            exp_name (str): the experiment name. If None, then use default_exp_name.
-        """
-        exp_name = self._get_exp_name(exp_name)
-        online_models = self.online_models(exp_name=exp_name)
-        for rec in online_models:
-            try:
-                updater = PredUpdater(rec, to_date=to_date, from_date=from_date)
-            except LoadObjectError as e:
-                # skip the recorder without pred
-                self.logger.warn(f"An exception `{str(e)}` happened when load `pred.pkl`, skip it.")
-                continue
-            updater.update()
-
-        self.logger.info(f"Finished updating {len(online_models)} online model predictions of {exp_name}.")
-
-    def _get_exp_name(self, exp_name):
-        if exp_name is None:
-            if self.default_exp_name is None:
-                raise ValueError(
-                    "Both default_exp_name and exp_name are None. OnlineToolR needs a specific experiment."
-                )
-            exp_name = self.default_exp_name
-        return exp_name
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+"""
+OnlineTool is a module to set and unset a series of `online` models.
+The `online` models are some decisive models in some time points, which can be changed with the change of time.
+This allows us to use efficient submodels as the market-style changing.
+"""
+
+from typing import List, Union
+
+from qlib.log import get_module_logger
+from qlib.utils.exceptions import LoadObjectError
+from qlib.workflow.online.update import PredUpdater
+from qlib.workflow.recorder import Recorder
+from qlib.workflow.task.utils import list_recorders
+
+
+class OnlineTool:
+    """
+    OnlineTool will manage `online` models in an experiment that includes the model recorders.
+    """
+
+    ONLINE_KEY = "online_status"  # the online status key in recorder
+    ONLINE_TAG = "online"  # the 'online' model
+    OFFLINE_TAG = "offline"  # the 'offline' model, not for online serving
+
+    def __init__(self):
+        """
+        Init OnlineTool.
+        """
+        self.logger = get_module_logger(self.__class__.__name__)
+
+    def set_online_tag(self, tag, recorder: Union[list, object]):
+        """
+        Set `tag` to the model to sign whether online.
+
+        Args:
+            tag (str): the tags in `ONLINE_TAG`, `OFFLINE_TAG`
+            recorder (Union[list,object]): the model's recorder
+        """
+        raise NotImplementedError(f"Please implement the `set_online_tag` method.")
+
+    def get_online_tag(self, recorder: object) -> str:
+        """
+        Given a model recorder and return its online tag.
+
+        Args:
+            recorder (Object): the model's recorder
+
+        Returns:
+            str: the online tag
+        """
+        raise NotImplementedError(f"Please implement the `get_online_tag` method.")
+
+    def reset_online_tag(self, recorder: Union[list, object]):
+        """
+        Offline all models and set the recorders to 'online'.
+
+        Args:
+            recorder (Union[list,object]):
+                the recorder you want to reset to 'online'.
+
+        """
+        raise NotImplementedError(f"Please implement the `reset_online_tag` method.")
+
+    def online_models(self) -> list:
+        """
+        Get current `online` models
+
+        Returns:
+            list: a list of `online` models.
+        """
+        raise NotImplementedError(f"Please implement the `online_models` method.")
+
+    def update_online_pred(self, to_date=None):
+        """
+        Update the predictions of `online` models to to_date.
+
+        Args:
+            to_date (pd.Timestamp): the pred before this date will be updated. None for updating to the latest.
+
+        """
+        raise NotImplementedError(f"Please implement the `update_online_pred` method.")
+
+
+class OnlineToolR(OnlineTool):
+    """
+    The implementation of OnlineTool based on (R)ecorder.
+    """
+
+    def __init__(self, default_exp_name: str = None):
+        """
+        Init OnlineToolR.
+
+        Args:
+            default_exp_name (str): the default experiment name.
+        """
+        super().__init__()
+        self.default_exp_name = default_exp_name
+
+    def set_online_tag(self, tag, recorder: Union[Recorder, List]):
+        """
+        Set `tag` to the model's recorder to sign whether online.
+
+        Args:
+            tag (str): the tags in `ONLINE_TAG`, `NEXT_ONLINE_TAG`, `OFFLINE_TAG`
+            recorder (Union[Recorder, List]): a list of Recorder or an instance of Recorder
+        """
+        if isinstance(recorder, Recorder):
+            recorder = [recorder]
+        for rec in recorder:
+            rec.set_tags(**{self.ONLINE_KEY: tag})
+        self.logger.info(f"Set {len(recorder)} models to '{tag}'.")
+
+    def get_online_tag(self, recorder: Recorder) -> str:
+        """
+        Given a model recorder and return its online tag.
+
+        Args:
+            recorder (Recorder): an instance of recorder
+
+        Returns:
+            str: the online tag
+        """
+        tags = recorder.list_tags()
+        return tags.get(self.ONLINE_KEY, self.OFFLINE_TAG)
+
+    def reset_online_tag(self, recorder: Union[Recorder, List], exp_name: str = None):
+        """
+        Offline all models and set the recorders to 'online'.
+
+        Args:
+            recorder (Union[Recorder, List]):
+                the recorder you want to reset to 'online'.
+            exp_name (str): the experiment name. If None, then use default_exp_name.
+
+        """
+        exp_name = self._get_exp_name(exp_name)
+        if isinstance(recorder, Recorder):
+            recorder = [recorder]
+        recs = list_recorders(exp_name)
+        self.set_online_tag(self.OFFLINE_TAG, list(recs.values()))
+        self.set_online_tag(self.ONLINE_TAG, recorder)
+
+    def online_models(self, exp_name: str = None) -> list:
+        """
+        Get current `online` models
+
+        Args:
+            exp_name (str): the experiment name. If None, then use default_exp_name.
+
+        Returns:
+            list: a list of `online` models.
+        """
+        exp_name = self._get_exp_name(exp_name)
+        return list(list_recorders(exp_name, lambda rec: self.get_online_tag(rec) == self.ONLINE_TAG).values())
+
+    def update_online_pred(self, to_date=None, from_date=None, exp_name: str = None):
+        """
+        Update the predictions of online models to to_date.
+
+        Args:
+            to_date (pd.Timestamp): the pred before this date will be updated. None for updating to latest time in Calendar.
+            exp_name (str): the experiment name. If None, then use default_exp_name.
+        """
+        exp_name = self._get_exp_name(exp_name)
+        online_models = self.online_models(exp_name=exp_name)
+        for rec in online_models:
+            try:
+                updater = PredUpdater(rec, to_date=to_date, from_date=from_date)
+            except LoadObjectError as e:
+                # skip the recorder without pred
+                self.logger.warn(f"An exception `{str(e)}` happened when load `pred.pkl`, skip it.")
+                continue
+            updater.update()
+
+        self.logger.info(f"Finished updating {len(online_models)} online model predictions of {exp_name}.")
+
+    def _get_exp_name(self, exp_name):
+        if exp_name is None:
+            if self.default_exp_name is None:
+                raise ValueError(
+                    "Both default_exp_name and exp_name are None. OnlineToolR needs a specific experiment."
+                )
+            exp_name = self.default_exp_name
+        return exp_name
```

## qlib/workflow/task/__init__.py

 * *Ordering differences only*

```diff
@@ -1,13 +1,13 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-"""
-Task related workflow is implemented in this folder
-
-A typical task workflow
-
-| Step                  | Description                                    |
-|-----------------------+------------------------------------------------|
-| TaskGen               | Generating tasks.                              |
-| TaskManager(optional) | Manage generated tasks                         |
-| run task              | retrieve  tasks from TaskManager and run tasks. |
-"""
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+"""
+Task related workflow is implemented in this folder
+
+A typical task workflow
+
+| Step                  | Description                                    |
+|-----------------------+------------------------------------------------|
+| TaskGen               | Generating tasks.                              |
+| TaskManager(optional) | Manage generated tasks                         |
+| run task              | retrieve  tasks from TaskManager and run tasks. |
+"""
```

## qlib/workflow/task/collect.py

 * *Ordering differences only*

```diff
@@ -1,258 +1,258 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-"""
-Collector module can collect objects from everywhere and process them such as merging, grouping, averaging and so on.
-"""
-
-from collections import defaultdict
-from qlib.log import TimeInspector
-from typing import Callable, Dict, Iterable, List
-from qlib.log import get_module_logger
-from qlib.utils.serial import Serializable
-from qlib.utils.exceptions import LoadObjectError
-from qlib.workflow import R
-from qlib.workflow.exp import Experiment
-from qlib.workflow.recorder import Recorder
-
-
-class Collector(Serializable):
-    """The collector to collect different results"""
-
-    pickle_backend = "dill"  # use dill to dump user method
-
-    def __init__(self, process_list=[]):
-        """
-        Init Collector.
-
-        Args:
-            process_list (list or Callable):  the list of processors or the instance of a processor to process dict.
-        """
-        if not isinstance(process_list, list):
-            process_list = [process_list]
-        self.process_list = process_list
-
-    def collect(self) -> dict:
-        """
-        Collect the results and return a dict like {key: things}
-
-        Returns:
-            dict: the dict after collecting.
-
-            For example:
-
-            {"prediction": pd.Series}
-
-            {"IC": {"Xgboost": pd.Series, "LSTM": pd.Series}}
-
-            ...
-        """
-        raise NotImplementedError(f"Please implement the `collect` method.")
-
-    @staticmethod
-    def process_collect(collected_dict, process_list=[], *args, **kwargs) -> dict:
-        """
-        Do a series of processing to the dict returned by collect and return a dict like {key: things}
-        For example, you can group and ensemble.
-
-        Args:
-            collected_dict (dict): the dict return by `collect`
-            process_list (list or Callable): the list of processors or the instance of a processor to process dict.
-                The processor order is the same as the list order.
-                For example: [Group1(..., Ensemble1()), Group2(..., Ensemble2())]
-
-        Returns:
-            dict: the dict after processing.
-        """
-        if not isinstance(process_list, list):
-            process_list = [process_list]
-        result = {}
-        for artifact in collected_dict:
-            value = collected_dict[artifact]
-            for process in process_list:
-                if not callable(process):
-                    raise NotImplementedError(f"{type(process)} is not supported in `process_collect`.")
-                value = process(value, *args, **kwargs)
-            result[artifact] = value
-        return result
-
-    def __call__(self, *args, **kwargs) -> dict:
-        """
-        Do the workflow including ``collect`` and ``process_collect``
-
-        Returns:
-            dict: the dict after collecting and processing.
-        """
-        collected = self.collect()
-        return self.process_collect(collected, self.process_list, *args, **kwargs)
-
-
-class MergeCollector(Collector):
-    """
-    A collector to collect the results of other Collectors
-
-    For example:
-
-        We have 2 collector, which named A and B.
-        A can collect {"prediction": pd.Series} and B can collect {"IC": {"Xgboost": pd.Series, "LSTM": pd.Series}}.
-        Then after this class's collect, we can collect {"A_prediction": pd.Series, "B_IC": {"Xgboost": pd.Series, "LSTM": pd.Series}}
-
-        ...
-
-    """
-
-    def __init__(self, collector_dict: Dict[str, Collector], process_list: List[Callable] = [], merge_func=None):
-        """
-        Init MergeCollector.
-
-        Args:
-            collector_dict (Dict[str,Collector]): the dict like {collector_key, Collector}
-            process_list (List[Callable]): the list of processors or the instance of processor to process dict.
-            merge_func (Callable): a method to generate outermost key. The given params are ``collector_key`` from collector_dict and ``key`` from every collector after collecting.
-                None for using tuple to connect them, such as "ABC"+("a","b") -> ("ABC", ("a","b")).
-        """
-        super().__init__(process_list=process_list)
-        self.collector_dict = collector_dict
-        self.merge_func = merge_func
-
-    def collect(self) -> dict:
-        """
-        Collect all results of collector_dict and change the outermost key to a recombination key.
-
-        Returns:
-            dict: the dict after collecting.
-        """
-        collect_dict = {}
-        for collector_key, collector in self.collector_dict.items():
-            tmp_dict = collector()
-            for key, value in tmp_dict.items():
-                if self.merge_func is not None:
-                    collect_dict[self.merge_func(collector_key, key)] = value
-                else:
-                    collect_dict[(collector_key, key)] = value
-        return collect_dict
-
-
-class RecorderCollector(Collector):
-    ART_KEY_RAW = "__raw"
-
-    def __init__(
-        self,
-        experiment,
-        process_list=[],
-        rec_key_func=None,
-        rec_filter_func=None,
-        artifacts_path={"pred": "pred.pkl"},
-        artifacts_key=None,
-        list_kwargs={},
-        status: Iterable = {Recorder.STATUS_FI},
-    ):
-        """
-        Init RecorderCollector.
-
-        Args:
-            experiment:
-                (Experiment or str): an instance of an Experiment or the name of an Experiment
-                (Callable): an callable function, which returns a list of experiments
-            process_list (list or Callable): the list of processors or the instance of a processor to process dict.
-            rec_key_func (Callable): a function to get the key of a recorder. If None, use recorder id.
-            rec_filter_func (Callable, optional): filter the recorder by return True or False. Defaults to None.
-            artifacts_path (dict, optional): The artifacts name and its path in Recorder. Defaults to {"pred": "pred.pkl", "IC": "sig_analysis/ic.pkl"}.
-            artifacts_key (str or List, optional): the artifacts key you want to get. If None, get all artifacts.
-            list_kwargs (str): arguments for list_recorders function.
-            status (Iterable): only collect recorders with specific status. None indicating collecting all the recorders
-        """
-        super().__init__(process_list=process_list)
-        if isinstance(experiment, str):
-            experiment = R.get_exp(experiment_name=experiment)
-        assert isinstance(experiment, (Experiment, Callable))
-        self.experiment = experiment
-        self.artifacts_path = artifacts_path
-        if rec_key_func is None:
-
-            def rec_key_func(rec):
-                return rec.info["id"]
-
-        if artifacts_key is None:
-            artifacts_key = list(self.artifacts_path.keys())
-        self.rec_key_func = rec_key_func
-        self.artifacts_key = artifacts_key
-        self.rec_filter_func = rec_filter_func
-        self.list_kwargs = list_kwargs
-        self.status = status
-
-    def collect(self, artifacts_key=None, rec_filter_func=None, only_exist=True) -> dict:
-        """
-        Collect different artifacts based on recorder after filtering.
-
-        Args:
-            artifacts_key (str or List, optional): the artifacts key you want to get. If None, use the default.
-            rec_filter_func (Callable, optional): filter the recorder by return True or False. If None, use the default.
-            only_exist (bool, optional): if only collect the artifacts when a recorder really has.
-                If True, the recorder with exception when loading will not be collected. But if False, it will raise the exception.
-
-        Returns:
-            dict: the dict after collected like {artifact: {rec_key: object}}
-        """
-        if artifacts_key is None:
-            artifacts_key = self.artifacts_key
-        if rec_filter_func is None:
-            rec_filter_func = self.rec_filter_func
-
-        if isinstance(artifacts_key, str):
-            artifacts_key = [artifacts_key]
-
-        collect_dict = {}
-        # filter records
-
-        if isinstance(self.experiment, Experiment):
-            with TimeInspector.logt("Time to `list_recorders` in RecorderCollector"):
-                recs = list(self.experiment.list_recorders(**self.list_kwargs).values())
-        elif isinstance(self.experiment, Callable):
-            recs = self.experiment()
-
-        recs = [
-            rec
-            for rec in recs
-            if (
-                (self.status is None or rec.status in self.status) and (rec_filter_func is None or rec_filter_func(rec))
-            )
-        ]
-
-        logger = get_module_logger("RecorderCollector")
-        status_stat = defaultdict(int)
-        for r in recs:
-            status_stat[r.status] += 1
-        logger.info(f"Nubmer of recorders after filter: {status_stat}")
-        for rec in recs:
-            rec_key = self.rec_key_func(rec)
-            for key in artifacts_key:
-                if self.ART_KEY_RAW == key:
-                    artifact = rec
-                else:
-                    try:
-                        artifact = rec.load_object(self.artifacts_path[key])
-                    except LoadObjectError as e:
-                        if only_exist:
-                            # only collect existing artifact
-                            logger.warning(f"Fail to load {self.artifacts_path[key]} and it is ignored.")
-                            continue
-                        raise e
-                # give user some warning if the values are overridden
-                cdd = collect_dict.setdefault(key, {})
-                if rec_key in cdd:
-                    logger.warning(
-                        f"key '{rec_key}' is duplicated. Previous value will be overrides. Please check you `rec_key_func`"
-                    )
-                cdd[rec_key] = artifact
-
-        return collect_dict
-
-    def get_exp_name(self) -> str:
-        """
-        Get experiment name
-
-        Returns:
-            str: experiment name
-        """
-        return self.experiment.name
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+"""
+Collector module can collect objects from everywhere and process them such as merging, grouping, averaging and so on.
+"""
+
+from collections import defaultdict
+from qlib.log import TimeInspector
+from typing import Callable, Dict, Iterable, List
+from qlib.log import get_module_logger
+from qlib.utils.serial import Serializable
+from qlib.utils.exceptions import LoadObjectError
+from qlib.workflow import R
+from qlib.workflow.exp import Experiment
+from qlib.workflow.recorder import Recorder
+
+
+class Collector(Serializable):
+    """The collector to collect different results"""
+
+    pickle_backend = "dill"  # use dill to dump user method
+
+    def __init__(self, process_list=[]):
+        """
+        Init Collector.
+
+        Args:
+            process_list (list or Callable):  the list of processors or the instance of a processor to process dict.
+        """
+        if not isinstance(process_list, list):
+            process_list = [process_list]
+        self.process_list = process_list
+
+    def collect(self) -> dict:
+        """
+        Collect the results and return a dict like {key: things}
+
+        Returns:
+            dict: the dict after collecting.
+
+            For example:
+
+            {"prediction": pd.Series}
+
+            {"IC": {"Xgboost": pd.Series, "LSTM": pd.Series}}
+
+            ...
+        """
+        raise NotImplementedError(f"Please implement the `collect` method.")
+
+    @staticmethod
+    def process_collect(collected_dict, process_list=[], *args, **kwargs) -> dict:
+        """
+        Do a series of processing to the dict returned by collect and return a dict like {key: things}
+        For example, you can group and ensemble.
+
+        Args:
+            collected_dict (dict): the dict return by `collect`
+            process_list (list or Callable): the list of processors or the instance of a processor to process dict.
+                The processor order is the same as the list order.
+                For example: [Group1(..., Ensemble1()), Group2(..., Ensemble2())]
+
+        Returns:
+            dict: the dict after processing.
+        """
+        if not isinstance(process_list, list):
+            process_list = [process_list]
+        result = {}
+        for artifact in collected_dict:
+            value = collected_dict[artifact]
+            for process in process_list:
+                if not callable(process):
+                    raise NotImplementedError(f"{type(process)} is not supported in `process_collect`.")
+                value = process(value, *args, **kwargs)
+            result[artifact] = value
+        return result
+
+    def __call__(self, *args, **kwargs) -> dict:
+        """
+        Do the workflow including ``collect`` and ``process_collect``
+
+        Returns:
+            dict: the dict after collecting and processing.
+        """
+        collected = self.collect()
+        return self.process_collect(collected, self.process_list, *args, **kwargs)
+
+
+class MergeCollector(Collector):
+    """
+    A collector to collect the results of other Collectors
+
+    For example:
+
+        We have 2 collector, which named A and B.
+        A can collect {"prediction": pd.Series} and B can collect {"IC": {"Xgboost": pd.Series, "LSTM": pd.Series}}.
+        Then after this class's collect, we can collect {"A_prediction": pd.Series, "B_IC": {"Xgboost": pd.Series, "LSTM": pd.Series}}
+
+        ...
+
+    """
+
+    def __init__(self, collector_dict: Dict[str, Collector], process_list: List[Callable] = [], merge_func=None):
+        """
+        Init MergeCollector.
+
+        Args:
+            collector_dict (Dict[str,Collector]): the dict like {collector_key, Collector}
+            process_list (List[Callable]): the list of processors or the instance of processor to process dict.
+            merge_func (Callable): a method to generate outermost key. The given params are ``collector_key`` from collector_dict and ``key`` from every collector after collecting.
+                None for using tuple to connect them, such as "ABC"+("a","b") -> ("ABC", ("a","b")).
+        """
+        super().__init__(process_list=process_list)
+        self.collector_dict = collector_dict
+        self.merge_func = merge_func
+
+    def collect(self) -> dict:
+        """
+        Collect all results of collector_dict and change the outermost key to a recombination key.
+
+        Returns:
+            dict: the dict after collecting.
+        """
+        collect_dict = {}
+        for collector_key, collector in self.collector_dict.items():
+            tmp_dict = collector()
+            for key, value in tmp_dict.items():
+                if self.merge_func is not None:
+                    collect_dict[self.merge_func(collector_key, key)] = value
+                else:
+                    collect_dict[(collector_key, key)] = value
+        return collect_dict
+
+
+class RecorderCollector(Collector):
+    ART_KEY_RAW = "__raw"
+
+    def __init__(
+        self,
+        experiment,
+        process_list=[],
+        rec_key_func=None,
+        rec_filter_func=None,
+        artifacts_path={"pred": "pred.pkl"},
+        artifacts_key=None,
+        list_kwargs={},
+        status: Iterable = {Recorder.STATUS_FI},
+    ):
+        """
+        Init RecorderCollector.
+
+        Args:
+            experiment:
+                (Experiment or str): an instance of an Experiment or the name of an Experiment
+                (Callable): an callable function, which returns a list of experiments
+            process_list (list or Callable): the list of processors or the instance of a processor to process dict.
+            rec_key_func (Callable): a function to get the key of a recorder. If None, use recorder id.
+            rec_filter_func (Callable, optional): filter the recorder by return True or False. Defaults to None.
+            artifacts_path (dict, optional): The artifacts name and its path in Recorder. Defaults to {"pred": "pred.pkl", "IC": "sig_analysis/ic.pkl"}.
+            artifacts_key (str or List, optional): the artifacts key you want to get. If None, get all artifacts.
+            list_kwargs (str): arguments for list_recorders function.
+            status (Iterable): only collect recorders with specific status. None indicating collecting all the recorders
+        """
+        super().__init__(process_list=process_list)
+        if isinstance(experiment, str):
+            experiment = R.get_exp(experiment_name=experiment)
+        assert isinstance(experiment, (Experiment, Callable))
+        self.experiment = experiment
+        self.artifacts_path = artifacts_path
+        if rec_key_func is None:
+
+            def rec_key_func(rec):
+                return rec.info["id"]
+
+        if artifacts_key is None:
+            artifacts_key = list(self.artifacts_path.keys())
+        self.rec_key_func = rec_key_func
+        self.artifacts_key = artifacts_key
+        self.rec_filter_func = rec_filter_func
+        self.list_kwargs = list_kwargs
+        self.status = status
+
+    def collect(self, artifacts_key=None, rec_filter_func=None, only_exist=True) -> dict:
+        """
+        Collect different artifacts based on recorder after filtering.
+
+        Args:
+            artifacts_key (str or List, optional): the artifacts key you want to get. If None, use the default.
+            rec_filter_func (Callable, optional): filter the recorder by return True or False. If None, use the default.
+            only_exist (bool, optional): if only collect the artifacts when a recorder really has.
+                If True, the recorder with exception when loading will not be collected. But if False, it will raise the exception.
+
+        Returns:
+            dict: the dict after collected like {artifact: {rec_key: object}}
+        """
+        if artifacts_key is None:
+            artifacts_key = self.artifacts_key
+        if rec_filter_func is None:
+            rec_filter_func = self.rec_filter_func
+
+        if isinstance(artifacts_key, str):
+            artifacts_key = [artifacts_key]
+
+        collect_dict = {}
+        # filter records
+
+        if isinstance(self.experiment, Experiment):
+            with TimeInspector.logt("Time to `list_recorders` in RecorderCollector"):
+                recs = list(self.experiment.list_recorders(**self.list_kwargs).values())
+        elif isinstance(self.experiment, Callable):
+            recs = self.experiment()
+
+        recs = [
+            rec
+            for rec in recs
+            if (
+                (self.status is None or rec.status in self.status) and (rec_filter_func is None or rec_filter_func(rec))
+            )
+        ]
+
+        logger = get_module_logger("RecorderCollector")
+        status_stat = defaultdict(int)
+        for r in recs:
+            status_stat[r.status] += 1
+        logger.info(f"Nubmer of recorders after filter: {status_stat}")
+        for rec in recs:
+            rec_key = self.rec_key_func(rec)
+            for key in artifacts_key:
+                if self.ART_KEY_RAW == key:
+                    artifact = rec
+                else:
+                    try:
+                        artifact = rec.load_object(self.artifacts_path[key])
+                    except LoadObjectError as e:
+                        if only_exist:
+                            # only collect existing artifact
+                            logger.warning(f"Fail to load {self.artifacts_path[key]} and it is ignored.")
+                            continue
+                        raise e
+                # give user some warning if the values are overridden
+                cdd = collect_dict.setdefault(key, {})
+                if rec_key in cdd:
+                    logger.warning(
+                        f"key '{rec_key}' is duplicated. Previous value will be overrides. Please check you `rec_key_func`"
+                    )
+                cdd[rec_key] = artifact
+
+        return collect_dict
+
+    def get_exp_name(self) -> str:
+        """
+        Get experiment name
+
+        Returns:
+            str: experiment name
+        """
+        return self.experiment.name
```

## qlib/workflow/task/gen.py

```diff
@@ -1,352 +1,351 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-"""
-TaskGenerator module can generate many tasks based on TaskGen and some task templates.
-"""
-import abc
-import copy
-import pandas as pd
-from typing import Dict, List, Union, Callable
-
-from qlib.utils import transform_end_date
-from .utils import TimeAdjuster
-
-
-def task_generator(tasks, generators) -> list:
-    """
-    Use a list of TaskGen and a list of task templates to generate different tasks.
-
-    For examples:
-
-        There are 3 task templates a,b,c and 2 TaskGen A,B. A will generates 2 tasks from a template and B will generates 3 tasks from a template.
-        task_generator([a, b, c], [A, B]) will finally generate 3*2*3 = 18 tasks.
-
-    Parameters
-    ----------
-    tasks : List[dict] or dict
-        a list of task templates or a single task
-    generators : List[TaskGen] or TaskGen
-        a list of TaskGen or a single TaskGen
-
-    Returns
-    -------
-    list
-        a list of tasks
-    """
-
-    if isinstance(tasks, dict):
-        tasks = [tasks]
-    if isinstance(generators, TaskGen):
-        generators = [generators]
-
-    # generate gen_task_list
-    for gen in generators:
-        new_task_list = []
-        for task in tasks:
-            new_task_list.extend(gen.generate(task))
-        tasks = new_task_list
-
-    return tasks
-
-
-class TaskGen(metaclass=abc.ABCMeta):
-    """
-    The base class for generating different tasks
-
-    Example 1:
-
-        input: a specific task template and rolling steps
-
-        output: rolling version of the tasks
-
-    Example 2:
-
-        input: a specific task template and losses list
-
-        output: a set of tasks with different losses
-
-    """
-
-    @abc.abstractmethod
-    def generate(self, task: dict) -> List[dict]:
-        """
-        Generate different tasks based on a task template
-
-        Parameters
-        ----------
-        task: dict
-            a task template
-
-        Returns
-        -------
-        typing.List[dict]:
-            A list of tasks
-        """
-
-    def __call__(self, *args, **kwargs):
-        """
-        This is just a syntactic sugar for generate
-        """
-        return self.generate(*args, **kwargs)
-
-
-def handler_mod(task: dict, rolling_gen):
-    """
-    Help to modify the handler end time when using RollingGen
-    It try to handle the following case
-
-    - Hander's data end_time is earlier than  dataset's test_data's segments.
-
-        - To handle this, handler's data's end_time is extended.
-
-    If the handler's end_time is None, then it is not necessary to change it's end time.
-
-    Args:
-        task (dict): a task template
-        rg (RollingGen): an instance of RollingGen
-    """
-    try:
-        interval = rolling_gen.ta.cal_interval(
-            task["dataset"]["kwargs"]["handler"]["kwargs"]["end_time"],
-            task["dataset"]["kwargs"]["segments"][rolling_gen.test_key][1],
-        )
-        # if end_time < the end of test_segments, then change end_time to allow load more data
-        if interval < 0:
-            task["dataset"]["kwargs"]["handler"]["kwargs"]["end_time"] = copy.deepcopy(
-                task["dataset"]["kwargs"]["segments"][rolling_gen.test_key][1]
-            )
-    except KeyError:
-        # Maybe dataset do not have handler, then do nothing.
-        pass
-    except TypeError:
-        # May be the handler is a string. `"handler.pkl"["kwargs"]` will raise TypeError
-        # e.g. a dumped file like file:///<file>/
-        pass
-
-
-def trunc_segments(ta: TimeAdjuster, segments: Dict[str, pd.Timestamp], days, test_key="test"):
-    """
-    To avoid the leakage of future information, the segments should be truncated according to the test start_time
-
-    NOTE:
-        This function will change segments **inplace**
-    """
-    # adjust segment
-    test_start = min(t for t in segments[test_key] if t is not None)
-    for k in list(segments.keys()):
-        if k != test_key:
-            segments[k] = ta.truncate(segments[k], test_start, days)
-
-
-class RollingGen(TaskGen):
-    ROLL_EX = TimeAdjuster.SHIFT_EX  # fixed start date, expanding end date
-    ROLL_SD = TimeAdjuster.SHIFT_SD  # fixed segments size, slide it from start date
-
-    def __init__(
-        self,
-        step: int = 40,
-        rtype: str = ROLL_EX,
-        ds_extra_mod_func: Union[None, Callable] = handler_mod,
-        test_key="test",
-        train_key="train",
-        trunc_days: int = None,
-        task_copy_func: Callable = copy.deepcopy,
-    ):
-        """
-        Generate tasks for rolling
-
-        Parameters
-        ----------
-        step : int
-            step to rolling
-        rtype : str
-            rolling type (expanding, sliding)
-        ds_extra_mod_func: Callable
-            A method like: handler_mod(task: dict, rg: RollingGen)
-            Do some extra action after generating a task. For example, use ``handler_mod`` to modify the end time of the handler of a dataset.
-        trunc_days: int
-            trunc some data to avoid future information leakage
-        task_copy_func: Callable
-            the function to copy entire task. This is very useful when user want to share something between tasks
-        """
-        self.step = step
-        self.rtype = rtype
-        self.ds_extra_mod_func = ds_extra_mod_func
-        self.ta = TimeAdjuster(future=True)
-
-        self.test_key = test_key
-        self.train_key = train_key
-        self.trunc_days = trunc_days
-        self.task_copy_func = task_copy_func
-
-    def _update_task_segs(self, task, segs):
-        # update segments of this task
-        task["dataset"]["kwargs"]["segments"] = copy.deepcopy(segs)
-        if self.ds_extra_mod_func is not None:
-            self.ds_extra_mod_func(task, self)
-
-    def gen_following_tasks(self, task: dict, test_end: pd.Timestamp) -> List[dict]:
-        """
-        generating following rolling tasks for `task` until test_end
-
-        Parameters
-        ----------
-        task : dict
-            Qlib task format
-        test_end : pd.Timestamp
-            the latest rolling task includes `test_end`
-
-        Returns
-        -------
-        List[dict]:
-            the following tasks of `task`(`task` itself is excluded)
-        """
-        prev_seg = task["dataset"]["kwargs"]["segments"]
-        while True:
-            segments = {}
-            try:
-                for k, seg in prev_seg.items():
-                    # decide how to shift
-                    # expanding only for train data, the segments size of test data and valid data won't change
-                    if k == self.train_key and self.rtype == self.ROLL_EX:
-                        rtype = self.ta.SHIFT_EX
-                    else:
-                        rtype = self.ta.SHIFT_SD
-                    # shift the segments data
-                    segments[k] = self.ta.shift(seg, step=self.step, rtype=rtype)
-                if segments[self.test_key][0] > test_end:
-                    break
-            except KeyError:
-                # We reach the end of tasks
-                # No more rolling
-                break
-
-            prev_seg = segments
-            t = self.task_copy_func(task)  # deepcopy is necessary to avoid replace task inplace
-            self._update_task_segs(t, segments)
-            yield t
-
-    def generate(self, task: dict) -> List[dict]:
-        """
-        Converting the task into a rolling task.
-
-        Parameters
-        ----------
-        task: dict
-            A dict describing a task. For example.
-
-            .. code-block:: python
-
-                DEFAULT_TASK = {
-                    "model": {
-                        "class": "LGBModel",
-                        "module_path": "qlib.contrib.model.gbdt",
-                    },
-                    "dataset": {
-                        "class": "DatasetH",
-                        "module_path": "qlib.data.dataset",
-                        "kwargs": {
-                            "handler": {
-                                "class": "Alpha158",
-                                "module_path": "qlib.contrib.data.handler",
-                                "kwargs": {
-                                    "start_time": "2008-01-01",
-                                    "end_time": "2020-08-01",
-                                    "fit_start_time": "2008-01-01",
-                                    "fit_end_time": "2014-12-31",
-                                    "instruments": "csi100",
-                                },
-                            },
-                            "segments": {
-                                "train": ("2008-01-01", "2014-12-31"),
-                                "valid": ("2015-01-01", "2016-12-20"),  # Please avoid leaking the future test data into validation
-                                "test": ("2017-01-01", "2020-08-01"),
-                            },
-                        },
-                    },
-                    "record": [
-                        {
-                            "class": "SignalRecord",
-                            "module_path": "qlib.workflow.record_temp",
-                        },
-                    ]
-                }
-
-        Returns
-        ----------
-        List[dict]: a list of tasks
-        """
-        res = []
-
-        t = self.task_copy_func(task)
-
-        # calculate segments
-
-        # First rolling
-        # 1) prepare the end point
-        segments: dict = copy.deepcopy(self.ta.align_seg(t["dataset"]["kwargs"]["segments"]))
-        test_end = transform_end_date(segments[self.test_key][1])
-        # 2) and init test segments
-        test_start_idx = self.ta.align_idx(segments[self.test_key][0])
-        segments[self.test_key] = (self.ta.get(test_start_idx), self.ta.get(test_start_idx + self.step - 1))
-        if self.trunc_days is not None:
-            trunc_segments(self.ta, segments, self.trunc_days, self.test_key)
-
-        # update segments of this task
-        self._update_task_segs(t, segments)
-
-        res.append(t)
-
-        # Update the following rolling
-        res.extend(self.gen_following_tasks(t, test_end))
-        return res
-
-
-class MultiHorizonGenBase(TaskGen):
-    def __init__(self, horizon: List[int] = [5], label_leak_n=2):
-        """
-        This task generator tries to generate tasks for different horizons based on an existing task
-
-        Parameters
-        ----------
-        horizon : List[int]
-            the possible horizons of the tasks
-        label_leak_n : int
-            How many future days it will take to get complete label after the day making prediction
-            For example:
-            - User make prediction on day `T`(after getting the close price on `T`)
-            - The label is the return of buying stock on `T + 1` and selling it on `T + 2`
-            - the `label_leak_n` will be 2 (e.g. two days of information is leaked to leverage this sample)
-        """
-        self.horizon = list(horizon)
-        self.label_leak_n = label_leak_n
-        self.ta = TimeAdjuster()
-        self.test_key = "test"
-
-    @abc.abstractmethod
-    def set_horizon(self, task: dict, hr: int):
-        """
-        This method is designed to change the task **in place**
-
-        Parameters
-        ----------
-        task : dict
-            Qlib's task
-        hr : int
-            the horizon of task
-        """
-
-    def generate(self, task: dict):
-        res = []
-        for hr in self.horizon:
-
-            # Add horizon
-            t = copy.deepcopy(task)
-            self.set_horizon(t, hr)
-
-            # adjust segment
-            segments = self.ta.align_seg(t["dataset"]["kwargs"]["segments"])
-            trunc_segments(self.ta, segments, days=hr + self.label_leak_n, test_key=self.test_key)
-            t["dataset"]["kwargs"]["segments"] = segments
-            res.append(t)
-        return res
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+"""
+TaskGenerator module can generate many tasks based on TaskGen and some task templates.
+"""
+import abc
+import copy
+import pandas as pd
+from typing import Dict, List, Union, Callable
+
+from qlib.utils import transform_end_date
+from .utils import TimeAdjuster
+
+
+def task_generator(tasks, generators) -> list:
+    """
+    Use a list of TaskGen and a list of task templates to generate different tasks.
+
+    For examples:
+
+        There are 3 task templates a,b,c and 2 TaskGen A,B. A will generates 2 tasks from a template and B will generates 3 tasks from a template.
+        task_generator([a, b, c], [A, B]) will finally generate 3*2*3 = 18 tasks.
+
+    Parameters
+    ----------
+    tasks : List[dict] or dict
+        a list of task templates or a single task
+    generators : List[TaskGen] or TaskGen
+        a list of TaskGen or a single TaskGen
+
+    Returns
+    -------
+    list
+        a list of tasks
+    """
+
+    if isinstance(tasks, dict):
+        tasks = [tasks]
+    if isinstance(generators, TaskGen):
+        generators = [generators]
+
+    # generate gen_task_list
+    for gen in generators:
+        new_task_list = []
+        for task in tasks:
+            new_task_list.extend(gen.generate(task))
+        tasks = new_task_list
+
+    return tasks
+
+
+class TaskGen(metaclass=abc.ABCMeta):
+    """
+    The base class for generating different tasks
+
+    Example 1:
+
+        input: a specific task template and rolling steps
+
+        output: rolling version of the tasks
+
+    Example 2:
+
+        input: a specific task template and losses list
+
+        output: a set of tasks with different losses
+
+    """
+
+    @abc.abstractmethod
+    def generate(self, task: dict) -> List[dict]:
+        """
+        Generate different tasks based on a task template
+
+        Parameters
+        ----------
+        task: dict
+            a task template
+
+        Returns
+        -------
+        typing.List[dict]:
+            A list of tasks
+        """
+
+    def __call__(self, *args, **kwargs):
+        """
+        This is just a syntactic sugar for generate
+        """
+        return self.generate(*args, **kwargs)
+
+
+def handler_mod(task: dict, rolling_gen):
+    """
+    Help to modify the handler end time when using RollingGen
+    It try to handle the following case
+
+    - Hander's data end_time is earlier than  dataset's test_data's segments.
+
+        - To handle this, handler's data's end_time is extended.
+
+    If the handler's end_time is None, then it is not necessary to change it's end time.
+
+    Args:
+        task (dict): a task template
+        rg (RollingGen): an instance of RollingGen
+    """
+    try:
+        interval = rolling_gen.ta.cal_interval(
+            task["dataset"]["kwargs"]["handler"]["kwargs"]["end_time"],
+            task["dataset"]["kwargs"]["segments"][rolling_gen.test_key][1],
+        )
+        # if end_time < the end of test_segments, then change end_time to allow load more data
+        if interval < 0:
+            task["dataset"]["kwargs"]["handler"]["kwargs"]["end_time"] = copy.deepcopy(
+                task["dataset"]["kwargs"]["segments"][rolling_gen.test_key][1]
+            )
+    except KeyError:
+        # Maybe dataset do not have handler, then do nothing.
+        pass
+    except TypeError:
+        # May be the handler is a string. `"handler.pkl"["kwargs"]` will raise TypeError
+        # e.g. a dumped file like file:///<file>/
+        pass
+
+
+def trunc_segments(ta: TimeAdjuster, segments: Dict[str, pd.Timestamp], days, test_key="test"):
+    """
+    To avoid the leakage of future information, the segments should be truncated according to the test start_time
+
+    NOTE:
+        This function will change segments **inplace**
+    """
+    # adjust segment
+    test_start = min(t for t in segments[test_key] if t is not None)
+    for k in list(segments.keys()):
+        if k != test_key:
+            segments[k] = ta.truncate(segments[k], test_start, days)
+
+
+class RollingGen(TaskGen):
+    ROLL_EX = TimeAdjuster.SHIFT_EX  # fixed start date, expanding end date
+    ROLL_SD = TimeAdjuster.SHIFT_SD  # fixed segments size, slide it from start date
+
+    def __init__(
+        self,
+        step: int = 40,
+        rtype: str = ROLL_EX,
+        ds_extra_mod_func: Union[None, Callable] = handler_mod,
+        test_key="test",
+        train_key="train",
+        trunc_days: int = None,
+        task_copy_func: Callable = copy.deepcopy,
+    ):
+        """
+        Generate tasks for rolling
+
+        Parameters
+        ----------
+        step : int
+            step to rolling
+        rtype : str
+            rolling type (expanding, sliding)
+        ds_extra_mod_func: Callable
+            A method like: handler_mod(task: dict, rg: RollingGen)
+            Do some extra action after generating a task. For example, use ``handler_mod`` to modify the end time of the handler of a dataset.
+        trunc_days: int
+            trunc some data to avoid future information leakage
+        task_copy_func: Callable
+            the function to copy entire task. This is very useful when user want to share something between tasks
+        """
+        self.step = step
+        self.rtype = rtype
+        self.ds_extra_mod_func = ds_extra_mod_func
+        self.ta = TimeAdjuster(future=True)
+
+        self.test_key = test_key
+        self.train_key = train_key
+        self.trunc_days = trunc_days
+        self.task_copy_func = task_copy_func
+
+    def _update_task_segs(self, task, segs):
+        # update segments of this task
+        task["dataset"]["kwargs"]["segments"] = copy.deepcopy(segs)
+        if self.ds_extra_mod_func is not None:
+            self.ds_extra_mod_func(task, self)
+
+    def gen_following_tasks(self, task: dict, test_end: pd.Timestamp) -> List[dict]:
+        """
+        generating following rolling tasks for `task` until test_end
+
+        Parameters
+        ----------
+        task : dict
+            Qlib task format
+        test_end : pd.Timestamp
+            the latest rolling task includes `test_end`
+
+        Returns
+        -------
+        List[dict]:
+            the following tasks of `task`(`task` itself is excluded)
+        """
+        prev_seg = task["dataset"]["kwargs"]["segments"]
+        while True:
+            segments = {}
+            try:
+                for k, seg in prev_seg.items():
+                    # decide how to shift
+                    # expanding only for train data, the segments size of test data and valid data won't change
+                    if k == self.train_key and self.rtype == self.ROLL_EX:
+                        rtype = self.ta.SHIFT_EX
+                    else:
+                        rtype = self.ta.SHIFT_SD
+                    # shift the segments data
+                    segments[k] = self.ta.shift(seg, step=self.step, rtype=rtype)
+                if segments[self.test_key][0] > test_end:
+                    break
+            except KeyError:
+                # We reach the end of tasks
+                # No more rolling
+                break
+
+            prev_seg = segments
+            t = self.task_copy_func(task)  # deepcopy is necessary to avoid replace task inplace
+            self._update_task_segs(t, segments)
+            yield t
+
+    def generate(self, task: dict) -> List[dict]:
+        """
+        Converting the task into a rolling task.
+
+        Parameters
+        ----------
+        task: dict
+            A dict describing a task. For example.
+
+            .. code-block:: python
+
+                DEFAULT_TASK = {
+                    "model": {
+                        "class": "LGBModel",
+                        "module_path": "qlib.contrib.model.gbdt",
+                    },
+                    "dataset": {
+                        "class": "DatasetH",
+                        "module_path": "qlib.data.dataset",
+                        "kwargs": {
+                            "handler": {
+                                "class": "Alpha158",
+                                "module_path": "qlib.contrib.data.handler",
+                                "kwargs": {
+                                    "start_time": "2008-01-01",
+                                    "end_time": "2020-08-01",
+                                    "fit_start_time": "2008-01-01",
+                                    "fit_end_time": "2014-12-31",
+                                    "instruments": "csi100",
+                                },
+                            },
+                            "segments": {
+                                "train": ("2008-01-01", "2014-12-31"),
+                                "valid": ("2015-01-01", "2016-12-20"),  # Please avoid leaking the future test data into validation
+                                "test": ("2017-01-01", "2020-08-01"),
+                            },
+                        },
+                    },
+                    "record": [
+                        {
+                            "class": "SignalRecord",
+                            "module_path": "qlib.workflow.record_temp",
+                        },
+                    ]
+                }
+
+        Returns
+        ----------
+        List[dict]: a list of tasks
+        """
+        res = []
+
+        t = self.task_copy_func(task)
+
+        # calculate segments
+
+        # First rolling
+        # 1) prepare the end point
+        segments: dict = copy.deepcopy(self.ta.align_seg(t["dataset"]["kwargs"]["segments"]))
+        test_end = transform_end_date(segments[self.test_key][1])
+        # 2) and init test segments
+        test_start_idx = self.ta.align_idx(segments[self.test_key][0])
+        segments[self.test_key] = (self.ta.get(test_start_idx), self.ta.get(test_start_idx + self.step - 1))
+        if self.trunc_days is not None:
+            trunc_segments(self.ta, segments, self.trunc_days, self.test_key)
+
+        # update segments of this task
+        self._update_task_segs(t, segments)
+
+        res.append(t)
+
+        # Update the following rolling
+        res.extend(self.gen_following_tasks(t, test_end))
+        return res
+
+
+class MultiHorizonGenBase(TaskGen):
+    def __init__(self, horizon: List[int] = [5], label_leak_n=2):
+        """
+        This task generator tries to generate tasks for different horizons based on an existing task
+
+        Parameters
+        ----------
+        horizon : List[int]
+            the possible horizons of the tasks
+        label_leak_n : int
+            How many future days it will take to get complete label after the day making prediction
+            For example:
+            - User make prediction on day `T`(after getting the close price on `T`)
+            - The label is the return of buying stock on `T + 1` and selling it on `T + 2`
+            - the `label_leak_n` will be 2 (e.g. two days of information is leaked to leverage this sample)
+        """
+        self.horizon = list(horizon)
+        self.label_leak_n = label_leak_n
+        self.ta = TimeAdjuster()
+        self.test_key = "test"
+
+    @abc.abstractmethod
+    def set_horizon(self, task: dict, hr: int):
+        """
+        This method is designed to change the task **in place**
+
+        Parameters
+        ----------
+        task : dict
+            Qlib's task
+        hr : int
+            the horizon of task
+        """
+
+    def generate(self, task: dict):
+        res = []
+        for hr in self.horizon:
+            # Add horizon
+            t = copy.deepcopy(task)
+            self.set_horizon(t, hr)
+
+            # adjust segment
+            segments = self.ta.align_seg(t["dataset"]["kwargs"]["segments"])
+            trunc_segments(self.ta, segments, days=hr + self.label_leak_n, test_key=self.test_key)
+            t["dataset"]["kwargs"]["segments"] = segments
+            res.append(t)
+        return res
```

## qlib/workflow/task/manage.py

 * *Ordering differences only*

```diff
@@ -1,556 +1,556 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-"""
-TaskManager can fetch unused tasks automatically and manage the lifecycle of a set of tasks with error handling.
-These features can run tasks concurrently and ensure every task will be used only once.
-Task Manager will store all tasks in `MongoDB <https://www.mongodb.com/>`_.
-Users **MUST** finished the configuration of `MongoDB <https://www.mongodb.com/>`_ when using this module.
-
-A task in TaskManager consists of 3 parts
-- tasks description: the desc will define the task
-- tasks status: the status of the task
-- tasks result: A user can get the task with the task description and task result.
-"""
-import concurrent
-import pickle
-import time
-from contextlib import contextmanager
-from typing import Callable, List
-
-import fire
-import pymongo
-from bson.binary import Binary
-from bson.objectid import ObjectId
-from pymongo.errors import InvalidDocument
-from qlib import auto_init, get_module_logger
-from tqdm.cli import tqdm
-
-from .utils import get_mongodb
-from ...config import C
-
-
-class TaskManager:
-    """
-    TaskManager
-
-    Here is what will a task looks like when it created by TaskManager
-
-    .. code-block:: python
-
-        {
-            'def': pickle serialized task definition.  using pickle will make it easier
-            'filter': json-like data. This is for filtering the tasks.
-            'status': 'waiting' | 'running' | 'done'
-            'res': pickle serialized task result,
-        }
-
-    The tasks manager assumes that you will only update the tasks you fetched.
-    The mongo fetch one and update will make it date updating secure.
-
-    This class can be used as a tool from commandline. Here are several examples.
-    You can view the help of manage module with the following commands:
-    python -m qlib.workflow.task.manage -h # show manual of manage module CLI
-    python -m qlib.workflow.task.manage wait -h # show manual of the wait command of manage
-
-    .. code-block:: shell
-
-        python -m qlib.workflow.task.manage -t <pool_name> wait
-        python -m qlib.workflow.task.manage -t <pool_name> task_stat
-
-
-    .. note::
-
-        Assumption: the data in MongoDB was encoded and the data out of MongoDB was decoded
-
-    Here are four status which are:
-
-        STATUS_WAITING: waiting for training
-
-        STATUS_RUNNING: training
-
-        STATUS_PART_DONE: finished some step and waiting for next step
-
-        STATUS_DONE: all work done
-    """
-
-    STATUS_WAITING = "waiting"
-    STATUS_RUNNING = "running"
-    STATUS_DONE = "done"
-    STATUS_PART_DONE = "part_done"
-
-    ENCODE_FIELDS_PREFIX = ["def", "res"]
-
-    def __init__(self, task_pool: str):
-        """
-        Init Task Manager, remember to make the statement of MongoDB url and database name firstly.
-        A TaskManager instance serves a specific task pool.
-        The static method of this module serves the whole MongoDB.
-
-        Parameters
-        ----------
-        task_pool: str
-            the name of Collection in MongoDB
-        """
-        self.task_pool: pymongo.collection.Collection = getattr(get_mongodb(), task_pool)
-        self.logger = get_module_logger(self.__class__.__name__)
-        self.logger.info(f"task_pool:{task_pool}")
-
-    @staticmethod
-    def list() -> list:
-        """
-        List the all collection(task_pool) of the db.
-
-        Returns:
-            list
-        """
-        return get_mongodb().list_collection_names()
-
-    def _encode_task(self, task):
-        for prefix in self.ENCODE_FIELDS_PREFIX:
-            for k in list(task.keys()):
-                if k.startswith(prefix):
-                    task[k] = Binary(pickle.dumps(task[k], protocol=C.dump_protocol_version))
-        return task
-
-    def _decode_task(self, task):
-        """
-        _decode_task is Serialization tool.
-        Mongodb needs JSON, so it needs to convert Python objects into JSON objects through pickle
-
-        Parameters
-        ----------
-        task : dict
-            task information
-
-        Returns
-        -------
-        dict
-            JSON required by mongodb
-        """
-        for prefix in self.ENCODE_FIELDS_PREFIX:
-            for k in list(task.keys()):
-                if k.startswith(prefix):
-                    task[k] = pickle.loads(task[k])
-        return task
-
-    def _dict_to_str(self, flt):
-        return {k: str(v) for k, v in flt.items()}
-
-    def _decode_query(self, query):
-        """
-        If the query includes any `_id`, then it needs `ObjectId` to decode.
-        For example, when using TrainerRM, it needs query `{"_id": {"$in": _id_list}}`. Then we need to `ObjectId` every `_id` in `_id_list`.
-
-        Args:
-            query (dict): query dict. Defaults to {}.
-
-        Returns:
-            dict: the query after decoding.
-        """
-        if "_id" in query:
-            if isinstance(query["_id"], dict):
-                for key in query["_id"]:
-                    query["_id"][key] = [ObjectId(i) for i in query["_id"][key]]
-            else:
-                query["_id"] = ObjectId(query["_id"])
-        return query
-
-    def replace_task(self, task, new_task):
-        """
-        Use a new task to replace a old one
-
-        Args:
-            task: old task
-            new_task: new task
-        """
-        new_task = self._encode_task(new_task)
-        query = {"_id": ObjectId(task["_id"])}
-        try:
-            self.task_pool.replace_one(query, new_task)
-        except InvalidDocument:
-            task["filter"] = self._dict_to_str(task["filter"])
-            self.task_pool.replace_one(query, new_task)
-
-    def insert_task(self, task):
-        """
-        Insert a task.
-
-        Args:
-            task: the task waiting for insert
-
-        Returns:
-            pymongo.results.InsertOneResult
-        """
-        try:
-            insert_result = self.task_pool.insert_one(task)
-        except InvalidDocument:
-            task["filter"] = self._dict_to_str(task["filter"])
-            insert_result = self.task_pool.insert_one(task)
-        return insert_result
-
-    def insert_task_def(self, task_def):
-        """
-        Insert a task to task_pool
-
-        Parameters
-        ----------
-        task_def: dict
-            the task definition
-
-        Returns
-        -------
-        pymongo.results.InsertOneResult
-        """
-        task = self._encode_task(
-            {
-                "def": task_def,
-                "filter": task_def,  # FIXME: catch the raised error
-                "status": self.STATUS_WAITING,
-            }
-        )
-        insert_result = self.insert_task(task)
-        return insert_result
-
-    def create_task(self, task_def_l, dry_run=False, print_nt=False) -> List[str]:
-        """
-        If the tasks in task_def_l are new, then insert new tasks into the task_pool, and record inserted_id.
-        If a task is not new, then just query its _id.
-
-        Parameters
-        ----------
-        task_def_l: list
-            a list of task
-        dry_run: bool
-            if insert those new tasks to task pool
-        print_nt: bool
-            if print new task
-
-        Returns
-        -------
-        List[str]
-            a list of the _id of task_def_l
-        """
-        new_tasks = []
-        _id_list = []
-        for t in task_def_l:
-            try:
-                r = self.task_pool.find_one({"filter": t})
-            except InvalidDocument:
-                r = self.task_pool.find_one({"filter": self._dict_to_str(t)})
-            # When r is none, it indicates that r s a new task
-            if r is None:
-                new_tasks.append(t)
-                if not dry_run:
-                    insert_result = self.insert_task_def(t)
-                    _id_list.append(insert_result.inserted_id)
-                else:
-                    _id_list.append(None)
-            else:
-                _id_list.append(self._decode_task(r)["_id"])
-
-        self.logger.info(f"Total Tasks: {len(task_def_l)}, New Tasks: {len(new_tasks)}")
-
-        if print_nt:  # print new task
-            for t in new_tasks:
-                print(t)
-
-        if dry_run:
-            return []
-
-        return _id_list
-
-    def fetch_task(self, query={}, status=STATUS_WAITING) -> dict:
-        """
-        Use query to fetch tasks.
-
-        Args:
-            query (dict, optional): query dict. Defaults to {}.
-            status (str, optional): [description]. Defaults to STATUS_WAITING.
-
-        Returns:
-            dict: a task(document in collection) after decoding
-        """
-        query = query.copy()
-        query = self._decode_query(query)
-        query.update({"status": status})
-        task = self.task_pool.find_one_and_update(
-            query, {"$set": {"status": self.STATUS_RUNNING}}, sort=[("priority", pymongo.DESCENDING)]
-        )
-        # null will be at the top after sorting when using ASCENDING, so the larger the number higher, the higher the priority
-        if task is None:
-            return None
-        task["status"] = self.STATUS_RUNNING
-        return self._decode_task(task)
-
-    @contextmanager
-    def safe_fetch_task(self, query={}, status=STATUS_WAITING):
-        """
-        Fetch task from task_pool using query with contextmanager
-
-        Parameters
-        ----------
-        query: dict
-            the dict of query
-
-        Returns
-        -------
-        dict: a task(document in collection) after decoding
-        """
-        task = self.fetch_task(query=query, status=status)
-        try:
-            yield task
-        except (Exception, KeyboardInterrupt):  # KeyboardInterrupt is not a subclass of Exception
-            if task is not None:
-                self.logger.info("Returning task before raising error")
-                self.return_task(task, status=status)  # return task as the original status
-                self.logger.info("Task returned")
-            raise
-
-    def task_fetcher_iter(self, query={}):
-        while True:
-            with self.safe_fetch_task(query=query) as task:
-                if task is None:
-                    break
-                yield task
-
-    def query(self, query={}, decode=True):
-        """
-        Query task in collection.
-        This function may raise exception `pymongo.errors.CursorNotFound: cursor id not found` if it takes too long to iterate the generator
-
-        python -m qlib.workflow.task.manage -t <your task pool> query '{"_id": "615498be837d0053acbc5d58"}'
-
-        Parameters
-        ----------
-        query: dict
-            the dict of query
-        decode: bool
-
-        Returns
-        -------
-        dict: a task(document in collection) after decoding
-        """
-        query = query.copy()
-        query = self._decode_query(query)
-        for t in self.task_pool.find(query):
-            yield self._decode_task(t)
-
-    def re_query(self, _id) -> dict:
-        """
-        Use _id to query task.
-
-        Args:
-            _id (str): _id of a document
-
-        Returns:
-            dict: a task(document in collection) after decoding
-        """
-        t = self.task_pool.find_one({"_id": ObjectId(_id)})
-        return self._decode_task(t)
-
-    def commit_task_res(self, task, res, status=STATUS_DONE):
-        """
-        Commit the result to task['res'].
-
-        Args:
-            task ([type]): [description]
-            res (object): the result you want to save
-            status (str, optional): STATUS_WAITING, STATUS_RUNNING, STATUS_DONE, STATUS_PART_DONE. Defaults to STATUS_DONE.
-        """
-        # A workaround to use the class attribute.
-        if status is None:
-            status = TaskManager.STATUS_DONE
-        self.task_pool.update_one(
-            {"_id": task["_id"]},
-            {"$set": {"status": status, "res": Binary(pickle.dumps(res, protocol=C.dump_protocol_version))}},
-        )
-
-    def return_task(self, task, status=STATUS_WAITING):
-        """
-        Return a task to status. Always using in error handling.
-
-        Args:
-            task ([type]): [description]
-            status (str, optional): STATUS_WAITING, STATUS_RUNNING, STATUS_DONE, STATUS_PART_DONE. Defaults to STATUS_WAITING.
-        """
-        if status is None:
-            status = TaskManager.STATUS_WAITING
-        update_dict = {"$set": {"status": status}}
-        self.task_pool.update_one({"_id": task["_id"]}, update_dict)
-
-    def remove(self, query={}):
-        """
-        Remove the task using query
-
-        Parameters
-        ----------
-        query: dict
-            the dict of query
-
-        """
-        query = query.copy()
-        query = self._decode_query(query)
-        self.task_pool.delete_many(query)
-
-    def task_stat(self, query={}) -> dict:
-        """
-        Count the tasks in every status.
-
-        Args:
-            query (dict, optional): the query dict. Defaults to {}.
-
-        Returns:
-            dict
-        """
-        query = query.copy()
-        query = self._decode_query(query)
-        tasks = self.query(query=query, decode=False)
-        status_stat = {}
-        for t in tasks:
-            status_stat[t["status"]] = status_stat.get(t["status"], 0) + 1
-        return status_stat
-
-    def reset_waiting(self, query={}):
-        """
-        Reset all running task into waiting status. Can be used when some running task exit unexpected.
-
-        Args:
-            query (dict, optional): the query dict. Defaults to {}.
-        """
-        query = query.copy()
-        # default query
-        if "status" not in query:
-            query["status"] = self.STATUS_RUNNING
-        return self.reset_status(query=query, status=self.STATUS_WAITING)
-
-    def reset_status(self, query, status):
-        query = query.copy()
-        query = self._decode_query(query)
-        print(self.task_pool.update_many(query, {"$set": {"status": status}}))
-
-    def prioritize(self, task, priority: int):
-        """
-        Set priority for task
-
-        Parameters
-        ----------
-        task : dict
-            The task query from the database
-        priority : int
-            the target priority
-        """
-        update_dict = {"$set": {"priority": priority}}
-        self.task_pool.update_one({"_id": task["_id"]}, update_dict)
-
-    def _get_undone_n(self, task_stat):
-        return (
-            task_stat.get(self.STATUS_WAITING, 0)
-            + task_stat.get(self.STATUS_RUNNING, 0)
-            + task_stat.get(self.STATUS_PART_DONE, 0)
-        )
-
-    def _get_total(self, task_stat):
-        return sum(task_stat.values())
-
-    def wait(self, query={}):
-        """
-        When multiprocessing, the main progress may fetch nothing from TaskManager because there are still some running tasks.
-        So main progress should wait until all tasks are trained well by other progress or machines.
-
-        Args:
-            query (dict, optional): the query dict. Defaults to {}.
-        """
-        task_stat = self.task_stat(query)
-        total = self._get_total(task_stat)
-        last_undone_n = self._get_undone_n(task_stat)
-        if last_undone_n == 0:
-            return
-        self.logger.warning(f"Waiting for {last_undone_n} undone tasks. Please make sure they are running.")
-        with tqdm(total=total, initial=total - last_undone_n) as pbar:
-            while True:
-                time.sleep(10)
-                undone_n = self._get_undone_n(self.task_stat(query))
-                pbar.update(last_undone_n - undone_n)
-                last_undone_n = undone_n
-                if undone_n == 0:
-                    break
-
-    def __str__(self):
-        return f"TaskManager({self.task_pool})"
-
-
-def run_task(
-    task_func: Callable,
-    task_pool: str,
-    query: dict = {},
-    force_release: bool = False,
-    before_status: str = TaskManager.STATUS_WAITING,
-    after_status: str = TaskManager.STATUS_DONE,
-    **kwargs,
-):
-    r"""
-    While the task pool is not empty (has WAITING tasks), use task_func to fetch and run tasks in task_pool
-
-    After running this method, here are 4 situations (before_status -> after_status):
-
-        STATUS_WAITING -> STATUS_DONE: use task["def"] as `task_func` param, it means that the task has not been started
-
-        STATUS_WAITING -> STATUS_PART_DONE: use task["def"] as `task_func` param
-
-        STATUS_PART_DONE -> STATUS_PART_DONE: use task["res"] as `task_func` param, it means that the task has been started but not completed
-
-        STATUS_PART_DONE -> STATUS_DONE: use task["res"] as `task_func` param
-
-    Parameters
-    ----------
-    task_func : Callable
-        def (task_def, \**kwargs) -> <res which will be committed>
-
-        the function to run the task
-    task_pool : str
-        the name of the task pool (Collection in MongoDB)
-    query: dict
-        will use this dict to query task_pool when fetching task
-    force_release : bool
-        will the program force to release the resource
-    before_status : str:
-        the tasks in before_status will be fetched and trained. Can be STATUS_WAITING, STATUS_PART_DONE.
-    after_status : str:
-        the tasks after trained will become after_status. Can be STATUS_WAITING, STATUS_PART_DONE.
-    kwargs
-        the params for `task_func`
-    """
-    tm = TaskManager(task_pool)
-
-    ever_run = False
-
-    while True:
-        with tm.safe_fetch_task(status=before_status, query=query) as task:
-            if task is None:
-                break
-            get_module_logger("run_task").info(task["def"])
-            # when fetching `WAITING` task, use task["def"] to train
-            if before_status == TaskManager.STATUS_WAITING:
-                param = task["def"]
-            # when fetching `PART_DONE` task, use task["res"] to train because the middle result has been saved to task["res"]
-            elif before_status == TaskManager.STATUS_PART_DONE:
-                param = task["res"]
-            else:
-                raise ValueError("The fetched task must be `STATUS_WAITING` or `STATUS_PART_DONE`!")
-            if force_release:
-                with concurrent.futures.ProcessPoolExecutor(max_workers=1) as executor:
-                    res = executor.submit(task_func, param, **kwargs).result()
-            else:
-                res = task_func(param, **kwargs)
-            tm.commit_task_res(task, res, status=after_status)
-            ever_run = True
-
-    return ever_run
-
-
-if __name__ == "__main__":
-    # This is for using it in cmd
-    # E.g. : `python -m qlib.workflow.task.manage list`
-    auto_init()
-    fire.Fire(TaskManager)
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+
+"""
+TaskManager can fetch unused tasks automatically and manage the lifecycle of a set of tasks with error handling.
+These features can run tasks concurrently and ensure every task will be used only once.
+Task Manager will store all tasks in `MongoDB <https://www.mongodb.com/>`_.
+Users **MUST** finished the configuration of `MongoDB <https://www.mongodb.com/>`_ when using this module.
+
+A task in TaskManager consists of 3 parts
+- tasks description: the desc will define the task
+- tasks status: the status of the task
+- tasks result: A user can get the task with the task description and task result.
+"""
+import concurrent
+import pickle
+import time
+from contextlib import contextmanager
+from typing import Callable, List
+
+import fire
+import pymongo
+from bson.binary import Binary
+from bson.objectid import ObjectId
+from pymongo.errors import InvalidDocument
+from qlib import auto_init, get_module_logger
+from tqdm.cli import tqdm
+
+from .utils import get_mongodb
+from ...config import C
+
+
+class TaskManager:
+    """
+    TaskManager
+
+    Here is what will a task looks like when it created by TaskManager
+
+    .. code-block:: python
+
+        {
+            'def': pickle serialized task definition.  using pickle will make it easier
+            'filter': json-like data. This is for filtering the tasks.
+            'status': 'waiting' | 'running' | 'done'
+            'res': pickle serialized task result,
+        }
+
+    The tasks manager assumes that you will only update the tasks you fetched.
+    The mongo fetch one and update will make it date updating secure.
+
+    This class can be used as a tool from commandline. Here are several examples.
+    You can view the help of manage module with the following commands:
+    python -m qlib.workflow.task.manage -h # show manual of manage module CLI
+    python -m qlib.workflow.task.manage wait -h # show manual of the wait command of manage
+
+    .. code-block:: shell
+
+        python -m qlib.workflow.task.manage -t <pool_name> wait
+        python -m qlib.workflow.task.manage -t <pool_name> task_stat
+
+
+    .. note::
+
+        Assumption: the data in MongoDB was encoded and the data out of MongoDB was decoded
+
+    Here are four status which are:
+
+        STATUS_WAITING: waiting for training
+
+        STATUS_RUNNING: training
+
+        STATUS_PART_DONE: finished some step and waiting for next step
+
+        STATUS_DONE: all work done
+    """
+
+    STATUS_WAITING = "waiting"
+    STATUS_RUNNING = "running"
+    STATUS_DONE = "done"
+    STATUS_PART_DONE = "part_done"
+
+    ENCODE_FIELDS_PREFIX = ["def", "res"]
+
+    def __init__(self, task_pool: str):
+        """
+        Init Task Manager, remember to make the statement of MongoDB url and database name firstly.
+        A TaskManager instance serves a specific task pool.
+        The static method of this module serves the whole MongoDB.
+
+        Parameters
+        ----------
+        task_pool: str
+            the name of Collection in MongoDB
+        """
+        self.task_pool: pymongo.collection.Collection = getattr(get_mongodb(), task_pool)
+        self.logger = get_module_logger(self.__class__.__name__)
+        self.logger.info(f"task_pool:{task_pool}")
+
+    @staticmethod
+    def list() -> list:
+        """
+        List the all collection(task_pool) of the db.
+
+        Returns:
+            list
+        """
+        return get_mongodb().list_collection_names()
+
+    def _encode_task(self, task):
+        for prefix in self.ENCODE_FIELDS_PREFIX:
+            for k in list(task.keys()):
+                if k.startswith(prefix):
+                    task[k] = Binary(pickle.dumps(task[k], protocol=C.dump_protocol_version))
+        return task
+
+    def _decode_task(self, task):
+        """
+        _decode_task is Serialization tool.
+        Mongodb needs JSON, so it needs to convert Python objects into JSON objects through pickle
+
+        Parameters
+        ----------
+        task : dict
+            task information
+
+        Returns
+        -------
+        dict
+            JSON required by mongodb
+        """
+        for prefix in self.ENCODE_FIELDS_PREFIX:
+            for k in list(task.keys()):
+                if k.startswith(prefix):
+                    task[k] = pickle.loads(task[k])
+        return task
+
+    def _dict_to_str(self, flt):
+        return {k: str(v) for k, v in flt.items()}
+
+    def _decode_query(self, query):
+        """
+        If the query includes any `_id`, then it needs `ObjectId` to decode.
+        For example, when using TrainerRM, it needs query `{"_id": {"$in": _id_list}}`. Then we need to `ObjectId` every `_id` in `_id_list`.
+
+        Args:
+            query (dict): query dict. Defaults to {}.
+
+        Returns:
+            dict: the query after decoding.
+        """
+        if "_id" in query:
+            if isinstance(query["_id"], dict):
+                for key in query["_id"]:
+                    query["_id"][key] = [ObjectId(i) for i in query["_id"][key]]
+            else:
+                query["_id"] = ObjectId(query["_id"])
+        return query
+
+    def replace_task(self, task, new_task):
+        """
+        Use a new task to replace a old one
+
+        Args:
+            task: old task
+            new_task: new task
+        """
+        new_task = self._encode_task(new_task)
+        query = {"_id": ObjectId(task["_id"])}
+        try:
+            self.task_pool.replace_one(query, new_task)
+        except InvalidDocument:
+            task["filter"] = self._dict_to_str(task["filter"])
+            self.task_pool.replace_one(query, new_task)
+
+    def insert_task(self, task):
+        """
+        Insert a task.
+
+        Args:
+            task: the task waiting for insert
+
+        Returns:
+            pymongo.results.InsertOneResult
+        """
+        try:
+            insert_result = self.task_pool.insert_one(task)
+        except InvalidDocument:
+            task["filter"] = self._dict_to_str(task["filter"])
+            insert_result = self.task_pool.insert_one(task)
+        return insert_result
+
+    def insert_task_def(self, task_def):
+        """
+        Insert a task to task_pool
+
+        Parameters
+        ----------
+        task_def: dict
+            the task definition
+
+        Returns
+        -------
+        pymongo.results.InsertOneResult
+        """
+        task = self._encode_task(
+            {
+                "def": task_def,
+                "filter": task_def,  # FIXME: catch the raised error
+                "status": self.STATUS_WAITING,
+            }
+        )
+        insert_result = self.insert_task(task)
+        return insert_result
+
+    def create_task(self, task_def_l, dry_run=False, print_nt=False) -> List[str]:
+        """
+        If the tasks in task_def_l are new, then insert new tasks into the task_pool, and record inserted_id.
+        If a task is not new, then just query its _id.
+
+        Parameters
+        ----------
+        task_def_l: list
+            a list of task
+        dry_run: bool
+            if insert those new tasks to task pool
+        print_nt: bool
+            if print new task
+
+        Returns
+        -------
+        List[str]
+            a list of the _id of task_def_l
+        """
+        new_tasks = []
+        _id_list = []
+        for t in task_def_l:
+            try:
+                r = self.task_pool.find_one({"filter": t})
+            except InvalidDocument:
+                r = self.task_pool.find_one({"filter": self._dict_to_str(t)})
+            # When r is none, it indicates that r s a new task
+            if r is None:
+                new_tasks.append(t)
+                if not dry_run:
+                    insert_result = self.insert_task_def(t)
+                    _id_list.append(insert_result.inserted_id)
+                else:
+                    _id_list.append(None)
+            else:
+                _id_list.append(self._decode_task(r)["_id"])
+
+        self.logger.info(f"Total Tasks: {len(task_def_l)}, New Tasks: {len(new_tasks)}")
+
+        if print_nt:  # print new task
+            for t in new_tasks:
+                print(t)
+
+        if dry_run:
+            return []
+
+        return _id_list
+
+    def fetch_task(self, query={}, status=STATUS_WAITING) -> dict:
+        """
+        Use query to fetch tasks.
+
+        Args:
+            query (dict, optional): query dict. Defaults to {}.
+            status (str, optional): [description]. Defaults to STATUS_WAITING.
+
+        Returns:
+            dict: a task(document in collection) after decoding
+        """
+        query = query.copy()
+        query = self._decode_query(query)
+        query.update({"status": status})
+        task = self.task_pool.find_one_and_update(
+            query, {"$set": {"status": self.STATUS_RUNNING}}, sort=[("priority", pymongo.DESCENDING)]
+        )
+        # null will be at the top after sorting when using ASCENDING, so the larger the number higher, the higher the priority
+        if task is None:
+            return None
+        task["status"] = self.STATUS_RUNNING
+        return self._decode_task(task)
+
+    @contextmanager
+    def safe_fetch_task(self, query={}, status=STATUS_WAITING):
+        """
+        Fetch task from task_pool using query with contextmanager
+
+        Parameters
+        ----------
+        query: dict
+            the dict of query
+
+        Returns
+        -------
+        dict: a task(document in collection) after decoding
+        """
+        task = self.fetch_task(query=query, status=status)
+        try:
+            yield task
+        except (Exception, KeyboardInterrupt):  # KeyboardInterrupt is not a subclass of Exception
+            if task is not None:
+                self.logger.info("Returning task before raising error")
+                self.return_task(task, status=status)  # return task as the original status
+                self.logger.info("Task returned")
+            raise
+
+    def task_fetcher_iter(self, query={}):
+        while True:
+            with self.safe_fetch_task(query=query) as task:
+                if task is None:
+                    break
+                yield task
+
+    def query(self, query={}, decode=True):
+        """
+        Query task in collection.
+        This function may raise exception `pymongo.errors.CursorNotFound: cursor id not found` if it takes too long to iterate the generator
+
+        python -m qlib.workflow.task.manage -t <your task pool> query '{"_id": "615498be837d0053acbc5d58"}'
+
+        Parameters
+        ----------
+        query: dict
+            the dict of query
+        decode: bool
+
+        Returns
+        -------
+        dict: a task(document in collection) after decoding
+        """
+        query = query.copy()
+        query = self._decode_query(query)
+        for t in self.task_pool.find(query):
+            yield self._decode_task(t)
+
+    def re_query(self, _id) -> dict:
+        """
+        Use _id to query task.
+
+        Args:
+            _id (str): _id of a document
+
+        Returns:
+            dict: a task(document in collection) after decoding
+        """
+        t = self.task_pool.find_one({"_id": ObjectId(_id)})
+        return self._decode_task(t)
+
+    def commit_task_res(self, task, res, status=STATUS_DONE):
+        """
+        Commit the result to task['res'].
+
+        Args:
+            task ([type]): [description]
+            res (object): the result you want to save
+            status (str, optional): STATUS_WAITING, STATUS_RUNNING, STATUS_DONE, STATUS_PART_DONE. Defaults to STATUS_DONE.
+        """
+        # A workaround to use the class attribute.
+        if status is None:
+            status = TaskManager.STATUS_DONE
+        self.task_pool.update_one(
+            {"_id": task["_id"]},
+            {"$set": {"status": status, "res": Binary(pickle.dumps(res, protocol=C.dump_protocol_version))}},
+        )
+
+    def return_task(self, task, status=STATUS_WAITING):
+        """
+        Return a task to status. Always using in error handling.
+
+        Args:
+            task ([type]): [description]
+            status (str, optional): STATUS_WAITING, STATUS_RUNNING, STATUS_DONE, STATUS_PART_DONE. Defaults to STATUS_WAITING.
+        """
+        if status is None:
+            status = TaskManager.STATUS_WAITING
+        update_dict = {"$set": {"status": status}}
+        self.task_pool.update_one({"_id": task["_id"]}, update_dict)
+
+    def remove(self, query={}):
+        """
+        Remove the task using query
+
+        Parameters
+        ----------
+        query: dict
+            the dict of query
+
+        """
+        query = query.copy()
+        query = self._decode_query(query)
+        self.task_pool.delete_many(query)
+
+    def task_stat(self, query={}) -> dict:
+        """
+        Count the tasks in every status.
+
+        Args:
+            query (dict, optional): the query dict. Defaults to {}.
+
+        Returns:
+            dict
+        """
+        query = query.copy()
+        query = self._decode_query(query)
+        tasks = self.query(query=query, decode=False)
+        status_stat = {}
+        for t in tasks:
+            status_stat[t["status"]] = status_stat.get(t["status"], 0) + 1
+        return status_stat
+
+    def reset_waiting(self, query={}):
+        """
+        Reset all running task into waiting status. Can be used when some running task exit unexpected.
+
+        Args:
+            query (dict, optional): the query dict. Defaults to {}.
+        """
+        query = query.copy()
+        # default query
+        if "status" not in query:
+            query["status"] = self.STATUS_RUNNING
+        return self.reset_status(query=query, status=self.STATUS_WAITING)
+
+    def reset_status(self, query, status):
+        query = query.copy()
+        query = self._decode_query(query)
+        print(self.task_pool.update_many(query, {"$set": {"status": status}}))
+
+    def prioritize(self, task, priority: int):
+        """
+        Set priority for task
+
+        Parameters
+        ----------
+        task : dict
+            The task query from the database
+        priority : int
+            the target priority
+        """
+        update_dict = {"$set": {"priority": priority}}
+        self.task_pool.update_one({"_id": task["_id"]}, update_dict)
+
+    def _get_undone_n(self, task_stat):
+        return (
+            task_stat.get(self.STATUS_WAITING, 0)
+            + task_stat.get(self.STATUS_RUNNING, 0)
+            + task_stat.get(self.STATUS_PART_DONE, 0)
+        )
+
+    def _get_total(self, task_stat):
+        return sum(task_stat.values())
+
+    def wait(self, query={}):
+        """
+        When multiprocessing, the main progress may fetch nothing from TaskManager because there are still some running tasks.
+        So main progress should wait until all tasks are trained well by other progress or machines.
+
+        Args:
+            query (dict, optional): the query dict. Defaults to {}.
+        """
+        task_stat = self.task_stat(query)
+        total = self._get_total(task_stat)
+        last_undone_n = self._get_undone_n(task_stat)
+        if last_undone_n == 0:
+            return
+        self.logger.warning(f"Waiting for {last_undone_n} undone tasks. Please make sure they are running.")
+        with tqdm(total=total, initial=total - last_undone_n) as pbar:
+            while True:
+                time.sleep(10)
+                undone_n = self._get_undone_n(self.task_stat(query))
+                pbar.update(last_undone_n - undone_n)
+                last_undone_n = undone_n
+                if undone_n == 0:
+                    break
+
+    def __str__(self):
+        return f"TaskManager({self.task_pool})"
+
+
+def run_task(
+    task_func: Callable,
+    task_pool: str,
+    query: dict = {},
+    force_release: bool = False,
+    before_status: str = TaskManager.STATUS_WAITING,
+    after_status: str = TaskManager.STATUS_DONE,
+    **kwargs,
+):
+    r"""
+    While the task pool is not empty (has WAITING tasks), use task_func to fetch and run tasks in task_pool
+
+    After running this method, here are 4 situations (before_status -> after_status):
+
+        STATUS_WAITING -> STATUS_DONE: use task["def"] as `task_func` param, it means that the task has not been started
+
+        STATUS_WAITING -> STATUS_PART_DONE: use task["def"] as `task_func` param
+
+        STATUS_PART_DONE -> STATUS_PART_DONE: use task["res"] as `task_func` param, it means that the task has been started but not completed
+
+        STATUS_PART_DONE -> STATUS_DONE: use task["res"] as `task_func` param
+
+    Parameters
+    ----------
+    task_func : Callable
+        def (task_def, \**kwargs) -> <res which will be committed>
+
+        the function to run the task
+    task_pool : str
+        the name of the task pool (Collection in MongoDB)
+    query: dict
+        will use this dict to query task_pool when fetching task
+    force_release : bool
+        will the program force to release the resource
+    before_status : str:
+        the tasks in before_status will be fetched and trained. Can be STATUS_WAITING, STATUS_PART_DONE.
+    after_status : str:
+        the tasks after trained will become after_status. Can be STATUS_WAITING, STATUS_PART_DONE.
+    kwargs
+        the params for `task_func`
+    """
+    tm = TaskManager(task_pool)
+
+    ever_run = False
+
+    while True:
+        with tm.safe_fetch_task(status=before_status, query=query) as task:
+            if task is None:
+                break
+            get_module_logger("run_task").info(task["def"])
+            # when fetching `WAITING` task, use task["def"] to train
+            if before_status == TaskManager.STATUS_WAITING:
+                param = task["def"]
+            # when fetching `PART_DONE` task, use task["res"] to train because the middle result has been saved to task["res"]
+            elif before_status == TaskManager.STATUS_PART_DONE:
+                param = task["res"]
+            else:
+                raise ValueError("The fetched task must be `STATUS_WAITING` or `STATUS_PART_DONE`!")
+            if force_release:
+                with concurrent.futures.ProcessPoolExecutor(max_workers=1) as executor:
+                    res = executor.submit(task_func, param, **kwargs).result()
+            else:
+                res = task_func(param, **kwargs)
+            tm.commit_task_res(task, res, status=after_status)
+            ever_run = True
+
+    return ever_run
+
+
+if __name__ == "__main__":
+    # This is for using it in cmd
+    # E.g. : `python -m qlib.workflow.task.manage list`
+    auto_init()
+    fire.Fire(TaskManager)
```

## qlib/workflow/task/utils.py

```diff
@@ -1,278 +1,308 @@
-# Copyright (c) Microsoft Corporation.
-# Licensed under the MIT License.
-
-"""
-Some tools for task management.
-"""
-
-import bisect
-import pandas as pd
-from qlib.data import D
-from qlib.workflow import R
-from qlib.config import C
-from qlib.log import get_module_logger
-from pymongo import MongoClient
-from pymongo.database import Database
-from typing import Union
-
-
-def get_mongodb() -> Database:
-
-    """
-    Get database in MongoDB, which means you need to declare the address and the name of a database at first.
-
-    For example:
-
-        Using qlib.init():
-
-            .. code-block:: python
-
-                mongo_conf = {
-                    "task_url": task_url,  # your MongoDB url
-                    "task_db_name": task_db_name,  # database name
-                }
-                qlib.init(..., mongo=mongo_conf)
-
-        After qlib.init():
-
-            .. code-block:: python
-
-                C["mongo"] = {
-                    "task_url" : "mongodb://localhost:27017/",
-                    "task_db_name" : "rolling_db"
-                }
-
-    Returns:
-        Database: the Database instance
-    """
-    try:
-        cfg = C["mongo"]
-    except KeyError:
-        get_module_logger("task").error("Please configure `C['mongo']` before using TaskManager")
-        raise
-    get_module_logger("task").info(f"mongo config:{cfg}")
-    client = MongoClient(cfg["task_url"])
-    return client.get_database(name=cfg["task_db_name"])
-
-
-def list_recorders(experiment, rec_filter_func=None):
-    """
-    List all recorders which can pass the filter in an experiment.
-
-    Args:
-        experiment (str or Experiment): the name of an Experiment or an instance
-        rec_filter_func (Callable, optional): return True to retain the given recorder. Defaults to None.
-
-    Returns:
-        dict: a dict {rid: recorder} after filtering.
-    """
-    if isinstance(experiment, str):
-        experiment = R.get_exp(experiment_name=experiment)
-    recs = experiment.list_recorders()
-    recs_flt = {}
-    for rid, rec in recs.items():
-        if rec_filter_func is None or rec_filter_func(rec):
-            recs_flt[rid] = rec
-
-    return recs_flt
-
-
-class TimeAdjuster:
-    """
-    Find appropriate date and adjust date.
-    """
-
-    def __init__(self, future=True, end_time=None):
-        self._future = future
-        self.cals = D.calendar(future=future, end_time=end_time)
-
-    def set_end_time(self, end_time=None):
-        """
-        Set end time. None for use calendar's end time.
-
-        Args:
-            end_time
-        """
-        self.cals = D.calendar(future=self._future, end_time=end_time)
-
-    def get(self, idx: int):
-        """
-        Get datetime by index.
-
-        Parameters
-        ----------
-        idx : int
-            index of the calendar
-        """
-        if idx is None or idx >= len(self.cals):
-            return None
-        return self.cals[idx]
-
-    def max(self) -> pd.Timestamp:
-        """
-        Return the max calendar datetime
-        """
-        return max(self.cals)
-
-    def align_idx(self, time_point, tp_type="start") -> int:
-        """
-        Align the index of time_point in the calendar.
-
-        Parameters
-        ----------
-        time_point
-        tp_type : str
-
-        Returns
-        -------
-        index : int
-        """
-        if time_point is None:
-            # `None` indicates unbounded index/boarder
-            return None
-        time_point = pd.Timestamp(time_point)
-        if tp_type == "start":
-            idx = bisect.bisect_left(self.cals, time_point)
-        elif tp_type == "end":
-            idx = bisect.bisect_right(self.cals, time_point) - 1
-        else:
-            raise NotImplementedError(f"This type of input is not supported")
-        return idx
-
-    def cal_interval(self, time_point_A, time_point_B) -> int:
-        """
-        Calculate the trading day interval (time_point_A - time_point_B)
-
-        Args:
-            time_point_A : time_point_A
-            time_point_B : time_point_B (is the past of time_point_A)
-
-        Returns:
-            int: the interval between A and B
-        """
-        return self.align_idx(time_point_A) - self.align_idx(time_point_B)
-
-    def align_time(self, time_point, tp_type="start") -> pd.Timestamp:
-        """
-        Align time_point to trade date of calendar
-
-        Args:
-            time_point
-                Time point
-            tp_type : str
-                time point type (`"start"`, `"end"`)
-
-        Returns:
-            pd.Timestamp
-        """
-        if time_point is None:
-            return None
-        return self.cals[self.align_idx(time_point, tp_type=tp_type)]
-
-    def align_seg(self, segment: Union[dict, tuple]) -> Union[dict, tuple]:
-        """
-        Align the given date to the trade date
-
-        for example:
-
-            .. code-block:: python
-
-                input: {'train': ('2008-01-01', '2014-12-31'), 'valid': ('2015-01-01', '2016-12-31'), 'test': ('2017-01-01', '2020-08-01')}
-
-                output: {'train': (Timestamp('2008-01-02 00:00:00'), Timestamp('2014-12-31 00:00:00')),
-                        'valid': (Timestamp('2015-01-05 00:00:00'), Timestamp('2016-12-30 00:00:00')),
-                        'test': (Timestamp('2017-01-03 00:00:00'), Timestamp('2020-07-31 00:00:00'))}
-
-        Parameters
-        ----------
-        segment
-
-        Returns
-        -------
-        Union[dict, tuple]: the start and end trade date (pd.Timestamp) between the given start and end date.
-        """
-        if isinstance(segment, dict):
-            return {k: self.align_seg(seg) for k, seg in segment.items()}
-        elif isinstance(segment, (tuple, list)):
-            return self.align_time(segment[0], tp_type="start"), self.align_time(segment[1], tp_type="end")
-        else:
-            raise NotImplementedError(f"This type of input is not supported")
-
-    def truncate(self, segment: tuple, test_start, days: int) -> tuple:
-        """
-        Truncate the segment based on the test_start date
-
-        Parameters
-        ----------
-        segment : tuple
-            time segment
-        test_start
-        days : int
-            The trading days to be truncated
-            the data in this segment may need 'days' data
-            `days` are based on the `test_start`.
-            For example, if the label contains the information of 2 days in the near future, the prediction horizon 1 day.
-            (e.g. the prediction target is `Ref($close, -2)/Ref($close, -1) - 1`)
-            the days should be 2 + 1 == 3 days.
-
-        Returns
-        ---------
-        tuple: new segment
-        """
-        test_idx = self.align_idx(test_start)
-        if isinstance(segment, tuple):
-            new_seg = []
-            for time_point in segment:
-                tp_idx = min(self.align_idx(time_point), test_idx - days)
-                assert tp_idx > 0
-                new_seg.append(self.get(tp_idx))
-            return tuple(new_seg)
-        else:
-            raise NotImplementedError(f"This type of input is not supported")
-
-    SHIFT_SD = "sliding"
-    SHIFT_EX = "expanding"
-
-    def _add_step(self, index, step):
-        if index is None:
-            return None
-        return index + step
-
-    def shift(self, seg: tuple, step: int, rtype=SHIFT_SD) -> tuple:
-        """
-        Shift the datatime of segment
-
-        If there are None (which indicates unbounded index) in the segment, this method will return None.
-
-        Parameters
-        ----------
-        seg :
-            datetime segment
-        step : int
-            rolling step
-        rtype : str
-            rolling type ("sliding" or "expanding")
-
-        Returns
-        --------
-        tuple: new segment
-
-        Raises
-        ------
-        KeyError:
-            shift will raise error if the index(both start and end) is out of self.cal
-        """
-        if isinstance(seg, tuple):
-            start_idx, end_idx = self.align_idx(seg[0], tp_type="start"), self.align_idx(seg[1], tp_type="end")
-            if rtype == self.SHIFT_SD:
-                start_idx = self._add_step(start_idx, step)
-                end_idx = self._add_step(end_idx, step)
-            elif rtype == self.SHIFT_EX:
-                end_idx = self._add_step(end_idx, step)
-            else:
-                raise NotImplementedError(f"This type of input is not supported")
-            if start_idx is not None and start_idx > len(self.cals):
-                raise KeyError("The segment is out of valid calendar")
-            return self.get(start_idx), self.get(end_idx)
-        else:
-            raise NotImplementedError(f"This type of input is not supported")
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+"""
+Some tools for task management.
+"""
+
+import bisect
+from copy import deepcopy
+import pandas as pd
+from qlib.data import D
+from qlib.utils import hash_args
+from qlib.utils.mod import init_instance_by_config
+from qlib.workflow import R
+from qlib.config import C
+from qlib.log import get_module_logger
+from pymongo import MongoClient
+from pymongo.database import Database
+from typing import Union
+from pathlib import Path
+
+
+def get_mongodb() -> Database:
+    """
+    Get database in MongoDB, which means you need to declare the address and the name of a database at first.
+
+    For example:
+
+        Using qlib.init():
+
+            .. code-block:: python
+
+                mongo_conf = {
+                    "task_url": task_url,  # your MongoDB url
+                    "task_db_name": task_db_name,  # database name
+                }
+                qlib.init(..., mongo=mongo_conf)
+
+        After qlib.init():
+
+            .. code-block:: python
+
+                C["mongo"] = {
+                    "task_url" : "mongodb://localhost:27017/",
+                    "task_db_name" : "rolling_db"
+                }
+
+    Returns:
+        Database: the Database instance
+    """
+    try:
+        cfg = C["mongo"]
+    except KeyError:
+        get_module_logger("task").error("Please configure `C['mongo']` before using TaskManager")
+        raise
+    get_module_logger("task").info(f"mongo config:{cfg}")
+    client = MongoClient(cfg["task_url"])
+    return client.get_database(name=cfg["task_db_name"])
+
+
+def list_recorders(experiment, rec_filter_func=None):
+    """
+    List all recorders which can pass the filter in an experiment.
+
+    Args:
+        experiment (str or Experiment): the name of an Experiment or an instance
+        rec_filter_func (Callable, optional): return True to retain the given recorder. Defaults to None.
+
+    Returns:
+        dict: a dict {rid: recorder} after filtering.
+    """
+    if isinstance(experiment, str):
+        experiment = R.get_exp(experiment_name=experiment)
+    recs = experiment.list_recorders()
+    recs_flt = {}
+    for rid, rec in recs.items():
+        if rec_filter_func is None or rec_filter_func(rec):
+            recs_flt[rid] = rec
+
+    return recs_flt
+
+
+class TimeAdjuster:
+    """
+    Find appropriate date and adjust date.
+    """
+
+    def __init__(self, future=True, end_time=None):
+        self._future = future
+        self.cals = D.calendar(future=future, end_time=end_time)
+
+    def set_end_time(self, end_time=None):
+        """
+        Set end time. None for use calendar's end time.
+
+        Args:
+            end_time
+        """
+        self.cals = D.calendar(future=self._future, end_time=end_time)
+
+    def get(self, idx: int):
+        """
+        Get datetime by index.
+
+        Parameters
+        ----------
+        idx : int
+            index of the calendar
+        """
+        if idx is None or idx >= len(self.cals):
+            return None
+        return self.cals[idx]
+
+    def max(self) -> pd.Timestamp:
+        """
+        Return the max calendar datetime
+        """
+        return max(self.cals)
+
+    def align_idx(self, time_point, tp_type="start") -> int:
+        """
+        Align the index of time_point in the calendar.
+
+        Parameters
+        ----------
+        time_point
+        tp_type : str
+
+        Returns
+        -------
+        index : int
+        """
+        if time_point is None:
+            # `None` indicates unbounded index/boarder
+            return None
+        time_point = pd.Timestamp(time_point)
+        if tp_type == "start":
+            idx = bisect.bisect_left(self.cals, time_point)
+        elif tp_type == "end":
+            idx = bisect.bisect_right(self.cals, time_point) - 1
+        else:
+            raise NotImplementedError(f"This type of input is not supported")
+        return idx
+
+    def cal_interval(self, time_point_A, time_point_B) -> int:
+        """
+        Calculate the trading day interval (time_point_A - time_point_B)
+
+        Args:
+            time_point_A : time_point_A
+            time_point_B : time_point_B (is the past of time_point_A)
+
+        Returns:
+            int: the interval between A and B
+        """
+        return self.align_idx(time_point_A) - self.align_idx(time_point_B)
+
+    def align_time(self, time_point, tp_type="start") -> pd.Timestamp:
+        """
+        Align time_point to trade date of calendar
+
+        Args:
+            time_point
+                Time point
+            tp_type : str
+                time point type (`"start"`, `"end"`)
+
+        Returns:
+            pd.Timestamp
+        """
+        if time_point is None:
+            return None
+        return self.cals[self.align_idx(time_point, tp_type=tp_type)]
+
+    def align_seg(self, segment: Union[dict, tuple]) -> Union[dict, tuple]:
+        """
+        Align the given date to the trade date
+
+        for example:
+
+            .. code-block:: python
+
+                input: {'train': ('2008-01-01', '2014-12-31'), 'valid': ('2015-01-01', '2016-12-31'), 'test': ('2017-01-01', '2020-08-01')}
+
+                output: {'train': (Timestamp('2008-01-02 00:00:00'), Timestamp('2014-12-31 00:00:00')),
+                        'valid': (Timestamp('2015-01-05 00:00:00'), Timestamp('2016-12-30 00:00:00')),
+                        'test': (Timestamp('2017-01-03 00:00:00'), Timestamp('2020-07-31 00:00:00'))}
+
+        Parameters
+        ----------
+        segment
+
+        Returns
+        -------
+        Union[dict, tuple]: the start and end trade date (pd.Timestamp) between the given start and end date.
+        """
+        if isinstance(segment, dict):
+            return {k: self.align_seg(seg) for k, seg in segment.items()}
+        elif isinstance(segment, (tuple, list)):
+            return self.align_time(segment[0], tp_type="start"), self.align_time(segment[1], tp_type="end")
+        else:
+            raise NotImplementedError(f"This type of input is not supported")
+
+    def truncate(self, segment: tuple, test_start, days: int) -> tuple:
+        """
+        Truncate the segment based on the test_start date
+
+        Parameters
+        ----------
+        segment : tuple
+            time segment
+        test_start
+        days : int
+            The trading days to be truncated
+            the data in this segment may need 'days' data
+            `days` are based on the `test_start`.
+            For example, if the label contains the information of 2 days in the near future, the prediction horizon 1 day.
+            (e.g. the prediction target is `Ref($close, -2)/Ref($close, -1) - 1`)
+            the days should be 2 + 1 == 3 days.
+
+        Returns
+        ---------
+        tuple: new segment
+        """
+        test_idx = self.align_idx(test_start)
+        if isinstance(segment, tuple):
+            new_seg = []
+            for time_point in segment:
+                tp_idx = min(self.align_idx(time_point), test_idx - days)
+                assert tp_idx > 0
+                new_seg.append(self.get(tp_idx))
+            return tuple(new_seg)
+        else:
+            raise NotImplementedError(f"This type of input is not supported")
+
+    SHIFT_SD = "sliding"
+    SHIFT_EX = "expanding"
+
+    def _add_step(self, index, step):
+        if index is None:
+            return None
+        return index + step
+
+    def shift(self, seg: tuple, step: int, rtype=SHIFT_SD) -> tuple:
+        """
+        Shift the datatime of segment
+
+        If there are None (which indicates unbounded index) in the segment, this method will return None.
+
+        Parameters
+        ----------
+        seg :
+            datetime segment
+        step : int
+            rolling step
+        rtype : str
+            rolling type ("sliding" or "expanding")
+
+        Returns
+        --------
+        tuple: new segment
+
+        Raises
+        ------
+        KeyError:
+            shift will raise error if the index(both start and end) is out of self.cal
+        """
+        if isinstance(seg, tuple):
+            start_idx, end_idx = self.align_idx(seg[0], tp_type="start"), self.align_idx(seg[1], tp_type="end")
+            if rtype == self.SHIFT_SD:
+                start_idx = self._add_step(start_idx, step)
+                end_idx = self._add_step(end_idx, step)
+            elif rtype == self.SHIFT_EX:
+                end_idx = self._add_step(end_idx, step)
+            else:
+                raise NotImplementedError(f"This type of input is not supported")
+            if start_idx is not None and start_idx > len(self.cals):
+                raise KeyError("The segment is out of valid calendar")
+            return self.get(start_idx), self.get(end_idx)
+        else:
+            raise NotImplementedError(f"This type of input is not supported")
+
+
+def replace_task_handler_with_cache(task: dict, cache_dir: Union[str, Path] = ".") -> dict:
+    """
+    Replace the handler in task with a cache handler.
+    It will automatically cache the file and save it in cache_dir.
+
+    >>> import qlib
+    >>> qlib.auto_init()
+    >>> import datetime
+    >>> # it is simplified task
+    >>> task = {"dataset": {"kwargs":{'handler': {'class': 'Alpha158', 'module_path': 'qlib.contrib.data.handler', 'kwargs': {'start_time': datetime.date(2008, 1, 1), 'end_time': datetime.date(2020, 8, 1), 'fit_start_time': datetime.date(2008, 1, 1), 'fit_end_time': datetime.date(2014, 12, 31), 'instruments': 'CSI300'}}}}}
+    >>> new_task = replace_task_handler_with_cache(task)
+    >>> print(new_task)
+    {'dataset': {'kwargs': {'handler': 'file...Alpha158.3584f5f8b4.pkl'}}}
+
+    """
+    cache_dir = Path(cache_dir)
+    task = deepcopy(task)
+    handler = task["dataset"]["kwargs"]["handler"]
+    if isinstance(handler, dict):
+        hash = hash_args(handler)
+        h_path = cache_dir / f"{handler['class']}.{hash[:10]}.pkl"
+        if not h_path.exists():
+            h = init_instance_by_config(handler)
+            h.to_pickle(h_path, dump_all=True)
+        task["dataset"]["kwargs"]["handler"] = f"file://{h_path}"
+    return task
```

## Comparing `pyqlib-0.9.2.1.dist-info/LICENSE` & `pyqlib-0.9.3.dist-info/LICENSE`

 * *Ordering differences only*

 * *Files 14% similar despite different names*

```diff
@@ -1,21 +1,21 @@
-    MIT License
-
-    Copyright (c) Microsoft Corporation.
-
-    Permission is hereby granted, free of charge, to any person obtaining a copy
-    of this software and associated documentation files (the "Software"), to deal
-    in the Software without restriction, including without limitation the rights
-    to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
-    copies of the Software, and to permit persons to whom the Software is
-    furnished to do so, subject to the following conditions:
-
-    The above copyright notice and this permission notice shall be included in all
-    copies or substantial portions of the Software.
-
-    THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
-    IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
-    FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
-    AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
-    LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
-    OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
-    SOFTWARE
+    MIT License
+
+    Copyright (c) Microsoft Corporation.
+
+    Permission is hereby granted, free of charge, to any person obtaining a copy
+    of this software and associated documentation files (the "Software"), to deal
+    in the Software without restriction, including without limitation the rights
+    to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+    copies of the Software, and to permit persons to whom the Software is
+    furnished to do so, subject to the following conditions:
+
+    The above copyright notice and this permission notice shall be included in all
+    copies or substantial portions of the Software.
+
+    THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+    IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+    FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+    AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+    LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+    OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+    SOFTWARE
```

## Comparing `pyqlib-0.9.2.1.dist-info/METADATA` & `pyqlib-0.9.3.dist-info/METADATA`

 * *Files 8% similar despite different names*

```diff
@@ -1,602 +1,616 @@
-Metadata-Version: 2.1
-Name: pyqlib
-Version: 0.9.2.1
-Summary: A Quantitative-research Platform
-Home-page: https://github.com/microsoft/qlib
-License: MIT Licence
-Classifier: Operating System :: POSIX :: Linux
-Classifier: Operating System :: Microsoft :: Windows
-Classifier: Operating System :: MacOS
-Classifier: License :: OSI Approved :: MIT License
-Classifier: Development Status :: 3 - Alpha
-Classifier: Programming Language :: Python
-Classifier: Programming Language :: Python :: 3
-Classifier: Programming Language :: Python :: 3.7
-Classifier: Programming Language :: Python :: 3.8
-Classifier: Programming Language :: Python :: 3.9
-Requires-Python: >=3.5.0
-Description-Content-Type: text/markdown
-License-File: LICENSE
-Requires-Dist: numpy (<1.24,>=1.12.0)
-Requires-Dist: pandas (>=0.25.1)
-Requires-Dist: scipy (>=1.0.0)
-Requires-Dist: requests (>=2.18.0)
-Requires-Dist: sacred (>=0.7.4)
-Requires-Dist: python-socketio
-Requires-Dist: redis (>=3.0.1)
-Requires-Dist: python-redis-lock (>=3.3.1)
-Requires-Dist: schedule (>=0.6.0)
-Requires-Dist: cvxpy (>=1.0.21)
-Requires-Dist: hyperopt (==0.1.2)
-Requires-Dist: fire (>=0.3.1)
-Requires-Dist: statsmodels
-Requires-Dist: xlrd (>=1.0.0)
-Requires-Dist: plotly (>=4.12.0)
-Requires-Dist: matplotlib (>=3.3)
-Requires-Dist: tables (>=3.6.1)
-Requires-Dist: pyyaml (>=5.3.1)
-Requires-Dist: mlflow (<=1.30.0,>=1.12.1)
-Requires-Dist: tqdm
-Requires-Dist: loguru
-Requires-Dist: lightgbm (>=3.3.0)
-Requires-Dist: tornado
-Requires-Dist: joblib (>=0.17.0)
-Requires-Dist: ruamel.yaml (>=0.16.12)
-Requires-Dist: pymongo (==3.7.2)
-Requires-Dist: scikit-learn (>=0.22)
-Requires-Dist: dill
-Requires-Dist: filelock
-Requires-Dist: jinja2 (<3.1.0)
-Requires-Dist: gym
-Requires-Dist: cryptography
-Requires-Dist: dataclasses ; python_version < "3.7"
-Requires-Dist: protobuf (<=3.20.1) ; python_version <= "3.8"
-Provides-Extra: dev
-Requires-Dist: coverage ; extra == 'dev'
-Requires-Dist: pytest (>=3) ; extra == 'dev'
-Requires-Dist: sphinx ; extra == 'dev'
-Requires-Dist: sphinx-rtd-theme ; extra == 'dev'
-Requires-Dist: pre-commit ; extra == 'dev'
-Requires-Dist: wheel ; extra == 'dev'
-Requires-Dist: setuptools ; extra == 'dev'
-Requires-Dist: black ; extra == 'dev'
-Requires-Dist: pylint ; extra == 'dev'
-Requires-Dist: mypy (<0.981) ; extra == 'dev'
-Requires-Dist: flake8 ; extra == 'dev'
-Requires-Dist: nbqa ; extra == 'dev'
-Requires-Dist: jupyter ; extra == 'dev'
-Requires-Dist: nbconvert ; extra == 'dev'
-Requires-Dist: importlib-metadata (<5.0.0) ; extra == 'dev'
-Requires-Dist: readthedocs-sphinx-ext ; extra == 'dev'
-Requires-Dist: cmake ; extra == 'dev'
-Requires-Dist: lxml ; extra == 'dev'
-Requires-Dist: baostock ; extra == 'dev'
-Requires-Dist: yahooquery ; extra == 'dev'
-Requires-Dist: beautifulsoup4 ; extra == 'dev'
-Requires-Dist: tianshou (<=0.4.10) ; extra == 'dev'
-Requires-Dist: gym (>=0.24) ; extra == 'dev'
-Provides-Extra: rl
-Requires-Dist: tianshou (<=0.4.10) ; extra == 'rl'
-Requires-Dist: torch ; extra == 'rl'
-
-[![Python Versions](https://img.shields.io/pypi/pyversions/pyqlib.svg?logo=python&logoColor=white)](https://pypi.org/project/pyqlib/#files)
-[![Platform](https://img.shields.io/badge/platform-linux%20%7C%20windows%20%7C%20macos-lightgrey)](https://pypi.org/project/pyqlib/#files)
-[![PypI Versions](https://img.shields.io/pypi/v/pyqlib)](https://pypi.org/project/pyqlib/#history)
-[![Upload Python Package](https://github.com/microsoft/qlib/workflows/Upload%20Python%20Package/badge.svg)](https://pypi.org/project/pyqlib/)
-[![Github Actions Test Status](https://github.com/microsoft/qlib/workflows/Test/badge.svg?branch=main)](https://github.com/microsoft/qlib/actions)
-[![Documentation Status](https://readthedocs.org/projects/qlib/badge/?version=latest)](https://qlib.readthedocs.io/en/latest/?badge=latest)
-[![License](https://img.shields.io/pypi/l/pyqlib)](LICENSE)
-[![Join the chat at https://gitter.im/Microsoft/qlib](https://badges.gitter.im/Microsoft/qlib.svg)](https://gitter.im/Microsoft/qlib?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)
-
-## :newspaper: **What's NEW!** &nbsp;   :sparkling_heart: 
-Recent released features
-| Feature | Status |
-| --                      | ------    |
-| KRNN and Sandwich models | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/1414/) on May 26, 2023 |
-| Release Qlib v0.9.0 | :octocat: [Released](https://github.com/microsoft/qlib/releases/tag/v0.9.0) on Dec 9, 2022 |
-| RL Learning Framework | :hammer: :chart_with_upwards_trend: Released on Nov 10, 2022. [#1332](https://github.com/microsoft/qlib/pull/1332), [#1322](https://github.com/microsoft/qlib/pull/1322), [#1316](https://github.com/microsoft/qlib/pull/1316),[#1299](https://github.com/microsoft/qlib/pull/1299),[#1263](https://github.com/microsoft/qlib/pull/1263), [#1244](https://github.com/microsoft/qlib/pull/1244), [#1169](https://github.com/microsoft/qlib/pull/1169), [#1125](https://github.com/microsoft/qlib/pull/1125), [#1076](https://github.com/microsoft/qlib/pull/1076)|
-| HIST and IGMTF models | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/1040) on Apr 10, 2022 |
-| Qlib [notebook tutorial](https://github.com/microsoft/qlib/tree/main/examples/tutorial) | 📖 [Released](https://github.com/microsoft/qlib/pull/1037) on Apr 7, 2022 | 
-| Ibovespa index data | :rice: [Released](https://github.com/microsoft/qlib/pull/990) on Apr 6, 2022 |
-| Point-in-Time database | :hammer: [Released](https://github.com/microsoft/qlib/pull/343) on Mar 10, 2022 |
-| Arctic Provider Backend & Orderbook data example | :hammer: [Released](https://github.com/microsoft/qlib/pull/744) on Jan 17, 2022 |
-| Meta-Learning-based framework & DDG-DA  | :chart_with_upwards_trend:  :hammer: [Released](https://github.com/microsoft/qlib/pull/743) on Jan 10, 2022 | 
-| Planning-based portfolio optimization | :hammer: [Released](https://github.com/microsoft/qlib/pull/754) on Dec 28, 2021 | 
-| Release Qlib v0.8.0 | :octocat: [Released](https://github.com/microsoft/qlib/releases/tag/v0.8.0) on Dec 8, 2021 |
-| ADD model | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/704) on Nov 22, 2021 |
-| ADARNN  model | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/689) on Nov 14, 2021 |
-| TCN  model | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/668) on Nov 4, 2021 |
-| Nested Decision Framework | :hammer: [Released](https://github.com/microsoft/qlib/pull/438) on Oct 1, 2021. [Example](https://github.com/microsoft/qlib/blob/main/examples/nested_decision_execution/workflow.py) and [Doc](https://qlib.readthedocs.io/en/latest/component/highfreq.html) |
-| Temporal Routing Adaptor (TRA) | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/531) on July 30, 2021 |
-| Transformer & Localformer | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/508) on July 22, 2021 |
-| Release Qlib v0.7.0 | :octocat: [Released](https://github.com/microsoft/qlib/releases/tag/v0.7.0) on July 12, 2021 |
-| TCTS Model | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/491) on July 1, 2021 |
-| Online serving and automatic model rolling | :hammer:  [Released](https://github.com/microsoft/qlib/pull/290) on May 17, 2021 | 
-| DoubleEnsemble Model | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/286) on Mar 2, 2021 | 
-| High-frequency data processing example | :hammer: [Released](https://github.com/microsoft/qlib/pull/257) on Feb 5, 2021  |
-| High-frequency trading example | :chart_with_upwards_trend: [Part of code released](https://github.com/microsoft/qlib/pull/227) on Jan 28, 2021  | 
-| High-frequency data(1min) | :rice: [Released](https://github.com/microsoft/qlib/pull/221) on Jan 27, 2021 |
-| Tabnet Model | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/205) on Jan 22, 2021 |
-
-Features released before 2021 are not listed here.
-
-<p align="center">
-  <img src="http://fintech.msra.cn/images_v070/logo/1.png" />
-</p>
-
-Qlib is an open-source, AI-oriented quantitative investment platform that aims to realize the potential, empower research, and create value using AI technologies in quantitative investment, from exploring ideas to implementing productions. Qlib supports diverse machine learning modeling paradigms, including supervised learning, market dynamics modeling, and reinforcement learning.
-
-An increasing number of SOTA Quant research works/papers in diverse paradigms are being released in Qlib to collaboratively solve key challenges in quantitative investment. For example, 1) using supervised learning to mine the market's complex non-linear patterns from rich and heterogeneous financial data, 2) modeling the dynamic nature of the financial market using adaptive concept drift technology, and 3) using reinforcement learning to model continuous investment decisions and assist investors in optimizing their trading strategies.
-
-It contains the full ML pipeline of data processing, model training, back-testing; and covers the entire chain of quantitative investment: alpha seeking, risk modeling, portfolio optimization, and order execution. 
-For more details, please refer to our paper ["Qlib: An AI-oriented Quantitative Investment Platform"](https://arxiv.org/abs/2009.11189).
-
-
-<table>
-  <tbody>
-    <tr>
-      <th>Frameworks, Tutorial, Data & DevOps</th>
-      <th>Main Challenges & Solutions in Quant Research</th>
-    </tr>
-    <tr>
-      <td>
-        <li><a href="#plans"><strong>Plans</strong></a></li>
-        <li><a href="#framework-of-qlib">Framework of Qlib</a></li>
-        <li><a href="#quick-start">Quick Start</a></li>
-          <ul dir="auto">
-            <li type="circle"><a href="#installation">Installation</a> </li>
-            <li type="circle"><a href="#data-preparation">Data Preparation</a></li>
-            <li type="circle"><a href="#auto-quant-research-workflow">Auto Quant Research Workflow</a></li>
-            <li type="circle"><a href="#building-customized-quant-research-workflow-by-code">Building Customized Quant Research Workflow by Code</a></li></ul>
-        <li><a href="#quant-dataset-zoo"><strong>Quant Dataset Zoo</strong></a></li>
-        <li><a href="#learning-framework">Learning Framework</a></li>
-        <li><a href="#more-about-qlib">More About Qlib</a></li>
-        <li><a href="#offline-mode-and-online-mode">Offline Mode and Online Mode</a>
-        <ul>
-          <li type="circle"><a href="#performance-of-qlib-data-server">Performance of Qlib Data Server</a></li></ul>
-        <li><a href="#related-reports">Related Reports</a></li>
-        <li><a href="#contact-us">Contact Us</a></li>
-        <li><a href="#contributing">Contributing</a></li>
-      </td>
-      <td valign="baseline">
-        <li><a href="#main-challenges--solutions-in-quant-research">Main Challenges &amp; Solutions in Quant Research</a>
-          <ul>
-            <li type="circle"><a href="#forecasting-finding-valuable-signalspatterns">Forecasting: Finding Valuable Signals/Patterns</a>
-              <ul>
-                <li type="disc"><a href="#quant-model-paper-zoo"><strong>Quant Model (Paper) Zoo</strong></a>
-                  <ul>
-                    <li type="circle"><a href="#run-a-single-model">Run a Single Model</a></li>
-                    <li type="circle"><a href="#run-multiple-models">Run Multiple Models</a></li>
-                  </ul>
-                </li>
-              </ul>
-            </li>
-          <li type="circle"><a href="#adapting-to-market-dynamics">Adapting to Market Dynamics</a></li>
-          </ul>
-        </li>
-      </td>
-    </tr>
-  </tbody>
-</table>
-
-# Plans
-New features under development(order by estimated release time).
-Your feedbacks about the features are very important.
-<!-- | Feature                        | Status      | -->
-<!-- | --                      | ------    | -->
-
-# Framework of Qlib
-
-<div style="align: center">
-<img src="docs/_static/img/framework-abstract.jpg" />
-</div>
-
-The high-level framework of Qlib can be found above(users can find the [detailed framework](https://qlib.readthedocs.io/en/latest/introduction/introduction.html#framework) of Qlib's design when getting into nitty gritty).
-The components are designed as loose-coupled modules, and each component could be used stand-alone.
-
-Qlib provides a strong infrastructure to support Quant research. [Data](https://qlib.readthedocs.io/en/latest/component/data.html) is always an important part.
-A strong learning framework is designed to support diverse learning paradigms (e.g. [reinforcement learning](https://qlib.readthedocs.io/en/latest/component/rl.html), [supervised learning](https://qlib.readthedocs.io/en/latest/component/workflow.html#model-section)) and patterns at different levels(e.g. [market dynamic modeling](https://qlib.readthedocs.io/en/latest/component/meta.html)).
-By modeling the market, [trading strategies](https://qlib.readthedocs.io/en/latest/component/strategy.html) will generate trade decisions that will be executed. Multiple trading strategies and executors in different levels or granularities can be [nested to be optimized and run together](https://qlib.readthedocs.io/en/latest/component/highfreq.html).
-At last, a comprehensive [analysis](https://qlib.readthedocs.io/en/latest/component/report.html) will be provided and the model can be [served online](https://qlib.readthedocs.io/en/latest/component/online.html) in a low cost.
-
-
-# Quick Start
-
-This quick start guide tries to demonstrate
-1. It's very easy to build a complete Quant research workflow and try your ideas with _Qlib_.
-2. Though with *public data* and *simple models*, machine learning technologies **work very well** in practical Quant investment.
-
-Here is a quick **[demo](https://terminalizer.com/view/3f24561a4470)** shows how to install ``Qlib``, and run LightGBM with ``qrun``. **But**, please make sure you have already prepared the data following the [instruction](#data-preparation).
-
-
-## Installation
-
-This table demonstrates the supported Python version of `Qlib`:
-|               | install with pip           | install from source  | plot |
-| ------------- |:---------------------:|:--------------------:|:----:|
-| Python 3.7    | :heavy_check_mark:    | :heavy_check_mark:   | :heavy_check_mark: |
-| Python 3.8    | :heavy_check_mark:    | :heavy_check_mark:   | :heavy_check_mark: |
-| Python 3.9    | :x:                   | :heavy_check_mark:   | :x: |
-
-**Note**: 
-1. **Conda** is suggested for managing your Python environment.
-1. Please pay attention that installing cython in Python 3.6 will raise some error when installing ``Qlib`` from source. If users use Python 3.6 on their machines, it is recommended to *upgrade* Python to version 3.7 or use `conda`'s Python to install ``Qlib`` from source.
-1. For Python 3.9, `Qlib` supports running workflows such as training models, doing backtest and plot most of the related figures (those included in [notebook](examples/workflow_by_code.ipynb)). However, plotting for the *model performance* is not supported for now and we will fix this when the dependent packages are upgraded in the future.
-1. `Qlib`Requires `tables` package, `hdf5` in tables does not support python3.9. 
-
-### Install with pip
-Users can easily install ``Qlib`` by pip according to the following command.
-
-```bash
-  pip install pyqlib
-```
-
-**Note**: pip will install the latest stable qlib. However, the main branch of qlib is in active development. If you want to test the latest scripts or functions in the main branch. Please install qlib with the methods below.
-
-### Install from source
-Also, users can install the latest dev version ``Qlib`` by the source code according to the following steps:
-
-* Before installing ``Qlib`` from source, users need to install some dependencies:
-
-  ```bash
-  pip install numpy
-  pip install --upgrade  cython
-  ```
-
-* Clone the repository and install ``Qlib`` as follows.
-    ```bash
-    git clone https://github.com/microsoft/qlib.git && cd qlib
-    pip install .
-    ```
-  **Note**:  You can install Qlib with `python setup.py install` as well. But it is not the recommended approach. It will skip `pip` and cause obscure problems. For example, **only** the command ``pip install .`` **can** overwrite the stable version installed by ``pip install pyqlib``, while the command ``python setup.py install`` **can't**.
-
-**Tips**: If you fail to install `Qlib` or run the examples in your environment,  comparing your steps and the [CI workflow](.github/workflows/test_qlib_from_source.yml) may help you find the problem.
-
-## Data Preparation
-Load and prepare data by running the following code:
-
-### Get with module
-  ```bash
-  # get 1d data
-  python -m qlib.run.get_data qlib_data --target_dir ~/.qlib/qlib_data/cn_data --region cn
-
-  # get 1min data
-  python -m qlib.run.get_data qlib_data --target_dir ~/.qlib/qlib_data/cn_data_1min --region cn --interval 1min
-
-  ```
-
-### Get from source
-
-  ```bash
-  # get 1d data
-  python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data --region cn
-
-  # get 1min data
-  python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data_1min --region cn --interval 1min
-
-  ```
-
-This dataset is created by public data collected by [crawler scripts](scripts/data_collector/), which have been released in
-the same repository.
-Users could create the same dataset with it. [Description of dataset](https://github.com/microsoft/qlib/tree/main/scripts/data_collector#description-of-dataset)
-
-*Please pay **ATTENTION** that the data is collected from [Yahoo Finance](https://finance.yahoo.com/lookup), and the data might not be perfect.
-We recommend users to prepare their own data if they have a high-quality dataset. For more information, users can refer to the [related document](https://qlib.readthedocs.io/en/latest/component/data.html#converting-csv-format-into-qlib-format)*.
-
-### Automatic update of daily frequency data (from yahoo finance)
-  > This step is *Optional* if users only want to try their models and strategies on history data.
-  > 
-  > It is recommended that users update the data manually once (--trading_date 2021-05-25) and then set it to update automatically.
-  >
-  > **NOTE**: Users can't incrementally  update data based on the offline data provided by Qlib(some fields are removed to reduce the data size). Users should use [yahoo collector](https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo#automatic-update-of-daily-frequency-datafrom-yahoo-finance) to download Yahoo data from scratch and then incrementally update it.
-  > 
-  > For more information, please refer to: [yahoo collector](https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo#automatic-update-of-daily-frequency-datafrom-yahoo-finance)
-
-  * Automatic update of data to the "qlib" directory each trading day(Linux)
-      * use *crontab*: `crontab -e`
-      * set up timed tasks:
-
-        ```
-        * * * * 1-5 python <script path> update_data_to_bin --qlib_data_1d_dir <user data dir>
-        ```
-        * **script path**: *scripts/data_collector/yahoo/collector.py*
-
-  * Manual update of data
-      ```
-      python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir <user data dir> --trading_date <start date> --end_date <end date>
-      ```
-      * *trading_date*: start of trading day
-      * *end_date*: end of trading day(not included)
-
-
-<!-- 
-- Run the initialization code and get stock data:
-
-  ```python
-  import qlib
-  from qlib.data import D
-  from qlib.constant import REG_CN
-
-  # Initialization
-  mount_path = "~/.qlib/qlib_data/cn_data"  # target_dir
-  qlib.init(mount_path=mount_path, region=REG_CN)
-
-  # Get stock data by Qlib
-  # Load trading calendar with the given time range and frequency
-  print(D.calendar(start_time='2010-01-01', end_time='2017-12-31', freq='day')[:2])
-
-  # Parse a given market name into a stockpool config
-  instruments = D.instruments('csi500')
-  print(D.list_instruments(instruments=instruments, start_time='2010-01-01', end_time='2017-12-31', as_list=True)[:6])
-
-  # Load features of certain instruments in given time range
-  instruments = ['SH600000']
-  fields = ['$close', '$volume', 'Ref($close, 1)', 'Mean($close, 3)', '$high-$low']
-  print(D.features(instruments, fields, start_time='2010-01-01', end_time='2017-12-31', freq='day').head())
-  ```
- -->
-
-## Auto Quant Research Workflow
-Qlib provides a tool named `qrun` to run the whole workflow automatically (including building dataset, training models, backtest and evaluation). You can start an auto quant research workflow and have a graphical reports analysis according to the following steps: 
-
-1. Quant Research Workflow: Run  `qrun` with lightgbm workflow config ([workflow_config_lightgbm_Alpha158.yaml](examples/benchmarks/LightGBM/workflow_config_lightgbm_Alpha158.yaml) as following.
-    ```bash
-      cd examples  # Avoid running program under the directory contains `qlib`
-      qrun benchmarks/LightGBM/workflow_config_lightgbm_Alpha158.yaml
-    ```
-    If users want to use `qrun` under debug mode, please use the following command:
-    ```bash
-    python -m pdb qlib/workflow/cli.py examples/benchmarks/LightGBM/workflow_config_lightgbm_Alpha158.yaml
-    ```
-    The result of `qrun` is as follows, please refer to [Intraday Trading](https://qlib.readthedocs.io/en/latest/component/backtest.html) for more details about the result. 
-
-    ```bash
-
-    'The following are analysis results of the excess return without cost.'
-                           risk
-    mean               0.000708
-    std                0.005626
-    annualized_return  0.178316
-    information_ratio  1.996555
-    max_drawdown      -0.081806
-    'The following are analysis results of the excess return with cost.'
-                           risk
-    mean               0.000512
-    std                0.005626
-    annualized_return  0.128982
-    information_ratio  1.444287
-    max_drawdown      -0.091078
-    ```
-    Here are detailed documents for `qrun` and [workflow](https://qlib.readthedocs.io/en/latest/component/workflow.html).
-
-2. Graphical Reports Analysis: Run `examples/workflow_by_code.ipynb` with `jupyter notebook` to get graphical reports
-    - Forecasting signal (model prediction) analysis
-      - Cumulative Return of groups
-      ![Cumulative Return](http://fintech.msra.cn/images_v070/analysis/analysis_model_cumulative_return.png?v=0.1)
-      - Return distribution
-      ![long_short](http://fintech.msra.cn/images_v070/analysis/analysis_model_long_short.png?v=0.1)
-      - Information Coefficient (IC)
-      ![Information Coefficient](http://fintech.msra.cn/images_v070/analysis/analysis_model_IC.png?v=0.1)
-      ![Monthly IC](http://fintech.msra.cn/images_v070/analysis/analysis_model_monthly_IC.png?v=0.1)
-      ![IC](http://fintech.msra.cn/images_v070/analysis/analysis_model_NDQ.png?v=0.1)
-      - Auto Correlation of forecasting signal (model prediction)
-      ![Auto Correlation](http://fintech.msra.cn/images_v070/analysis/analysis_model_auto_correlation.png?v=0.1)
-
-    - Portfolio analysis
-      - Backtest return
-      ![Report](http://fintech.msra.cn/images_v070/analysis/report.png?v=0.1)
-      <!-- 
-      - Score IC
-      ![Score IC](docs/_static/img/score_ic.png)
-      - Cumulative Return
-      ![Cumulative Return](docs/_static/img/cumulative_return.png)
-      - Risk Analysis
-      ![Risk Analysis](docs/_static/img/risk_analysis.png)
-      - Rank Label
-      ![Rank Label](docs/_static/img/rank_label.png)
-      -->
-   - [Explanation](https://qlib.readthedocs.io/en/latest/component/report.html) of above results
-
-## Building Customized Quant Research Workflow by Code
-The automatic workflow may not suit the research workflow of all Quant researchers. To support a flexible Quant research workflow, Qlib also provides a modularized interface to allow researchers to build their own workflow by code. [Here](examples/workflow_by_code.ipynb) is a demo for customized Quant research workflow by code.
-
-# Main Challenges & Solutions in Quant Research
-Quant investment is an very unique scenario with lots of key challenges to be solved.
-Currently, Qlib provides some solutions for several of them.
-
-## Forecasting: Finding Valuable Signals/Patterns
-Accurate forecasting of the stock price trend is a very important part to construct profitable portfolios.
-However, huge amount of data with various formats in the financial market which make it challenging to build forecasting models.
-
-An increasing number of SOTA Quant research works/papers, which focus on building forecasting models to mine valuable signals/patterns in complex financial data, are released in `Qlib`
-
-
-### [Quant Model (Paper) Zoo](examples/benchmarks)
-
-Here is a list of models built on `Qlib`.
-- [GBDT based on XGBoost (Tianqi Chen, et al. KDD 2016)](examples/benchmarks/XGBoost/)
-- [GBDT based on LightGBM (Guolin Ke, et al. NIPS 2017)](examples/benchmarks/LightGBM/)
-- [GBDT based on Catboost (Liudmila Prokhorenkova, et al. NIPS 2018)](examples/benchmarks/CatBoost/)
-- [MLP based on pytorch](examples/benchmarks/MLP/)
-- [LSTM based on pytorch (Sepp Hochreiter, et al. Neural computation 1997)](examples/benchmarks/LSTM/)
-- [GRU based on pytorch (Kyunghyun Cho, et al. 2014)](examples/benchmarks/GRU/)
-- [ALSTM based on pytorch (Yao Qin, et al. IJCAI 2017)](examples/benchmarks/ALSTM)
-- [GATs based on pytorch (Petar Velickovic, et al. 2017)](examples/benchmarks/GATs/)
-- [SFM based on pytorch (Liheng Zhang, et al. KDD 2017)](examples/benchmarks/SFM/)
-- [TFT based on tensorflow (Bryan Lim, et al. International Journal of Forecasting 2019)](examples/benchmarks/TFT/)
-- [TabNet based on pytorch (Sercan O. Arik, et al. AAAI 2019)](examples/benchmarks/TabNet/)
-- [DoubleEnsemble based on LightGBM (Chuheng Zhang, et al. ICDM 2020)](examples/benchmarks/DoubleEnsemble/)
-- [TCTS based on pytorch (Xueqing Wu, et al. ICML 2021)](examples/benchmarks/TCTS/)
-- [Transformer based on pytorch (Ashish Vaswani, et al. NeurIPS 2017)](examples/benchmarks/Transformer/)
-- [Localformer based on pytorch (Juyong Jiang, et al.)](examples/benchmarks/Localformer/)
-- [TRA based on pytorch (Hengxu, Dong, et al. KDD 2021)](examples/benchmarks/TRA/)
-- [TCN based on pytorch (Shaojie Bai, et al. 2018)](examples/benchmarks/TCN/)
-- [ADARNN based on pytorch (YunTao Du, et al. 2021)](examples/benchmarks/ADARNN/)
-- [ADD based on pytorch (Hongshun Tang, et al.2020)](examples/benchmarks/ADD/)
-- [IGMTF based on pytorch (Wentao Xu, et al.2021)](examples/benchmarks/IGMTF/)
-- [HIST based on pytorch (Wentao Xu, et al.2021)](examples/benchmarks/HIST/)
-- [KRNN based on pytorch](examples/benchmarks/KRNN/)
-- [Sandwich based on pytorch](examples/benchmarks/Sandwich/)
-
-Your PR of new Quant models is highly welcomed.
-
-The performance of each model on the `Alpha158` and `Alpha360` dataset can be found [here](examples/benchmarks/README.md).
-
-### Run a single model
-All the models listed above are runnable with ``Qlib``. Users can find the config files we provide and some details about the model through the [benchmarks](examples/benchmarks) folder. More information can be retrieved at the model files listed above.
-
-`Qlib` provides three different ways to run a single model, users can pick the one that fits their cases best:
-- Users can use the tool `qrun` mentioned above to run a model's workflow based from a config file.
-- Users can create a `workflow_by_code` python script based on the [one](examples/workflow_by_code.py) listed in the `examples` folder.
-
-- Users can use the script [`run_all_model.py`](examples/run_all_model.py) listed in the `examples` folder to run a model. Here is an example of the specific shell command to be used: `python run_all_model.py run --models=lightgbm`, where the `--models` arguments can take any number of models listed above(the available models can be found  in [benchmarks](examples/benchmarks/)). For more use cases, please refer to the file's [docstrings](examples/run_all_model.py).
-    - **NOTE**: Each baseline has different environment dependencies, please make sure that your python version aligns with the requirements(e.g. TFT only supports Python 3.6~3.7 due to the limitation of `tensorflow==1.15.0`)
-
-### Run multiple models
-`Qlib` also provides a script [`run_all_model.py`](examples/run_all_model.py) which can run multiple models for several iterations. (**Note**: the script only support *Linux* for now. Other OS will be supported in the future. Besides, it doesn't support parallel running the same model for multiple times as well, and this will be fixed in the future development too.)
-
-The script will create a unique virtual environment for each model, and delete the environments after training. Thus, only experiment results such as `IC` and `backtest` results will be generated and stored.
-
-Here is an example of running all the models for 10 iterations:
-```python
-python run_all_model.py run 10
-```
-
-It also provides the API to run specific models at once. For more use cases, please refer to the file's [docstrings](examples/run_all_model.py). 
-
-## [Adapting to Market Dynamics](examples/benchmarks_dynamic)
-
-Due to the non-stationary nature of the environment of the financial market, the data distribution may change in different periods, which makes the performance of models build on training data decays in the future test data.
-So adapting the forecasting models/strategies to market dynamics is very important to the model/strategies' performance.
-
-Here is a list of solutions built on `Qlib`.
-- [Rolling Retraining](examples/benchmarks_dynamic/baseline/)
-- [DDG-DA on pytorch (Wendi, et al. AAAI 2022)](examples/benchmarks_dynamic/DDG-DA/)
-
-# Quant Dataset Zoo
-Dataset plays a very important role in Quant. Here is a list of the datasets built on `Qlib`:
-
-| Dataset                                    | US Market | China Market |
-| --                                         | --        | --           |
-| [Alpha360](./qlib/contrib/data/handler.py) |  √        |  √           |
-| [Alpha158](./qlib/contrib/data/handler.py) |  √        |  √           |
-
-[Here](https://qlib.readthedocs.io/en/latest/advanced/alpha.html) is a tutorial to build dataset with `Qlib`.
-Your PR to build new Quant dataset is highly welcomed.
-
-
-# Learning Framework
-Qlib is high customizable and a lot of its components are learnable.
-The learnable components are instances of `Forecast Model` and `Trading Agent`. They are learned based on the `Learning Framework` layer and then applied to multiple scenarios in `Workflow` layer.
-The learning framework leverages the `Workflow` layer as well(e.g. sharing `Information Extractor`, creating environments based on `Execution Env`).
-
-Based on learning paradigms, they can be categorized into reinforcement learning and supervised learning.
-- For supervised learning, the detailed docs can be found [here](https://qlib.readthedocs.io/en/latest/component/model.html).
-- For reinforcement learning, the detailed docs can be found [here](https://qlib.readthedocs.io/en/latest/component/rl.html). Qlib's RL learning framework leverages `Execution Env` in `Workflow` layer to create environments.  It's worth noting that `NestedExecutor` is supported as well. This empowers users to optimize different level of strategies/models/agents together (e.g. optimizing an order execution strategy for a specific portfolio management strategy).
-
-
-# More About Qlib
-If you want to have a quick glance at the most frequently used components of qlib, you can try notebooks [here](examples/tutorial/).
-
-The detailed documents are organized in [docs](docs/).
-[Sphinx](http://www.sphinx-doc.org) and the readthedocs theme is required to build the documentation in html formats. 
-```bash
-cd docs/
-conda install sphinx sphinx_rtd_theme -y
-# Otherwise, you can install them with pip
-# pip install sphinx sphinx_rtd_theme
-make html
-```
-You can also view the [latest document](http://qlib.readthedocs.io/) online directly.
-
-Qlib is in active and continuing development. Our plan is in the roadmap, which is managed as a [github project](https://github.com/microsoft/qlib/projects/1).
-
-
-
-# Offline Mode and Online Mode
-The data server of Qlib can either deployed as `Offline` mode or `Online` mode. The default mode is offline mode.
-
-Under `Offline` mode, the data will be deployed locally. 
-
-Under `Online` mode, the data will be deployed as a shared data service. The data and their cache will be shared by all the clients. The data retrieval performance is expected to be improved due to a higher rate of cache hits. It will consume less disk space, too. The documents of the online mode can be found in [Qlib-Server](https://qlib-server.readthedocs.io/). The online mode can be deployed automatically with [Azure CLI based scripts](https://qlib-server.readthedocs.io/en/latest/build.html#one-click-deployment-in-azure). The source code of online data server can be found in [Qlib-Server repository](https://github.com/microsoft/qlib-server).
-
-## Performance of Qlib Data Server
-The performance of data processing is important to data-driven methods like AI technologies. As an AI-oriented platform, Qlib provides a solution for data storage and data processing. To demonstrate the performance of Qlib data server, we
-compare it with several other data storage solutions. 
-
-We evaluate the performance of several storage solutions by finishing the same task,
-which creates a dataset (14 features/factors) from the basic OHLCV daily data of a stock market (800 stocks each day from 2007 to 2020). The task involves data queries and processing.
-
-|                         | HDF5      | MySQL     | MongoDB   | InfluxDB  | Qlib -E -D  | Qlib +E -D   | Qlib +E +D  |
-| --                      | ------    | ------    | --------  | --------- | ----------- | ------------ | ----------- |
-| Total (1CPU) (seconds)  | 184.4±3.7 | 365.3±7.5 | 253.6±6.7 | 368.2±3.6 | 147.0±8.8   | 47.6±1.0     | **7.4±0.3** |
-| Total (64CPU) (seconds) |           |           |           |           | 8.8±0.6     | **4.2±0.2**  |             |
-* `+(-)E` indicates with (out) `ExpressionCache`
-* `+(-)D` indicates with (out) `DatasetCache`
-
-Most general-purpose databases take too much time to load data. After looking into the underlying implementation, we find that data go through too many layers of interfaces and unnecessary format transformations in general-purpose database solutions.
-Such overheads greatly slow down the data loading process.
-Qlib data are stored in a compact format, which is efficient to be combined into arrays for scientific computation.
-
-# Related Reports
-- [Guide To Qlib: Microsoft’s AI Investment Platform](https://analyticsindiamag.com/qlib/)
-- [微软也搞AI量化平台？还是开源的！](https://mp.weixin.qq.com/s/47bP5YwxfTp2uTHjUBzJQQ)
-- [微矿Qlib：业内首个AI量化投资开源平台](https://mp.weixin.qq.com/s/vsJv7lsgjEi-ALYUz4CvtQ)
-
-# Contact Us
-- If you have any issues, please create issue [here](https://github.com/microsoft/qlib/issues/new/choose) or send messages in [gitter](https://gitter.im/Microsoft/qlib).
-- If you want to make contributions to `Qlib`, please [create pull requests](https://github.com/microsoft/qlib/compare). 
-- For other reasons, you are welcome to contact us by email([qlib@microsoft.com](mailto:qlib@microsoft.com)).
-  - We are recruiting new members(both FTEs and interns), your resumes are welcome!
-
-Join IM discussion groups:
-|[Gitter](https://gitter.im/Microsoft/qlib)|
-|----|
-|![image](http://fintech.msra.cn/images_v070/qrcode/gitter_qr.png)|
-
-# Contributing
-We appreciate all contributions and thank all the contributors!
-<a href="https://github.com/microsoft/qlib/graphs/contributors"><img src="https://contrib.rocks/image?repo=microsoft/qlib" /></a>
-
-Before we released Qlib as an open-source project on Github in Sep 2020, Qlib is an internal project in our group. Unfortunately, the internal commit history is not kept. A lot of members in our group have also contributed a lot to Qlib, which includes Ruihua Wang, Yinda Zhang, Haisu Yu, Shuyu Wang, Bochen Pang, and [Dong Zhou](https://github.com/evanzd/evanzd). Especially thanks to [Dong Zhou](https://github.com/evanzd/evanzd) due to his initial version of Qlib.
-
-## Guidance
-
-This project welcomes contributions and suggestions.  
-**Here are some 
-[code standards and development guidance](docs/developer/code_standard_and_dev_guide.rst) for submiting a pull request.**
-
-Making contributions is not a hard thing. Solving an issue(maybe just answering a question raised in [issues list](https://github.com/microsoft/qlib/issues) or [gitter](https://gitter.im/Microsoft/qlib)), fixing/issuing a bug, improving the documents and even fixing a typo are important contributions to Qlib.
-
-For example, if you want to contribute to Qlib's document/code, you can follow the steps in the figure below.
-<p align="center">
-  <img src="https://github.com/demon143/qlib/blob/main/docs/_static/img/change%20doc.gif" />
-</p>
-
-If you don't know how to start to contribute, you can refer to the following examples.
-| Type | Examples |
-| -- | -- |
-| Solving issues | [Answer a question](https://github.com/microsoft/qlib/issues/749);  [issuing](https://github.com/microsoft/qlib/issues/765) or [fixing](https://github.com/microsoft/qlib/pull/792) a bug |
-| Docs | [Improve docs quality](https://github.com/microsoft/qlib/pull/797/files) ;  [Fix a typo](https://github.com/microsoft/qlib/pull/774) | 
-| Feature |  Implement a [requested feature](https://github.com/microsoft/qlib/projects) like [this](https://github.com/microsoft/qlib/pull/754); [Refactor interfaces](https://github.com/microsoft/qlib/pull/539/files) |
-| Dataset | [Add a dataset](https://github.com/microsoft/qlib/pull/733) | 
-| Models |  [Implement a new model](https://github.com/microsoft/qlib/pull/689), [some instructions to contribute models](https://github.com/microsoft/qlib/tree/main/examples/benchmarks#contributing) |
-
-[Good first issues](https://github.com/microsoft/qlib/labels/good%20first%20issue) are labelled to indicate that they are easy to start your contributions.
-
-You can find some impefect implementation in Qlib by  `rg 'TODO|FIXME' qlib`
- 
-If you would like to become one of Qlib's maintainers to contribute more (e.g. help merge PR, triage issues), please contact us by email([qlib@microsoft.com](mailto:qlib@microsoft.com)).  We are glad to help to upgrade your permission.
-
-## Licence
-Most contributions require you to agree to a
-Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
-the right to use your contribution. For details, visit https://cla.opensource.microsoft.com.
-
-When you submit a pull request, a CLA bot will automatically determine whether you need to provide
-a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions
-provided by the bot. You will only need to do this once across all repos using our CLA.
-
-This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
-For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
-contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.
+Metadata-Version: 2.1
+Name: pyqlib
+Version: 0.9.3
+Summary: A Quantitative-research Platform
+Home-page: https://github.com/microsoft/qlib
+License: MIT Licence
+Platform: UNKNOWN
+Classifier: Operating System :: POSIX :: Linux
+Classifier: Operating System :: Microsoft :: Windows
+Classifier: Operating System :: MacOS
+Classifier: License :: OSI Approved :: MIT License
+Classifier: Development Status :: 3 - Alpha
+Classifier: Programming Language :: Python
+Classifier: Programming Language :: Python :: 3
+Classifier: Programming Language :: Python :: 3.7
+Classifier: Programming Language :: Python :: 3.8
+Classifier: Programming Language :: Python :: 3.9
+Requires-Python: >=3.5.0
+Description-Content-Type: text/markdown
+Requires-Dist: numpy (<1.24,>=1.12.0)
+Requires-Dist: pandas (>=0.25.1)
+Requires-Dist: scipy (>=1.0.0)
+Requires-Dist: requests (>=2.18.0)
+Requires-Dist: sacred (>=0.7.4)
+Requires-Dist: python-socketio
+Requires-Dist: redis (>=3.0.1)
+Requires-Dist: python-redis-lock (>=3.3.1)
+Requires-Dist: schedule (>=0.6.0)
+Requires-Dist: cvxpy (>=1.0.21)
+Requires-Dist: hyperopt (==0.1.2)
+Requires-Dist: fire (>=0.3.1)
+Requires-Dist: statsmodels
+Requires-Dist: xlrd (>=1.0.0)
+Requires-Dist: plotly (>=4.12.0)
+Requires-Dist: matplotlib (>=3.3)
+Requires-Dist: tables (>=3.6.1)
+Requires-Dist: pyyaml (>=5.3.1)
+Requires-Dist: mlflow (<=1.30.0,>=1.12.1)
+Requires-Dist: tqdm
+Requires-Dist: loguru
+Requires-Dist: lightgbm (>=3.3.0)
+Requires-Dist: tornado
+Requires-Dist: joblib (>=0.17.0)
+Requires-Dist: ruamel.yaml (>=0.16.12)
+Requires-Dist: pymongo (==3.7.2)
+Requires-Dist: scikit-learn (>=0.22)
+Requires-Dist: dill
+Requires-Dist: filelock
+Requires-Dist: jinja2 (<3.1.0)
+Requires-Dist: gym
+Requires-Dist: cryptography
+Requires-Dist: dataclasses ; python_version < "3.7"
+Requires-Dist: protobuf (<=3.20.1) ; python_version <= "3.8"
+Provides-Extra: dev
+Requires-Dist: coverage ; extra == 'dev'
+Requires-Dist: pytest (>=3) ; extra == 'dev'
+Requires-Dist: sphinx ; extra == 'dev'
+Requires-Dist: sphinx-rtd-theme ; extra == 'dev'
+Requires-Dist: pre-commit ; extra == 'dev'
+Requires-Dist: wheel ; extra == 'dev'
+Requires-Dist: setuptools ; extra == 'dev'
+Requires-Dist: black ; extra == 'dev'
+Requires-Dist: pylint ; extra == 'dev'
+Requires-Dist: mypy (<0.981) ; extra == 'dev'
+Requires-Dist: flake8 ; extra == 'dev'
+Requires-Dist: nbqa ; extra == 'dev'
+Requires-Dist: jupyter ; extra == 'dev'
+Requires-Dist: nbconvert ; extra == 'dev'
+Requires-Dist: importlib-metadata (<5.0.0) ; extra == 'dev'
+Requires-Dist: readthedocs-sphinx-ext ; extra == 'dev'
+Requires-Dist: cmake ; extra == 'dev'
+Requires-Dist: lxml ; extra == 'dev'
+Requires-Dist: baostock ; extra == 'dev'
+Requires-Dist: yahooquery ; extra == 'dev'
+Requires-Dist: beautifulsoup4 ; extra == 'dev'
+Requires-Dist: tianshou (<=0.4.10) ; extra == 'dev'
+Requires-Dist: gym (>=0.24) ; extra == 'dev'
+Provides-Extra: rl
+Requires-Dist: tianshou (<=0.4.10) ; extra == 'rl'
+Requires-Dist: torch ; extra == 'rl'
+
+[![Python Versions](https://img.shields.io/pypi/pyversions/pyqlib.svg?logo=python&logoColor=white)](https://pypi.org/project/pyqlib/#files)
+[![Platform](https://img.shields.io/badge/platform-linux%20%7C%20windows%20%7C%20macos-lightgrey)](https://pypi.org/project/pyqlib/#files)
+[![PypI Versions](https://img.shields.io/pypi/v/pyqlib)](https://pypi.org/project/pyqlib/#history)
+[![Upload Python Package](https://github.com/microsoft/qlib/workflows/Upload%20Python%20Package/badge.svg)](https://pypi.org/project/pyqlib/)
+[![Github Actions Test Status](https://github.com/microsoft/qlib/workflows/Test/badge.svg?branch=main)](https://github.com/microsoft/qlib/actions)
+[![Documentation Status](https://readthedocs.org/projects/qlib/badge/?version=latest)](https://qlib.readthedocs.io/en/latest/?badge=latest)
+[![License](https://img.shields.io/pypi/l/pyqlib)](LICENSE)
+[![Join the chat at https://gitter.im/Microsoft/qlib](https://badges.gitter.im/Microsoft/qlib.svg)](https://gitter.im/Microsoft/qlib?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)
+
+## :newspaper: **What's NEW!** &nbsp;   :sparkling_heart: 
+Recent released features
+| Feature | Status |
+| --                      | ------    |
+| KRNN and Sandwich models | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/1414/) on May 26, 2023 |
+| Release Qlib v0.9.0 | :octocat: [Released](https://github.com/microsoft/qlib/releases/tag/v0.9.0) on Dec 9, 2022 |
+| RL Learning Framework | :hammer: :chart_with_upwards_trend: Released on Nov 10, 2022. [#1332](https://github.com/microsoft/qlib/pull/1332), [#1322](https://github.com/microsoft/qlib/pull/1322), [#1316](https://github.com/microsoft/qlib/pull/1316),[#1299](https://github.com/microsoft/qlib/pull/1299),[#1263](https://github.com/microsoft/qlib/pull/1263), [#1244](https://github.com/microsoft/qlib/pull/1244), [#1169](https://github.com/microsoft/qlib/pull/1169), [#1125](https://github.com/microsoft/qlib/pull/1125), [#1076](https://github.com/microsoft/qlib/pull/1076)|
+| HIST and IGMTF models | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/1040) on Apr 10, 2022 |
+| Qlib [notebook tutorial](https://github.com/microsoft/qlib/tree/main/examples/tutorial) | 📖 [Released](https://github.com/microsoft/qlib/pull/1037) on Apr 7, 2022 | 
+| Ibovespa index data | :rice: [Released](https://github.com/microsoft/qlib/pull/990) on Apr 6, 2022 |
+| Point-in-Time database | :hammer: [Released](https://github.com/microsoft/qlib/pull/343) on Mar 10, 2022 |
+| Arctic Provider Backend & Orderbook data example | :hammer: [Released](https://github.com/microsoft/qlib/pull/744) on Jan 17, 2022 |
+| Meta-Learning-based framework & DDG-DA  | :chart_with_upwards_trend:  :hammer: [Released](https://github.com/microsoft/qlib/pull/743) on Jan 10, 2022 | 
+| Planning-based portfolio optimization | :hammer: [Released](https://github.com/microsoft/qlib/pull/754) on Dec 28, 2021 | 
+| Release Qlib v0.8.0 | :octocat: [Released](https://github.com/microsoft/qlib/releases/tag/v0.8.0) on Dec 8, 2021 |
+| ADD model | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/704) on Nov 22, 2021 |
+| ADARNN  model | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/689) on Nov 14, 2021 |
+| TCN  model | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/668) on Nov 4, 2021 |
+| Nested Decision Framework | :hammer: [Released](https://github.com/microsoft/qlib/pull/438) on Oct 1, 2021. [Example](https://github.com/microsoft/qlib/blob/main/examples/nested_decision_execution/workflow.py) and [Doc](https://qlib.readthedocs.io/en/latest/component/highfreq.html) |
+| Temporal Routing Adaptor (TRA) | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/531) on July 30, 2021 |
+| Transformer & Localformer | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/508) on July 22, 2021 |
+| Release Qlib v0.7.0 | :octocat: [Released](https://github.com/microsoft/qlib/releases/tag/v0.7.0) on July 12, 2021 |
+| TCTS Model | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/491) on July 1, 2021 |
+| Online serving and automatic model rolling | :hammer:  [Released](https://github.com/microsoft/qlib/pull/290) on May 17, 2021 | 
+| DoubleEnsemble Model | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/286) on Mar 2, 2021 | 
+| High-frequency data processing example | :hammer: [Released](https://github.com/microsoft/qlib/pull/257) on Feb 5, 2021  |
+| High-frequency trading example | :chart_with_upwards_trend: [Part of code released](https://github.com/microsoft/qlib/pull/227) on Jan 28, 2021  | 
+| High-frequency data(1min) | :rice: [Released](https://github.com/microsoft/qlib/pull/221) on Jan 27, 2021 |
+| Tabnet Model | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/205) on Jan 22, 2021 |
+
+Features released before 2021 are not listed here.
+
+<p align="center">
+  <img src="http://fintech.msra.cn/images_v070/logo/1.png" />
+</p>
+
+Qlib is an open-source, AI-oriented quantitative investment platform that aims to realize the potential, empower research, and create value using AI technologies in quantitative investment, from exploring ideas to implementing productions. Qlib supports diverse machine learning modeling paradigms, including supervised learning, market dynamics modeling, and reinforcement learning.
+
+An increasing number of SOTA Quant research works/papers in diverse paradigms are being released in Qlib to collaboratively solve key challenges in quantitative investment. For example, 1) using supervised learning to mine the market's complex non-linear patterns from rich and heterogeneous financial data, 2) modeling the dynamic nature of the financial market using adaptive concept drift technology, and 3) using reinforcement learning to model continuous investment decisions and assist investors in optimizing their trading strategies.
+
+It contains the full ML pipeline of data processing, model training, back-testing; and covers the entire chain of quantitative investment: alpha seeking, risk modeling, portfolio optimization, and order execution. 
+For more details, please refer to our paper ["Qlib: An AI-oriented Quantitative Investment Platform"](https://arxiv.org/abs/2009.11189).
+
+
+<table>
+  <tbody>
+    <tr>
+      <th>Frameworks, Tutorial, Data & DevOps</th>
+      <th>Main Challenges & Solutions in Quant Research</th>
+    </tr>
+    <tr>
+      <td>
+        <li><a href="#plans"><strong>Plans</strong></a></li>
+        <li><a href="#framework-of-qlib">Framework of Qlib</a></li>
+        <li><a href="#quick-start">Quick Start</a></li>
+          <ul dir="auto">
+            <li type="circle"><a href="#installation">Installation</a> </li>
+            <li type="circle"><a href="#data-preparation">Data Preparation</a></li>
+            <li type="circle"><a href="#auto-quant-research-workflow">Auto Quant Research Workflow</a></li>
+            <li type="circle"><a href="#building-customized-quant-research-workflow-by-code">Building Customized Quant Research Workflow by Code</a></li></ul>
+        <li><a href="#quant-dataset-zoo"><strong>Quant Dataset Zoo</strong></a></li>
+        <li><a href="#learning-framework">Learning Framework</a></li>
+        <li><a href="#more-about-qlib">More About Qlib</a></li>
+        <li><a href="#offline-mode-and-online-mode">Offline Mode and Online Mode</a>
+        <ul>
+          <li type="circle"><a href="#performance-of-qlib-data-server">Performance of Qlib Data Server</a></li></ul>
+        <li><a href="#related-reports">Related Reports</a></li>
+        <li><a href="#contact-us">Contact Us</a></li>
+        <li><a href="#contributing">Contributing</a></li>
+      </td>
+      <td valign="baseline">
+        <li><a href="#main-challenges--solutions-in-quant-research">Main Challenges &amp; Solutions in Quant Research</a>
+          <ul>
+            <li type="circle"><a href="#forecasting-finding-valuable-signalspatterns">Forecasting: Finding Valuable Signals/Patterns</a>
+              <ul>
+                <li type="disc"><a href="#quant-model-paper-zoo"><strong>Quant Model (Paper) Zoo</strong></a>
+                  <ul>
+                    <li type="circle"><a href="#run-a-single-model">Run a Single Model</a></li>
+                    <li type="circle"><a href="#run-multiple-models">Run Multiple Models</a></li>
+                  </ul>
+                </li>
+              </ul>
+            </li>
+          <li type="circle"><a href="#adapting-to-market-dynamics">Adapting to Market Dynamics</a></li>
+          <li type="circle"><a href="#reinforcement-learning-modeling-continuous-decisions">Reinforcement Learning: modeling continuous decisions</a></li>
+          </ul>
+        </li>
+      </td>
+    </tr>
+  </tbody>
+</table>
+
+# Plans
+New features under development(order by estimated release time).
+Your feedbacks about the features are very important.
+<!-- | Feature                        | Status      | -->
+<!-- | --                      | ------    | -->
+
+# Framework of Qlib
+
+<div style="align: center">
+<img src="docs/_static/img/framework-abstract.jpg" />
+</div>
+
+The high-level framework of Qlib can be found above(users can find the [detailed framework](https://qlib.readthedocs.io/en/latest/introduction/introduction.html#framework) of Qlib's design when getting into nitty gritty).
+The components are designed as loose-coupled modules, and each component could be used stand-alone.
+
+Qlib provides a strong infrastructure to support Quant research. [Data](https://qlib.readthedocs.io/en/latest/component/data.html) is always an important part.
+A strong learning framework is designed to support diverse learning paradigms (e.g. [reinforcement learning](https://qlib.readthedocs.io/en/latest/component/rl.html), [supervised learning](https://qlib.readthedocs.io/en/latest/component/workflow.html#model-section)) and patterns at different levels(e.g. [market dynamic modeling](https://qlib.readthedocs.io/en/latest/component/meta.html)).
+By modeling the market, [trading strategies](https://qlib.readthedocs.io/en/latest/component/strategy.html) will generate trade decisions that will be executed. Multiple trading strategies and executors in different levels or granularities can be [nested to be optimized and run together](https://qlib.readthedocs.io/en/latest/component/highfreq.html).
+At last, a comprehensive [analysis](https://qlib.readthedocs.io/en/latest/component/report.html) will be provided and the model can be [served online](https://qlib.readthedocs.io/en/latest/component/online.html) in a low cost.
+
+
+# Quick Start
+
+This quick start guide tries to demonstrate
+1. It's very easy to build a complete Quant research workflow and try your ideas with _Qlib_.
+2. Though with *public data* and *simple models*, machine learning technologies **work very well** in practical Quant investment.
+
+Here is a quick **[demo](https://terminalizer.com/view/3f24561a4470)** shows how to install ``Qlib``, and run LightGBM with ``qrun``. **But**, please make sure you have already prepared the data following the [instruction](#data-preparation).
+
+
+## Installation
+
+This table demonstrates the supported Python version of `Qlib`:
+|               | install with pip           | install from source  | plot |
+| ------------- |:---------------------:|:--------------------:|:----:|
+| Python 3.7    | :heavy_check_mark:    | :heavy_check_mark:   | :heavy_check_mark: |
+| Python 3.8    | :heavy_check_mark:    | :heavy_check_mark:   | :heavy_check_mark: |
+| Python 3.9    | :x:                   | :heavy_check_mark:   | :x: |
+
+**Note**: 
+1. **Conda** is suggested for managing your Python environment.
+1. Please pay attention that installing cython in Python 3.6 will raise some error when installing ``Qlib`` from source. If users use Python 3.6 on their machines, it is recommended to *upgrade* Python to version 3.7 or use `conda`'s Python to install ``Qlib`` from source.
+1. For Python 3.9, `Qlib` supports running workflows such as training models, doing backtest and plot most of the related figures (those included in [notebook](examples/workflow_by_code.ipynb)). However, plotting for the *model performance* is not supported for now and we will fix this when the dependent packages are upgraded in the future.
+1. `Qlib`Requires `tables` package, `hdf5` in tables does not support python3.9. 
+
+### Install with pip
+Users can easily install ``Qlib`` by pip according to the following command.
+
+```bash
+  pip install pyqlib
+```
+
+**Note**: pip will install the latest stable qlib. However, the main branch of qlib is in active development. If you want to test the latest scripts or functions in the main branch. Please install qlib with the methods below.
+
+### Install from source
+Also, users can install the latest dev version ``Qlib`` by the source code according to the following steps:
+
+* Before installing ``Qlib`` from source, users need to install some dependencies:
+
+  ```bash
+  pip install numpy
+  pip install --upgrade  cython
+  ```
+
+* Clone the repository and install ``Qlib`` as follows.
+    ```bash
+    git clone https://github.com/microsoft/qlib.git && cd qlib
+    pip install .
+    ```
+  **Note**:  You can install Qlib with `python setup.py install` as well. But it is not the recommended approach. It will skip `pip` and cause obscure problems. For example, **only** the command ``pip install .`` **can** overwrite the stable version installed by ``pip install pyqlib``, while the command ``python setup.py install`` **can't**.
+
+**Tips**: If you fail to install `Qlib` or run the examples in your environment,  comparing your steps and the [CI workflow](.github/workflows/test_qlib_from_source.yml) may help you find the problem.
+
+## Data Preparation
+Load and prepare data by running the following code:
+
+### Get with module
+  ```bash
+  # get 1d data
+  python -m qlib.run.get_data qlib_data --target_dir ~/.qlib/qlib_data/cn_data --region cn
+
+  # get 1min data
+  python -m qlib.run.get_data qlib_data --target_dir ~/.qlib/qlib_data/cn_data_1min --region cn --interval 1min
+
+  ```
+
+### Get from source
+
+  ```bash
+  # get 1d data
+  python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data --region cn
+
+  # get 1min data
+  python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data_1min --region cn --interval 1min
+
+  ```
+
+This dataset is created by public data collected by [crawler scripts](scripts/data_collector/), which have been released in
+the same repository.
+Users could create the same dataset with it. [Description of dataset](https://github.com/microsoft/qlib/tree/main/scripts/data_collector#description-of-dataset)
+
+*Please pay **ATTENTION** that the data is collected from [Yahoo Finance](https://finance.yahoo.com/lookup), and the data might not be perfect.
+We recommend users to prepare their own data if they have a high-quality dataset. For more information, users can refer to the [related document](https://qlib.readthedocs.io/en/latest/component/data.html#converting-csv-format-into-qlib-format)*.
+
+### Automatic update of daily frequency data (from yahoo finance)
+  > This step is *Optional* if users only want to try their models and strategies on history data.
+  > 
+  > It is recommended that users update the data manually once (--trading_date 2021-05-25) and then set it to update automatically.
+  >
+  > **NOTE**: Users can't incrementally  update data based on the offline data provided by Qlib(some fields are removed to reduce the data size). Users should use [yahoo collector](https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo#automatic-update-of-daily-frequency-datafrom-yahoo-finance) to download Yahoo data from scratch and then incrementally update it.
+  > 
+  > For more information, please refer to: [yahoo collector](https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo#automatic-update-of-daily-frequency-datafrom-yahoo-finance)
+
+  * Automatic update of data to the "qlib" directory each trading day(Linux)
+      * use *crontab*: `crontab -e`
+      * set up timed tasks:
+
+        ```
+        * * * * 1-5 python <script path> update_data_to_bin --qlib_data_1d_dir <user data dir>
+        ```
+        * **script path**: *scripts/data_collector/yahoo/collector.py*
+
+  * Manual update of data
+      ```
+      python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir <user data dir> --trading_date <start date> --end_date <end date>
+      ```
+      * *trading_date*: start of trading day
+      * *end_date*: end of trading day(not included)
+
+
+<!-- 
+- Run the initialization code and get stock data:
+
+  ```python
+  import qlib
+  from qlib.data import D
+  from qlib.constant import REG_CN
+
+  # Initialization
+  mount_path = "~/.qlib/qlib_data/cn_data"  # target_dir
+  qlib.init(mount_path=mount_path, region=REG_CN)
+
+  # Get stock data by Qlib
+  # Load trading calendar with the given time range and frequency
+  print(D.calendar(start_time='2010-01-01', end_time='2017-12-31', freq='day')[:2])
+
+  # Parse a given market name into a stockpool config
+  instruments = D.instruments('csi500')
+  print(D.list_instruments(instruments=instruments, start_time='2010-01-01', end_time='2017-12-31', as_list=True)[:6])
+
+  # Load features of certain instruments in given time range
+  instruments = ['SH600000']
+  fields = ['$close', '$volume', 'Ref($close, 1)', 'Mean($close, 3)', '$high-$low']
+  print(D.features(instruments, fields, start_time='2010-01-01', end_time='2017-12-31', freq='day').head())
+  ```
+ -->
+
+## Auto Quant Research Workflow
+Qlib provides a tool named `qrun` to run the whole workflow automatically (including building dataset, training models, backtest and evaluation). You can start an auto quant research workflow and have a graphical reports analysis according to the following steps: 
+
+1. Quant Research Workflow: Run  `qrun` with lightgbm workflow config ([workflow_config_lightgbm_Alpha158.yaml](examples/benchmarks/LightGBM/workflow_config_lightgbm_Alpha158.yaml) as following.
+    ```bash
+      cd examples  # Avoid running program under the directory contains `qlib`
+      qrun benchmarks/LightGBM/workflow_config_lightgbm_Alpha158.yaml
+    ```
+    If users want to use `qrun` under debug mode, please use the following command:
+    ```bash
+    python -m pdb qlib/workflow/cli.py examples/benchmarks/LightGBM/workflow_config_lightgbm_Alpha158.yaml
+    ```
+    The result of `qrun` is as follows, please refer to [Intraday Trading](https://qlib.readthedocs.io/en/latest/component/backtest.html) for more details about the result. 
+
+    ```bash
+
+    'The following are analysis results of the excess return without cost.'
+                           risk
+    mean               0.000708
+    std                0.005626
+    annualized_return  0.178316
+    information_ratio  1.996555
+    max_drawdown      -0.081806
+    'The following are analysis results of the excess return with cost.'
+                           risk
+    mean               0.000512
+    std                0.005626
+    annualized_return  0.128982
+    information_ratio  1.444287
+    max_drawdown      -0.091078
+    ```
+    Here are detailed documents for `qrun` and [workflow](https://qlib.readthedocs.io/en/latest/component/workflow.html).
+
+2. Graphical Reports Analysis: Run `examples/workflow_by_code.ipynb` with `jupyter notebook` to get graphical reports
+    - Forecasting signal (model prediction) analysis
+      - Cumulative Return of groups
+      ![Cumulative Return](http://fintech.msra.cn/images_v070/analysis/analysis_model_cumulative_return.png?v=0.1)
+      - Return distribution
+      ![long_short](http://fintech.msra.cn/images_v070/analysis/analysis_model_long_short.png?v=0.1)
+      - Information Coefficient (IC)
+      ![Information Coefficient](http://fintech.msra.cn/images_v070/analysis/analysis_model_IC.png?v=0.1)
+      ![Monthly IC](http://fintech.msra.cn/images_v070/analysis/analysis_model_monthly_IC.png?v=0.1)
+      ![IC](http://fintech.msra.cn/images_v070/analysis/analysis_model_NDQ.png?v=0.1)
+      - Auto Correlation of forecasting signal (model prediction)
+      ![Auto Correlation](http://fintech.msra.cn/images_v070/analysis/analysis_model_auto_correlation.png?v=0.1)
+
+    - Portfolio analysis
+      - Backtest return
+      ![Report](http://fintech.msra.cn/images_v070/analysis/report.png?v=0.1)
+      <!-- 
+      - Score IC
+      ![Score IC](docs/_static/img/score_ic.png)
+      - Cumulative Return
+      ![Cumulative Return](docs/_static/img/cumulative_return.png)
+      - Risk Analysis
+      ![Risk Analysis](docs/_static/img/risk_analysis.png)
+      - Rank Label
+      ![Rank Label](docs/_static/img/rank_label.png)
+      -->
+   - [Explanation](https://qlib.readthedocs.io/en/latest/component/report.html) of above results
+
+## Building Customized Quant Research Workflow by Code
+The automatic workflow may not suit the research workflow of all Quant researchers. To support a flexible Quant research workflow, Qlib also provides a modularized interface to allow researchers to build their own workflow by code. [Here](examples/workflow_by_code.ipynb) is a demo for customized Quant research workflow by code.
+
+# Main Challenges & Solutions in Quant Research
+Quant investment is an very unique scenario with lots of key challenges to be solved.
+Currently, Qlib provides some solutions for several of them.
+
+## Forecasting: Finding Valuable Signals/Patterns
+Accurate forecasting of the stock price trend is a very important part to construct profitable portfolios.
+However, huge amount of data with various formats in the financial market which make it challenging to build forecasting models.
+
+An increasing number of SOTA Quant research works/papers, which focus on building forecasting models to mine valuable signals/patterns in complex financial data, are released in `Qlib`
+
+
+### [Quant Model (Paper) Zoo](examples/benchmarks)
+
+Here is a list of models built on `Qlib`.
+- [GBDT based on XGBoost (Tianqi Chen, et al. KDD 2016)](examples/benchmarks/XGBoost/)
+- [GBDT based on LightGBM (Guolin Ke, et al. NIPS 2017)](examples/benchmarks/LightGBM/)
+- [GBDT based on Catboost (Liudmila Prokhorenkova, et al. NIPS 2018)](examples/benchmarks/CatBoost/)
+- [MLP based on pytorch](examples/benchmarks/MLP/)
+- [LSTM based on pytorch (Sepp Hochreiter, et al. Neural computation 1997)](examples/benchmarks/LSTM/)
+- [GRU based on pytorch (Kyunghyun Cho, et al. 2014)](examples/benchmarks/GRU/)
+- [ALSTM based on pytorch (Yao Qin, et al. IJCAI 2017)](examples/benchmarks/ALSTM)
+- [GATs based on pytorch (Petar Velickovic, et al. 2017)](examples/benchmarks/GATs/)
+- [SFM based on pytorch (Liheng Zhang, et al. KDD 2017)](examples/benchmarks/SFM/)
+- [TFT based on tensorflow (Bryan Lim, et al. International Journal of Forecasting 2019)](examples/benchmarks/TFT/)
+- [TabNet based on pytorch (Sercan O. Arik, et al. AAAI 2019)](examples/benchmarks/TabNet/)
+- [DoubleEnsemble based on LightGBM (Chuheng Zhang, et al. ICDM 2020)](examples/benchmarks/DoubleEnsemble/)
+- [TCTS based on pytorch (Xueqing Wu, et al. ICML 2021)](examples/benchmarks/TCTS/)
+- [Transformer based on pytorch (Ashish Vaswani, et al. NeurIPS 2017)](examples/benchmarks/Transformer/)
+- [Localformer based on pytorch (Juyong Jiang, et al.)](examples/benchmarks/Localformer/)
+- [TRA based on pytorch (Hengxu, Dong, et al. KDD 2021)](examples/benchmarks/TRA/)
+- [TCN based on pytorch (Shaojie Bai, et al. 2018)](examples/benchmarks/TCN/)
+- [ADARNN based on pytorch (YunTao Du, et al. 2021)](examples/benchmarks/ADARNN/)
+- [ADD based on pytorch (Hongshun Tang, et al.2020)](examples/benchmarks/ADD/)
+- [IGMTF based on pytorch (Wentao Xu, et al.2021)](examples/benchmarks/IGMTF/)
+- [HIST based on pytorch (Wentao Xu, et al.2021)](examples/benchmarks/HIST/)
+- [KRNN based on pytorch](examples/benchmarks/KRNN/)
+- [Sandwich based on pytorch](examples/benchmarks/Sandwich/)
+
+Your PR of new Quant models is highly welcomed.
+
+The performance of each model on the `Alpha158` and `Alpha360` dataset can be found [here](examples/benchmarks/README.md).
+
+### Run a single model
+All the models listed above are runnable with ``Qlib``. Users can find the config files we provide and some details about the model through the [benchmarks](examples/benchmarks) folder. More information can be retrieved at the model files listed above.
+
+`Qlib` provides three different ways to run a single model, users can pick the one that fits their cases best:
+- Users can use the tool `qrun` mentioned above to run a model's workflow based from a config file.
+- Users can create a `workflow_by_code` python script based on the [one](examples/workflow_by_code.py) listed in the `examples` folder.
+
+- Users can use the script [`run_all_model.py`](examples/run_all_model.py) listed in the `examples` folder to run a model. Here is an example of the specific shell command to be used: `python run_all_model.py run --models=lightgbm`, where the `--models` arguments can take any number of models listed above(the available models can be found  in [benchmarks](examples/benchmarks/)). For more use cases, please refer to the file's [docstrings](examples/run_all_model.py).
+    - **NOTE**: Each baseline has different environment dependencies, please make sure that your python version aligns with the requirements(e.g. TFT only supports Python 3.6~3.7 due to the limitation of `tensorflow==1.15.0`)
+
+### Run multiple models
+`Qlib` also provides a script [`run_all_model.py`](examples/run_all_model.py) which can run multiple models for several iterations. (**Note**: the script only support *Linux* for now. Other OS will be supported in the future. Besides, it doesn't support parallel running the same model for multiple times as well, and this will be fixed in the future development too.)
+
+The script will create a unique virtual environment for each model, and delete the environments after training. Thus, only experiment results such as `IC` and `backtest` results will be generated and stored.
+
+Here is an example of running all the models for 10 iterations:
+```python
+python run_all_model.py run 10
+```
+
+It also provides the API to run specific models at once. For more use cases, please refer to the file's [docstrings](examples/run_all_model.py). 
+
+## [Adapting to Market Dynamics](examples/benchmarks_dynamic)
+
+Due to the non-stationary nature of the environment of the financial market, the data distribution may change in different periods, which makes the performance of models build on training data decays in the future test data.
+So adapting the forecasting models/strategies to market dynamics is very important to the model/strategies' performance.
+
+Here is a list of solutions built on `Qlib`.
+- [Rolling Retraining](examples/benchmarks_dynamic/baseline/)
+- [DDG-DA on pytorch (Wendi, et al. AAAI 2022)](examples/benchmarks_dynamic/DDG-DA/)
+
+##  Reinforcement Learning: modeling continuous decisions
+Qlib now supports reinforcement learning, a feature designed to model continuous investment decisions. This functionality assists investors in optimizing their trading strategies by learning from interactions with the environment to maximize some notion of cumulative reward.
+
+Here is a list of solutions built on `Qlib` categorized by scenarios.
+
+### [RL for order execution](examples/rl_order_execution)
+[Here](https://qlib.readthedocs.io/en/latest/component/rl/overall.html#order-execution) is the introduction of this scenario.  All the methods below are compared [here](examples/rl_order_execution).
+- [TWAP](examples/rl_order_execution/exp_configs/backtest_twap.yml)
+- [PPO: "An End-to-End Optimal Trade Execution Framework based on Proximal Policy Optimization", IJCAL 2020](examples/rl_order_execution/exp_configs/backtest_ppo.yml)
+- [OPDS: "Universal Trading for Order Execution with Oracle Policy Distillation", AAAI 2021](examples/rl_order_execution/exp_configs/backtest_opds.yml)
+
+# Quant Dataset Zoo
+Dataset plays a very important role in Quant. Here is a list of the datasets built on `Qlib`:
+
+| Dataset                                    | US Market | China Market |
+| --                                         | --        | --           |
+| [Alpha360](./qlib/contrib/data/handler.py) |  √        |  √           |
+| [Alpha158](./qlib/contrib/data/handler.py) |  √        |  √           |
+
+[Here](https://qlib.readthedocs.io/en/latest/advanced/alpha.html) is a tutorial to build dataset with `Qlib`.
+Your PR to build new Quant dataset is highly welcomed.
+
+
+# Learning Framework
+Qlib is high customizable and a lot of its components are learnable.
+The learnable components are instances of `Forecast Model` and `Trading Agent`. They are learned based on the `Learning Framework` layer and then applied to multiple scenarios in `Workflow` layer.
+The learning framework leverages the `Workflow` layer as well(e.g. sharing `Information Extractor`, creating environments based on `Execution Env`).
+
+Based on learning paradigms, they can be categorized into reinforcement learning and supervised learning.
+- For supervised learning, the detailed docs can be found [here](https://qlib.readthedocs.io/en/latest/component/model.html).
+- For reinforcement learning, the detailed docs can be found [here](https://qlib.readthedocs.io/en/latest/component/rl.html). Qlib's RL learning framework leverages `Execution Env` in `Workflow` layer to create environments.  It's worth noting that `NestedExecutor` is supported as well. This empowers users to optimize different level of strategies/models/agents together (e.g. optimizing an order execution strategy for a specific portfolio management strategy).
+
+
+# More About Qlib
+If you want to have a quick glance at the most frequently used components of qlib, you can try notebooks [here](examples/tutorial/).
+
+The detailed documents are organized in [docs](docs/).
+[Sphinx](http://www.sphinx-doc.org) and the readthedocs theme is required to build the documentation in html formats. 
+```bash
+cd docs/
+conda install sphinx sphinx_rtd_theme -y
+# Otherwise, you can install them with pip
+# pip install sphinx sphinx_rtd_theme
+make html
+```
+You can also view the [latest document](http://qlib.readthedocs.io/) online directly.
+
+Qlib is in active and continuing development. Our plan is in the roadmap, which is managed as a [github project](https://github.com/microsoft/qlib/projects/1).
+
+
+
+# Offline Mode and Online Mode
+The data server of Qlib can either deployed as `Offline` mode or `Online` mode. The default mode is offline mode.
+
+Under `Offline` mode, the data will be deployed locally. 
+
+Under `Online` mode, the data will be deployed as a shared data service. The data and their cache will be shared by all the clients. The data retrieval performance is expected to be improved due to a higher rate of cache hits. It will consume less disk space, too. The documents of the online mode can be found in [Qlib-Server](https://qlib-server.readthedocs.io/). The online mode can be deployed automatically with [Azure CLI based scripts](https://qlib-server.readthedocs.io/en/latest/build.html#one-click-deployment-in-azure). The source code of online data server can be found in [Qlib-Server repository](https://github.com/microsoft/qlib-server).
+
+## Performance of Qlib Data Server
+The performance of data processing is important to data-driven methods like AI technologies. As an AI-oriented platform, Qlib provides a solution for data storage and data processing. To demonstrate the performance of Qlib data server, we
+compare it with several other data storage solutions. 
+
+We evaluate the performance of several storage solutions by finishing the same task,
+which creates a dataset (14 features/factors) from the basic OHLCV daily data of a stock market (800 stocks each day from 2007 to 2020). The task involves data queries and processing.
+
+|                         | HDF5      | MySQL     | MongoDB   | InfluxDB  | Qlib -E -D  | Qlib +E -D   | Qlib +E +D  |
+| --                      | ------    | ------    | --------  | --------- | ----------- | ------------ | ----------- |
+| Total (1CPU) (seconds)  | 184.4±3.7 | 365.3±7.5 | 253.6±6.7 | 368.2±3.6 | 147.0±8.8   | 47.6±1.0     | **7.4±0.3** |
+| Total (64CPU) (seconds) |           |           |           |           | 8.8±0.6     | **4.2±0.2**  |             |
+* `+(-)E` indicates with (out) `ExpressionCache`
+* `+(-)D` indicates with (out) `DatasetCache`
+
+Most general-purpose databases take too much time to load data. After looking into the underlying implementation, we find that data go through too many layers of interfaces and unnecessary format transformations in general-purpose database solutions.
+Such overheads greatly slow down the data loading process.
+Qlib data are stored in a compact format, which is efficient to be combined into arrays for scientific computation.
+
+# Related Reports
+- [Guide To Qlib: Microsoft’s AI Investment Platform](https://analyticsindiamag.com/qlib/)
+- [微软也搞AI量化平台？还是开源的！](https://mp.weixin.qq.com/s/47bP5YwxfTp2uTHjUBzJQQ)
+- [微矿Qlib：业内首个AI量化投资开源平台](https://mp.weixin.qq.com/s/vsJv7lsgjEi-ALYUz4CvtQ)
+
+# Contact Us
+- If you have any issues, please create issue [here](https://github.com/microsoft/qlib/issues/new/choose) or send messages in [gitter](https://gitter.im/Microsoft/qlib).
+- If you want to make contributions to `Qlib`, please [create pull requests](https://github.com/microsoft/qlib/compare). 
+- For other reasons, you are welcome to contact us by email([qlib@microsoft.com](mailto:qlib@microsoft.com)).
+  - We are recruiting new members(both FTEs and interns), your resumes are welcome!
+
+Join IM discussion groups:
+|[Gitter](https://gitter.im/Microsoft/qlib)|
+|----|
+|![image](http://fintech.msra.cn/images_v070/qrcode/gitter_qr.png)|
+
+# Contributing
+We appreciate all contributions and thank all the contributors!
+<a href="https://github.com/microsoft/qlib/graphs/contributors"><img src="https://contrib.rocks/image?repo=microsoft/qlib" /></a>
+
+Before we released Qlib as an open-source project on Github in Sep 2020, Qlib is an internal project in our group. Unfortunately, the internal commit history is not kept. A lot of members in our group have also contributed a lot to Qlib, which includes Ruihua Wang, Yinda Zhang, Haisu Yu, Shuyu Wang, Bochen Pang, and [Dong Zhou](https://github.com/evanzd/evanzd). Especially thanks to [Dong Zhou](https://github.com/evanzd/evanzd) due to his initial version of Qlib.
+
+## Guidance
+
+This project welcomes contributions and suggestions.  
+**Here are some 
+[code standards and development guidance](docs/developer/code_standard_and_dev_guide.rst) for submiting a pull request.**
+
+Making contributions is not a hard thing. Solving an issue(maybe just answering a question raised in [issues list](https://github.com/microsoft/qlib/issues) or [gitter](https://gitter.im/Microsoft/qlib)), fixing/issuing a bug, improving the documents and even fixing a typo are important contributions to Qlib.
+
+For example, if you want to contribute to Qlib's document/code, you can follow the steps in the figure below.
+<p align="center">
+  <img src="https://github.com/demon143/qlib/blob/main/docs/_static/img/change%20doc.gif" />
+</p>
+
+If you don't know how to start to contribute, you can refer to the following examples.
+| Type | Examples |
+| -- | -- |
+| Solving issues | [Answer a question](https://github.com/microsoft/qlib/issues/749);  [issuing](https://github.com/microsoft/qlib/issues/765) or [fixing](https://github.com/microsoft/qlib/pull/792) a bug |
+| Docs | [Improve docs quality](https://github.com/microsoft/qlib/pull/797/files) ;  [Fix a typo](https://github.com/microsoft/qlib/pull/774) | 
+| Feature |  Implement a [requested feature](https://github.com/microsoft/qlib/projects) like [this](https://github.com/microsoft/qlib/pull/754); [Refactor interfaces](https://github.com/microsoft/qlib/pull/539/files) |
+| Dataset | [Add a dataset](https://github.com/microsoft/qlib/pull/733) | 
+| Models |  [Implement a new model](https://github.com/microsoft/qlib/pull/689), [some instructions to contribute models](https://github.com/microsoft/qlib/tree/main/examples/benchmarks#contributing) |
+
+[Good first issues](https://github.com/microsoft/qlib/labels/good%20first%20issue) are labelled to indicate that they are easy to start your contributions.
+
+You can find some impefect implementation in Qlib by  `rg 'TODO|FIXME' qlib`
+
+If you would like to become one of Qlib's maintainers to contribute more (e.g. help merge PR, triage issues), please contact us by email([qlib@microsoft.com](mailto:qlib@microsoft.com)).  We are glad to help to upgrade your permission.
+
+## Licence
+Most contributions require you to agree to a
+Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
+the right to use your contribution. For details, visit https://cla.opensource.microsoft.com.
+
+When you submit a pull request, a CLA bot will automatically determine whether you need to provide
+a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions
+provided by the bot. You will only need to do this once across all repos using our CLA.
+
+This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
+For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
+contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.
+
+
```

## Comparing `pyqlib-0.9.2.1.dist-info/RECORD` & `pyqlib-0.9.3.dist-info/RECORD`

 * *Files 12% similar despite different names*

```diff
@@ -1,233 +1,238 @@
-qlib/__init__.py,sha256=bDytqRQU0zPa5FCYksGbfree2wJEXk_jM09Op6qm-RA,11664
-qlib/config.py,sha256=2X4QoGzWpUAAsD3tvJt3KMW2QiFRng8PvENQP7YmXUI,17640
-qlib/constant.py,sha256=krDMjQJ3QlkVWLuu6TWQm-mliqnFVBtxT0Hw4TOP3pQ,499
-qlib/log.py,sha256=QsQty27yTLE3ncDtRxw0PnAVfDXUrq7O9KxTHx6DtU8,7601
-qlib/typehint.py,sha256=xRBIgY6KnAXJA8l6T9eVkfQB30a4WGsWxymumXkH3D4,1995
-qlib/backtest/__init__.py,sha256=2FGniNWUUgR7ODtIgWOTsQSE7VTbt9F6SbUHDA7CswE,11817
-qlib/backtest/account.py,sha256=AkegZe4A8GVjIPz-TbXibiPMEPV6_rHgY5Qc9AEb8Q4,17999
-qlib/backtest/backtest.py,sha256=AupuqYBJ8KwckGQOT3qKGEXoX5LOiyJEulmTsTQKEsA,4039
-qlib/backtest/decision.py,sha256=FcVG8UIPLhZGvpVLxceiCyTW8ynLgFkGCsO4XH_NRZw,21870
-qlib/backtest/exchange.py,sha256=tgyxm8a-DSsycyCpQVOqvi_DqHDQZGNWlx7E991EczA,44185
-qlib/backtest/executor.py,sha256=g8JZb1aZBmY8GN9txIAz5X8KNl9wb5Y9R_bmMBxT6hw,26612
-qlib/backtest/high_performance_ds.py,sha256=mfvGnNwB6qzLbl3Gbvi_LYFBhtrgl4-JPP72bApP2BY,23234
-qlib/backtest/position.py,sha256=mElzmnKSMzenWlSSrVszM0S_kDeb6aHrgoZuQ1wUB-E,20047
-qlib/backtest/profit_attribution.py,sha256=AL4Eb_brqlkwX0Tw16zPhbDbMvv85rS0LkEjgT92dKI,14992
-qlib/backtest/report.py,sha256=ifz1i5XJvMQRdW_JQLaQ5LMMeutt8KMugfkAKFq39Jg,27673
-qlib/backtest/signal.py,sha256=Fg9c6tjACVrc4ltWC4vM-m-KBMchpyFCvaf129ZZ_m4,4003
-qlib/backtest/utils.py,sha256=t8gbVdYwrwLBaMNCQWBAvdX-Bi3kFsSILo90FvV-zM4,10536
+qlib/__init__.py,sha256=NKC_sLh7iRppNCCzyFWY4q2RNZaV-4d8mMyBnY0JSnI,11957
+qlib/config.py,sha256=UvAfRa4hxtNU1LC70x6YFRifD2WZy0NhityiBlI_NDQ,18129
+qlib/constant.py,sha256=UtvceKWAt1x72ptlMvZdeR0eiC2RGBoea4yhB-bDnag,521
+qlib/log.py,sha256=XAsom8NRutyf9sTBLbaXx3Fp4EsNDRish5X_vRJHfVk,7862
+qlib/typehint.py,sha256=rEL9jK_E9a7zRd_ybaO3ehEN1CP0PPaALUFA36ZhJD0,2058
+qlib/backtest/__init__.py,sha256=bJBGs8AozsZut0QI63V5uSxQKHILeswtHSBpFddz_a8,12163
+qlib/backtest/account.py,sha256=cBiG8yo2vs2cXXiyjktadR7eAb7MEhRqnM-ZQAI1U4s,18416
+qlib/backtest/backtest.py,sha256=nJvipPMNJ0W7eLU68zoa7SVnuMYPaIqih6sqw0StZGE,4149
+qlib/backtest/decision.py,sha256=poZtFbyPOtHHW_w4VszeUkXUPwot4Hsa1WXW5-INglE,22466
+qlib/backtest/exchange.py,sha256=ZrXXoYTukjzshZMfDlHE0UVx8e6US5hkQYJ4TUyMeAA,45141
+qlib/backtest/executor.py,sha256=dquUzndpFIfabNQbz-P-FLUUnpF4Nbyqug7RdK_Q-og,27240
+qlib/backtest/high_performance_ds.py,sha256=Rl3jP-6XyLj8DFHzZCMOQYcKGPD-d6ers18g8ZoSyfc,23892
+qlib/backtest/position.py,sha256=rs2y2khG_JEZNWDsuk6RFId1vnirGNvpEcuRasg_CK4,20612
+qlib/backtest/profit_attribution.py,sha256=CsjdEO58UkUr9JZ_IG3UgD6s7TrdhA6M41kqD661S2Q,15326
+qlib/backtest/report.py,sha256=L9bgt_h0llCPuiQWdAuDG9S_3RvKh3KXtb04EOKnQxM,28314
+qlib/backtest/signal.py,sha256=XG8GDzig9f7QUDUnHyYKWBTyuXyD2Vb33Gefjmy_HPw,4108
+qlib/backtest/utils.py,sha256=nV2t_EVAIoV2yirceiJhXJWOKybO4YrugSWC4n3qFio,10826
 qlib/contrib/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-qlib/contrib/evaluate.py,sha256=c-6blhvdD18H1aSQ19Epagufy_NPZ8k7o7F22rDb1P8,14067
-qlib/contrib/evaluate_portfolio.py,sha256=gbNQZNmV7q6acGKzIJN6cLu0BXvhymmDTdRZW_ECe-M,6395
-qlib/contrib/torch.py,sha256=d_a5O5XPhrPFaAKwuDFFvMse_0fvI8BPhzzNg9CYTNc,1074
+qlib/contrib/evaluate.py,sha256=dZQqm0CXuOdR9Spnr4QBUuQ_Q9qdo2HdglukZDgT05A,14471
+qlib/contrib/evaluate_portfolio.py,sha256=QeD63NM6L6Fe6FuRggumwE7ZBOpS1TqUbkIxm6D0rwk,6639
+qlib/contrib/torch.py,sha256=fxnEquHKFlQJ9qLF3QkFkkNdpK1_0qoltYlFJljek2Y,1105
 qlib/contrib/data/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-qlib/contrib/data/data.py,sha256=QrGG63BWrNOMCGcmzAi3S9Mux8fMR6GUrykOcSUBZ-8,2130
-qlib/contrib/data/dataset.py,sha256=ju2CokxxJbNVxFSLWaQoijFIcRY_HFaNq-TOGnk6png,13599
-qlib/contrib/data/handler.py,sha256=lza8QoZLeLtH57wInGaLmc1Dx9BMEANohM5izDPxg0Y,20209
-qlib/contrib/data/highfreq_handler.py,sha256=Gb6xbIdx9h_2jYfJUiiKmNpZHK2zRIRmbi-ttQz0JOo,18736
-qlib/contrib/data/highfreq_processor.py,sha256=Xnt064wvBVPgBfipbeQ5pUYvP7K54csluYMP__WFZ78,3070
-qlib/contrib/data/highfreq_provider.py,sha256=zMCL5qODhAzUu8VRhGslbvIB-ujk-nRmsiktmX-XDZo,12284
-qlib/contrib/data/processor.py,sha256=lZxduH_6P8EQvxd4GUd3dH5jSuFo6-iejXHQBcQqfn8,4456
+qlib/contrib/data/data.py,sha256=AN61Z4vCcFnAwnFkBUrhVo60izc8VUjMoApII1q_wng,2185
+qlib/contrib/data/dataset.py,sha256=yd-6FSJC0-TCnjh4pmuYQn-fXYxsMVZ8Xo63cRMNsH4,13947
+qlib/contrib/data/handler.py,sha256=gqa6GIn5bWtfaPs9yjXmU_dStkxSGCPQdtM6rkMM6jc,20641
+qlib/contrib/data/highfreq_handler.py,sha256=gKJfGN9Er8kgiOPfID9AegREB4TbI0CNw-g3PWTqS-0,19273
+qlib/contrib/data/highfreq_processor.py,sha256=ih1zUJXmuGCHpezwmor4XdTHX17rDM5szb7y57ztxbo,3149
+qlib/contrib/data/highfreq_provider.py,sha256=nuySm_1RGRncadso_7TcLUzxlABVo53o4vWVpxHwT_g,12588
+qlib/contrib/data/processor.py,sha256=5sHlEibP65uum-0J9X9RP1yh0FBO4dT70GS5OgwK-gQ,4571
 qlib/contrib/data/utils/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-qlib/contrib/data/utils/sepdf.py,sha256=RIywfOyQF6KnhgcDd28qKebNxNSMLSPp09ggODVsHdE,6953
+qlib/contrib/data/utils/sepdf.py,sha256=e__2Ipkx8f55YnMx-OPkzFsW-7prESnHo38ZPoRZzFs,7163
 qlib/contrib/eva/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-qlib/contrib/eva/alpha.py,sha256=iOvgy9_zE5awGBeRYzbqJOGmflp9XocXUheP9o0rFPE,6635
-qlib/contrib/meta/__init__.py,sha256=-1WYDd1x7C_ySjzyA3_TfIatOl7uzX4ctTbmfNlzbio,200
-qlib/contrib/meta/data_selection/__init__.py,sha256=l4_gm8N0RfFDn5Gh3UES4z6NEdkjnbIkykAXblBzFDU,211
-qlib/contrib/meta/data_selection/dataset.py,sha256=m97lCYY1ZyoiYsVQEoDlFojp93ybwlFOnTwNsxxDuxQ,17758
-qlib/contrib/meta/data_selection/model.py,sha256=G69crFL2nyt6BmPz-4QvU-4dYMxGfg-MfordMXKeW7s,6431
-qlib/contrib/meta/data_selection/net.py,sha256=PzXNwGdyDx03h74hWTd--1XPaXeRnFyIqu1cz0ytejE,3024
-qlib/contrib/meta/data_selection/utils.py,sha256=48m6CW1GNq6ymaIgB8RBAtpfwuJgmsQZwArvsKef1cs,3962
-qlib/contrib/model/__init__.py,sha256=j2eeP9CZhRWmSykf6j59Qs0usYGSkAWPTPBiGzbLI4Y,1711
-qlib/contrib/model/catboost_model.py,sha256=e2yIfYhFasvEHiWZCrMZPDeuP47ACDzizHiosap_ZT8,3778
-qlib/contrib/model/double_ensemble.py,sha256=lJOibA79JflRhov1msGvaAyUupgDXd7wclt_D1fjxX0,12182
-qlib/contrib/model/gbdt.py,sha256=ntD3nc8a9bTwgbGwlnbtOyO2TTU6_hQZ6k1wj6yNxKQ,4937
-qlib/contrib/model/highfreq_gdbt_model.py,sha256=1wl4COZPaloXKJ1NfWS6SPeNWyb7iffd__DDEyRopt8,6645
-qlib/contrib/model/linear.py,sha256=2bEdpHwRgIYUKrzQ81QXix0pX-85WZkiEgcFR6Wb2Fg,4203
-qlib/contrib/model/pytorch_adarnn.py,sha256=z9Jm_kAUEjvWzlbu_7tqHLcZRi0vemH8v-GrR3FBdmQ,27941
-qlib/contrib/model/pytorch_add.py,sha256=3Yjpylb3eCAhh3hg0BE1ZgmtarEqzDXXc67_cQIlWvM,21515
-qlib/contrib/model/pytorch_alstm.py,sha256=v12HFBVRUXIwzUT-DiiaxBRoJLPL1NYsw3trGJl3i3M,11339
-qlib/contrib/model/pytorch_alstm_ts.py,sha256=4pztzZCKV_RtBg46DYUmNQsbD9F-nCnZL12jNiMTbOQ,11569
-qlib/contrib/model/pytorch_gats.py,sha256=V2Dzt4HANflR-KSQ1Qiw5v0quQ8cdb7yCDq0fDGZXYU,12709
-qlib/contrib/model/pytorch_gats_ts.py,sha256=1DupMU58p0wDmKmIju0R0BOmJoQhd8gLA57H8Hbc_Ac,13149
-qlib/contrib/model/pytorch_gru.py,sha256=OhqlBtdYEZ51BK1OLrJcM8EAYQwiWa4DmnYEY6vrGG8,9652
-qlib/contrib/model/pytorch_gru_ts.py,sha256=0UyjBKKZQTt9x5yEJ7bc-T7H4FlAqCwCbwtc7vC-dDw,9863
-qlib/contrib/model/pytorch_hist.py,sha256=xoZCt6LJ0s8lPJKXidZ_RwcHrYBRaNc1IAj9xfLJ9W0,18671
-qlib/contrib/model/pytorch_igmtf.py,sha256=NHetgRAmvDt5uS6p-i2sIKRABh-MRd8f5mfb3OZGqjQ,15847
+qlib/contrib/eva/alpha.py,sha256=jC3RCExBlLn7fvz31FI4KhQE7qh3zIPucwBEStt117g,6847
+qlib/contrib/meta/__init__.py,sha256=bhS50Fc9PCZP7DBdnG8mkA2_k6MERSUBPdeV-YAtIaY,207
+qlib/contrib/meta/data_selection/__init__.py,sha256=NHHRX2tUJXq6bBC4hukoF9ffH9u-oSCrEMGSTRHGxfM,219
+qlib/contrib/meta/data_selection/dataset.py,sha256=qHww2CIBYOYi6PECpn9OYmaE7gqtQEEFBriD7C4e7mU,18286
+qlib/contrib/meta/data_selection/model.py,sha256=bCmnUwpwYElAlRsUEzBNkaMtMTNC7OTzm1pD1NB_VoE,6615
+qlib/contrib/meta/data_selection/net.py,sha256=aLERCdcTPweZhoFBQO19AMG1X64oNrvdYCZhHSKR0S8,3098
+qlib/contrib/meta/data_selection/utils.py,sha256=kehCNJkEzPoR1zAnNpWF6MRGTx03GU64BMEL48RYQR4,4075
+qlib/contrib/model/__init__.py,sha256=_kfQ7WmL5yT4xsf_MLqip2znd_1PG0BPS7s6Ehyj_us,1754
+qlib/contrib/model/catboost_model.py,sha256=w9ivJqmvc_PUw52WiwvImCrjWc9x6U11whU1puXC4J0,3878
+qlib/contrib/model/double_ensemble.py,sha256=h3QvMQCxQc1rhTyGpx0a1b5t9FRclsFtu5b9ZPISH3U,12459
+qlib/contrib/model/gbdt.py,sha256=Bih0uEfr1kJrX1SiY9XbzkmxjVKOo12ulrajHb9_PGY,5061
+qlib/contrib/model/highfreq_gdbt_model.py,sha256=zqefgzTBMMZTLtHuIVdK5VBYMnuJecW--_zL8sOFkE0,6810
+qlib/contrib/model/linear.py,sha256=y0_PWTSUusKtcvJM0KH33bJ7G-oEH9tu3Es-ngtrrC8,4315
+qlib/contrib/model/pytorch_adarnn.py,sha256=5OLNkE-dhGNyqvkRXtmNjDcHgAeo73nvM4KZd4Y2ghk,28729
+qlib/contrib/model/pytorch_add.py,sha256=T7mEfYiAezSCV1Ngl8kHuy16kZq9-_AtA9t81XkbXGc,22112
+qlib/contrib/model/pytorch_alstm.py,sha256=ReMCzyFun7pCyjkacVBD1YXXnZK6Fb4X_TVJwfVfVFQ,11676
+qlib/contrib/model/pytorch_alstm_ts.py,sha256=m9ZTs9FGUhX13Ggw32Bg1RXbkbOXaiUg_9GfRLV25_w,11911
+qlib/contrib/model/pytorch_gats.py,sha256=i2uT5rskSBxSBPto5iUfaEPsrTbUV0Np22YL_7NtzYs,13089
+qlib/contrib/model/pytorch_gats_ts.py,sha256=QAPvK94f6q-TNwCcSduyqGeCEjD9jU1Aq4v9VA-m6a4,13532
+qlib/contrib/model/pytorch_gru.py,sha256=d5b09QBkb0P7tIzD9jlizsmb290j9hvd6M2YDZODJ5o,9959
+qlib/contrib/model/pytorch_gru_ts.py,sha256=_pgE19BrGNBMtQloNyjW1OqRAAocO5r9FoZMR0250RE,10173
+qlib/contrib/model/pytorch_hist.py,sha256=TOmeEfHCsbyQ1orzRTAdKq7gOA1uZ3B47L6zGgNAQbI,19168
+qlib/contrib/model/pytorch_igmtf.py,sha256=5qsHGuMR4MeMMRbFLAUANIUjT6Y5116N7cCmw90dI6Y,16285
 qlib/contrib/model/pytorch_krnn.py,sha256=kHgK-jv5lFe2vQNA0i_eistz-swBC7nj3CrklWROWj8,16228
-qlib/contrib/model/pytorch_localformer.py,sha256=zpt4-kxhvNEG2ke32qppm-f1sQ-G8XCiEKcs-3y0puQ,11054
-qlib/contrib/model/pytorch_localformer_ts.py,sha256=2LrQXhTiHGCp704lsvjjMAbBWQIEa3jqwU8GWTJV4ro,10377
-qlib/contrib/model/pytorch_lstm.py,sha256=KVvZzTcYIykIHHCIDSfO4b6qEwLQ82nBj5SnTmzJOxk,9416
-qlib/contrib/model/pytorch_lstm_ts.py,sha256=XrTNL7hnjAVJG0SQ6BB49udwajpCqeAgNYVDfsPMvhg,9655
-qlib/contrib/model/pytorch_nn.py,sha256=TZsBIMtcpuezyZ4GqJfq84YGyA3e-dUR-DfqpxJzYRc,17140
-qlib/contrib/model/pytorch_sandwich.py,sha256=twoPJ5BQwB0XvlTg30yLbOllXiez6fEGiSi1mgSxBQw,11661
-qlib/contrib/model/pytorch_sfm.py,sha256=4vsuGUQeEKI6bS0tcH_PAEKjymgjyeJ-ZZvLzjSpI-0,15892
-qlib/contrib/model/pytorch_tabnet.py,sha256=6S9fy6VeuZlHKNIj5IuwroObcSHr1Wz3XGAom_T9KgI,22859
-qlib/contrib/model/pytorch_tcn.py,sha256=k4bOhsyhkOu-wpBcW91Ku3fjA5uTzCBXMzvkemwENNI,9587
-qlib/contrib/model/pytorch_tcn_ts.py,sha256=_J7_8a1CU0ssb9ApJgYAasJrRggG6OhXW9X6gZFMlbY,9167
-qlib/contrib/model/pytorch_tcts.py,sha256=aU6ahRgSm-ncu14FNNOuaZd49WV9n_MqO1yjJfL30jQ,14296
-qlib/contrib/model/pytorch_tra.py,sha256=BEmjtXAQPfuOGV6AsHcOxBpVuldsSVJu_x83Ah6Gkn8,34231
-qlib/contrib/model/pytorch_transformer.py,sha256=kx4BxNj2CjXpPXbS4Qn_JMh31QNqaUsmfGbXh5bnYjw,9881
-qlib/contrib/model/pytorch_transformer_ts.py,sha256=brJXZPXx8wyp-uDNdc_9PCbxfWQsdrgg9D8Jwgrvnew,9179
-qlib/contrib/model/pytorch_utils.py,sha256=Jnq_0vJFPAxsJk-Hal0MIgGjLyT23AbH3tA-_YPvYsc,1197
-qlib/contrib/model/tcn.py,sha256=BSZAo_SMMj5NN1rcMDogbSIZgU8n1x8lQhYaeIaJPD8,2606
-qlib/contrib/model/xgboost.py,sha256=E8i-Bc8u2oHNNQdK-drqz6L3BKuOnpBIUMYq_0NUk_s,3084
-qlib/contrib/online/__init__.py,sha256=qOkUyzuAn2awv6p45VZinttdlsEjEC2t52Jo6KbapZ0,586
-qlib/contrib/online/manager.py,sha256=2YScKQrx4tbiAR3sGIz5B6XUp9gjd_CJ7SGJOBTlu4U,5489
-qlib/contrib/online/online_model.py,sha256=Np261q3R789p1x6JsgICmk43x4FMpMJPL1hZrFyQOVw,1110
+qlib/contrib/model/pytorch_localformer.py,sha256=oAnyDy-eUVaAoFgFIUA41lcxIB7CcPhtAZQ5nIW3gfg,11038
+qlib/contrib/model/pytorch_localformer_ts.py,sha256=c1mESI5iCkabtFFuH3PCja4t1j7RrzgtIW-Pfn5VRTk,10365
+qlib/contrib/model/pytorch_lstm.py,sha256=dqYyv26msgIe_2R7GO_ZHQZkgfK58HvZCpINmq1Pctk,9716
+qlib/contrib/model/pytorch_lstm_ts.py,sha256=059CK8X-zpPMBbl9yRGMcvE_fAgNsGYzQQWB6ZYMj08,9960
+qlib/contrib/model/pytorch_nn.py,sha256=JX58XDI52Jz_O2JzY-1hN2hP68EUClvIIoZ0sZkLY0o,17587
+qlib/contrib/model/pytorch_sandwich.py,sha256=u51_bpuQfrvF0YuwoCp7XxbMHIhSQkbD8gfaWZxNU5I,12042
+qlib/contrib/model/pytorch_sfm.py,sha256=Jo1w8dUG1beM7t_38BJkJ50vpB-8LWZyy6ShHAiiMjY,16365
+qlib/contrib/model/pytorch_tabnet.py,sha256=f0jm4oJQeKYg6LyWFiKbuLWpXzdWvAp2VgCwX09xEF4,23498
+qlib/contrib/model/pytorch_tcn.py,sha256=af6JADWVm1S4vvq2Ur6Eo6xaJgKyQ5JCPZxdf3HMdHQ,9891
+qlib/contrib/model/pytorch_tcn_ts.py,sha256=tv41Ieg6LlNRjx1e3A2S8a6f7SIRmI1wG8vyK6W6QsI,9460
+qlib/contrib/model/pytorch_tcts.py,sha256=05zmdSfgUwXzVE7Vuwq_ZEf3og5Kv5rOrIdsBhwZ9TU,14713
+qlib/contrib/model/pytorch_tra.py,sha256=98S39LzmNPWYK_AL0BPowtyqcTFgdxa_0QBC9mLHxQw,35154
+qlib/contrib/model/pytorch_transformer.py,sha256=kRggzmbITPvDJ_7QPpnL_Lq0f3G7g8YhexBHsrmTswU,9865
+qlib/contrib/model/pytorch_transformer_ts.py,sha256=MiIvdKPA73ir0U8QzEgDCL_6TdKiFAPvdMZ9wDOQxb8,9167
+qlib/contrib/model/pytorch_utils.py,sha256=SSysDG8nhbvCBK7gjCcI1NIZEf5KbcPZPa99psig7XQ,1234
+qlib/contrib/model/tcn.py,sha256=cV4JdS77oeCOpbRRJ65u2Ebj9X61dn1VdptNT42r19M,2682
+qlib/contrib/model/xgboost.py,sha256=ZqAby_ubVeWyajJ8uk7fgnYmBE7IAK0LqTJpUL7yTlQ,3168
+qlib/contrib/online/__init__.py,sha256=sR8osPGs_a8HsW2IZ9QRQuNUbeCgXNPjsbRIEArmxcU,607
+qlib/contrib/online/manager.py,sha256=PQJA75Rb7Ym5zsZ4dnneM-vwKxIjjVDvrPEs8D-w-Sg,5636
+qlib/contrib/online/online_model.py,sha256=ot93XYONg4egpK09bnr3mlXh77Z5gBDrPrlBwGOyYZE,1149
 qlib/contrib/online/operator.py,sha256=mn2W7-plBAWSBKT8KlzPeE-CYOyYkzM1lt3kWG-R698,13138
-qlib/contrib/online/user.py,sha256=Vw9U2hIU7t6gOWgwT4D26MRZbxqicEuDPK1AAj38v-Q,2980
-qlib/contrib/online/utils.py,sha256=I8JkfIlmc2sRnWy3V32b4MbMYdI96464orL-9yHrR6s,3079
+qlib/contrib/online/user.py,sha256=rBysCTPLnmpUXYbwGGUyKSchOUi_NYR9v7-9xgS76EM,3057
+qlib/contrib/online/utils.py,sha256=EJ-4fagemHmNu6NAqkNer9dPfRpdhRYR439fqqm9DN8,3177
 qlib/contrib/ops/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-qlib/contrib/ops/high_freq.py,sha256=lWWlq5pTRgkBk7_0KT0lKyNjOdg30dIFOfxqBm1Gbuk,8151
-qlib/contrib/report/__init__.py,sha256=wFLAod-1VFey2wH1TygZA69eVaF1Vo_06ZEX7Go-UyM,356
-qlib/contrib/report/graph.py,sha256=lxoB8WWCYCk6_LEzqYhIg6sF08GVYpnng9CwSdAcFi0,11189
-qlib/contrib/report/utils.py,sha256=DPEHMrfTyNod_l6A7ruqTvWvOc3MglJ0nSCjQj4RE_I,2467
-qlib/contrib/report/analysis_model/__init__.py,sha256=dnfKOoiFiGwr2tKsVDxBRIRIoU8G7MUtQ9GlveNvLrI,178
-qlib/contrib/report/analysis_model/analysis_model_performance.py,sha256=MO0lMCpyI0CbWAQNz40oG5VFhm7AS1SIcjsHTdeZDs4,11382
-qlib/contrib/report/analysis_position/__init__.py,sha256=xbGv4t71BNvpcM_T8g64P-NtCBEKOBfwVUY-422ovpI,404
-qlib/contrib/report/analysis_position/cumulative_return.py,sha256=qb1_33VQTHxuZRE-BD_j6Y1RilUWx8byWxyMTQ9jH6Q,9452
-qlib/contrib/report/analysis_position/parse_position.py,sha256=rPoTnMs36xx4qXLZPCjOt2oWeNQwgk3suyzVuq0VUPg,6559
-qlib/contrib/report/analysis_position/rank_label.py,sha256=ohJmbOKELhf-aGTgM9HS6Zams6h77mQWOqvcUDe68KU,4322
-qlib/contrib/report/analysis_position/report.py,sha256=2e_fXekJyz7oS_wHd93g3g4ilZB6dGe9GmwMBtFOZtY,8413
-qlib/contrib/report/analysis_position/risk_analysis.py,sha256=gbXypfXhdy-nyHLWTP4tKXJNcZWDqfm32dR-5xeTzK0,10651
-qlib/contrib/report/analysis_position/score_ic.py,sha256=mGVgdS0UY92pdNs3yGfNaFYpbzouOrc8SuoI5d74yeY,2572
-qlib/contrib/report/data/__init__.py,sha256=TsYH4Lg3_1mCZbNQwUUwgIoWCbVeCodkO4O-PN1vAWM,124
-qlib/contrib/report/data/ana.py,sha256=WbQ0YDHbAQqIGRudnAanrmV_6CAffIIQrhUq1lqOAXA,6608
-qlib/contrib/report/data/base.py,sha256=rxSzEKfwnsuhLPJheoDkpLDhZF9QW995SMjSp9k3Z6A,931
-qlib/contrib/strategy/__init__.py,sha256=p6GFa_Pom2vywDWWC2JWM8kgphtsj2E1OfQSO3_UONM,512
-qlib/contrib/strategy/cost_control.py,sha256=5Yko4gk38CDl6vXRixEgOdQJ3ATa99YT1M_vk3YCjio,3827
-qlib/contrib/strategy/order_generator.py,sha256=rvIDMIrczR1GvczKShaJ1-rngwdxEagaVkztFPoanyY,8260
-qlib/contrib/strategy/rule_strategy.py,sha256=Hm3MbseAz3HwtteXNyCNkiFYfzYvecYpERC8S8a5-pg,29374
-qlib/contrib/strategy/signal_strategy.py,sha256=hi54pEpfp1eYf4ntcgCxe_y8VRlkRXL_lzGsWeAbtPM,22498
-qlib/contrib/strategy/optimizer/__init__.py,sha256=Choav7jAo6dWYzpozZ-Y5sv-i3UsHntp7b3sNhY91fk,286
-qlib/contrib/strategy/optimizer/base.py,sha256=wpknTRNDw1UTBqceEmmuLunJWPRNkcMYJi_o3DW6omw,314
-qlib/contrib/strategy/optimizer/enhanced_indexing.py,sha256=JRvmNAIycIrCmdQb7HKUMmcigUd3qD2GeZlPPt7i9Bk,6515
-qlib/contrib/strategy/optimizer/optimizer.py,sha256=PVtRkk_cMSwMF3iBYnkoGhvi9juqstT2rEmsrY7NB8k,8640
-qlib/contrib/tuner/__init__.py,sha256=HoggAQsvLU8t7QxOpcoQQPpwbV2rAVDK0hRIUd0M4GI,35
-qlib/contrib/tuner/config.py,sha256=PiKyyKYp6TX2RSIEgnmPGRO-ePDED8zzabe_Uyhwfj0,3629
-qlib/contrib/tuner/launcher.py,sha256=acmYeLvrmAS505d4aMD74IrdEiYydYHGGcr9h2V_AaI,821
-qlib/contrib/tuner/pipeline.py,sha256=9na4uYzORrf1bb7uxXQlZv2unBOaFL9Bw1Wq6lFaCQ4,3451
-qlib/contrib/tuner/space.py,sha256=sPhP5UL7kTiQqI4mclYKJ2V-v0UW8-QGQeX48uXjshw,433
-qlib/contrib/tuner/tuner.py,sha256=LBC7IAZ7b5E6Ws5XTvxKhiOx8lc8cCMp-0pp2DBA9Qg,8009
-qlib/contrib/workflow/__init__.py,sha256=JoluMT9kns0-2ffSIUVSt3RAVdr29sh6Waycep1d5jY,206
-qlib/contrib/workflow/record_temp.py,sha256=2mSpRiRO86BSxCoChmGjjCLrw3023gw-OpYWBcqMNO8,3368
-qlib/data/__init__.py,sha256=HQXySdtZOLVjscNs3nD-gxsQVkPKdb6MB4vHHa7HtIE,1410
-qlib/data/base.py,sha256=MDgt0aPEdQbmFRTzdTOjQH9j_QHJqf1aErjAzOhcXVw,8387
-qlib/data/cache.py,sha256=cMo4tEYLzNFeLCHuWMvcWH8s3LSkXL81kJzUBwX25Y4,47251
-qlib/data/client.py,sha256=a2ZcwMA2W9Qv24YxFhuWN2WE61z584i4BwZ7CydNvLs,3749
-qlib/data/data.py,sha256=RD8Qwekn0GrXJiBPs_IKfVwW0M42LU9AQm1c_GmJRjQ,49488
-qlib/data/filter.py,sha256=WQ1jEBELiG2HS6Um-jEcuAM9JevNPzKNy6hOEXOZ_gs,13924
-qlib/data/inst_processor.py,sha256=SViB1JaOxCJhFkUEQtiVJfb60Wa-B3a8ZlPugP0DLtI,598
-qlib/data/ops.py,sha256=b2SDVXJahanxdSjYZCgfwGX4pPiHJhqZCfdJXV20J2A,45446
-qlib/data/pit.py,sha256=f2Dy6Jo9yYzEU9cP19aL8gY1n-E6IvYBHKKWkFVSqQU,3232
-qlib/data/_libs/__init__.py,sha256=HIog-luBqQnLkza1Q8b34Rr1QVu6tuiAy5sOry0vIPg,73
-qlib/data/_libs/expanding.cpython-38-darwin.so,sha256=kaTpSSmqW6GDvAtVqKjgYowt-RSjtHIx1p-mTeieUAs,156960
-qlib/data/_libs/expanding.pyx,sha256=DuPZ0lGqPe9vJ4hOMqDX2kEizKBdeGiAXr2F_s5I3VM,4151
-qlib/data/_libs/rolling.cpython-38-darwin.so,sha256=s2Ho1e2tmtHeNZ0gLZIqK00oS8swp1QtfMadPR2zrtQ,104808
-qlib/data/_libs/rolling.pyx,sha256=WLLkGKeFWBNcseytiL_P9oyumLK7JpXY0vreizDF2_E,6111
-qlib/data/dataset/__init__.py,sha256=ZR6d-2jPwbhGgXHlR6L1rpX_2cK3XZqDa7KXVklbgSE,27197
-qlib/data/dataset/handler.py,sha256=KbLOmqUWDSf79CrE1dRoLtS1mFhCF9NFu7Y30AzpBQg,26735
-qlib/data/dataset/loader.py,sha256=BBPTUf0QT0uf9UDDSS5FK5yDEmWQUxOKO5zU-1D6__U,12348
-qlib/data/dataset/processor.py,sha256=vZlX7Jv9P28LnvwJ2HXL9sflzr5JEJ4m232BXZosAIM,14380
-qlib/data/dataset/storage.py,sha256=PUQlHr3hP6MSuVBCdLbZ_uUxykCt3y00aOmlE0iwIfA,6349
-qlib/data/dataset/utils.py,sha256=alxL6wcdqhMjfxHerVbF41n6eJzRw5AA_mVPgEx7Ctg,4184
-qlib/data/dataset/weight.py,sha256=FKjDhozu3FzM4xu2iaWcdEXX8MqayKeKTLStMQTmqFo,754
-qlib/data/storage/__init__.py,sha256=mi7ZtiLmNNN71Jf-MxzkJoQ7uZEiwLwH6_k726frXmA,269
-qlib/data/storage/file_storage.py,sha256=m1cNbIlPZu0rVOHzetFRJ6E0VlRbYQaCy_Nn1E0_0Rs,14386
-qlib/data/storage/storage.py,sha256=fZZEIU5yrK2-hF54wnUt6SiFpobExYB3plA-WVOCqrs,14662
-qlib/model/__init__.py,sha256=1aHgnZ7OBDn8pmi93j277bFoMMx9kyviPLeusxip1Pc,149
-qlib/model/base.py,sha256=MddRQP94j3pPhG8PXmvW_gdrq-g_k_jkByN29TKkrS4,3771
-qlib/model/trainer.py,sha256=oC0TxVSSOAcgfMLuGY7AJfQ5nE6gQXYAI_pxCg_ZmIU,22767
-qlib/model/utils.py,sha256=53UYP1s9e3SFwgMwfSqKyn3H6M6L2UNPnoApFzqxIzE,579
+qlib/contrib/ops/high_freq.py,sha256=jn1Ml0FcKptf8Y2JOsPjjnH1tmpU8tF_wrmBQSN0Z_4,8428
+qlib/contrib/report/__init__.py,sha256=6VBXkJS3olq0729kDPOvM7MYX4r_RC3T95HhIUdp1-M,367
+qlib/contrib/report/graph.py,sha256=IJjsIVY_WCQDX6YqsN4bpRHROJfHvjz-Nkq6fpbBR28,11572
+qlib/contrib/report/utils.py,sha256=oAVJmFEq8OT6RBDp-wK4Y1OX3LQlid3dlJJInztDerg,2541
+qlib/contrib/report/analysis_model/__init__.py,sha256=CZyrTgVMoJCH_H9i8nSwy3SEvoPuz9TdkT8GsnJ_44s,185
+qlib/contrib/report/analysis_model/analysis_model_performance.py,sha256=VXEZcMaIegqUwWeZGUWk2Kx2qJUoSIGuSSsUF-LDzlE,11719
+qlib/contrib/report/analysis_position/__init__.py,sha256=DF6zU5wUrZtaSnm64s5QrutuxpkvpAEl9JGsg6ZXo3Q,415
+qlib/contrib/report/analysis_position/cumulative_return.py,sha256=yiEXG7MVYUm4QIBlFERy-YnklirQBqMlhei8eNtkvTU,9725
+qlib/contrib/report/analysis_position/parse_position.py,sha256=RAIT4bcJfWdoA-hFxMhRowrPXFGeR8Qv1hai0SPnICA,6734
+qlib/contrib/report/analysis_position/rank_label.py,sha256=PuJXEmAKATV1wiRTdAzeZqfsfiGEL3nWh2XmBdKUycg,4450
+qlib/contrib/report/analysis_position/report.py,sha256=CmJkWCFHouNVIRGrtfpuSxqxD1lQ0tW3k-4J1Hok_fc,8661
+qlib/contrib/report/analysis_position/risk_analysis.py,sha256=09NcA8oicNXGplkMKt-ZAUivu70gabk4Lda0TRpzlhM,10946
+qlib/contrib/report/analysis_position/score_ic.py,sha256=OavmzPZL2qeMAUuHvhTxc-0zVlZ4qTuoXmEmXWyCo8w,2641
+qlib/contrib/report/data/__init__.py,sha256=dtBK1Y9yoq-fppZV8R59BmF53UGTFK9gQrQDc1mEz1I,131
+qlib/contrib/report/data/ana.py,sha256=NeMyo_dCm144kovUNKf4lP_1eCP3y2WHLUaYOI0srUg,6808
+qlib/contrib/report/data/base.py,sha256=lg_IWc_FW7hevkB3VN-60skhEAOXLz5VnAElEl7F_QE,965
+qlib/contrib/rolling/__init__.py,sha256=hGXQ9h0JwbmQOIPhrB03sTasi4CRT8MZZTyy3ULj6mo,326
+qlib/contrib/rolling/__main__.py,sha256=kDzi6Q_GJOA-O1ZU5aY15FivgecrKyBF2750oUkxRRo,658
+qlib/contrib/rolling/base.py,sha256=M6DFGTubY7b-aFLhzdVPdEadY_vleahWjiwnaI0HTy8,10803
+qlib/contrib/rolling/ddgda.py,sha256=LmIZfFT0n_GdDlIgfBOpQAGuN4Nn8NDWSTDBhbPs4Dg,13592
+qlib/contrib/strategy/__init__.py,sha256=HU42uZOVXcNvHOYumuYBt9KXxD-KYbxfLClI0JURzeA,540
+qlib/contrib/strategy/cost_control.py,sha256=3uY6HhccfqHcza2mr6xOmzDYMDBLXwOkCNWu0kBIaM0,3928
+qlib/contrib/strategy/order_generator.py,sha256=Q3N6_Cvb8Dkrfiu-Gahc8rcGXtAQ1CLzeV60tr2fylo,8478
+qlib/contrib/strategy/rule_strategy.py,sha256=SyrSxVYRFrbaPRT4D2prcHGKtieBJN3sMqv8tZ9GhIQ,30043
+qlib/contrib/strategy/signal_strategy.py,sha256=KL9v2O2kGXo7vr-7Om07RLs0aYYYwUPnQJQw0OxlJLk,23019
+qlib/contrib/strategy/optimizer/__init__.py,sha256=F0-1v-SqvAIikXBMBQLj6i-O2ZDhw3pwKI5ENuDYg1s,295
+qlib/contrib/strategy/optimizer/base.py,sha256=42FYnLSCIKbd_-8oOzjum71-RC809l7x0W-NsWHQPWI,326
+qlib/contrib/strategy/optimizer/enhanced_indexing.py,sha256=e7ZdbR-Jtf7XP6PQO-bsQX4Q0pTSVZ_tL2aWk92DPmg,6717
+qlib/contrib/strategy/optimizer/optimizer.py,sha256=xy3hppNYTBy4lo2PzTRYX5CaU1iPn9s-23Ts2eFey5M,8904
+qlib/contrib/tuner/__init__.py,sha256=mzpYtSSiJ3FA4-1qo7EgF2bIPiU33Tiz0Fyk0vK93OM,37
+qlib/contrib/tuner/config.py,sha256=rWYtk6vDiYPWVt61O-Bbo1AjyT5LUmJPTJzwxg5eAUI,3716
+qlib/contrib/tuner/launcher.py,sha256=wsVhHizr6cSlF6l3k_JnjItdG0TiafdwDEFiXXoaH4o,858
+qlib/contrib/tuner/pipeline.py,sha256=WRcC9yzPlMBci7hBwTx4t_CP1UUPRI-kl6r2b1uwuas,3532
+qlib/contrib/tuner/space.py,sha256=VRqzPOfOx2FGBc-NwEyELcr9pMy8PRJsl_LuQKnkXYM,453
+qlib/contrib/tuner/tuner.py,sha256=4yCd8n0T4bhXJeKxGNZ6J-OxBFBR5StMXK-b_ffaNX0,8217
+qlib/contrib/workflow/__init__.py,sha256=sDKpsibnrzwU26YBp0H5LQG5nyKqYvG-NficMUZ5Oao,213
+qlib/contrib/workflow/record_temp.py,sha256=Ru1QGd3iYFBZ02Ie8vs6hC6938DJxfA3aqcEi4guAxw,3454
+qlib/data/__init__.py,sha256=oDv-vXV31G47m0ZYAUh57DPf1PJggtr4SWf7AuSVqbk,1476
+qlib/data/base.py,sha256=zCkQO0_LWnyH8pGU_D8P0KErYhJQpv3q285RctsjBHk,8668
+qlib/data/cache.py,sha256=MXr6IxLMrigq0oLaSpztvm22RS1eeOF-zInhCkOQ7-A,48444
+qlib/data/client.py,sha256=Y0ELILOtNQScrwMbxyOBo9_mtVNHQVM3kix2j8KTwf8,3852
+qlib/data/data.py,sha256=6lzNd4Yqd_fJT_PuS_w7189-0gpcVJC1xlpJYEES6ac,50820
+qlib/data/filter.py,sha256=VUGNrTvYZx7zloterDLZ--GPbQNPyA_3B8BrKngsFvg,14298
+qlib/data/inst_processor.py,sha256=4sxpGYaj60TxCsmihI-pR7rhVZRbQ4nSEBF1-HUC9lM,620
+qlib/data/ops.py,sha256=b3hWJEiq7KMzdSdOJAY2imGXRNTKJXM4N-dGM2mFhpE,47127
+qlib/data/pit.py,sha256=-vjWuqj3lnNSTxGkzYvpUQPcdDoL2dLi3MeV3apvyIQ,3302
+qlib/data/_libs/__init__.py,sha256=aCOr6sEsQpv9z4cJgWFA4qOs4xJqclqYYnxOVcxiK2Q,75
+qlib/data/_libs/expanding.cp38-win_amd64.pyd,sha256=p0GH4T4PdVp1OOllRB9cFTafLdMXgrHA1PeKJuFBZMI,156160
+qlib/data/_libs/expanding.pyx,sha256=KsoPvgUfdS4sciHql8v4XNTxUAbaQenCMwKDMm23Eh0,4303
+qlib/data/_libs/rolling.cp38-win_amd64.pyd,sha256=U5ZoJ7bWvvBhtBrMIPZBwWg7TBLeTDhH5sf92PLUGmk,120320
+qlib/data/_libs/rolling.pyx,sha256=VIJKjdYk_yjaVnZGWzymHaqjskejV3DjQDA0IsScrsA,6318
+qlib/data/dataset/__init__.py,sha256=h8VcuMzJaIjJhxuGZ0NUbX3ViSB0nEyv0pVGJcbAehA,27919
+qlib/data/dataset/handler.py,sha256=DU9YZUTFEZxM8cu667fonEnanIrRlupYj7Y2ptaInT0,27480
+qlib/data/dataset/loader.py,sha256=nW2NtHGtrW9CtS7PNaimmMtmH7ZyHUIbmCe2mVi_XZw,12690
+qlib/data/dataset/processor.py,sha256=5JY-ULQ_AxV4fAwf_LwLpIs4b6-TGZS14U_M7cb97Pw,14799
+qlib/data/dataset/storage.py,sha256=WmqGTnypXbS_h6UkVrPxZotbl-Tfz4FZWYkILiIdRWA,6507
+qlib/data/dataset/utils.py,sha256=L9HLCDQtKPsKHVHS53XXEntS4Y55YZbrI2KRVs5zHwI,4294
+qlib/data/dataset/weight.py,sha256=6smHSNpcnB5VeQk88ceFx06pTJrA-Rvlyl4bRAflEwg,781
+qlib/data/storage/__init__.py,sha256=EoHku26sqCZypwS5xwubeUZg2BR2wlA8A854xUWy-pA,276
+qlib/data/storage/file_storage.py,sha256=bWTUq7xTCwEUQVHY_-5sWvh7Ag0Km2Q-_RvUXFfWwCI,14762
+qlib/data/storage/storage.py,sha256=vjiruoreGF96amrTx70Vp4JqgSCvAMVvYemUabKvwAU,15156
+qlib/model/__init__.py,sha256=XBWjB3NKBzYTxkjsmtA1gWWL19jF43fiCPk-uxPuO5Q,158
+qlib/model/base.py,sha256=huwwfvNWlgGrQAnQ-vvbCa24b701ko5yId0sdyNxKPc,3881
+qlib/model/trainer.py,sha256=OJs5q8kjEb1WSMWwElDtRs0hiFWa-qR8WTErQ1oW6cI,23384
+qlib/model/utils.py,sha256=StxM6k4pKHyYln4UO8mhQ9pWHTPTU553Tf9EY2SGlJ8,605
 qlib/model/ens/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-qlib/model/ens/ensemble.py,sha256=djy2tqkSKoijgnS8lgiiPxM5-UAkcuOtp9sX364FrlM,4535
-qlib/model/ens/group.py,sha256=k895guJsT4MWg3-Hhy1vrLhixbvdfsNZ2PSh-dJn820,3912
+qlib/model/ens/ensemble.py,sha256=3QLUVC8LN98r3Smd0bWkiXMQmGaR7sQ7ODe6p3RhTdo,4669
+qlib/model/ens/group.py,sha256=T5yyr6eOb40IXI45Hh3H4Z6vHgl_cBscsAkc1HhLmzc,4027
 qlib/model/interpret/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-qlib/model/interpret/base.py,sha256=EymsCQ2wL1lUL-j1ZidEu9Jsf26Ie7envNWsRTuJN4c,1133
-qlib/model/meta/__init__.py,sha256=mUf_cA9XvAqLQ99dpK6ug5pfiFo9isxY9r6BRD9XgAk,182
-qlib/model/meta/dataset.py,sha256=sgzxQYenBEPHdNbPz7vy0Q6F7WKQdK6Kk1SiFbhM9iA,2779
-qlib/model/meta/model.py,sha256=5OJgoaK8Bf5ZJMrUJmTij2AXpOfFB58NHCPQhweh-Qw,2355
-qlib/model/meta/task.py,sha256=ankt4uPzHHSlqXyt7NeDnQ6oqSkcVbXFXwYKYFmQoq0,1655
-qlib/model/riskmodel/__init__.py,sha256=rXG2ZcTw2QfntrUIr6fqGzvayRyCRWk6XH_EoBdxFeQ,336
-qlib/model/riskmodel/base.py,sha256=4BXJGDukkojM_fqzRXICXC5UmH25_m3uiOvHiR7FGTg,5035
-qlib/model/riskmodel/poet.py,sha256=-0BphUELG7BqnWA-Mjl7Lv0hELgwixp9qjYmf-3koeU,3206
-qlib/model/riskmodel/shrink.py,sha256=N0bShmorfhArmjFi0W0Mlh1Z_UUI2_GML1iFw9vqMls,10441
-qlib/model/riskmodel/structured.py,sha256=XoFQ0vcj-2IgjA2U8AgsWy1w8gk2AWeHcxV5Ws6mMD4,3801
-qlib/rl/__init__.py,sha256=1yQ-WhAr-Mpg0M_UUQhIgzeSH3vXmKqtxdHkitLQ9Kw,339
-qlib/rl/aux_info.py,sha256=KIoN56J3HOGz6VfQAlbUCioNGQbv_LO7cKE6wPrh_Aw,1122
-qlib/rl/interpreter.py,sha256=NFgtFr0-B9qGeCxmG7KlyRsn_IhKYSKoaNJdrztwYgo,5235
-qlib/rl/reward.py,sha256=UmiO2UEzrLfv4loMYZ-xjXT-CnMTE2dGAZKH1uQcmbM,2703
-qlib/rl/seed.py,sha256=BMcYvOrnCkaj4T1TxLlz3V0tVTOAjX3FCiVZIK7BXHk,339
-qlib/rl/simulator.py,sha256=m4Uou_hpbsqKJyWpad82G_UShAk5h4NoBQQ3m8UIGe0,3031
+qlib/model/interpret/base.py,sha256=6wCjxWtDPtZubraPBuOajOnoqwbn0_dgzOtRepHDU9Y,1178
+qlib/model/meta/__init__.py,sha256=tycr1KT8R5NHcv3vjADvGxYnKWFE640R17R7Eh6A90Y,190
+qlib/model/meta/dataset.py,sha256=sPxOWc_Al63qGRngBRSXtsfYG8xdPCvln3NYKNFN2Fw,2856
+qlib/model/meta/model.py,sha256=wW0YOr3_qycfZED61nIHSYKnZ5QyiojoVJzvUt7t3Es,2430
+qlib/model/meta/task.py,sha256=A3QTaf6BE2cpZxQQwlpC4D5UU705cti3Momh4yfFG-o,1708
+qlib/model/riskmodel/__init__.py,sha256=zKZq1p8FAYqiEocyPKOD8JggR2HxQbYqsXbEFCYbU9w,351
+qlib/model/riskmodel/base.py,sha256=3An3fU04SS6FjWPQN2XQXhiITY49JQt5xKQjaTFn6bo,5182
+qlib/model/riskmodel/poet.py,sha256=VPbn_mnriZORRQcQbD693ML4BvoLTsh4NpSMYEr4tSY,3288
+qlib/model/riskmodel/shrink.py,sha256=dR4xG3pkzBuCaZ7w-8EfUOHNepaFL0iRc02GkE421sY,10702
+qlib/model/riskmodel/structured.py,sha256=tCmQaHNnn2w68oy56j25u54ES7icD8W5hR0PgBsqAmo,3895
+qlib/rl/__init__.py,sha256=IEE54Pl3SKG9AqVKeiRAQ8KoDsrylIrHT4ZaANvgYJI,347
+qlib/rl/aux_info.py,sha256=7xAOr5BAj4Mr1eJ4TPvXYhI0hjlSI4gy0w8shwssrmQ,1165
+qlib/rl/interpreter.py,sha256=z_3n8_Zf3Toqf11-SzkZ2Vx5UfjOwv54Bb0sM4kYueg,5376
+qlib/rl/reward.py,sha256=7qsPJSyKHALRn5faCbo1Lbb95c_AL_ZvsLEXh9igVm0,2788
+qlib/rl/seed.py,sha256=vIbUeVlf2yjMH1Rqh-l8RcBgp4RsBm5Yg0sROK-q_7k,351
+qlib/rl/simulator.py,sha256=Jk1XWXYRbrq5yahE-VH7Gda24PLFz7IPx3pqBb2npMI,3106
 qlib/rl/contrib/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-qlib/rl/contrib/backtest.py,sha256=JfzJfGO8RVsFzYBAm1GMXv3Q13FXpL3OUAvrYab2lLU,13230
-qlib/rl/contrib/naive_config_parser.py,sha256=wvTlWIdn3-cAbNPld3Eb8wCc0HadzUmRjOsUs2DMwtc,3387
-qlib/rl/contrib/train_onpolicy.py,sha256=tFvmlKKAghwBFNtsCqKzHsPCnDq7OPjV_j_tdQpeL1s,10105
-qlib/rl/contrib/utils.py,sha256=zrWMhL045FqLzY0UyunLXLXQdU9G-0Hddi6IcgN_7Qs,836
-qlib/rl/data/__init__.py,sha256=AqsQiQ7Ml4Yh1COlpGTcaYHBGS-roG8NtlnOG1EYed0,245
-qlib/rl/data/base.py,sha256=MUpNdUy5KXhGtZPIhqOVpQwv8Brk9Dd1_tOMS9Wr-Og,1754
-qlib/rl/data/integration.py,sha256=GBBHwj_incV4BzMGi1BBiEZP_za2fXNiZ28a0Sf4FlY,3111
-qlib/rl/data/native.py,sha256=4H4FCVkouE-hOLp6_QqZLid7CtlhWC1n72TfUuTQqYU,7412
-qlib/rl/data/pickle_styled.py,sha256=Zsq3cAte9W_T9IGcDtpLjFSuGqGxAbpCvCTCAUGGhlc,10544
-qlib/rl/order_execution/__init__.py,sha256=vEx8FQ3Dbv3Bqu4042VfUI-m4KzKtC_eX3WjtN1Wl3I,1008
-qlib/rl/order_execution/interpreter.py,sha256=Mw6zXApMoO4dQ38VuRS0FF--RT4Ax0XMCDYU19ScmI0,9669
-qlib/rl/order_execution/network.py,sha256=lYQDWFiUNlK96TC6xRKKJhIrP-EIPSai7P6n7RICIQw,4830
-qlib/rl/order_execution/policy.py,sha256=O7Aen52CvL9DC_Yr9CIJzozDrsbeyOWowNQJdk2xbcM,7008
-qlib/rl/order_execution/reward.py,sha256=99htz7mGAlLvnf_vuxTIeFEW4eCsDW3JNGZ0zJ7ozNE,3549
-qlib/rl/order_execution/simulator_qlib.py,sha256=QRS396Y28YmP5g1HYsel1g22Vonw8wwnw8sl_R-VqF0,4765
-qlib/rl/order_execution/simulator_simple.py,sha256=CEXb8WPqco_B3oSB4cC9YYdTihLveFE85U4QN8g86vs,14742
-qlib/rl/order_execution/state.py,sha256=iT8xDaAToQAGmK6hdcx6WX168naE0__LzXeiVjgjd0U,3690
-qlib/rl/order_execution/strategy.py,sha256=697x-AnkDh6HJA2mXGB_d9rKpCmTOsHM0zcXxtrbnUY,21177
-qlib/rl/order_execution/utils.py,sha256=SjtCNtQzAEHGZziB-lJHM6fD_UpjLlEUxbtk7LMsH4o,1631
-qlib/rl/strategy/__init__.py,sha256=1psNcxliGbRL1CdbP6f2DdhEOPPhSsHTL5BHm4zWYOI,154
-qlib/rl/strategy/single_order.py,sha256=6QyezvFCe_LyMtRcc7EtPdDgEn37dovJrLsehL8deUA,1049
-qlib/rl/trainer/__init__.py,sha256=4_0_mHmoGIxtD93Zbx6OxmvyFwGJ-DWmKZAjxtEJtuo,463
-qlib/rl/trainer/api.py,sha256=06DdUc_i-7_7wYjEQQDTG-mSERI3MTE5X12AmfHnFoc,3650
-qlib/rl/trainer/callbacks.py,sha256=iY9M2b7NL81aU7EwQxNR3DrGcE_9WSOhOHcFiljzmkQ,11576
-qlib/rl/trainer/trainer.py,sha256=_f8MEO_A8mYzdqBUFXIocg-xNloCWEo0IhqA4cbSprE,13476
-qlib/rl/trainer/vessel.py,sha256=gi7aKkAsC7gbWFyL7vM6Lr51aR0fG7Pc0qYXhoAnOdg,9887
-qlib/rl/utils/__init__.py,sha256=clRgbCLshbYXQP-dRsqzRQeuiSIGQ9AI2HDrhZCmagU,527
-qlib/rl/utils/data_queue.py,sha256=iWQgnMU7w6709oO9uuM35gWZJicThWcgfNDX4-YAMJU,6597
-qlib/rl/utils/env_wrapper.py,sha256=gTqq94l9MU9cn5SH3JRPupGacOe5zgwHrfQ16DUFuLM,9844
-qlib/rl/utils/finite_env.py,sha256=IXnKBSxNyymJDsPV6oOwUuo-cUX9Ku6w0tOqSKN-xoI,13367
-qlib/rl/utils/log.py,sha256=xOLjC59ISWMRlAYn9VImjSl8rU75lQdGECjpafzkbF8,18541
+qlib/rl/contrib/backtest.py,sha256=zMyPv46Y9lvqcmxc7JW8SS63dW9hSLXZLxA5J1kjf8o,13614
+qlib/rl/contrib/naive_config_parser.py,sha256=bQ4-i9Rrdww2E2hR21mLHvjKpzhohiNXFyJQJPjajYQ,3494
+qlib/rl/contrib/train_onpolicy.py,sha256=DCNIRQ8eMCXW5L1yxd6vnlOm8fpvomLER5w35wuQYQc,10373
+qlib/rl/contrib/utils.py,sha256=GsGl7_5IRTYSwFHGoaoE4276mXVnS9EnVKiFGBSeSms,865
+qlib/rl/data/__init__.py,sha256=1_nFqflPuHiX76gWQG6JL5Uxlox1vAxg0z4aDQKr1GA,253
+qlib/rl/data/base.py,sha256=sQms-RLBkOdkNH9X5Rfo9rzOJhqQrLRv6Riv_yMq_ck,1819
+qlib/rl/data/integration.py,sha256=BcIaXtjKWxviHcBxpDxITcVax8JOviW-92PizqgxSfk,3193
+qlib/rl/data/native.py,sha256=fO5ipPPvQiWaLJz12U7aBLbA0J5iNLOVu7e4XqE9_Yg,7645
+qlib/rl/data/pickle_styled.py,sha256=7PWyb4DewP4Mz8ckbLd74c28dZj6ovFuKvPL55BsrwQ,10840
+qlib/rl/order_execution/__init__.py,sha256=lJ6yNdMqqlpk3zuDmd9AiTWJ-ABgr2t_qP62DL_ABY4,1046
+qlib/rl/order_execution/interpreter.py,sha256=XVNpMmRAiCApCoGJRYroQGVWBNb8D0RChk__lRFGzxA,9926
+qlib/rl/order_execution/network.py,sha256=vuDld-Bz779xC1wd2KnrWVPatE07cBn3xop6okTIOCE,4970
+qlib/rl/order_execution/policy.py,sha256=j8TKU2CkNshmOa8ZX02jgb8S4XEZr2uQhAgeSSV_Jjo,7245
+qlib/rl/order_execution/reward.py,sha256=b67Fi5p6ghzZSmFG2xWjZuz--xfVXx-no0bPsq7h_Qk,3648
+qlib/rl/order_execution/simulator_qlib.py,sha256=wIcL6Pr_Sphj6NgPHQTmCjGcaOzzG_C9GDsGT_B6Fv0,4906
+qlib/rl/order_execution/simulator_simple.py,sha256=pcsrnD_8ubC1DYrWahRSZh1vsEzhLY9prULgHOFI5_A,15104
+qlib/rl/order_execution/state.py,sha256=VFlY5KlpMU0eEs-iKsN1rz7v4TRRJTmfR-QN9TlRNTg,3791
+qlib/rl/order_execution/strategy.py,sha256=3EpOjLBXfBl1R_8sLjGVo1iIYbkXbRZpYMaPHV0TtJ8,21728
+qlib/rl/order_execution/utils.py,sha256=iSZ_XPyeOHUcssOjfHTKcS6MOV9JLVPtAvAppmJyB08,1683
+qlib/rl/strategy/__init__.py,sha256=ocvADP5YmkFQo-gDraKgGI2ZbD6XHt4-B45Cz02WElI,159
+qlib/rl/strategy/single_order.py,sha256=kEATByqUe3KXAFzNJLP2mdjxwWuV5HJZhgmgYqrlZnU,1082
+qlib/rl/trainer/__init__.py,sha256=Cxtr3smPraMbX82dMIJ4FH9rXHPE6dwead4f-l-5NY8,483
+qlib/rl/trainer/api.py,sha256=EIYKzOyEdxkG6sZF8Yj9vBsRbcD6rQnmB5FZ8Z0HpB8,3768
+qlib/rl/trainer/callbacks.py,sha256=z6TdUjt3MuOQ0bzwpsexAfQVHkaDKdb3vgzm9bh6GOI,11867
+qlib/rl/trainer/trainer.py,sha256=6VHuWIiZrP91PSoGf6HSLp1QqUYAMAP-mp-_uJKH7lc,13831
+qlib/rl/trainer/vessel.py,sha256=y6tOVS3PHMjvHtJqcLfE681wyC3dLY2FJ0EGFdCv_YU,10103
+qlib/rl/utils/__init__.py,sha256=HXkTy-nDy6OHiwVGKwDBbTvdW2ECeaFPl278wePVgPs,548
+qlib/rl/utils/data_queue.py,sha256=n7afyWB2WEeyiYzDTDEQw2JTw0aUMmUltJiSbV2pRJ0,6785
+qlib/rl/utils/env_wrapper.py,sha256=_wN0hHhjgYTM0Zb-766qStiJ0o8U75u05UwWmBo_oio,10094
+qlib/rl/utils/finite_env.py,sha256=YH3729mMOLC0BHPo6sn3oc6VhuAf-hgqsoSU0ZUXEyI,13736
+qlib/rl/utils/log.py,sha256=RjdNEJbiI8R3JbeU2M_z1_G_c7e3nRyYfQdwZMi_LX8,19064
 qlib/run/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-qlib/run/get_data.py,sha256=E4uO5f7ledVcDs27hEekhB4-90aKozq-SMlt5qgHzwA,176
-qlib/strategy/__init__.py,sha256=HIog-luBqQnLkza1Q8b34Rr1QVu6tuiAy5sOry0vIPg,73
-qlib/strategy/base.py,sha256=wUHgKb-WsT9jQIVq486aL64e6HP58nSxe6DmObdbruA,11155
-qlib/tests/__init__.py,sha256=LK24HdgK41pWd-F6Vptddn-V_HoyetJdCXEIJc0SCRo,12220
-qlib/tests/config.py,sha256=sthyf9FkHxcZf5QQ-2CXxd3st4bqyBgugfCYrM7X57Y,4834
-qlib/tests/data.py,sha256=Wp_VsUQbhfXDX_eIiMz2q4x0ZPmdWaVeAWHH_ln_f1g,8001
-qlib/utils/__init__.py,sha256=WVo8G_dXrnz2j1o-ctNNU-Kptg9_vfWaNiO48lsYTyQ,34192
-qlib/utils/data.py,sha256=2YYWOw_y_dv2kDDsgVZ8ox5ivsvf6CR3eyxumAgcBis,3085
-qlib/utils/exceptions.py,sha256=k2Z36ENkQnf4u82mJ7wBHsv8LrBWxxX7xi7Xn61Cds8,438
-qlib/utils/file.py,sha256=uAj4nf2GfFVp5zZjiF5-QunGD9mM1ExAJ2uNVUKF39I,5738
-qlib/utils/index_data.py,sha256=dP4PWYwIiYaeyYSEoOJ23Ws7BHcqkuTtRaqd4DxqWXg,21893
-qlib/utils/objm.py,sha256=N119wCCgeRHULlSmgEpeUtrpuNwn-hPLEygEuk6PHzE,3302
-qlib/utils/paral.py,sha256=tVMF6cjJ4FJOItGl0bYmS7n8qxWiAIQBKYWlmkhqceI,9118
-qlib/utils/resam.py,sha256=2zmc9LTRvaIkkkB7q71NPK0LXDg3S80dio0yf40whJI,9387
-qlib/utils/serial.py,sha256=FT-tBu5sJWl0xLjHxU6obtd9xMHTMc-JTnNU4Y3LRpw,6074
-qlib/utils/time.py,sha256=nL9LbFKJwfEHYidiFFqIxCc5-Ylz-yeaqGlBWb-osx0,11778
-qlib/workflow/__init__.py,sha256=4UwRgbUSPocbOkGr3j09jr_DxOjmKSeQtOg_ZqzkQh4,24582
-qlib/workflow/cli.py,sha256=WHG69DOXBnyc7Fn4unvDBtrGdr-2SSi3KBShm-_ZqgM,3690
+qlib/run/get_data.py,sha256=_v_-H5iMalXUVn8hJhyKygpUhJFM_vUev0O3l7_Q4AY,185
+qlib/strategy/__init__.py,sha256=aCOr6sEsQpv9z4cJgWFA4qOs4xJqclqYYnxOVcxiK2Q,75
+qlib/strategy/base.py,sha256=jYBFiS1rUvsHQAJaL-RG7kMxCsHAzEOEzRRr63z9Pwg,11451
+qlib/tests/__init__.py,sha256=Off67XIckTmRiW8srVhcgS_-Y_T5nZQlDEW6b-LuRSQ,12507
+qlib/tests/config.py,sha256=3hzC5qMEFpGLUSvDQ57K41a2_d0HI88aw5zTBgoPbiw,5001
+qlib/tests/data.py,sha256=6fa2pXprRv5FY0_1t8akai6oeop7S5zrFfqlDxhCqcc,8193
+qlib/utils/__init__.py,sha256=_La0YYt5QjO6-zUaBmABR9u5ipCSitqhZhg87WbdjYw,30095
+qlib/utils/data.py,sha256=UYlAnNI8MW4mPhaa9-ovNoGTmj-DennPaBweaWJg4iE,3190
+qlib/utils/exceptions.py,sha256=ZU8DxuFaGxPmWFljQ3cX3xPh2isRkY9oKoFpLlHVkGc,457
+qlib/utils/file.py,sha256=5XNW4D9e9JKa4FOSGwV6xvdN5lKVGPVLJyELRjEWnXw,5928
+qlib/utils/index_data.py,sha256=X4IzXQTY9S3a8pPXu9sYfsQz53E6wnWl-9GKP-Ypa54,22535
+qlib/utils/mod.py,sha256=Ar6kFDgYLIqkQw2lAY4uf1ghMVC2GC3qY2AAV4xj-0s,7669
+qlib/utils/objm.py,sha256=x8pzFJgJABmAdQWcpYvhBfF6gV6ckbroc0TWcKu7Hwg,3435
+qlib/utils/paral.py,sha256=Yez9UkLr08691xohG7T3CaVjnl6MYc0EwW6GIbiRz3c,9433
+qlib/utils/resam.py,sha256=n6g7ZY7MwGniSi_H3MxZYfUTysOGp7oBD1H97kSf7C8,9626
+qlib/utils/serial.py,sha256=lSl_3YVqFk82DKMXYkhhvOy8T1bVcg9ZaWPz3T3njis,6263
+qlib/utils/time.py,sha256=cLKB_hhcxJr8BBZcSC0W64wIXgmyJLsFdnhs8xumGkw,12155
+qlib/workflow/__init__.py,sha256=RXAWBqSzfFR8fz7Y1iTSansBhWQbMk99otClH9AaOsE,25250
+qlib/workflow/cli.py,sha256=qU6hI2834JUghU24odU5DE3YjB3eERgwWczz0I5NGck,3809
 qlib/workflow/exp.py,sha256=vomFDMJW38e6STx396swFBEEmxCBgeHj7a7982ricFA,15192
-qlib/workflow/expm.py,sha256=MUobw2G_Skuzfu20F0jNvvHFaavIknHjNP6LBmBeFHs,17579
-qlib/workflow/record_temp.py,sha256=YsfcpMxn12vnC2qb92YybnT9gFNwCZ5bxuDcfKSDMt8,21860
+qlib/workflow/expm.py,sha256=GNIyaBvAao2pJTJhEZ6CPXdxWV57VESaCmoFakiiZw8,18008
+qlib/workflow/record_temp.py,sha256=VVTF5kjCuGcMhIkDF7DJE2rtxhsO5M9qmINHCFFrWSc,22414
 qlib/workflow/recorder.py,sha256=VGcOxMut49FeiUeaB2RCAzNbGGR4ekBz3Q5z1KzzqYQ,18257
-qlib/workflow/utils.py,sha256=5E8XBezXrld-CfTPMn9b_3jqkoMn9F55wQlMCK-GbLo,1616
+qlib/workflow/utils.py,sha256=-G6VMOO7V2mXUtm01lclB_jnC6B6Jr-QsCDvMfumBuk,1663
 qlib/workflow/online/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-qlib/workflow/online/manager.py,sha256=7vECQQQ02dYEuZxuWxH0XSpOp4FFlAF0RJ-rbWTFwzI,17366
-qlib/workflow/online/strategy.py,sha256=lDWxzswbOq5gblMogUMCnKiZ3L4Mjd8S6kSJjNT8oNs,8411
-qlib/workflow/online/update.py,sha256=ujWGVtZKf5uDgK81ji9alhoj1WBq3DWkoGnC5_nWxZo,10586
-qlib/workflow/online/utils.py,sha256=0SNDYtFMlQIdW6Fw0qhtQFtZjqBJjFE6AFGR4_kLpjE,6475
-qlib/workflow/task/__init__.py,sha256=R132i8ht1HfVejPF11U2DjEQiJ9BR7Iwe6SSEq0GLjo,535
-qlib/workflow/task/collect.py,sha256=7X08tz-zMHU_IUglsPtlA42mNIieIDjJzSU58WWc2KU,10057
-qlib/workflow/task/gen.py,sha256=c7wUVENqMuWvo_wCQdCcKsEPRsPeFqO1_NSAD1ez-aM,11767
-qlib/workflow/task/manage.py,sha256=bRgHJiXgwrRJBQ4qi_uZxr93p5Jz1Kc3eNvbpDvE8VY,18412
-qlib/workflow/task/utils.py,sha256=H7yxk87ZxAIhLBqJK_i5byLrvwo3N0rTaSyf5JB3j40,8612
-pyqlib-0.9.2.1.dist-info/LICENSE,sha256=ws_MuBL-SCEBqPBFl9_FqZkaaydIJmxHrJG2parhU4M,1141
-pyqlib-0.9.2.1.dist-info/METADATA,sha256=Ic4e78sf4vAAZvEyzCjwxjTQ4Pqcfrive6R-t6lnI7U,39367
-pyqlib-0.9.2.1.dist-info/WHEEL,sha256=FD7-AF0nEqaeImnGafj5_AW9t4FL2N8tDssGZwowrv0,110
-pyqlib-0.9.2.1.dist-info/entry_points.txt,sha256=8g030NI7vaEG2_kQV5ENHHuAMcJ1bcDNREh3ea3ILbo,47
-pyqlib-0.9.2.1.dist-info/top_level.txt,sha256=NEJ4tcoKeCgATGsti9esDbItwLBkgN8elNBsfGExUlE,5
-pyqlib-0.9.2.1.dist-info/RECORD,,
+qlib/workflow/online/manager.py,sha256=W3pOdrx8WsBh5WrqoInLeGx3BDQ6hK9hXGeR1y2uPRM,17748
+qlib/workflow/online/strategy.py,sha256=ZIjV4gUpyyzvwhcHKFcmc1crvjfI9VjXoAabxTLL_ao,8620
+qlib/workflow/online/update.py,sha256=qL0iR4BVK597yMWKZjG9RtWqah88ysBOqji27_b9ITs,10884
+qlib/workflow/online/utils.py,sha256=AmvMTq7BlzoioPs8UVZF8RbyzebQ_RA_U3AIGoMeIOs,6662
+qlib/workflow/task/__init__.py,sha256=7DnkUttb90Qmrl6CT2Usad5SUd-vicF0KIDDj5_fV6Y,548
+qlib/workflow/task/collect.py,sha256=XFWBC1LTPj90jGP8ffcmqYqZJAR2vsp7b9PvjSGMRdo,10315
+qlib/workflow/task/gen.py,sha256=h2OSSsoQvXMILLo27_kHssSsZpwxO1hAUbnOil4uJYc,12117
+qlib/workflow/task/manage.py,sha256=W1VLYwz6jIrK5AIpTcHF0mmU5PjCxSJnH5esEPUTgcw,18968
+qlib/workflow/task/utils.py,sha256=Z1jDblp0WAHxEEXZGlDSenMmCYw96rlRFCVRCxV6bHo,10322
+pyqlib-0.9.3.dist-info/LICENSE,sha256=mQaUD2Gx8LUz-n2ZuvVReLKAj74RPqUd-_rYVyzNXys,1162
+pyqlib-0.9.3.dist-info/METADATA,sha256=UnieRtil6AGSxdFKbRK5-Ve7T1QFCyE26_VUWrEHydY,41184
+pyqlib-0.9.3.dist-info/WHEEL,sha256=F2aDWtWrilNM3-px1vBtJOEd7aRVTetQfFuDYmYbAUQ,100
+pyqlib-0.9.3.dist-info/entry_points.txt,sha256=dwunoTIpn4pHpIdn84W_tcL4QhuvJu2Ui-fnfa540uQ,48
+pyqlib-0.9.3.dist-info/top_level.txt,sha256=NEJ4tcoKeCgATGsti9esDbItwLBkgN8elNBsfGExUlE,5
+pyqlib-0.9.3.dist-info/RECORD,,
```

